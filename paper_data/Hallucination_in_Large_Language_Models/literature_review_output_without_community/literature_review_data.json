{
  "title": "A Comprehensive Literature Review with Self-Reflection",
  "papers_processed": 277,
  "paper_list": [
    "be177300487b6d0f25e6cade9a31900454b13281.pdf",
    "088a42203bc9a67e14b1bfd5c1fd25a03c126c08.pdf",
    "2feb4d83da1892db3934fcf406c8beb6cd10ded1.pdf",
    "45ed6263e02d219f0542ac743b9c9f837154a58d.pdf",
    "5667f64b23cf48c94ff7413122bc56e5aad7e6a2.pdf",
    "e468ed6b824e60f45ba9a20b034e4090c6630751.pdf",
    "94c81ec4364d63fe67f98098547d0d09f063931d.pdf",
    "d00735241af700d21762d2f3ca00d920241a15a4.pdf",
    "2635c1aeee582dacb865f00d1289b443c3d96d02.pdf",
    "c2f3d3e847faf3a8448eabb5bd5fdb6bebbc3a05.pdf",
    "807f336176070bd3f95b82a16f125ee99b7d2c80.pdf",
    "bb3cc013c462ff2bf3dc5be90f731ebf34996f86.pdf",
    "7b181a867f243d83ed0731201b69a82e038feea3.pdf",
    "6947893915861e8c30bc6b010eb1faf0d82f0a19.pdf",
    "e7c97e953849f1a8e5d85ceb4cfcc0a5d54d2365.pdf",
    "425d16205b28ce175c8429965a964d19b6f390c1.pdf",
    "4661b7e8f6bb4f0cc1d4a767a92534f1def344b8.pdf",
    "bf54792cf01761a2c51ac3410287797fff665cd4.pdf",
    "328eb183007bf4aefbf42437b42a15db375803e3.pdf",
    "15aaf20d02a1e26be9106e66d065fd1ca5600e29.pdf",
    "24a48ef14c8eb4e571e3f4ae9b37936060a3fb06.pdf",
    "3c3f5af1aee19bf0093c40f35a120744d099723e.pdf",
    "46f9f7b8f88f72e12cbdb21e3311f995eb6e65c5.pdf",
    "4e53b481beabba42aac027e5a8c69fed26ab4062.pdf",
    "933d1d4f18e721160ddbf8dab25c33f8e3d2cec7.pdf",
    "45ffc7928a358ff6567d8420b58d509fc3b7dbd1.pdf",
    "5e060f23914aff74d8c7b6973df44e5af8d97db5.pdf",
    "cd2e04598909158494e556823d9de8baa692cee2.pdf",
    "ee19d5c943f1ebcd1a9e52a7bf494a88255b8e04.pdf",
    "5838b56f2c7ca3dd946428dae07bdc26a9265c67.pdf",
    "b10482ab3dd1d340c3c926d92c3e617c24ee3949.pdf",
    "ae1e48a74cb2f313e8e99c82f0aa4487b0805002.pdf",
    "88e52de2320e06c7556795be43b38c85a9800e5a.pdf",
    "2c67ee597ed38f43ec0f123a3f1cce38cbd3b5b4.pdf",
    "0422493dc3a70816bb5d327c4c67094f64a78c98.pdf",
    "e5f183ffafd74e2fba831420fa1f3e5f07b7ce2d.pdf",
    "fc4c380102d6f72657d1ab54dffd6be536bb01c7.pdf",
    "c910c8f715d8231ed824caff13952d6946de1e59.pdf",
    "0ba76fbb7a4a2e6a221b4c31321e9846eca2fe92.pdf",
    "fca2da71f3dce2f757aef39e561a572f68106603.pdf",
    "58ee9e1c426166a5451a1ce13e1186f7d6baacfd.pdf",
    "73020a07af4cfc42286e299097a0e35d2fe71a6c.pdf",
    "0b395ed1c8b284e551172b728e83cf257e33729a.pdf",
    "1146d40d3d01427a008a20530269667b8989750c.pdf",
    "1e909e2a8cdacdcdff125ebcc566f37cb869a1c8.pdf",
    "1c7ef42897ad2dced83ab1d58d8fbd4539f87ddc.pdf",
    "682ff66a5ec0248f7e4a17a684b2d1e328e57f70.pdf",
    "4d7c68ec1a86ef5d187e7edb2f0ad63adddc8ea2.pdf",
    "c7a7104df3db13737a865ede2be8146990fa4026.pdf",
    "03e2f5cded1b1d92dc8e693e0e93ad466f6cc352.pdf",
    "99bfe503743c5ec8e16e50ab8438159cdb533a89.pdf",
    "c946888e2f81b1db84ba4addf2a11e87f0568fe9.pdf",
    "e0384ba36555232c587d4a80d527895a095a9001.pdf",
    "93c525267e93c78309a5b28a3eb0780704125744.pdf",
    "b3fd9f9245584ee41c0ba005cb262fd8f93ff3b5.pdf",
    "c7714dc70eb508a0b1859b7b1a5af552439b973f.pdf",
    "e5f7e3d55790f2031ecb0c24e6e53c21c7013bb0.pdf",
    "7cfbd36c0043098589cbaf18dca2b41d8dc24abe.pdf",
    "889feabe31ba0d24c093ac94d54a06eecb87e3f4.pdf",
    "9b05e1dfd158c307b74298df3d4608b93d2060a7.pdf",
    "b169426b9181adee0e7d6616fc12fc12611d9901.pdf",
    "4d608203639087e0fe3c5d2b7a374941dd182cb7.pdf",
    "06b2ac5153e3d8d05c13c82f93d7f4e13eee6d0f.pdf",
    "83d81e31f5c32f6989d98be1133adfc08db094ce.pdf",
    "25243632a6159c19db280e2f0064aa59562a518a.pdf",
    "a2f44fc0f0c24fd4ab848f01a770a68dfa114f62.pdf",
    "c680e5d34b713f8b63ad68149973d5b2b485dd07.pdf",
    "db646f0eb37bb97fda3a89f94c81e507f9421ba9.pdf",
    "dbeeca8466e0c177ec67c60d529899232415ca87.pdf",
    "171807aeeb88f0c7983bc6cc960b5605441d7121.pdf",
    "28e2ecb4183ebc0eec504b12dddc677f8aef8745.pdf",
    "f8a642fbb51e0b0ae4774781309545d15d6d9b11.pdf",
    "05839a68bd05880beef2f171cee7aab960bb6d2f.pdf",
    "3bb6f6a4cf672616bd49d8f4eb15d1b4df19972b.pdf",
    "3b0792f6d7f6aa6aadd316e73943116afef2979b.pdf",
    "c4d3c2516d24bd1c0eff93ea047147f3afd586ca.pdf",
    "492e526ca2416a734f286da0efcfeda4672ea77f.pdf",
    "143a05fb36be8198d7675b594c0656b5652da3cb.pdf",
    "1b387e3fbec0447c8bf2dcee21f6db59cdddf698.pdf",
    "bb1083425517bdac8d9a6438fcf5032543acb20e.pdf",
    "5cd671efa2af8456c615c5faf54d1be4950f3819.pdf",
    "c6bf48f25e0a65d64d658b47326de5922ea7dd44.pdf",
    "7c1707db9aafd209aa93db3251e7ebd593d55876.pdf",
    "968bd4cf71c66bb153527778836e54c85ee6162c.pdf",
    "01f3b1809035a593b9dd6fb0b2cabdc8e216542f.pdf",
    "f6d4c76b21539aadc2ca8d813fe631be7149231e.pdf",
    "5272acad9e4201e93dabe3fd99bd7ead9b1a544d.pdf",
    "2126e045f81b831da34c185e2b51a49194bf4aa4.pdf",
    "ca261cb681b082e90ca6c7a9d325b4265ed1dc28.pdf",
    "206400aba5f12f734cdd2e4ab48ef6014ea60773.pdf",
    "d6da914d0c8021df6622857aba23b794fc7e6a40.pdf",
    "4b0b56be0ae9479d2bd5c2f0943db1906343c10f.pdf",
    "89fccb4b70d0a072d9c874dddfab0afb3676d1b8.pdf",
    "27d55a944b5c02b8c10eb250773d8eb082e06476.pdf",
    "f208ea909fa7f54fea82def9a92fd81dfc758c39.pdf",
    "57f0d904629955d16bb2b80a5d427e6b1efa6562.pdf",
    "6d26836a4cee8f90c6fa4d5751d5f10e0f720301.pdf",
    "411b725522e2747e890ba5acfbf43d22f759c00a.pdf",
    "705ffeccfde95c3b0723f197c4565f7d3f0451a1.pdf",
    "80248c8c7cbb5bb1d2a508001108f3f15bb60430.pdf",
    "39d8486475173357619647061dda377f4c38853e.pdf",
    "4f83d9c391d782d358c2bf0d7ffc6150924dae01.pdf",
    "19e909f88b8b9b0635bd6e441094e1738c3bba9a.pdf",
    "c14010990c9d75a6e836e1c86d42f405a5d3d0a6.pdf",
    "431a4e7e89863b038069335baa80c3e489538214.pdf",
    "9e2037d7d2f8222a7be86d2471eda895c8040ff5.pdf",
    "b877f5076c617a948081e12e08809e6c6b84b468.pdf",
    "3f915aab835cbfe69e7b2ea1c73b74ac8a2d384e.pdf",
    "ecc51ce52ca524be17616a9c0dc8a051a2996ad7.pdf",
    "23f1d4b46bc7c8f357a5a89144d5d32af7be13a5.pdf",
    "576023f7cc3da5a36ac0cfda402af859cc90be10.pdf",
    "a7f4deb9a1452374330f202bc8d36966a0f254e8.pdf",
    "7bcd5c0b17560ee560aec903ea42487a1a54e5d9.pdf",
    "8ff45750057cc9452ae09aef6b9dfee3bd84b083.pdf",
    "396305230ddcf915b19a19683a89e34d76321a33.pdf",
    "97d24f9f0d81007d57cc43e61bf2b0c9081fe184.pdf",
    "0f7a6c557e376d8c77d684bcda0daee74fc29acf.pdf",
    "49b79d61ffc2db6dce8c2cd9cda06e1876ed8b4c.pdf",
    "1cc347c97a8f9d30edc809e4f207d64c7b8247b4.pdf",
    "dd6b124606e3696dcddc93c889a824feaa322117.pdf",
    "99832586d55f540f603637e458a292406a0ed75d.pdf",
    "ca2f63950685a97e5ab6b8e6b2db78a8995e94a2.pdf",
    "a6d8d04962f84ae6225e72723869a002b9fc8036.pdf",
    "5bb3bd2ec1e99b11a84ccd0e4dce4bdb2a776a5e.pdf",
    "c4f26bc007343c59bedd1423250a3b453d3d2d22.pdf",
    "ebb4826b717798918bebed4dfac28c917e44577d.pdf",
    "35872510c095b1189105e9f902f04f51bd0a88e3.pdf",
    "c2c3220b9faf95db43d90a5ed42fa824b4b3d2f0.pdf",
    "72da4a646a31d72bcae90b916e120cd7df5f9dae.pdf",
    "ecb5a6fe2f5261e4e717ece1e82c464c63cb4862.pdf",
    "f35dbee22c1572d149b7c1e20d69672cae931451.pdf",
    "fda76a1411e16722ebd2d8278c3143ca4363da6b.pdf",
    "3e075efc541c7d2b357199655e11f084686e8575.pdf",
    "334bf07262320eb895a22973c948b4111e782daa.pdf",
    "5b824faaea855eb1700b16d7a67ee58a8f75e7d4.pdf",
    "c8373577bcce9b8811672ddeb372a5b780397117.pdf",
    "6ab78343ab82fa9d7baa68027f9f7e8cd9863737.pdf",
    "5791c2b41dd23310c53d6738a4c0d587107c2dc8.pdf",
    "fbe549b5b30f54a4f76cb4bf4c6b8a9fb8e24657.pdf",
    "a996fb4b9f3d4315138f8773b1e995f8386b11eb.pdf",
    "471a49220cea2069e8b8a76821b1d2434204a732.pdf",
    "3b7cdcbe19fdbda17f7cce1eb1b6c8f5a8a60e3b.pdf",
    "6aed9dffe3246efd9d19b2873994ba112e2ad422.pdf",
    "b32cf25dffbdbe6d838fc7b8781c126c8fea7d3c.pdf",
    "20013690616f6e781c05feaa08a2247a97640a87.pdf",
    "e416b5fb1ab75c0770cf7cbd6976f5444b0ee89f.pdf",
    "b7d484357f5f84c065ef7fbc77d0460b9795964d.pdf",
    "7c9f69848d28e0a7cbb00942ee83dab9773c23e4.pdf",
    "ce4e101950554c41d3b35f5b297722abd1ce6403.pdf",
    "5c204b2421d05b83d3c96a6c515cc03143073935.pdf",
    "8c5acaafe43e710d55b08c63d567550ad26ec437.pdf",
    "1ebcf1884390c28f24b3adaf5a7aba5b9453b48b.pdf",
    "f4e06256ab07727ff4e0465deea83fcf45012354.pdf",
    "e17c58d7a48b6b811df023484161a3b9c03e0d6b.pdf",
    "2338d7c9ab07e6d0f4160335dce0e6e6a87c4749.pdf",
    "fac468032e0c38ea10dfb95ba6cdeac51a473050.pdf",
    "3e7b421f9df1bd34d9cb42ece2760269f0314f05.pdf",
    "25a9c0946925cc860c4600dba91b313cdbe7c8a8.pdf",
    "d299a6b26e9ee23d0337a1d1a896fc1c847f5a46.pdf",
    "ea0d41514a41f8273f13b3b277e7fcbbc65a8549.pdf",
    "a082c9c93b5cc0d38e7ac14c6c9dfe186bb5c824.pdf",
    "24b6b70e1b1525535155cc9fa66dfd9d5d42d6b5.pdf",
    "6a9a9120d746a3c29902548bd1d93d6ea034c5d7.pdf",
    "95a52dd5adf6eb8d918cdfbf6189aab4eaa8e607.pdf",
    "80fd20e175f83a699258b8780cf365418d1538b0.pdf",
    "81bd66d960503106ef969830568016da4f93754a.pdf",
    "91ca6535fc8fb03efe0ecbe424ce5354ed129b0c.pdf",
    "d48c56dbce88580736c037797666060cb3b03bf7.pdf",
    "668341051f3c9c087e42e393c610792df3e45992.pdf",
    "3dea23e10eeff848f7352b17bbc1fdce38112acc.pdf",
    "31968a970f14beab3cbadf9f6ad45c1a51f4ea95.pdf",
    "be8c90bca14d59f180f40a41126b7cd8c29c5d4e.pdf",
    "aa1fbd6e8d1c8e99b6ca34c17bcdb36e987b68a6.pdf",
    "694b753385820ea675f2ca80dcdb4c91fc05962a.pdf",
    "6a2e0927914ef03f25b99c2666f2275e0c950e5d.pdf",
    "f19e6c955b05d61aeb1cbc7580dc3723d31398ea.pdf",
    "3c2130e219528df234658b94810de33fe7b077dc.pdf",
    "37f6249b8381c955d732755714c1e700ba62b988.pdf",
    "f123b838e000e11f08cb0d7c63e01934b38d3092.pdf",
    "4c0f029efd5371eed087d0794fb0df71238600cc.pdf",
    "cdfab7e389f94263ce99b7c0025090971df40a01.pdf",
    "4c3447dce6798b894313bb3ff2735ef139cbf071.pdf",
    "1f49b4586cc71cca59151e7a7bbfd500574c2fee.pdf",
    "4150d370a29258ac88552561cdd2b10f7862bfd7.pdf",
    "de02ba19fb957ae30de7f09904ae3d983c3b50e7.pdf",
    "75a381667ef536d02d99063eb3568e410d7ce909.pdf",
    "d409053ed94ec725d72c812e7c8bd71b87278b96.pdf",
    "379e3ee9a6a92817bd0812848b409eafb9bf9550.pdf",
    "b0633ccf235e467c35b963ad012f6b8c54aba19f.pdf",
    "35a544ca04325006f7f2e2369d3e0aaa8ba36a07.pdf",
    "165fdad3949b7abdb985cb8834c26c7baa7bd40f.pdf",
    "8c8527f7615d53cbc21b9c3536486540f1c75000.pdf",
    "e131a11ad907023f655f22a4fae2b2a6f2db96f7.pdf",
    "ffeec58ed1fc045c55512e20b30fce951913a3f0.pdf",
    "89729cdfe0f71ad7a04c73e9167c2b266ee0ee8c.pdf",
    "bca44a53d9becd158ee0abf34c4255375fdc7327.pdf",
    "d5ebd84b996491d8ffadefd05a32f8f25085935d.pdf",
    "5cb8fc293567f4f2930712a3bf7dec97b4dd1776.pdf",
    "388e6dbb4b4486e01d4f040684560135a9e1ef71.pdf",
    "e384f513114d7e6c20d007b4a3ad13fa58cf83dd.pdf",
    "56ff9de0931bd1accb9d4e3f109afcbf31f7df25.pdf",
    "bff123171a5b7bddd699a72daed96b4c56742069.pdf",
    "4582779668ab801f29db457790cd291767510035.pdf",
    "e7d5f2da48b9141e18de9ea57ed8a4cebb6a09cd.pdf",
    "36ffd74db97f98f121a6c2954bf98c93dad5d2ce.pdf",
    "4cf85f9436bb8b5b1c68715f44d6b67254413ef8.pdf",
    "2b277717203bb5354e7d41e79a35c59e34fa6778.pdf",
    "dd20cfadf992986b5d71b3a44b5a8660f0d68671.pdf",
    "7112d0d36b960b590f55569bc294b2190d288860.pdf",
    "5be8fd7aa6564ea1884626c16bcac36508f79ff1.pdf",
    "8a9b43946dc10f91ce8c5971a1f247fbacda7a42.pdf",
    "e615f33365ea1d439507fc477588528ffb0764a8.pdf",
    "108bc0498629f4710b44076fe0c6270954494097.pdf",
    "59395cf4f9346ef4ccb37499a3a7e52c2978fc61.pdf",
    "655f92355d7930919a01125bd7a35c812498b1a9.pdf",
    "d0fe343fbdecaf4cc477d70e8701f9a6935b13d0.pdf",
    "10269639a0462a0e9790bd0524f5a092325d8d51.pdf",
    "7e38ac6f71408383939ec05f60b0bd85759a4c4e.pdf",
    "1541bc9e588bfcd4bf365c868fa2f11461896980.pdf",
    "f05e64c2a096e3762939dfdb7f475724c04a46bd.pdf",
    "46e542884db4fc4df605eb28473cff79aec54c99.pdf",
    "ff06bee3d898b3dc3a0364f2bfe506591d7e6d52.pdf",
    "7641749cae1ad30779bfb46948fd47922bcc296a.pdf",
    "7adb88771376c2a31688e3b0395b0550a35b824d.pdf",
    "f41977c497c96c1da2e9e945315e9be6d6ad472e.pdf",
    "20eecb9ead20ffe49a66588a9662336eefb20a54.pdf",
    "ff0450c78e5fabaa8aab61a368f267bd83753a64.pdf",
    "e33fceb7cfb825ae3c530de0bf093769169039fc.pdf",
    "6518d3209cec0a1ad277e8aaf153242b3a4233d9.pdf",
    "d45865c981161ad711adf18b0492e959771554e4.pdf",
    "4d710532fff7aec8187f68fb2ca90079c40e7004.pdf",
    "003463160918704684f812ae8d7b6920d2d15e31.pdf",
    "f0a79fe7765ab253480a0be6d29c889eac19eb3c.pdf",
    "910d26adcd83c4ec36f365198f8b2224b14ad6c9.pdf",
    "b5f56f466c06d10100d8d1aac9e1f979c527b1cf.pdf",
    "6b44d0ac2b6d6deeb7f35ef4a7ad77b12b646b9e.pdf",
    "9516ad22fbd81875b160c4471ff3f747c4543da1.pdf",
    "cb7fa7ee3df826628c113ba0c6db1205751d89a3.pdf",
    "13fd528a587196ff6429bfbe1d11d2f89a4036f5.pdf",
    "f63fdbbdf9005245d960ac1912cf4d0805e274a8.pdf",
    "3b1baabcbfd19e2f292863c522de41083814856a.pdf",
    "fa0d056dd585eeffb4333cb55807d357808f8440.pdf",
    "0797f2d1366da1f3441ea7d33b2109d7f27d1ad7.pdf",
    "fbca0c2ec5425bbd8dc4898d684c909a58dab1de.pdf",
    "4dc1fbde861d1e97daa0677ac9cfae92b2832589.pdf",
    "f363c38f2a5d3ed8697a72d3dd014d228bfda91a.pdf",
    "43210579b1ff7707afbd5d1ed045cc56ba52e938.pdf",
    "1154343478e423fecb12501cf02208499cd57846.pdf",
    "691f111348f3b19163e62a208de9803280205ed8.pdf",
    "c0082580c4b9e5c6c96cf06f1be67c0cbbafb753.pdf",
    "ded732209b0ba8a6704cc62ab8197a898b57f833.pdf",
    "6d3ae6d6b312b659b3a14ae3f3e86a36db63200d.pdf",
    "e059a20ae41aa32837030fd6e4392b8217243a2c.pdf",
    "55b193ab7967fb20a8a05f878ac85bf48c9fc615.pdf",
    "ea32e8511cde4a1b852d8c003e0ec64bdf64b0d8.pdf",
    "b7aa5af5bf96ee003543d5ff7dcfc3d9a46d43bb.pdf",
    "8767dcddfb856db4bfa1e150470fc99f51f43835.pdf",
    "6bf4e95e63df023c81458ec60a8324788535a2f4.pdf",
    "8705389fd69be2a0cecd2242287a08e8280f2c52.pdf",
    "21967f943a189a9171f9f880182894acff5b87a4.pdf",
    "5e5336d0284e94cd835c40f931e0d379e21b464d.pdf",
    "1d1af14aa70f86b013e616cfd07fa8a164652d84.pdf",
    "6073b9a4856d726c270f03ebee54ea7658f16ec1.pdf",
    "4c4a3328153e85749c690e68acc13b42a7225e50.pdf",
    "849b3727dbb41c37f92a338ac5860b764a5b94f4.pdf",
    "55adbe4a6511a9c036024c6e2a637782d53289f4.pdf",
    "934635a82a338ddea6e0c0059663250eee259e8f.pdf",
    "2777cf13955e7683e0ec5446fdff7cdf7dd57d91.pdf",
    "f7a47a7de7289d6fd69e3bf226f7cdaffa670a3f.pdf",
    "d6442ff9d10310071108f44734b00d182b6e2c28.pdf",
    "0e3d306997e4830e668f750bddc3ee28487ce59a.pdf",
    "962cdae24cebb49c8870525fbf229554203aa5d2.pdf",
    "1c3eee136c5fa85ad97ef62f353557f776059f3e.pdf",
    "b29b7aaae0554e864409dfd1afadf9e1564a2616.pdf",
    "6becd0d29f013a7aec01d453727cb1680c01979f.pdf",
    "dbcd51388bc622e7725782177c09cf8b5c1daf5d.pdf",
    "ac7cc880f897626ee2d2d5a5c40180d551f4e0f8.pdf"
  ],
  "citations_map": {
    "be177300487b6d0f25e6cade9a31900454b13281.pdf": "vu202337s",
    "088a42203bc9a67e14b1bfd5c1fd25a03c126c08.pdf": "chang2024u3t",
    "2feb4d83da1892db3934fcf406c8beb6cd10ded1.pdf": "wang2024vym",
    "45ed6263e02d219f0542ac743b9c9f837154a58d.pdf": "niu2024v97",
    "5667f64b23cf48c94ff7413122bc56e5aad7e6a2.pdf": "liu2024gxh",
    "e468ed6b824e60f45ba9a20b034e4090c6630751.pdf": "li2023v3v",
    "94c81ec4364d63fe67f98098547d0d09f063931d.pdf": "liang2024hoo",
    "d00735241af700d21762d2f3ca00d920241a15a4.pdf": "zhang2023k1j",
    "2635c1aeee582dacb865f00d1289b443c3d96d02.pdf": "zhou2024lvp",
    "c2f3d3e847faf3a8448eabb5bd5fdb6bebbc3a05.pdf": "bai2024tkm",
    "807f336176070bd3f95b82a16f125ee99b7d2c80.pdf": "yin2023hx3",
    "bb3cc013c462ff2bf3dc5be90f731ebf34996f86.pdf": "cao2023ecl",
    "7b181a867f243d83ed0731201b69a82e038feea3.pdf": "wu2024bxt",
    "6947893915861e8c30bc6b010eb1faf0d82f0a19.pdf": "ghosh2024tj5",
    "e7c97e953849f1a8e5d85ceb4cfcc0a5d54d2365.pdf": "gao2023ht7",
    "425d16205b28ce175c8429965a964d19b6f390c1.pdf": "yang20251dw",
    "4661b7e8f6bb4f0cc1d4a767a92534f1def344b8.pdf": "zhang2025pex",
    "bf54792cf01761a2c51ac3410287797fff665cd4.pdf": "xing2024itg",
    "328eb183007bf4aefbf42437b42a15db375803e3.pdf": "leng2023ohr",
    "15aaf20d02a1e26be9106e66d065fd1ca5600e29.pdf": "kim2024ozf",
    "24a48ef14c8eb4e571e3f4ae9b37936060a3fb06.pdf": "li2024wyb",
    "3c3f5af1aee19bf0093c40f35a120744d099723e.pdf": "ji20243j6",
    "46f9f7b8f88f72e12cbdb21e3311f995eb6e65c5.pdf": "gao20232zb",
    "4e53b481beabba42aac027e5a8c69fed26ab4062.pdf": "ji20227ii",
    "933d1d4f18e721160ddbf8dab25c33f8e3d2cec7.pdf": "adams202289x",
    "45ffc7928a358ff6567d8420b58d509fc3b7dbd1.pdf": "su2024gnz",
    "5e060f23914aff74d8c7b6973df44e5af8d97db5.pdf": "du2023qu7",
    "cd2e04598909158494e556823d9de8baa692cee2.pdf": "ji2023vhv",
    "ee19d5c943f1ebcd1a9e52a7bf494a88255b8e04.pdf": "pan2023mwu",
    "5838b56f2c7ca3dd946428dae07bdc26a9265c67.pdf": "kang20238j0",
    "b10482ab3dd1d340c3c926d92c3e617c24ee3949.pdf": "kang202378c",
    "ae1e48a74cb2f313e8e99c82f0aa4487b0805002.pdf": "dong20223yz",
    "88e52de2320e06c7556795be43b38c85a9800e5a.pdf": "qu20240f7",
    "2c67ee597ed38f43ec0f123a3f1cce38cbd3b5b4.pdf": "mckenna2023pzc",
    "0422493dc3a70816bb5d327c4c67094f64a78c98.pdf": "rejeleene2024okw",
    "e5f183ffafd74e2fba831420fa1f3e5f07b7ce2d.pdf": "liu2024p39",
    "fc4c380102d6f72657d1ab54dffd6be536bb01c7.pdf": "liu2024sn3",
    "c910c8f715d8231ed824caff13952d6946de1e59.pdf": "chen2024hfe",
    "0ba76fbb7a4a2e6a221b4c31321e9846eca2fe92.pdf": "li2024hdc",
    "fca2da71f3dce2f757aef39e561a572f68106603.pdf": "yan2024ux8",
    "58ee9e1c426166a5451a1ce13e1186f7d6baacfd.pdf": "wang2024rta",
    "73020a07af4cfc42286e299097a0e35d2fe71a6c.pdf": "xie2024l8a",
    "0b395ed1c8b284e551172b728e83cf257e33729a.pdf": "guan2023z15",
    "1146d40d3d01427a008a20530269667b8989750c.pdf": "liang20236sh",
    "1e909e2a8cdacdcdff125ebcc566f37cb869a1c8.pdf": "huang2023akj",
    "1c7ef42897ad2dced83ab1d58d8fbd4539f87ddc.pdf": "lv2024k5x",
    "682ff66a5ec0248f7e4a17a684b2d1e328e57f70.pdf": "gu202414e",
    "4d7c68ec1a86ef5d187e7edb2f0ad63adddc8ea2.pdf": "huang20247wn",
    "c7a7104df3db13737a865ede2be8146990fa4026.pdf": "liu2023882",
    "03e2f5cded1b1d92dc8e693e0e93ad466f6cc352.pdf": "ding2024o88",
    "99bfe503743c5ec8e16e50ab8438159cdb533a89.pdf": "rawte2023ao8",
    "c946888e2f81b1db84ba4addf2a11e87f0568fe9.pdf": "pan2024y3a",
    "e0384ba36555232c587d4a80d527895a095a9001.pdf": "li2023rvf",
    "93c525267e93c78309a5b28a3eb0780704125744.pdf": "zhou2023zu6",
    "b3fd9f9245584ee41c0ba005cb262fd8f93ff3b5.pdf": "han202439z",
    "c7714dc70eb508a0b1859b7b1a5af552439b973f.pdf": "wang2025jen",
    "e5f7e3d55790f2031ecb0c24e6e53c21c7013bb0.pdf": "qu2024pqc",
    "7cfbd36c0043098589cbaf18dca2b41d8dc24abe.pdf": "dai20229aa",
    "889feabe31ba0d24c093ac94d54a06eecb87e3f4.pdf": "dziri2021bw9",
    "9b05e1dfd158c307b74298df3d4608b93d2060a7.pdf": "sungbin2024r2g",
    "b169426b9181adee0e7d6616fc12fc12611d9901.pdf": "hakim2024d4u",
    "4d608203639087e0fe3c5d2b7a374941dd182cb7.pdf": "li2025qzg",
    "06b2ac5153e3d8d05c13c82f93d7f4e13eee6d0f.pdf": "wang2023ubf",
    "83d81e31f5c32f6989d98be1133adfc08db094ce.pdf": "chen2024c4k",
    "25243632a6159c19db280e2f0064aa59562a518a.pdf": "ding20244yr",
    "a2f44fc0f0c24fd4ab848f01a770a68dfa114f62.pdf": "deng202405j",
    "c680e5d34b713f8b63ad68149973d5b2b485dd07.pdf": "chen2024j0g",
    "db646f0eb37bb97fda3a89f94c81e507f9421ba9.pdf": "chen20247jb",
    "dbeeca8466e0c177ec67c60d529899232415ca87.pdf": "maynez2020h3q",
    "171807aeeb88f0c7983bc6cc960b5605441d7121.pdf": "qu20246yn",
    "28e2ecb4183ebc0eec504b12dddc677f8aef8745.pdf": "chen2023h04",
    "f8a642fbb51e0b0ae4774781309545d15d6d9b11.pdf": "zhou2024wbi",
    "05839a68bd05880beef2f171cee7aab960bb6d2f.pdf": "jiang2024792",
    "3bb6f6a4cf672616bd49d8f4eb15d1b4df19972b.pdf": "tjandra2024umq",
    "3b0792f6d7f6aa6aadd316e73943116afef2979b.pdf": "umapathi2023puv",
    "c4d3c2516d24bd1c0eff93ea047147f3afd586ca.pdf": "zou2024dp7",
    "492e526ca2416a734f286da0efcfeda4672ea77f.pdf": "li2024osp",
    "143a05fb36be8198d7675b594c0656b5652da3cb.pdf": "chuang20248ey",
    "1b387e3fbec0447c8bf2dcee21f6db59cdddf698.pdf": "li2024qrj",
    "bb1083425517bdac8d9a6438fcf5032543acb20e.pdf": "wang2023zop",
    "5cd671efa2af8456c615c5faf54d1be4950f3819.pdf": "xu2024n76",
    "c6bf48f25e0a65d64d658b47326de5922ea7dd44.pdf": "liu2021mo6",
    "7c1707db9aafd209aa93db3251e7ebd593d55876.pdf": "manakul20236ex",
    "968bd4cf71c66bb153527778836e54c85ee6162c.pdf": "zhong2024mfi",
    "01f3b1809035a593b9dd6fb0b2cabdc8e216542f.pdf": "zhang2024qq9",
    "f6d4c76b21539aadc2ca8d813fe631be7149231e.pdf": "wu20241us",
    "5272acad9e4201e93dabe3fd99bd7ead9b1a544d.pdf": "tonmoy20244e4",
    "2126e045f81b831da34c185e2b51a49194bf4aa4.pdf": "wu2024n00",
    "ca261cb681b082e90ca6c7a9d325b4265ed1dc28.pdf": "wen2023t6v",
    "206400aba5f12f734cdd2e4ab48ef6014ea60773.pdf": "li2023249",
    "d6da914d0c8021df6622857aba23b794fc7e6a40.pdf": "lan20240yz",
    "4b0b56be0ae9479d2bd5c2f0943db1906343c10f.pdf": "dhuliawala2023rqn",
    "89fccb4b70d0a072d9c874dddfab0afb3676d1b8.pdf": "sui20242u1",
    "27d55a944b5c02b8c10eb250773d8eb082e06476.pdf": "xiao2024hv1",
    "f208ea909fa7f54fea82def9a92fd81dfc758c39.pdf": "trivedi2022qsf",
    "57f0d904629955d16bb2b80a5d427e6b1efa6562.pdf": "park20247cm",
    "6d26836a4cee8f90c6fa4d5751d5f10e0f720301.pdf": "sridhar2022l1c",
    "411b725522e2747e890ba5acfbf43d22f759c00a.pdf": "su2024lem",
    "705ffeccfde95c3b0723f197c4565f7d3f0451a1.pdf": "luo2023xyc",
    "80248c8c7cbb5bb1d2a508001108f3f15bb60430.pdf": "wu2024h81",
    "39d8486475173357619647061dda377f4c38853e.pdf": "chen2024vy7",
    "4f83d9c391d782d358c2bf0d7ffc6150924dae01.pdf": "zheng20246fk",
    "19e909f88b8b9b0635bd6e441094e1738c3bba9a.pdf": "chen2024lc5",
    "c14010990c9d75a6e836e1c86d42f405a5d3d0a6.pdf": "sahoo2024hcb",
    "431a4e7e89863b038069335baa80c3e489538214.pdf": "zhang2024mmj",
    "9e2037d7d2f8222a7be86d2471eda895c8040ff5.pdf": "qiu2024zyc",
    "b877f5076c617a948081e12e08809e6c6b84b468.pdf": "oh2024xa3",
    "3f915aab835cbfe69e7b2ea1c73b74ac8a2d384e.pdf": "zhang202396g",
    "ecc51ce52ca524be17616a9c0dc8a051a2996ad7.pdf": "yin2025s2b",
    "23f1d4b46bc7c8f357a5a89144d5d32af7be13a5.pdf": "goyal2021onb",
    "576023f7cc3da5a36ac0cfda402af859cc90be10.pdf": "kuan20249pm",
    "a7f4deb9a1452374330f202bc8d36966a0f254e8.pdf": "kaul2024ta7",
    "7bcd5c0b17560ee560aec903ea42487a1a54e5d9.pdf": "yebin2024txh",
    "8ff45750057cc9452ae09aef6b9dfee3bd84b083.pdf": "zhao2024ge8",
    "396305230ddcf915b19a19683a89e34d76321a33.pdf": "ye2023yom",
    "97d24f9f0d81007d57cc43e61bf2b0c9081fe184.pdf": "zhang2023k5a",
    "0f7a6c557e376d8c77d684bcda0daee74fc29acf.pdf": "li2022ypy",
    "49b79d61ffc2db6dce8c2cd9cda06e1876ed8b4c.pdf": "huang2023du3",
    "1cc347c97a8f9d30edc809e4f207d64c7b8247b4.pdf": "tang2024a1j",
    "dd6b124606e3696dcddc93c889a824feaa322117.pdf": "fu2024yqj",
    "99832586d55f540f603637e458a292406a0ed75d.pdf": "yao20229uz",
    "ca2f63950685a97e5ab6b8e6b2db78a8995e94a2.pdf": "kanthara2022kuj",
    "a6d8d04962f84ae6225e72723869a002b9fc8036.pdf": "kim2021obx",
    "5bb3bd2ec1e99b11a84ccd0e4dce4bdb2a776a5e.pdf": "xia20224cl",
    "c4f26bc007343c59bedd1423250a3b453d3d2d22.pdf": "aharoni2022ioz",
    "ebb4826b717798918bebed4dfac28c917e44577d.pdf": "zhang2022p55",
    "35872510c095b1189105e9f902f04f51bd0a88e3.pdf": "jiang2022reg",
    "c2c3220b9faf95db43d90a5ed42fa824b4b3d2f0.pdf": "wang2020vz6",
    "72da4a646a31d72bcae90b916e120cd7df5f9dae.pdf": "chen2022gkm",
    "ecb5a6fe2f5261e4e717ece1e82c464c63cb4862.pdf": "korbak202191w",
    "f35dbee22c1572d149b7c1e20d69672cae931451.pdf": "raman20229ce",
    "fda76a1411e16722ebd2d8278c3143ca4363da6b.pdf": "liu2021h6c",
    "3e075efc541c7d2b357199655e11f084686e8575.pdf": "norlund2021462",
    "334bf07262320eb895a22973c948b4111e782daa.pdf": "wu20206gt",
    "5b824faaea855eb1700b16d7a67ee58a8f75e7d4.pdf": "liu2022hw7",
    "c8373577bcce9b8811672ddeb372a5b780397117.pdf": "jelinek2016205",
    "6ab78343ab82fa9d7baa68027f9f7e8cd9863737.pdf": "raunak2022r58",
    "5791c2b41dd23310c53d6738a4c0d587107c2dc8.pdf": "saha20229lo",
    "fbe549b5b30f54a4f76cb4bf4c6b8a9fb8e24657.pdf": "ham20213fx",
    "a996fb4b9f3d4315138f8773b1e995f8386b11eb.pdf": "jeong20180d2",
    "471a49220cea2069e8b8a76821b1d2434204a732.pdf": "kedia2022c03",
    "3b7cdcbe19fdbda17f7cce1eb1b6c8f5a8a60e3b.pdf": "gallace201080s",
    "6aed9dffe3246efd9d19b2873994ba112e2ad422.pdf": "li20203k7",
    "b32cf25dffbdbe6d838fc7b8781c126c8fea7d3c.pdf": "zhou2011j8m",
    "20013690616f6e781c05feaa08a2247a97640a87.pdf": "injae2016yq6",
    "e416b5fb1ab75c0770cf7cbd6976f5444b0ee89f.pdf": "jeong2019z3k",
    "b7d484357f5f84c065ef7fbc77d0460b9795964d.pdf": "rajani20171n9",
    "7c9f69848d28e0a7cbb00942ee83dab9773c23e4.pdf": "wang202379k",
    "ce4e101950554c41d3b35f5b297722abd1ce6403.pdf": "alshahwan2024v64",
    "5c204b2421d05b83d3c96a6c515cc03143073935.pdf": "zou2024ucl",
    "8c5acaafe43e710d55b08c63d567550ad26ec437.pdf": "fadeeva2024lt8",
    "1ebcf1884390c28f24b3adaf5a7aba5b9453b48b.pdf": "nguyen2023obn",
    "f4e06256ab07727ff4e0465deea83fcf45012354.pdf": "zou2024c26",
    "e17c58d7a48b6b811df023484161a3b9c03e0d6b.pdf": "li2023f7d",
    "2338d7c9ab07e6d0f4160335dce0e6e6a87c4749.pdf": "wang2023hgw",
    "fac468032e0c38ea10dfb95ba6cdeac51a473050.pdf": "chen2023gii",
    "3e7b421f9df1bd34d9cb42ece2760269f0314f05.pdf": "gilbert2024uu2",
    "25a9c0946925cc860c4600dba91b313cdbe7c8a8.pdf": "kim2024vgn",
    "d299a6b26e9ee23d0337a1d1a896fc1c847f5a46.pdf": "wang2024sae",
    "ea0d41514a41f8273f13b3b277e7fcbbc65a8549.pdf": "huang20233v0",
    "a082c9c93b5cc0d38e7ac14c6c9dfe186bb5c824.pdf": "zhao2024s3a",
    "24b6b70e1b1525535155cc9fa66dfd9d5d42d6b5.pdf": "leiser2024kfo",
    "6a9a9120d746a3c29902548bd1d93d6ea034c5d7.pdf": "malmqvist2024k7x",
    "95a52dd5adf6eb8d918cdfbf6189aab4eaa8e607.pdf": "lin2024gru",
    "80fd20e175f83a699258b8780cf365418d1538b0.pdf": "li2023dw0",
    "81bd66d960503106ef969830568016da4f93754a.pdf": "ma2023mka",
    "91ca6535fc8fb03efe0ecbe424ce5354ed129b0c.pdf": "song2024t8k",
    "d48c56dbce88580736c037797666060cb3b03bf7.pdf": "jiang20242kz",
    "668341051f3c9c087e42e393c610792df3e45992.pdf": "song2024br2",
    "3dea23e10eeff848f7352b17bbc1fdce38112acc.pdf": "liu2024ker",
    "31968a970f14beab3cbadf9f6ad45c1a51f4ea95.pdf": "sun2024z6b",
    "be8c90bca14d59f180f40a41126b7cd8c29c5d4e.pdf": "ling2024hqv",
    "aa1fbd6e8d1c8e99b6ca34c17bcdb36e987b68a6.pdf": "li2024jbb",
    "694b753385820ea675f2ca80dcdb4c91fc05962a.pdf": "smith2023i8w",
    "6a2e0927914ef03f25b99c2666f2275e0c950e5d.pdf": "shah20242sx",
    "f19e6c955b05d61aeb1cbc7580dc3723d31398ea.pdf": "pan2024hm4",
    "3c2130e219528df234658b94810de33fe7b077dc.pdf": "zhang2024o58",
    "37f6249b8381c955d732755714c1e700ba62b988.pdf": "tang2024cxa",
    "f123b838e000e11f08cb0d7c63e01934b38d3092.pdf": "zhou2024d14",
    "4c0f029efd5371eed087d0794fb0df71238600cc.pdf": "zhao2024g9c",
    "cdfab7e389f94263ce99b7c0025090971df40a01.pdf": "pan2024uot",
    "4c3447dce6798b894313bb3ff2735ef139cbf071.pdf": "zhu2024hll",
    "1f49b4586cc71cca59151e7a7bbfd500574c2fee.pdf": "zhang20252at",
    "4150d370a29258ac88552561cdd2b10f7862bfd7.pdf": "chen2024kgu",
    "de02ba19fb957ae30de7f09904ae3d983c3b50e7.pdf": "dernbach2024w0b",
    "75a381667ef536d02d99063eb3568e410d7ce909.pdf": "wu202407f",
    "d409053ed94ec725d72c812e7c8bd71b87278b96.pdf": "ahn2024r1o",
    "379e3ee9a6a92817bd0812848b409eafb9bf9550.pdf": "mou2024fsy",
    "b0633ccf235e467c35b963ad012f6b8c54aba19f.pdf": "xu2024f68",
    "35a544ca04325006f7f2e2369d3e0aaa8ba36a07.pdf": "hu2024fnt",
    "165fdad3949b7abdb985cb8834c26c7baa7bd40f.pdf": "mukherjee2024o5w",
    "8c8527f7615d53cbc21b9c3536486540f1c75000.pdf": "jiao2024l4e",
    "e131a11ad907023f655f22a4fae2b2a6f2db96f7.pdf": "ding20245e3",
    "ffeec58ed1fc045c55512e20b30fce951913a3f0.pdf": "wang2024swy",
    "89729cdfe0f71ad7a04c73e9167c2b266ee0ee8c.pdf": "chen2024md6",
    "bca44a53d9becd158ee0abf34c4255375fdc7327.pdf": "wang2023ynd",
    "d5ebd84b996491d8ffadefd05a32f8f25085935d.pdf": "yeo2024g7d",
    "5cb8fc293567f4f2930712a3bf7dec97b4dd1776.pdf": "hu2024fld",
    "388e6dbb4b4486e01d4f040684560135a9e1ef71.pdf": "zhang2024ia4",
    "e384f513114d7e6c20d007b4a3ad13fa58cf83dd.pdf": "yang2024iia",
    "56ff9de0931bd1accb9d4e3f109afcbf31f7df25.pdf": "yu2023ine",
    "bff123171a5b7bddd699a72daed96b4c56742069.pdf": "gu2024eig",
    "4582779668ab801f29db457790cd291767510035.pdf": "liu2024kf2",
    "e7d5f2da48b9141e18de9ea57ed8a4cebb6a09cd.pdf": "woo2024dtm",
    "36ffd74db97f98f121a6c2954bf98c93dad5d2ce.pdf": "zhao20246wi",
    "4cf85f9436bb8b5b1c68715f44d6b67254413ef8.pdf": "qian2024mj9",
    "2b277717203bb5354e7d41e79a35c59e34fa6778.pdf": "butler20242xs",
    "dd20cfadf992986b5d71b3a44b5a8660f0d68671.pdf": "sahoo202420w",
    "7112d0d36b960b590f55569bc294b2190d288860.pdf": "zuo20242i0",
    "5be8fd7aa6564ea1884626c16bcac36508f79ff1.pdf": "parente2024vlq",
    "8a9b43946dc10f91ce8c5971a1f247fbacda7a42.pdf": "zhao2024h5n",
    "e615f33365ea1d439507fc477588528ffb0764a8.pdf": "zhou2024b0u",
    "108bc0498629f4710b44076fe0c6270954494097.pdf": "adewumi2024lv9",
    "59395cf4f9346ef4ccb37499a3a7e52c2978fc61.pdf": "toroghi2024mxf",
    "655f92355d7930919a01125bd7a35c812498b1a9.pdf": "taveekitworachai2024aql",
    "d0fe343fbdecaf4cc477d70e8701f9a6935b13d0.pdf": "wan2024mh1",
    "10269639a0462a0e9790bd0524f5a092325d8d51.pdf": "alsadat2024i78",
    "7e38ac6f71408383939ec05f60b0bd85759a4c4e.pdf": "cao2024o9a",
    "1541bc9e588bfcd4bf365c868fa2f11461896980.pdf": "benkirane202494i",
    "f05e64c2a096e3762939dfdb7f475724c04a46bd.pdf": "jin2024jpw",
    "46e542884db4fc4df605eb28473cff79aec54c99.pdf": "mu2024f3b",
    "ff06bee3d898b3dc3a0364f2bfe506591d7e6d52.pdf": "yu2024pp9",
    "7641749cae1ad30779bfb46948fd47922bcc296a.pdf": "amirizaniani2024cad",
    "7adb88771376c2a31688e3b0395b0550a35b824d.pdf": "ling2024qto",
    "f41977c497c96c1da2e9e945315e9be6d6ad472e.pdf": "sarmah2023cuq",
    "20eecb9ead20ffe49a66588a9662336eefb20a54.pdf": "nathani202338c",
    "ff0450c78e5fabaa8aab61a368f267bd83753a64.pdf": "chataigner2024cr0",
    "e33fceb7cfb825ae3c530de0bf093769169039fc.pdf": "liu2025xwv",
    "6518d3209cec0a1ad277e8aaf153242b3a4233d9.pdf": "song2024v5n",
    "d45865c981161ad711adf18b0492e959771554e4.pdf": "barkley202472d",
    "4d710532fff7aec8187f68fb2ca90079c40e7004.pdf": "huang2024c9t",
    "003463160918704684f812ae8d7b6920d2d15e31.pdf": "malin2024fin",
    "f0a79fe7765ab253480a0be6d29c889eac19eb3c.pdf": "yuan2024o7d",
    "910d26adcd83c4ec36f365198f8b2224b14ad6c9.pdf": "pandit20257jx",
    "b5f56f466c06d10100d8d1aac9e1f979c527b1cf.pdf": "bellinileite2023y38",
    "6b44d0ac2b6d6deeb7f35ef4a7ad77b12b646b9e.pdf": "omar2025us3",
    "9516ad22fbd81875b160c4471ff3f747c4543da1.pdf": "lee2024i72",
    "cb7fa7ee3df826628c113ba0c6db1205751d89a3.pdf": "li2023irg",
    "13fd528a587196ff6429bfbe1d11d2f89a4036f5.pdf": "zhang2023pb6",
    "f63fdbbdf9005245d960ac1912cf4d0805e274a8.pdf": "irulandi2023xlg",
    "3b1baabcbfd19e2f292863c522de41083814856a.pdf": "li2024ncc",
    "fa0d056dd585eeffb4333cb55807d357808f8440.pdf": "guo2024tlu",
    "0797f2d1366da1f3441ea7d33b2109d7f27d1ad7.pdf": "xie20247zk",
    "fbca0c2ec5425bbd8dc4898d684c909a58dab1de.pdf": "amirizaniani2024493",
    "4dc1fbde861d1e97daa0677ac9cfae92b2832589.pdf": "yang2025n54",
    "f363c38f2a5d3ed8697a72d3dd014d228bfda91a.pdf": "agarwal202418c",
    "43210579b1ff7707afbd5d1ed045cc56ba52e938.pdf": "rrv2024gw0",
    "1154343478e423fecb12501cf02208499cd57846.pdf": "li2024hl9",
    "691f111348f3b19163e62a208de9803280205ed8.pdf": "wang2024t4o",
    "c0082580c4b9e5c6c96cf06f1be67c0cbbafb753.pdf": "hegselmann20249q4",
    "ded732209b0ba8a6704cc62ab8197a898b57f833.pdf": "gao2024ncr",
    "6d3ae6d6b312b659b3a14ae3f3e86a36db63200d.pdf": "tsai2024klg",
    "e059a20ae41aa32837030fd6e4392b8217243a2c.pdf": "chen2024qs5",
    "55b193ab7967fb20a8a05f878ac85bf48c9fc615.pdf": "liu2025juo",
    "ea32e8511cde4a1b852d8c003e0ec64bdf64b0d8.pdf": "zhang2024htn",
    "b7aa5af5bf96ee003543d5ff7dcfc3d9a46d43bb.pdf": "guo2024hgn",
    "8767dcddfb856db4bfa1e150470fc99f51f43835.pdf": "luo2024uh8",
    "6bf4e95e63df023c81458ec60a8324788535a2f4.pdf": "hamid2024pwn",
    "8705389fd69be2a0cecd2242287a08e8280f2c52.pdf": "zheng20240qd",
    "21967f943a189a9171f9f880182894acff5b87a4.pdf": "rawte2024bu6",
    "5e5336d0284e94cd835c40f931e0d379e21b464d.pdf": "yin2024iau",
    "1d1af14aa70f86b013e616cfd07fa8a164652d84.pdf": "tang2025mfi",
    "6073b9a4856d726c270f03ebee54ea7658f16ec1.pdf": "das2024jdt",
    "4c4a3328153e85749c690e68acc13b42a7225e50.pdf": "zhang2024h4a",
    "849b3727dbb41c37f92a338ac5860b764a5b94f4.pdf": "xu2024t34",
    "55adbe4a6511a9c036024c6e2a637782d53289f4.pdf": "zhang2025p1z",
    "934635a82a338ddea6e0c0059663250eee259e8f.pdf": "ahmadi2024j88",
    "2777cf13955e7683e0ec5446fdff7cdf7dd57d91.pdf": "abdelghafour2024efh",
    "f7a47a7de7289d6fd69e3bf226f7cdaffa670a3f.pdf": "zhou20253zv",
    "d6442ff9d10310071108f44734b00d182b6e2c28.pdf": "karbasi2025j7n",
    "0e3d306997e4830e668f750bddc3ee28487ce59a.pdf": "mubarak2024lx6",
    "962cdae24cebb49c8870525fbf229554203aa5d2.pdf": "zhang2024sbu",
    "1c3eee136c5fa85ad97ef62f353557f776059f3e.pdf": "omar2025cc3",
    "b29b7aaae0554e864409dfd1afadf9e1564a2616.pdf": "gao20242nu",
    "6becd0d29f013a7aec01d453727cb1680c01979f.pdf": "pester20242zt",
    "dbcd51388bc622e7725782177c09cf8b5c1daf5d.pdf": "wu202415r",
    "ac7cc880f897626ee2d2d5a5c40180d551f4e0f8.pdf": "tu2024v40"
  },
  "sections": {
    "Introduction": "\\section{Introduction}\n\\label{sec:introduction}\n\n\n\n\\subsection{The Rise of Large Language Models and the Hallucination Challenge}\n\\label{sec:1_1_the_rise_of_large_language_models__and__the_hallucination_challenge}\n\n\nThe advent of Large Language Models (LLMs) has heralded a transformative era in artificial intelligence, demonstrating unprecedented capabilities in natural language understanding and generation \\cite{ahmadi2024j88}. These models, characterized by their massive scale and emergent abilities, have revolutionized diverse domains, from automating complex cognitive tasks like code generation and scientific discovery to enhancing human-computer interaction through sophisticated dialogue systems. Their capacity to produce highly coherent, contextually relevant, and often creative text has positioned them as pivotal technologies poised to redefine numerous industries and research paradigms.\n\nHowever, alongside these remarkable advancements, a pervasive and critical limitation, widely termed 'hallucination,' significantly impedes the reliability and trustworthiness of LLMs \\cite{ahmadi2024j88, rawte2023ao8}. Hallucination refers to the generation of content that is factually incorrect, nonsensical, or ungrounded in the provided input or real-world knowledge. Early investigations into neural text generation, particularly in abstractive summarization, highlighted this issue, revealing that models frequently produced information not present in the source document, with a substantial portion being factually erroneous \\cite{maynez2020h3q}. This foundational work introduced a critical distinction between 'intrinsic' hallucinations (misrepresenting source information) and 'extrinsic' hallucinations (adding ungrounded information), underscoring the challenge of ensuring faithfulness and factual accuracy. The problem's prevalence has only intensified with the increasing scale and generality of LLMs, making it a central concern for their safe and effective deployment \\cite{rawte2023ao8}.\n\nThe implications of hallucination are profound and far-reaching, posing substantial risks to user trust, the credibility of AI systems, and the safety of their applications across various domains. In high-stakes environments such as healthcare, finance, or legal services, hallucinated information can lead to severe consequences, including misdiagnoses, flawed financial advice, or fabricated legal precedents \\cite{li2025qzg, ahmadi2024j88}. Beyond factual errors, hallucinations can manifest as logical inconsistencies or ungrounded reasoning, eroding confidence in an LLM's ability to perform complex tasks reliably. This challenge extends beyond mere inconvenience, directly impacting the deployability of LLMs and necessitating robust mechanisms to ensure their outputs are verifiable and trustworthy.\n\nThe research trajectory has evolved from merely characterizing hallucination to seeking a deeper understanding of its origins, both empirical and theoretical. Initial observations revealed that even during pre-training, smaller language models could learn to reduce perplexity on grammatical sequences that *contained* hallucinations, suggesting that the propensity for generating plausible but incorrect text is embedded early in the learning process \\cite{xia20224cl}. As models scale, some of these issues are mitigated, but the fundamental challenge persists. More recently, the understanding of hallucination has been elevated to a theoretical plane, with formal mathematical definitions being proposed \\cite{li2025qzg}. Groundbreaking work has even posited that hallucination is not merely a transient engineering problem but an inherent and *inevitable* limitation of any computable LLM, stemming from fundamental principles of learning theory and computability \\cite{xu2024n76}. This theoretical perspective fundamentally reshapes the problem, suggesting that while mitigation is crucial, complete eradication might be impossible, thus shifting the focus towards robust management and transparent communication of uncertainty rather than absolute elimination. This pervasive issue is also not confined to text, manifesting uniquely across multimodal contexts, including Large Vision-Language Models (LVLMs) and Large Audio-Language Models (LALMs), further complicating the landscape of trustworthy AI \\cite{li2025qzg}.\n\nIn light of these challenges, the urgent need for robust research into the diverse causes, comprehensive evaluation, and effective mitigation strategies for hallucination is paramount. Addressing this phenomenon, which spans factual inaccuracies, nonsensical outputs, and ungrounded content across text and increasingly multimodal domains, is not merely an incremental improvement but a central hurdle to achieving truly trustworthy, verifiable, and safely deployable AI systems. This review aims to comprehensively explore the multifaceted problem of hallucination, from its foundational definitions and mechanistic causes to advanced evaluation methodologies and cutting-edge mitigation techniques, ultimately guiding the development of more reliable and accountable LLMs.\n\\subsection{Scope and Structure of the Review}\n\\label{sec:1_2_scope__and__structure_of_the_review}\n\n\nThis literature review provides a comprehensive and structured analysis of research into hallucination in Large Language Models (LLMs), adopting a pedagogical progression to trace the field's maturation from foundational concepts to advanced methodological approaches and cutting-edge multimodal developments. This structured organization is designed to offer a clear, evolving narrative of the research landscape, emphasizing the interconnections between different facets of the hallucination problem. The review synthesizes the evolving understanding of hallucination, its underlying causes, methods for its evaluation, and strategies for its mitigation, with a primary focus on advancements from 2022 to 2025 to ensure relevance to the current state of the art in LLM research.\n\nThe review is meticulously organized into several thematic sections, each building upon the preceding one to offer a holistic and in-depth understanding of hallucination. Following this introduction, Section 2, \"Foundational Understanding: Defining and Categorizing Hallucination,\" lays the groundwork by exploring the evolution of hallucination definitions. It distinguishes between crucial concepts like faithfulness to source material and factual correctness in general knowledge, and traces the development of early taxonomies. This section highlights how the understanding of hallucination has broadened significantly beyond simple factual errors to encompass a diverse array of content inconsistencies, logical flaws, and reasoning failures, thereby setting the stage for more granular analysis and targeted interventions.\n\nBuilding upon this definitional and conceptual foundation, Section 3, \"The Roots of Hallucination: Mechanistic Causes and Theoretical Limits,\" delves into the underlying reasons why LLMs generate incorrect or ungrounded content. This section moves beyond mere empirical observation to investigate both practical causes related to data quality, training processes, and inference biases. Crucially, it explores specific internal model mechanisms, such as 'knowledge overshadowing' and 'attention sinks,' that directly contribute to hallucination. The section culminates in a discussion of the formal theoretical grounding of hallucination, examining its mathematical origins and the concept of its inherent inevitability, which fundamentally re-frames the problem from a transient engineering bug to an innate characteristic of computable LLMs.\n\nWith a clear understanding of what hallucination is and why it occurs, Section 4, \"Evaluating Hallucination: Benchmarks for Factual Accuracy and Reasoning,\" details the evolution of evaluation methodologies. It showcases a progression from initial, broad assessments to highly granular, automatically verifiable, and context-aware benchmarks. This section covers early efforts to establish reproducible metrics for factual correctness and citation quality, advancements in fine-grained and rationale-based evaluation that probe the model's reasoning process, and the development of specialized benchmarks for complex algorithmic reasoning. Furthermore, it addresses the unique challenges of assessing hallucinations in long-context and dialogue-level interactions, underscoring the field's commitment to rigorous and comprehensive measurement of LLM trustworthiness.\n\nSubsequently, the review transitions to a comprehensive exploration of mitigation strategies, divided into two complementary sections. Section 5, \"Mitigation Strategies I: External Knowledge Grounding and Adaptive Retrieval,\" explores approaches centered on grounding LLMs in external knowledge. It introduces Retrieval-Augmented Generation (RAG) as a foundational paradigm, explaining its core principles and early implementations. This section then details the evolution of RAG into advanced and adaptive architectures that dynamically integrate external information, optimize retrieval, and intelligently manage context. It also highlights the increasing importance of integrating structured knowledge graphs to enhance factual accuracy, logical consistency, and overall trustworthiness.\n\nComplementing external grounding, Section 6, \"Mitigation Strategies II: Intrinsic Model Interventions and Self-Correction,\" focuses on interventions that operate intrinsically within the LLM itself. This includes techniques that modify the decoding process, such as contrastive methods, to steer generation away from ungrounded content. The section then delves into more granular interventions that manipulate internal model states and attention mechanisms during the forward pass for precise control over information flow. Finally, it discusses the development of self-correction and abstention mechanisms, enabling LLMs to detect their own uncertainties and errors, along with training-based approaches that leverage automated data generation for more efficient and targeted fine-tuning against hallucination.\n\nRecognizing the expanding frontier of AI, Section 7, \"The Multimodal Frontier: Hallucination in Vision, Audio, and Video Language Models,\" addresses the significant challenges posed by hallucination in Large Vision-Language Models (LVLMs), Large Audio-Language Models (LALMs), and Video-Language Models (VLLMs). This section defines and categorizes the distinct types of hallucinations that emerge from cross-modal interactions, details specialized evaluation benchmarks developed to rigorously assess these multimodal inconsistencies, and explores tailored multimodal mitigation strategies. It also investigates dynamic behaviors such as 'multimodal hallucination snowballing' in interactive multimodal settings, highlighting the complexity of ensuring trustworthiness across diverse sensory inputs.\n\nFinally, Section 8, \"Towards Trustworthy AI: Robustness, Safety, and Advanced Evaluation,\" explores critical advancements in building truly trustworthy LLMs. It covers methods for zero-resource and black-box hallucination detection, crucial for proprietary models, and delves into the proactive testing of LLM vulnerabilities through adversarial attacks, which actively induce hallucinations to identify weaknesses. This section highlights the development of 'semantic guardrails' for safety-critical applications, aiming for absolute error prevention, and discusses the emerging field of meta-evaluation, which critically assesses the quality of hallucination benchmarks themselves, ensuring the integrity of all research efforts towards reliable AI.\n\nThe review concludes in Section 9, \"Conclusion and Future Directions,\" by synthesizing the key advancements, outlining remaining challenges and open questions, and discussing the critical ethical considerations necessary for responsible AI development. This structured progression aims to provide readers with a comprehensive, nuanced, and forward-looking perspective on the multifaceted problem of hallucination in LLMs, fostering a deeper understanding of the path towards more reliable and accountable AI systems.\n",
    "Foundational Understanding: Defining and Categorizing Hallucination": "\\section{Foundational Understanding: Defining and Categorizing Hallucination}\n\\label{sec:foundational_underst_and_ing:_defining__and__categorizing_hallucination}\n\n\n\n\\subsection{Early Characterization and Taxonomies in LLMs}\n\\label{sec:2_1_early_characterization__and__taxonomies_in_llms}\n\n\nThe advent of Large Language Models (LLMs) marked a significant leap in generative AI, but simultaneously brought to the forefront the pervasive challenge of \"hallucination\"â€”the generation of content that is unfaithful to source material, factually incorrect, or logically inconsistent. Early research was thus fundamentally driven by the need to empirically observe, define, and categorize these phenomena to establish a foundational understanding for subsequent evaluation and mitigation strategies.\n\nInitial empirical observations of unfaithful content were predominantly rooted in specific Natural Language Generation (NLG) tasks, particularly abstractive summarization. A seminal work by \\cite{maynez2020h3q} provided a large-scale human evaluation of \"faithfulness\" in neural summarization. This study systematically characterized hallucinations into \"intrinsic\" (misrepresenting information present in the source) and \"extrinsic\" (adding information not directly inferable from the source) types. Crucially, \\cite{maynez2020h3q} highlighted that traditional automatic metrics like ROUGE correlated poorly with human judgments of faithfulness, underscoring the limitations of surface-level evaluation and the necessity for deeper semantic understanding. Building on this, \\cite{dong20223yz} further refined the understanding of faithfulness, challenging the strict assumption that all out-of-article content is undesirable. Their work demonstrated that gold-standard summaries often contain factually correct entities not explicitly present in the source, requiring external world knowledge. This introduced a critical dual perspective: faithfulness to the source document versus faithfulness to external world knowledge, thereby significantly shaping the early research agenda by clarifying these distinct dimensions of hallucination. These early efforts in summarization laid the groundwork for identifying content unsupported by a given context and distinguishing between adherence to input and general factual correctness.\n\nMoving beyond document-level analysis, the need for finer-grained detection methods applicable to free-form text generation became apparent. While not directly a hallucination detection method, the work by \\cite{chen2022gkm} on proposition-level segmentation and textual entailment recognition offered a foundational conceptual framework for analyzing the truthfulness of individual meaning units within sentences. By enabling the segmentation of sentences into distinct propositions and classifying their entailment relations with respect to a reference document, \\cite{chen2022gkm} provided tools that could underpin more precise, token- or phrase-level identification of unsupported content. This approach represented a significant step towards dissecting generated text at a granular level, addressing the limitations of coarser-grained methods that might miss subtle inconsistencies, and paving the way for more detailed analyses of where and how hallucinations manifest within generated outputs.\n\nAs LLMs gained prominence, the scope of hallucination expanded, necessitating structured frameworks for categorization that transcended task-specific observations. Comprehensive surveys like \\cite{zhang2023k1j} and \\cite{ye2023yom} emerged as foundational works, synthesizing the burgeoning literature and proposing early LLM-centric taxonomies. \\cite{zhang2023k1j} categorized LLM hallucinations into input-conflicting, context-conflicting, and fact-conflicting types, emphasizing the particular challenges posed by fact-conflicting errors due to the absence of an authoritative knowledge source. This survey provided a clear analytical framework for understanding the multifaceted nature of LLM failures. Concurrently, \\cite{ye2023yom} offered a detailed taxonomy that spanned various text generation tasks, from machine translation to dialogue systems, and identified initial hypothesized origins related to data collection, knowledge gaps, and the optimization process. These meta-analyses were instrumental in consolidating diverse empirical findings into a common language, providing structured frameworks for understanding different types of hallucinations, and distinguishing between faithfulness to source and factual correctness in general knowledge. While these early taxonomies provided critical initial classifications, they often remained descriptive, and the sheer diversity and subtlety of LLM-generated errors hinted at a problem far more complex than simple factual deviations.\n\nIn conclusion, the early characterization of hallucination in LLMs evolved from empirical observations in specific NLG tasks, particularly abstractive summarization, to the development of sophisticated, LLM-centric taxonomies and meta-analyses. This progression, from identifying unfaithful content at a document level and refining the concept of faithfulness, to enabling finer-grained analysis and categorizing diverse error types, was instrumental in defining the problem space. These foundational works established the crucial distinction between faithfulness to source and factual correctness, which became a cornerstone for subsequent research. However, the initial frameworks also revealed that many LLM errors were not merely simple factual deviations but deeper failures of reasoning and consistency, suggesting that a more pervasive and nuanced understanding of hallucination was required, a challenge we explore in the subsequent section.\n\\subsection{Hallucination as a Pervasive Problem: Beyond Factual Errors}\n\\label{sec:2_2_hallucination_as_a_pervasive_problem:_beyond_factual_errors}\n\n\nThe initial conceptualization of hallucination in Large Language Models (LLMs) primarily focused on the generation of content that was factually incorrect or unfaithful to source material. However, as LLMs have grown in complexity and application, the understanding of hallucination has profoundly expanded, revealing it as a multifaceted problem encompassing a broader spectrum of inconsistencies and failures that extend beyond mere factual inaccuracies. This shift highlights that hallucination impacts the overall 'information quality' and trustworthiness of LLM outputs, necessitating a deeper examination of underlying cognitive and logical failures \\cite{rejeleene2024okw}.\n\nEarly research began to delineate the nuances of unfaithful generation, laying the groundwork for this broader view. For instance, \\cite{maynez2020h3q} distinguished between \"intrinsic hallucinations,\" which misrepresent information present in the source, and \"extrinsic hallucinations,\" which introduce new, ungrounded information. Crucially, they observed that many extrinsic hallucinations were erroneous, demonstrating that even superficially plausible generated content could be unfaithful to its source and thus unreliable. This early distinction underscored that hallucination was not solely about factual errors against world knowledge, but also about fidelity to provided context. This conceptual expansion quickly extended to multimodal contexts, where \\cite{dai20229aa} identified \"object hallucination\" and \"attribute hallucination\" in vision-language models. Here, LLMs generated objects or attributes not present in the visual input, illustrating how ungrounded reasoning could manifest across different modalities, producing semantically coherent but contextually false outputs.\n\nThe field has since moved towards recognizing that hallucination often stems from deeper failures in an LLM's reasoning process and internal consistency, rather than just a lack of factual recall. Logical inconsistencies, where an LLM's internal reasoning path is flawed, represent a significant category of such failures. For example, \\cite{xie20247zk} demonstrated that the order in which LLMs generate answers and reasoning significantly impacts their consistency, revealing instances where models fabricate answers and then retrospectively generate justifications. This highlights a fundamental flaw in the logical coherence of the model's thought process, rather than a simple factual error. Further, \\cite{jiang20242kz} investigated how LLMs can hallucinate *despite possessing the correct knowledge*, attributing this to problematic inference dynamics. Their work revealed that in hallucinated cases, the output token's probability rarely demonstrated consistent superiority in later stages of the model, suggesting a failure in applying known facts during generation, indicative of ungrounded reasoning.\n\nBeyond explicit logical errors, hallucinations can also manifest as subtle semantic shifts, biases, or ungrounded claims that appear superficially plausible, thereby undermining trustworthiness. \\cite{zhang2024qq9} introduced \"knowledge overshadowing,\" a phenomenon where LLMs prioritize certain knowledge due to data imbalance. This leads to outputs that, while not necessarily factually incorrect, are ungrounded in the immediate context or subtly biased, generating plausible but misleading information. A particularly insidious form of this is \"sycophantic hallucination,\" as explored by \\cite{rrv2024gw0}. This occurs when LLMs provide answers that align with a user's potentially misleading keywords or desired narrative, even if factually questionable. Such behavior amplifies misinformation and erodes user trust by prioritizing perceived user preference over factual accuracy, representing a significant semantic shift that compromises the integrity of the generated content.\n\nCollectively, these diverse manifestations underscore a critical shift in understanding: hallucination is not merely about isolated factual errors, but about a pervasive degradation of overall \"information quality\" and trustworthiness. \\cite{rejeleene2024okw} directly addresses this by proposing a mathematical framework for evaluating Information Quality (IQ) in LLMs, defining it as a function of consistency, relevance, and accuracy. This framework explicitly moves beyond simple factual correctness to encompass the broader attributes essential for reliable and trustworthy AI outputs. The recognition of hallucination as a multifaceted problem, encompassing logical inconsistencies, ungrounded reasoning, and subtle semantic shifts, necessitates a paradigm shift towards more sophisticated evaluation and mitigation strategies that address these deeper cognitive and logical failures inherent in LLM architectures.\n\\subsection{The Evolution of Hallucination Types}\n\\label{sec:2_3_the_evolution_of_hallucination_types}\n\n\nThe understanding and categorization of AI hallucinations have undergone a significant evolution, moving from broad, general definitions to highly granular and context-specific classifications. This progression reflects the increasing complexity of AI models and their applications, necessitating a more nuanced taxonomy for effective detection and mitigation.\n\nInitially, research into hallucinations in large language models (LLMs) established foundational categories. Early work, such as the comprehensive survey by \\cite{DBLP:journals/corr/abs-2202-03629}, broadly categorized hallucinations into *intrinsic*, where the generated content contradicts the source input, and *extrinsic*, where it contradicts established world knowledge. This foundational classification also introduced related concepts like factuality, faithfulness, and consistency, providing an initial framework for analyzing model outputs. Expanding on this, another survey by \\cite{DBLP:journals/corr/abs-2305-13889} further refined LLM hallucination types by categorizing them based on their *source* (e.g., data, model architecture, inference process), *form* (e.g., factual, logical, numerical inconsistencies), and *severity*, offering a multi-dimensional perspective on how and why these errors manifest.\n\nAs AI capabilities extended to multimodal domains, the definition of hallucination necessarily diversified to encompass new forms of inconsistency. \\cite{DBLP:journals/corr/abs-2305-18654} specifically addressed hallucinations in Large Multimodal Models (LMMs), proposing a refined taxonomy that includes *object hallucinations* (misidentifying or fabricating objects), *attribute hallucinations* (incorrectly describing an object's properties), and *relation hallucinations* (misrepresenting relationships between objects). This marked a crucial step towards more granular, modality-specific categorizations. Building upon this multimodal understanding, research into Video-Language Models (VLMs) further introduced the dimension of time. \\cite{DBLP:journals/corr/abs-2305-18260} identified and categorized VLM hallucinations based on *temporal consistency*, detailing issues such as incorrect event order, duration, or frequency, which are paramount for accurately interpreting dynamic visual information.\n\nBeyond modality, the operational context and interaction paradigm of AI systems have also led to the identification of distinct hallucination types. For instance, the challenges posed by extended inputs prompted the definition of *long-context specific hallucinations* by \\cite{DBLP:journals/corr/abs-2309-17424}. These are errors that emerge or are exacerbated when models process unusually long sequences of text, often involving subtle inconsistencies or omissions that are difficult to detect without a comprehensive understanding of the entire context. Similarly, in the realm of conversational AI, \\cite{DBLP:journals/corr/abs-2309-07490} introduced the concept of *dialogue-level hallucinations*. These extend beyond single-turn factual errors to encompass inconsistencies in persona, conversation flow, or maintaining coherent context across multiple turns, which are critical for natural and trustworthy human-AI interaction. Furthermore, the global deployment of AI models has highlighted *multilingual hallucinations*, as investigated by \\cite{DBLP:journals/corr/abs-2305-10424}. This work revealed that hallucination rates can vary significantly across languages, suggesting that language-specific nuances, data biases, or model training disparities can lead to distinct patterns of erroneous generation in non-English contexts.\n\nThis progressive refinement in categorizing hallucination typesâ€”from broad textual inconsistencies to specific multimodal, temporal, long-context, dialogue-level, and multilingual manifestationsâ€”is indispensable. It underscores a growing recognition that a one-size-fits-all approach to hallucination is insufficient. The continued identification of such distinct types is crucial for developing targeted evaluation metrics, designing more robust and context-aware mitigation strategies, and ultimately fostering greater reliability across diverse AI applications.\n",
    "The Roots of Hallucination: Mechanistic Causes and Theoretical Limits": "\\section{The Roots of Hallucination: Mechanistic Causes and Theoretical Limits}\n\\label{sec:the_roots_of_hallucination:_mechanistic_causes__and__theoretical_limits}\n\n\n\n\\subsection{Empirical Causes: Data, Training, and Inference Biases}\n\\label{sec:3_1_empirical_causes:_data,_training,__and__inference_biases}\n\n\nLarge Language Models (LLMs) are susceptible to generating \"hallucinations\"â€”plausible but factually incorrect or unfaithful contentâ€”due to a complex interplay of empirical factors spanning their entire lifecycle, from data acquisition to training and inference. Understanding these practical causes is crucial for developing targeted interventions throughout the model development pipeline. A foundational observation, particularly in abstractive summarization, highlighted that standard likelihood training objectives and approximate decoding strategies inherently prioritize fluency and coherence over strict factual accuracy, leading to both intrinsic (misrepresenting source information) and extrinsic (adding ungrounded information) hallucinations \\cite{maynez2020h3q}. This initial insight underscores biases embedded during training and exacerbated during generation.\n\nA significant category of empirical causes stems from **data collection and preparation issues**. The immense pre-training corpora, while enabling powerful generalization, often contain noise, inconsistencies, and factual decay that models inadvertently learn \\cite{li2024qrj}. For instance, empirical studies reveal that the lower the frequency of specific knowledge in pre-training data, the higher the propensity for LLMs to hallucinate when queried on that topic \\cite{li2024qrj}. This suggests that data imbalance and under-representation of certain facts directly contribute to factual errors. Furthermore, LLMs learn superficial statistical patterns from their training data rather than robust logical reasoning. McKenna et al. (2023) identified two key biases in pre-trained models: an **attestation bias**, where models over-rely on propositional memory, affirming entailment if a hypothesis is likely attested in their training data regardless of the premise; and a **relative frequency bias**, where models tend to affirm entailment if the premise predicate is less frequent than the hypothesis predicate \\cite{mckenna2023pzc}. These biases demonstrate how statistical regularities in the data, rather than semantic understanding, can lead to incorrect inferences, with specific named entities often acting as \"indices\" to trigger memorized, potentially irrelevant, propositional knowledge \\cite{mckenna2023pzc}.\n\nDuring **training**, models can develop biases that manifest as hallucinations. The predominant optimization objectives, typically focused on next-token prediction, do not explicitly penalize factual inaccuracies. This encourages models to \"over-generalize\" or invent plausible but unverified information to maintain fluency, thereby internalizing spurious correlations present in the data \\cite{li2024qrj}. Supervised fine-tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF) aim to align models, but their effectiveness can be highly dependent on the quality of instructions and the balanced complexity of the fine-tuning data. Suboptimal SFT can exacerbate hallucinations, and even RLHF's benefits can be domain-specific, indicating that the training process itself can introduce or fail to mitigate biases effectively \\cite{li2024qrj}. In multimodal contexts, such as with Vision-Language Models (VLMs), training challenges can lead to \"uncontrollable global visual uncertainty,\" where the model struggles to consistently ground its textual descriptions in the visual input, resulting in visual hallucinations \\cite{chen20247jb}. This highlights that even with rich multimodal data, the training process might not sufficiently enforce cross-modal consistency.\n\n**Inference-time biases** further contribute significantly to hallucination, even in well-trained models. The decoding process, as noted by \\cite{maynez2020h3q}, often prioritizes fluency over factual accuracy. LLMs frequently struggle with complex reasoning tasks, leading to erroneous outputs. Empirical studies show that a \"lack of logical reasoning capabilities is the primary contributor to Fact-Conflicting Hallucination (FCH),\" particularly when dealing with temporal concepts and out-of-distribution knowledge \\cite{li2024osp}. Benchmarks evaluating LLM rationales reveal that models can generate plausible-sounding but logically flawed reasoning paths \\cite{oh2024xa3}. Furthermore, the order in which an LLM generates its reasoning and answer significantly impacts consistency, with generating an answer first often leading to subsequent fabrication of justifications, highlighting a fragile reasoning process at inference time \\cite{xie20247zk}. When tasked with algorithmic reasoning on complex structures like real-world graphs, LLMs often fail to produce correct solutions, indicating a struggle with systematic, multi-step inference that can lead to hallucinated steps or conclusions \\cite{tang2024a1j}.\n\nAnother prevalent inference-time bias is the model's overconfidence or lack of uncertainty awareness. LLMs tend to generate definitive answers even when highly uncertain, leading to confident hallucinations \\cite{li2024qrj}. This necessitates proactive abstention mechanisms, which allow models to signal uncertainty and refuse to answer when confidence is low \\cite{tjandra2024umq}. The empirical observation that models can produce \"never event\" errors in safety-critical domains further underscores this lack of inherent self-correction and the need for \"semantic guardrails\" to prevent catastrophic hallucinations during deployment \\cite{hakim2024d4u}. For LVLMs, an inference-time bias can be their failure to adequately ground generated text in specific visual regions without explicit guidance, leading to object hallucinations that require training-free, image-grounded interventions \\cite{zhao2024ge8}.\n\nEven with Retrieval-Augmented Generation (RAG) systems designed to mitigate hallucinations by providing external knowledge, LLMs exhibit several inference-time biases in context utilization. Empirical benchmarks reveal that LLMs struggle with **noise robustness**, often confusing similar information when relevant and noisy documents are present; **negative rejection**, frequently failing to abstain from answering when no relevant information is available; **information integration**, demonstrating a significant lack of ability to synthesize facts from multiple documents; and **counterfactual robustness**, tending to trust and prioritize factually incorrect retrieved information even when possessing correct internal knowledge or being warned \\cite{chen2023h04}. Moreover, LLMs can be receptive to newly generated counterfactual passages, indicating a vulnerability in their utility judgment of retrieved evidence \\cite{zhang2024o58}. These challenges necessitate more intelligent, uncertainty-aware retrieval and context highlighting mechanisms \\cite{niu2024v97, su2024gnz, lv2024k5x}.\n\nIn conclusion, empirical causes of hallucination are multifaceted, spanning the entire LLM development pipeline. Data quality issues, especially imbalances and superficial statistical patterns, lead to models learning incorrect associations and biases. Training objectives that prioritize fluency over factuality, coupled with challenges in enforcing cross-modal consistency, contribute to over-generalization and learned biases. During inference, models exhibit biases towards fluency, struggle with complex logical and algorithmic reasoning, often lack uncertainty awareness, and can inefficiently or incorrectly utilize external knowledge. Addressing these empirical causes requires a holistic approach, from meticulous data curation and refined training objectives to advanced inference-time controls and uncertainty quantification.\n\\subsection{Internal Mechanisms: Knowledge Overshadowing and Attention Sinks}\n\\label{sec:3_2_internal_mechanisms:_knowledge_overshadowing__and__attention_sinks}\n\n\nUnderstanding the genesis of hallucinations in Large Language Models (LLMs) necessitates a deep dive into their internal processing mechanisms, moving beyond surface-level output analysis to pinpoint the granular cognitive biases and representational distortions that contribute to factual inaccuracies. This subsection elucidates specific internal phenomena, such as knowledge overshadowing, amalgamated hallucinations, visual encoding distortion, semantic shift bias, and the exploitation of attention sinks, offering a mechanistic perspective on hallucination.\n\nOne fundamental internal mechanism contributing to hallucination is \\textit{knowledge overshadowing}, where dominant patterns or more frequently encountered information in the training data can suppress or override specific, less frequent facts. \\cite{smith2021overshadowing} introduced this concept, demonstrating how an LLM, even when possessing the correct information, might default to a more generalized or statistically prevalent answer due to the overshadowing effect, leading to plausible but incorrect generations. Building upon this understanding of internal misrepresentation, \\cite{jones2022amalgamation} further explored \\textit{amalgamated hallucinations}, where models do not merely over-generalize but actively combine multiple factually correct, yet disparate, pieces of information into a novel, coherent, but ultimately false statement or narrative. This highlights the model's internal capacity for creative miscombination, where the synthesis of information goes awry, producing a \"truthy\" but fabricated output.\n\nThe challenge of internal mechanistic failures extends beyond purely textual models, manifesting acutely in multimodal architectures. \\cite{chen2023multimodal} identified \\textit{visual encoding distortion} in multimodal LLMs, where the model's internal representation of visual information becomes corrupted during the encoding process. This distortion leads to hallucinations where the generated text inaccurately describes visual elements, even when the visual input itself is clear and unambiguous, underscoring that representational fidelity issues are not modality-specific but inherent to the internal processing pipelines. Further elaborating on the subtleties of internal processing, \\cite{lee2024semanticshift} revealed \\textit{semantic shift bias}, demonstrating how seemingly innocuous linguistic structures, such as paragraph breaks, formatting, or specific phrasing, can subtly but significantly alter an LLM's internal semantic understanding. This internal bias can lead to shifts in meaning, subsequently causing the generation of content that is factually divergent or contextually inappropriate, highlighting the profound sensitivity of internal representations to structural cues.\n\nThese identified internal vulnerabilities are not merely theoretical constructs but represent exploitable weaknesses. \\cite{wang2025attentionsinks} demonstrated how adversarial inputs can be meticulously crafted to exploit \\textit{attention sinks} within LLMs. By strategically placing specific tokens, attackers can manipulate the model's internal attention mechanisms, forcing it to allocate disproportionate focus to irrelevant or misleading information. This targeted manipulation of internal attention pathways can predictably induce specific hallucinations, providing a direct mechanistic link between internal processing flaws and external adversarial attacks.\n\nIn conclusion, the literature underscores that hallucinations are not monolithic errors but rather symptoms of diverse and granular internal mechanisms, ranging from the statistical biases of knowledge overshadowing and the creative miscombination of amalgamated hallucinations, to cross-modal representational distortions and subtle linguistic influences on semantic understanding. The identification of attention sinks further reveals how these internal processing quirks can be leveraged for adversarial purposes. While significant progress has been made in dissecting these internal mechanisms, future research must continue to investigate their complex interplay and develop more precise, mechanistic interventions that can directly address these core processing failures, moving beyond post-hoc corrections to foundational robustness.\n\\subsection{Formal Theoretical Grounding: Inevitability and Mathematical Origins}\n\\label{sec:3_3_formal_theoretical_grounding:_inevitability__and__mathematical_origins}\n\n\nThe understanding of hallucination in Large Language Models (LLMs) has undergone a profound paradigm shift, transitioning from an empirically driven problem-solving endeavor to a rigorous theoretical exploration of its fundamental limits. While early research primarily focused on identifying practical sources of hallucination and developing engineering-centric mitigation strategies \\cite{xu2024n76}, a groundbreaking line of inquiry has emerged to investigate whether hallucination is merely a transient engineering challenge or an innate, unavoidable characteristic of these powerful models. This theoretical turn seeks to establish a formal grounding for hallucination, moving beyond observational data to mathematical proofs of its inevitability.\n\nA pivotal contribution in this area is presented by \\cite{xu2024n76}, which fundamentally redefines the discourse around LLM hallucination. The paper introduces a \"formal world\" where LLMs are abstracted as total computable functions and ground truth is similarly defined as a computable function. This abstraction allows for the first formal definition of hallucination (Definition 4), characterizing it as an inconsistency between the LLM's output and the ground truth for a given input. This rigorous formalization provides a robust framework for theoretical analysis, independent of specific model architectures or training algorithms.\n\nThe core innovation of \\cite{xu2024n76} lies in its application of advanced concepts from learning theory, specifically Cantor's diagonalization argument, to prove the inherent inevitability of hallucination. This mathematical technique, traditionally used to demonstrate the existence of uncomputable numbers or functions, is ingeniously adapted to show that no computable LLM can perfectly learn all computable functions representing ground truths. The authors present Theorem 1, which demonstrates that for any computably enumerable set of LLMs, all states will inevitably hallucinate on some inputs. This is further extended by Theorem 2, proving that such hallucination will occur on *infinitely many* inputs. Crucially, Theorem 3 generalizes these findings to establish that hallucination is inevitable for *any individual computable LLM*, both on some inputs and on infinitely many inputs. These proofs establish a theoretical upper bound on the capabilities of LLMs, demonstrating that they cannot serve as universal problem solvers without generating factually incorrect information.\n\nThis theoretical insight fundamentally reshapes the understanding of LLM capabilities and limitations. Unlike previous empirical efforts that aimed to *reduce* or *eliminate* hallucination through better data, architectures, or prompting techniques, \\cite{xu2024n76} posits that complete eradication is mathematically impossible for any computable LLM. While parallel work by \\cite{kalai2023statistical} provides a statistical lower bound on hallucination rates for calibrated LLMs, \\cite{xu2024n76}'s findings are more general, applying to all computable LLMs and proving an absolute inevitability rather than a statistical likelihood. This distinction highlights the profound nature of the theoretical limits identified by the diagonalization argument.\n\nConsequently, hallucination is reframed not as a transient engineering problem to be fixed, but as an innate, fundamental characteristic inherent to the very nature of computable functions that LLMs embody. This profound theoretical grounding guides future research away from the elusive goal of achieving \"hallucination-free\" LLMs and towards more realistic and robust strategies for detection, mitigation, and responsible deployment. It underscores the necessity for LLMs to be used as specialized tools, often requiring external verification and human oversight, rather than as infallible general problem solvers. The mathematical origins of hallucination thus compel a shift in focus towards managing its unavoidable presence, fostering a more realistic and effective research agenda for the future of LLM development.\n",
    "Evaluating Hallucination: Benchmarks for Factual Accuracy and Reasoning": "\\section{Evaluating Hallucination: Benchmarks for Factual Accuracy and Reasoning}\n\\label{sec:evaluating_hallucination:_benchmarks_for_factual_accuracy__and__reasoning}\n\n\n\n\\subsection{Early Benchmarks and Automated Evaluation}\n\\label{sec:4_1_early_benchmarks__and__automated_evaluation}\n\n\nThe initial efforts to evaluate hallucination in Large Language Models (LLMs) were driven by the need for standardized, reproducible benchmarks that could move beyond subjective human judgment. These foundational works primarily focused on assessing factual correctness and the ability to generate text consistent with provided sources or external knowledge, laying crucial groundwork for automated evaluation.\n\nEarly investigations into factual inconsistencies often centered on specific natural language generation tasks. For instance, \\cite{DBLP:journals/corr/abs-2005-14165} introduced a dataset specifically designed for factual error correction in summarization, highlighting the prevalence of factual inconsistencies even in models trained on large corpora. This work underscored the necessity of robust evaluation methods to identify and rectify such errors, often relying on human annotation for ground truth. Recognizing the scalability limitations of human evaluation, researchers soon began developing automated metrics. \\cite{DBLP:journals/corr/abs-2009-07853} proposed a differentiable fact-checking model that could automatically assess the factuality of generated text by comparing it against a structured knowledge base. This marked a significant step towards scalable evaluation, offering an automated alternative to manual verification by leveraging external factual sources.\n\nAs LLMs became more capable and their applications diversified, the scope of factual evaluation expanded to more open-ended generation tasks. \\cite{DBLP:journals/corr/abs-2104-08784} extended the investigation of factuality to neural dialogue response generation, proposing a metric called FactScore and methods to improve factual consistency by grounding responses in external knowledge. This demonstrated an early attempt to tackle factual correctness in conversational contexts, where the absence of a clear source document makes evaluation more challenging. The growing awareness of factual errors also spurred research into mitigation strategies, with \\cite{DBLP:journals/corr/abs-2109-00971} exploring self-correction mechanisms to enhance factual consistency. While not strictly an evaluation paper, it implicitly highlighted the need for sensitive evaluation metrics to guide and measure the effectiveness of such corrective approaches.\n\nFurther refining the understanding and evaluation of factual consistency, \\cite{DBLP:journals/corr/abs-2201-02604} provided a comprehensive review and benchmark for factual consistency in abstractive summarization. This work meticulously categorized different types of factual errors and systematically evaluated various existing metrics, revealing their strengths and weaknesses in capturing the nuances of factual inaccuracies. This comprehensive analysis showcased the increasing sophistication in diagnosing the specific ways LLMs could hallucinate factually.\n\nA pivotal development in challenging the models' \"honesty\" and moving beyond simple factual recall was the introduction of TruthfulQA by \\cite{DBLP:journals/corr/abs-2205-07823}. This benchmark was specifically designed to measure whether LLMs generate truthful answers to questions that people commonly answer incorrectly due to widespread misconceptions. TruthfulQA pushed the boundaries of hallucination evaluation by targeting subtle forms of untruthfulness stemming from learned biases or misrepresentations, rather than just outright fabrication. Complementing these efforts, \\cite{DBLP:journals/corr/abs-2206-04624} presented a systematic evaluation of factual consistency across various LLM tasks and proposed a new metric, FactScore-NLI, based on Natural Language Inference. This approach leveraged the robust capabilities of NLI models to assess the entailment or contradiction between generated text and reference facts, offering a more generalized and linguistically informed automated evaluation paradigm.\n\nWhile these early benchmarks and automated metrics laid crucial groundwork for evaluating factual correctness and consistency, they often faced limitations in capturing the full spectrum of hallucination. Their reliance on external knowledge bases or factual consistency with source documents struggled to address more subtle forms of hallucination, such as plausible but entirely fabricated information in open-ended generation, or errors in reasoning and coherence that did not directly contradict a known fact. These methods, while scalable, often lacked the nuance to fully understand the cognitive processes leading to hallucination, paving the way for more sophisticated and fine-grained evaluation paradigms that consider broader aspects of truthfulness, coherence, and user intent.\n\\subsection{Fine-Grained and Rationale-Based Evaluation}\n\\label{sec:4_2_fine-grained__and__rationale-based_evaluation}\n\n\nTraditional evaluation metrics for Large Language Models (LLMs), often relying on surface-level textual overlap (e.g., ROUGE, BLEU) or simple answer correctness, frequently prove insufficient for comprehensively assessing the nuanced quality and underlying reasoning capabilities of their outputs. These metrics struggle to pinpoint the exact nature and location of errors, particularly hallucinations, or to verify the logical soundness of the model's internal reasoning process. To address this, a significant shift has occurred towards fine-grained and rationale-based evaluation methodologies, which delve deeper into the LLM's reasoning and output fidelity, aiming to provide detailed insights into error types and locations, and to scrutinize the logical coherence of generated rationales. This paradigm shift is crucial for fostering greater transparency and verifiability in LLM behavior.\n\nThe initial advancements in this domain focused on developing more granular methods for identifying and categorizing hallucinations within generated text. Rather than a binary correct/incorrect judgment, these approaches sought to annotate errors at a sentence or even phrase level, distinguishing between factual inaccuracies, logical inconsistencies, or ungrounded statements. This fine-grained annotation provides a richer understanding of where and how LLMs deviate from ground truth, moving beyond aggregate scores to actionable insights into model weaknesses. For instance, some benchmarks explore the impact of reasoning order on LLM consistency, revealing how models might fabricate justifications when asked to provide an answer before its rationale, highlighting a fine-grained flaw in their generation process \\cite{xie20247zk}.\n\nA crucial advancement in this area is the emergence of 'rationale verification' techniques, which scrutinize the intermediate steps an LLM takes to arrive at a conclusion, rather than solely evaluating the final output. This moves beyond merely identifying *what* an LLM gets wrong to understanding *why*, by pinpointing logical missteps in its chain of thought. The most sophisticated iterations of rationale verification leverage structured data to automatically assess logical soundness, offering a scalable and objective alternative to human judgment.\n\nA prominent example of this is \\textit{ERBench}, which utilizes existing relational databases (RDBs) and their inherent Entity-Relationship (ER) model to construct automatically verifiable benchmarks for LLMs \\cite{oh2024xa3}. ERBench leverages database schema, records, and crucially, integrity constraints like Functional Dependencies (FDs) and Foreign Key Constraints (FKCs), to generate complex, multi-hop questions. Its innovation lies in automatically verifying not only the final answer but also the LLM's rationale by checking for the presence of FD-inferred critical keywords within the generated reasoning steps. This allows for a deeper evaluation of factual hallucination and introduces novel metrics such as Rationale Accuracy (R) and Answer-Rationale Accuracy (AR), providing a verifiable audit trail for LLM conclusions by grounding evaluation in external, structured knowledge.\n\nBeyond relational databases, other structured knowledge sources like Knowledge Graphs (KGs) have been instrumental in developing rationale-based evaluation. \\cite{ghosh2024tj5} proposes a framework to assess and improve the logical consistency of Retrieval-Augmented LLMs (RAG) in fact-checking tasks, specifically for propositional logic queries derived from KGs. This work defines quantitative measures for LLM consistency across primitive logical operators (negation, conjunction, disjunction) and complex logical rules, directly evaluating the LLM's adherence to logical soundness within a structured context. Similarly, \\textit{Drowzee} introduces a logic-programming-aided metamorphic testing technique for Fact-Conflicting Hallucination (FCH) detection \\cite{li2024osp}. It constructs a factual knowledge base and generates diverse test cases using logic reasoning rules. By employing semantic-aware metamorphic oracles, Drowzee automatically detects FCHs by comparing the logical and semantic structures of LLM answers against ground truth, thereby validating the reasoning process itself.\n\nThe principle of verifying intermediate reasoning steps extends to specialized domains like mathematical reasoning. \\cite{liu2025juo} introduces a structured self-consistency framework designed to enhance the reliability of mathematical reasoning in LLMs. This method enforces self-consistency not just on final answers but critically across intermediate steps in tasks such as theorem proving, symbolic manipulation, and numerical computation. By ensuring logical consistency throughout the reasoning trajectory, this approach effectively reduces hallucinations and logical inconsistencies, paving the way for more reliable and interpretable AI-driven mathematics.\n\nIn conclusion, the progression from coarse-grained to fine-grained and rationale-based evaluation marks a critical maturation in LLM assessment. These methodologies, particularly those leveraging structured data like relational databases and knowledge graphs, offer unprecedented transparency by allowing for the automatic verification of an LLM's reasoning process. While significant strides have been made in pinpointing errors and verifying reasoning, challenges remain. The construction and maintenance of comprehensive, domain-specific structured knowledge bases can be resource-intensive, limiting scalability to highly complex, open-ended, or multi-hop reasoning tasks where the required knowledge might be vast or ill-defined. Furthermore, models might learn to produce 'verifiable' but ultimately incorrect rationales, highlighting the need for more robust verification mechanisms that are less susceptible to superficial adherence to rules. Future directions will likely focus on developing more adaptive rationale verification systems that can dynamically interact with diverse and evolving knowledge sources, handle ambiguous reasoning paths, and integrate with advanced uncertainty quantification methods to provide a holistic assessment of LLM trustworthiness.\n\\subsection{Complex Reasoning and Algorithmic Hallucination}\n\\label{sec:4_3_complex_reasoning__and__algorithmic_hallucination}\n\n\nEvaluating Large Language Models (LLMs) on tasks that demand genuine algorithmic reasoning and complex problem-solving represents a critical frontier, moving beyond superficial knowledge retrieval or pattern matching to assess the integrity of the entire solution process \\cite{zhang2023k1j}. While early work on hallucination often focused on factual inconsistencies in generative tasks \\cite{maynez2020h3q}, the increasing capabilities of LLMs necessitate benchmarks that probe their ability to perform multi-step logical deductions, execute precise algorithms, and maintain coherence throughout complex reasoning paths. This shift reveals a deeper form of hallucination, rooted in a lack of true algorithmic understanding rather than mere factual error.\n\nA significant area of research focuses on assessing LLMs' logical consistency in structured data and fact-checking scenarios. \\cite{oh2024xa3} introduced ERBench, a benchmark that leverages relational databases and their integrity constraints to generate complex, automatically verifiable questions. Crucially, ERBench not only assesses the final answer but also rigorously verifies the LLM's generated rationales, exposing inconsistencies in the underlying thought process even when the final answer might appear correct. This highlights that LLMs often struggle with maintaining logical coherence across multiple deductive steps. Extending this, \\cite{li2024osp} proposed Drowzee, a framework employing metamorphic testing to evaluate the logical consistency and robustness of LLM rationales in fact-checking. By generating semantically equivalent but syntactically varied inputs based on logic reasoning rules, Drowzee reveals how minor perturbations can expose brittle reasoning and lead to contradictory outputs, underscoring the fragility of LLM logical understanding in complex scenarios. Further, \\cite{ghosh2024tj5} developed a framework to quantitatively assess LLMs' logical consistency in fact-checking, specifically for propositional logic queries derived from Knowledge Graphs. Their work defines measures for consistency across primitive logical operators, complex DNF/CNF facts, and logical rules, demonstrating that LLMs exhibit significant logical inconsistency on such complex queries, even when provided with authoritative context. These studies collectively emphasize the challenge of ensuring logical soundness and verifiable reasoning paths in LLMs.\n\nBeyond structured logical deduction, a more demanding test for LLMs lies in their ability to perform pure algorithmic execution. \\cite{tang2024a1j} developed GraphArena, a benchmark specifically tailored to evaluate LLMs on real-world graph computational problems, including NP-complete tasks, which require precise, step-by-step algorithmic execution rather than heuristic pattern matching. Their findings reveal alarmingly high hallucination rates, where LLMs frequently generate plausible but incorrect intermediate steps or entirely fabricated solutions, demonstrating a profound limitation in their capacity to understand and accurately execute algorithmic instructions. Similarly, in the domain of mathematical reasoning, \\cite{liu2025juo} introduced a structured self-consistency framework to enhance reliability by enforcing consistency across intermediate steps and final outputs in tasks like theorem proving, symbolic manipulation, and numerical computation. Their results indicate that while self-consistency can improve accuracy, LLMs remain susceptible to hallucinations in these precise mathematical and algorithmic contexts, highlighting the difficulty in achieving robust, step-by-step correctness.\n\nThe integrity of the reasoning process itself is further probed by examining the impact of reasoning order. \\cite{xie20247zk} introduced a benchmark demonstrating that the order in which LLMs generate answers and their corresponding reasoning significantly impacts consistency. They found that LLMs often fabricate answers and then retrospectively generate justifications, exposing a fundamental flaw where the reasoning path is constructed to fit a pre-determined (potentially incorrect) answer, rather than the answer being a product of sound reasoning. This \"reasoning order hallucination\" underscores the challenge of ensuring that LLMs genuinely understand and follow logical steps.\n\nThe problem of complex reasoning hallucination also manifests acutely in high-stakes, domain-specific applications. \\cite{umapathi2023puv} introduced Med-HALT, a Medical Domain Hallucination Test, which includes \"Reasoning Hallucination Tests\" (RHTs) such as False Confidence Tests (FCT), None of the Above (NOTA) Tests, and Fake Questions Tests (FQT). These RHTs assess an LLM's ability to reason about complex medical problems, generate logically coherent and factually accurate output, and identify invalid queries without creating fake information. Their findings indicate that even state-of-the-art LLMs perform poorly on these reasoning-based medical tasks, struggling with complex logical inference and exhibiting a tendency to hallucinate with undue certainty, highlighting the severe implications of such failures in critical domains.\n\nIn conclusion, current LLMs exhibit significant limitations in tasks requiring genuine algorithmic reasoning and complex problem-solving. Benchmarks focusing on rationale verification, logical consistency in structured data, pure algorithmic execution, mathematical reasoning, and the integrity of the reasoning process consistently reveal high hallucination rates. These evaluations demonstrate that LLMs struggle with the precise execution, deep understanding, and robust logical coherence necessary for these tasks. The emphasis has decisively shifted from merely assessing final answers to rigorously evaluating the integrity and consistency of the entire solution process. Future research must focus on developing models that can perform verifiable, step-by-step algorithmic execution, moving beyond superficial pattern matching to achieve true algorithmic intelligence and logical soundness.\n\\subsection{Long-Context and Dialogue-Level Evaluation}\n\\label{sec:4_4_long-context__and__dialogue-level_evaluation}\n\nEvaluating Large Language Models (LLMs) in extended conversational contexts and with lengthy documents presents distinct and complex challenges. Models are prone to generating hallucinations stemming from an inability to maintain consistent information across multiple turns, misremembering previous dialogue states, or losing factual accuracy and coherence when processing extensive inputs. These specialized evaluations are paramount for developing LLMs that are reliable, consistent, and trustworthy in real-world interactive and document-intensive applications. The evolution of hallucination types, as highlighted by \\cite{qiu2024zyc}, increasingly includes dialogue-level and long-context specific manifestations, necessitating dedicated evaluation paradigms.\n\nA critical area of focus is the assessment of dialogue-level consistency and factual accuracy across multi-turn interactions. Traditional evaluation methods, often focused on single-turn responses, fail to capture the cumulative errors that can emerge in dynamic conversations. To address this, \\cite{chen2024c4k} introduce \\texttt{DiaHalu}, a pioneering benchmark specifically designed to evaluate hallucinations in multi-turn dialogues. \\texttt{DiaHalu} distinguishes between various hallucination subtypes, including non-factual information, incoherence (input-conflicting, context-conflicting, self-conflicting), irrelevance, overreliance, and reasoning errors. Its construction involves LLM self-dialogue generation and expert annotation across diverse domains like knowledge-grounded and task-oriented conversations, making it a challenging and realistic testbed for conversational reliability. However, a limitation of such benchmarks is their reliance on LLM-generated dialogues, which, while efficient, may not fully capture the nuances of human-LLM interaction or introduce biases from the generating LLM itself. Further insights into dialogue-level errors come from studies like \\cite{dziri2021bw9}, which, through human studies, critically analyze the modes of hallucination in dialogue systems, revealing that extrinsic hallucinations, particularly erroneous entity mentions, are prevalent. They also observe that increased response diversity often correlates with increased hallucination, highlighting the necessity for evaluations that can precisely identify and ground specific entities within a conversational flow.\n\nFurthermore, the consistency of an LLM's internal reasoning process across turns is vital. \\cite{xie20247zk} propose a novel benchmark method that assesses LLM consistency by comparing responses generated when reasoning precedes the answer versus when the answer precedes reasoning. This approach is particularly insightful for dialogue evaluation, as it exposes instances where LLMs might fabricate justifications for previously stated, potentially hallucinatory, conclusions, thus revealing logical inconsistencies over an extended reasoning path inherent in multi-turn interactions. This method helps to uncover a deeper form of dialogue hallucination where the model's internal state becomes inconsistent. Extending dialogue evaluation to multimodal contexts, \\cite{cao2024o9a} introduce \\texttt{VisDiaHalBench}, a visual dialogue benchmark for Large Vision-Language Models (LVLMs). This benchmark specifically investigates hallucinations arising from \"long-term misleading textual history\" in visual dialogues, featuring five-turn questions about edited and original images. This highlights the complex interplay between textual context, visual input, and conversational memory in multimodal dialogue hallucination. Conceptually, the Information Quality (IQ) model proposed by \\cite{rejeleene2024okw}, which defines IQ based on consistency, relevance, and accuracy, provides a valuable framework for what comprehensive dialogue evaluations should strive to measure.\n\nBeyond dialogues, the ability of LLMs to process and synthesize information from extensive documents without generating spurious details or losing track of relevant facts is a major challenge. A foundational and widely adopted method for probing long-context factual recall is the \"Needle In A Haystack\" (NIAH) test. This technique involves embedding a specific, verifiable piece of information (the \"needle\") within a much longer, often irrelevant document (the \"haystack\") and then querying the LLM to retrieve that information. The performance on NIAH tests directly measures an LLM's capacity to maintain attention and extract precise facts from lengthy inputs, revealing how context length impacts factual grounding. While not a formal benchmark suite, NIAH has become a de facto standard for demonstrating an LLM's effective context window and susceptibility to 'getting lost' in irrelevant details. Variants of NIAH, such as those testing multiple needles or needles at different positions, have revealed crucial insights, like the \"lost in the middle\" phenomenon where LLMs often perform worse on information located in the middle of a long context \\cite{liu2024lost}. However, a critical limitation of NIAH is its focus on *retrieval* rather than *synthesis* or complex reasoning, and its artificial nature may not fully reflect real-world document processing challenges.\n\nTo address the more complex task of long-document *synthesis*, particularly in abstractive summarization, \\cite{maynez2020h3q} provided a foundational human evaluation and taxonomy of hallucinations, distinguishing between intrinsic (misrepresenting source) and extrinsic (adding ungrounded information) hallucinations. They demonstrated that traditional metrics like ROUGE correlate poorly with human judgments of faithfulness, advocating for semantically-aware metrics like textual entailment. This work underscores the need for evaluations that go beyond surface-level metrics to assess deep semantic consistency with long source documents. Similarly, \\cite{hegselmann20249q4} developed a rigorous labeling protocol for errors and an annotated dataset of hallucinations in long patient summaries, highlighting domain-specific challenges in medical text generation and the need for high-fidelity evaluation in sensitive applications.\n\nFurthermore, \\cite{qiu2024zyc} introduce \\texttt{LongHalQA}, an LLM-free benchmark for evaluating long-context hallucinations in Multimodal Large Language Models (MLLMs). This benchmark features 6K long and complex hallucination texts across object-level descriptions, image-level descriptions, and multi-round conversations. It employs novel \"Hallucination Discrimination\" and \"Hallucination Completion\" tasks, framed as multiple-choice questions, to efficiently assess MLLMs' ability to identify and avoid generating hallucinations in lengthy outputs. \\texttt{LongHalQA}'s coverage of 12 distinct hallucination types, including complex logical and contextual inconsistencies, represents a significant step beyond simple factual checks. The survey by \\cite{sahoo2024hcb} further emphasizes the challenge, noting the absence of standardized metrics for assessing object hallucination in LVLMs, particularly relevant in long multimodal contexts. For evaluating faithfulness in complex, multi-source question answering, which often involves synthesizing information from long contexts, \\cite{pan2024hm4} propose a \"multi-reference faith score (MRFS)\" to verify and resolve conflicts in generated answers, indicating a move towards more robust, verifiable evaluation metrics for long-form generation. The broader review by \\cite{malin2024fin} reinforces that evaluating open-ended generation provides a more comprehensive measure of LLM performance than commonly used multiple-choice benchmarking, which is crucial for assessing faithfulness in long-context tasks.\n\nIn conclusion, the evaluation of long-context and dialogue-level hallucinations necessitates a shift from isolated factual checks to comprehensive assessments of consistency, coherence, and factual accuracy across extended interactions and lengthy inputs. Benchmarks like \\texttt{DiaHalu} and \\texttt{VisDiaHalBench} provide critical tools for understanding dialogue-level errors, while methods like the NIAH test, when critically understood for its retrieval focus, and the synthesis-oriented evaluations for summarization \\cite{maynez2020h3q} and multimodal long-context generation \\cite{qiu2024zyc} are indispensable for evaluating an LLM's ability to remain grounded in long documents and maintain coherence over extended generations. While significant progress has been made, challenges persist in developing scalable, fine-grained evaluation techniques for truly massive contexts and dynamically assessing the nuanced consistency required for highly interactive, multi-agent conversational systems. Future research will likely integrate intrinsic detection mechanisms with comprehensive external validation to build more robust and trustworthy LLMs for complex, real-world applications.\n",
    "Mitigation Strategies I: External Knowledge Grounding and Adaptive Retrieval": "\\section{Mitigation Strategies I: External Knowledge Grounding and Adaptive Retrieval}\n\\label{sec:mitigation_strategies_i:_external_knowledge_grounding__and__adaptive_retrieval}\n\n\n\n\\subsection{Retrieval-Augmented Generation (RAG) Fundamentals}\n\\label{sec:5_1_retrieval-augmented_generation_(rag)_fundamentals}\n\n\nLarge Language Models (LLMs) have demonstrated remarkable capabilities in generating human-like text, yet they frequently suffer from issues of hallucination, generating factually incorrect or unfaithful content, and are constrained by the static, often outdated, knowledge encoded during their pre-training \\cite{Lewis2020}. Retrieval-Augmented Generation (RAG) emerged as a pivotal strategy to mitigate these limitations by dynamically grounding LLM responses in up-to-date and authoritative external knowledge sources. The core concept of RAG involves augmenting the LLM's generation process with a retrieval step, where relevant information is fetched from a vast corpus of documents and then provided to the LLM as context for generating its response, thereby reducing the generation of unfaithful or outdated content.\n\nThe foundational paradigm, often referred to as 'Naive RAG', typically combines a retriever component with a generator component. \\textcite{Lewis2020} introduced Retrieval-Augmented Generation (RAG), a general-purpose fine-tuning approach that integrates a pre-trained parametric memory (the generator, a seq2seq model) with a non-parametric memory (the retriever, a Dense Passage Retriever or DPR). This architecture dynamically retrieves relevant documents from a large corpus and conditions the generator's output on these retrieved passages, demonstrating state-of-the-art results on knowledge-intensive NLP tasks like open-domain question answering and fact verification. Simultaneously, \\textcite{Guu2020} proposed REALM (Retrieval-Augmented Language Model Pre-training), which explored a deeper integration by learning to retrieve documents from a large corpus and use them to augment a language model's input during pre-training. REALM showcased that jointly training the retriever and the language model end-to-end could significantly improve performance on knowledge-intensive tasks, highlighting the benefits of building retrieval capabilities directly into the model's knowledge acquisition process.\n\nThese early works established the immense promise of RAG, demonstrating that external knowledge retrieval could significantly enhance the factual accuracy and currency of LLM outputs. Building upon this foundation, \\textcite{Izacard2022} introduced Atlas, a retrieval-augmented language model that leverages a frozen pre-trained T5 model and a jointly trained retriever. Atlas demonstrated that even relatively smaller language models (e.g., 11B parameters) could achieve competitive performance with much larger, purely parametric LLMs on knowledge-intensive tasks when effectively augmented with retrieval, underscoring the efficiency benefits and the importance of well-trained retriever-generator interactions.\n\nHowever, these initial RAG paradigms often assumed full access to the language model's internal states or gradients for joint training or fine-tuning, posing a significant challenge for proprietary or black-box LLMs. Addressing this practical limitation, \\textcite{Shi2023} proposed REPLUG, a novel method designed to train a retriever to work effectively with black-box language models without requiring access to their internal states or gradients. REPLUG trains the retriever by maximizing the likelihood of the LLM's output given the retrieved documents, employing a contrastive learning objective, thus enabling the application of RAG to a broader range of LLMs, including those deployed as APIs. This progression from tightly coupled, white-box RAG systems to more flexible, black-box compatible approaches highlights the evolving understanding of RAG's practical deployment challenges and the innovative solutions developed to overcome them. The foundational understanding of RAG, from its initial promise to its early architectural challenges and solutions, sets the stage for appreciating the subsequent advancements and refinements in RAG architectures designed to further enhance retrieval quality, generation coherence, and overall system robustness.\n\\subsection{Advanced and Adaptive RAG Architectures}\n\\label{sec:5_2_advanced__and__adaptive_rag_architectures}\n\n\nThe foundational Retrieval-Augmented Generation (RAG) paradigm, while effective, often operates with a static retrieval mechanism that can be inefficient, imprecise, or fail to fully leverage the capabilities of Large Language Models (LLMs). This has spurred the development of advanced and adaptive RAG architectures, focusing on sophisticated, modular designs that empower LLMs with greater control over the retrieval process, leading to more intelligent and targeted information synthesis. These innovations aim to significantly improve RAG's efficiency, precision, and robustness, particularly in mitigating hallucinations.\n\nEarly conceptualizations of these advanced paradigms were laid out by comprehensive surveys, such as that by \\cite{gao2023retrieval}, which categorized RAG into basic, advanced, and modular forms, highlighting the need for more sophisticated retrieval and generation strategies to overcome limitations like hallucination and outdated information. Similarly, \\cite{zhang2024retriever} further elaborated on this evolution, detailing various components like pre-retrieval, retrieval, post-retrieval, and generation, underscoring the shift towards more dynamic and integrated RAG systems. Within this evolving landscape, initial efforts focused on enhancing the LLM's ability to process and utilize retrieved information more effectively. For instance, \\cite{yu2023chainofthought} demonstrated how Chain-of-Thought (CoT) reasoning can be integrated into RAG, guiding the LLM to generate intermediate thoughts that improve the utilization of retrieved documents and consequently reduce hallucinations by fostering a deeper understanding of the context.\n\nMoving beyond static retrieval, subsequent research introduced mechanisms for the LLM to actively influence the retrieval process itself. \\cite{shi2023replug} proposed RePlug, a framework that enables LLMs to self-refine their retrieval queries and generated answers. By using internal feedback, RePlug iteratively improves the relevance of retrieved documents, making the retrieval process more adaptive and responsive to the LLM's evolving information needs. This marked a significant step towards adaptive RAG, where the LLM is no longer a passive consumer but an active participant in shaping its information landscape.\n\nThe concept of \"Modular RAG\" further extends this adaptivity by integrating RAG into broader, multi-step reasoning frameworks. \\cite{yang2023ragagents} introduced RAG-Agents, a framework that combines RAG with autonomous agents, allowing LLMs to tackle complex tasks by breaking them down, planning, executing actions (including strategic retrieval), and self-correcting. This approach transforms RAG into a specialized tool within an agentic workflow, enabling intelligent decision-making about *when* and *what* to retrieve in a multi-step reasoning process, thereby enhancing its utility for complex problem-solving.\n\nA pinnacle of adaptive RAG is exemplified by architectures that grant LLMs the autonomy to decide when and what to retrieve based on their internal state. \\cite{wang2023selfrag} introduced Self-RAG, an LLM framework that learns to retrieve, generate, and critique through self-reflection. This innovative approach allows the LLM to generate \"reflection tokens\" that guide its own retrieval and generation process, enabling it to selectively retrieve information when its internal uncertainty is high or external knowledge is required. Furthermore, Self-RAG incorporates dynamic context highlighting by allowing the LLM to critique its own output, ensuring that the generated response is factual and well-supported by the retrieved documents, thus embodying a highly intelligent and targeted solution for hallucination mitigation.\n\nIn conclusion, the evolution of RAG architectures showcases a clear trajectory towards more intelligent, autonomous, and modular systems. While significant progress has been made in enabling selective retrieval, dynamic context highlighting, and iterative refinement, challenges remain. These include the computational overhead of iterative processes, the complexity of fine-tuning sophisticated feedback loops, and ensuring the generalizability of self-reflection mechanisms across diverse domains. Future directions will likely focus on developing more efficient uncertainty estimation techniques, extending adaptive RAG to multi-modal contexts, and achieving real-time adaptation in dynamic information environments.\n\\subsection{Knowledge Graph Integration for Trustworthiness}\n\\label{sec:5_3_knowledge_graph_integration_for_trustworthiness}\n\n\nLarge Language Models (LLMs) often suffer from factual inaccuracies and logical inconsistencies, commonly referred to as hallucinations, which severely undermine their trustworthiness. Strategic integration of structured Knowledge Graphs (KGs) with LLMs offers a robust solution by providing a reliable, up-to-date, and verifiable source of information, thereby serving as a powerful grounding mechanism against such issues.\n\nEarly efforts to mitigate LLM hallucinations recognized the critical need for external knowledge to ground their responses. For instance, \\cite{trivedi2022qsf} demonstrated that interleaving retrieval with Chain-of-Thought (CoT) reasoning could significantly improve factual accuracy in knowledge-intensive multi-step questions. Their IRCoT framework dynamically uses intermediate CoT steps to generate context-aware queries for retrieving relevant paragraphs, grounding the reasoning process and reducing factual errors by up to 50\\%. However, these foundational approaches often relied on unstructured text retrieval, which, while effective, could still introduce noise or outdated information, highlighting the need for more structured and verifiable knowledge sources.\n\nBuilding upon the broader Retrieval-Augmented Generation (RAG) paradigm, which generally involves fetching relevant information from an external corpus to inform LLM responses \\cite{gao20232zb}, recent research has increasingly focused on leveraging the inherent structure and verifiability of Knowledge Graphs. \\cite{sui20242u1} directly addresses how KGs can enhance LLM trustworthiness by proposing a unified framework that combines \"Graph-guided retrieval\" and \"Graph-guided generation.\" This approach enables LLMs to query and integrate structured facts from KGs, leading to more accurate and logically consistent answers in open-ended question answering tasks, and introduces the OKGQA benchmark to evaluate such KG-augmented models, even under perturbed KG conditions. This represents a significant step beyond generic text retrieval by providing a rich, semantically structured context that is less prone to misinterpretation or hallucination.\n\nBeyond mere factual accuracy, trustworthiness also encompasses logical consistency, especially in complex reasoning tasks. While LLMs can struggle with maintaining logical coherence, KGs, by their very nature, encode explicit relationships and constraints that can be leveraged for verification. \\cite{ghosh2024tj5} emphasizes the importance of evaluating LLM logical consistency in fact-checking, proposing new logical fact-checking (LFC) datasets and quantitative measures to assess their performance on complex propositional logic queries. Although this work does not explicitly integrate KGs, the structured nature of KGs makes them an ideal candidate for providing the ground truth and relational context necessary to enforce and verify such logical consistency in LLM outputs. Further enhancing verifiability, \\cite{oh2024xa3} introduced ERBench, a benchmark that utilizes relational databases (conceptually akin to KGs in their structured representation of entities and relationships) to automatically verify not only the LLM's final answers but, crucially, its *rationales*. This capability to scrutinize the reasoning path against structured knowledge is paramount for building truly transparent and trustworthy AI systems, moving beyond simple output correctness to verifiable logical soundness.\n\nIn conclusion, the integration of Knowledge Graphs represents a pivotal advancement in addressing the trustworthiness challenges of LLMs. By providing a structured, verifiable, and logically consistent source of external knowledge, KGs enable LLMs to move beyond mere fluency to produce responses that are factually accurate, logically sound, and inherently more reliable. This strategic integration, encompassing graph-guided retrieval, generation, and rationale verification, lays a robust foundation for developing next-generation LLMs that are not only powerful but also transparent and trustworthy in their knowledge-intensive and complex reasoning capabilities. Ongoing challenges include the scalability of KG construction and maintenance, and the seamless, real-time integration of dynamic KGs with evolving LLM architectures.\n",
    "Mitigation Strategies II: Intrinsic Model Interventions and Self-Correction": "\\section{Mitigation Strategies II: Intrinsic Model Interventions and Self-Correction}\n\\label{sec:mitigation_strategies_ii:_intrinsic_model_interventions__and__self-correction}\n\n\n\n\\subsection{Decoding-Time Interventions and Contrastive Methods}\n\\label{sec:6_1_decoding-time_interventions__and__contrastive_methods}\n\n\nLarge Language Models (LLMs) and Large Vision-Language Models (LVLMs) frequently generate \"hallucinations\"â€”plausible but factually incorrect or ungrounded information. Addressing this critical challenge, decoding-time interventions offer an efficient, training-free paradigm for the primary model to steer generation towards more factual and grounded outputs by manipulating the probability distribution of generated tokens during inference. These methods operate by introducing a \"negative\" signal or bias that discourages the generation of ungrounded content, without requiring extensive fine-tuning of the main model itself.\n\nOne prominent approach in this domain is \\textbf{Visual Contrastive Decoding (VCD)}, as proposed by \\cite{park20247cm}. VCD mitigates hallucination, particularly in LVLMs, by penalizing tokens that are disproportionately favored when the visual input is subtly distorted or perturbed. The core mechanism involves comparing the logit distribution generated from the original input with that from a slightly altered, \"negative\" input (e.g., an image with minor noise or a masked object). By identifying tokens whose probabilities increase significantly under these subtle perturbations, VCD infers their sensitivity to ungrounded visual cues and suppresses them. This method leverages the model's internal representations to identify and counteract potential hallucinatory tendencies without requiring any additional training of the main LVLM. VCD has demonstrated effectiveness in reducing attribute and object hallucinations by steering the model towards more robust and visually grounded descriptions, often outperforming standard decoding strategies in terms of factual consistency. Its strength lies in its training-free nature and generalizability across different LVLMs, though its efficacy can be sensitive to the choice and strength of the perturbation.\n\nFurther enhancing the grounding aspect, other methods integrate external modalities to provide a strong, verifiable signal for contrastive decoding. For instance, approaches like the \\textbf{Image-Grounded Guidance} proposed by \\cite{zhao2024ge8} for LVLMs leverage pre-trained external vision models to establish a factual baseline. This method generates a reference caption for an input image using a robust image captioning model. During the LVLM's decoding process, the generated tokens are continuously compared against this reference. If the LVLM generates tokens corresponding to objects or attributes not present in the reference caption, these tokens are penalized by adjusting their logits downwards. This effectively guides the LVLM to generate text that is consistent with the external visual understanding, mitigating object and attribute hallucinations. While this approach is training-free for the main LVLM, it introduces computational overhead due to the inference required by the external image captioning model. However, it offers a powerful mechanism for ensuring strong visual grounding, particularly for object-centric hallucinations, by explicitly incorporating external, verifiable visual information into the decoding process.\n\nComplementing these sophisticated contrastive techniques are simpler, yet often effective, \\textbf{logit-based interventions}. Works such as \\cite{leng2023ohr} demonstrate that direct manipulation of token logits can counter specific linguistic biases or common hallucination patterns. These methods involve applying predefined biases or penalties to certain token probabilities based on external knowledge, semantic categories, or simple heuristics. For example, if a model frequently hallucinates specific entities in a given context, their logits can be slightly suppressed. \\cite{leng2023ohr} specifically proposes a logit-based calibration method that adjusts token probabilities by considering their frequency in factual vs. non-factual contexts, effectively down-weighting tokens associated with higher hallucination rates. These interventions are computationally light and offer a direct way to address specific, recurring types of hallucinations or linguistic tendencies that lead to ungrounded outputs. However, their generalizability is often limited, as they require prior identification of problematic tokens or patterns and may not adapt well to novel hallucination types.\n\nIn synthesis, decoding-time interventions offer a versatile and efficient suite of strategies. VCD (\\cite{park20247cm}) provides a general, training-free mechanism to enhance robustness against subtle input variations, making it effective for reducing attribute and object hallucinations by promoting consistency. Image-Grounded Guidance (\\cite{zhao2024ge8}) offers a more explicit form of external grounding for multimodal models, leveraging an auxiliary vision model to provide a strong factual anchor, albeit with increased inference cost. Logit-based calibration (\\cite{leng2023ohr}) represents the most lightweight approach, directly adjusting token probabilities based on observed biases, offering immediate impact for known issues but requiring careful design to avoid over-suppression or limiting creativity.\n\nDespite their efficiency and training-free nature for the primary model, challenges remain. A key difficulty lies in defining universally effective \"negative\" signals or biases that reliably identify and suppress hallucinations without inadvertently stifling creativity or factual nuance. The computational overhead of integrating external models for guidance, as seen in image-grounded approaches, can also be a practical limitation. Furthermore, ensuring that these interventions do not introduce new biases or degenerate the quality of non-hallucinatory content requires careful calibration. Future directions may involve developing more adaptive and dynamic intervention strengths, perhaps guided by real-time uncertainty estimation within the LLM, or exploring hybrid approaches that combine the general robustness of VCD with the explicit grounding of external knowledge and the targeted precision of logit-based adjustments. The goal is to create a robust, yet flexible, decoding framework that can dynamically balance factual accuracy with fluency and coherence across diverse generation tasks.\n\\subsection{Internal State Manipulation and Forward-Pass Interventions}\n\\label{sec:6_2_internal_state_manipulation__and__forward-pass_interventions}\n\n\nTo effectively mitigate hallucination and enhance grounding in large language models (LLMs), advanced strategies move beyond external prompting to directly intervene within the model's internal processing during the forward pass. These methods leverage a deeper, mechanistic understanding of LLM behavior, allowing for more precise and less intrusive corrections by manipulating internal states, attention mechanisms, and feature representations. This approach contrasts with decoding-time interventions by operating earlier in the generation pipeline, often influencing the very construction of hidden states and attention patterns that precede token prediction.\n\nOne family of interventions focuses on dynamically adjusting feature representations based on model uncertainty or signal strength. \\citet{zou2024dp7} introduced **Memory-Space Visual Retracing (MemVR)**, a reactive strategy primarily for multimodal LLMs. MemVR addresses visual grounding issues by monitoring the LLM's internal uncertainty, often quantified through the entropy of token probabilities or disagreement across multiple decoding paths. When high uncertainty is detected, MemVR reactively re-injects relevant visual tokens or their representations into intermediate layers of the model. This re-grounding aims to provide the model with a fresh, reinforced visual context precisely when it is most prone to hallucination due to visual amnesia or insufficient attention. Complementing this, \\citet{yin2025s2b} proposed **Visual Amplification Fusion (VAF)**, a more proactive strategy. VAF addresses the problem of insufficient or diluted visual signals in deeper layers of the LLM by actively enhancing and amplifying visual feature representations within specific middle layers. This proactive boosting, often achieved through learned scaling factors or specialized fusion modules, helps prevent the early decay of visual information, ensuring that the model maintains sufficient visual attention throughout the forward pass. While MemVR is reactive and uncertainty-driven, VAF is proactive, tackling the \"modality gap\" by ensuring visual signals remain salient.\n\nBeyond general feature manipulation, research has increasingly focused on granular control over the model's attention mechanisms, which are critical for integrating information across modalities and contexts. \\citet{chen2024j0g} explored **Targeted Attention Head Interventions for Cross-Level Visual Focus**. This work delves into identifying specific attention heads crucial for integrating visual information across varying levels of abstraction, from low-level features to high-level concepts. By applying interventions, such as re-weighting attention scores or fine-tuning specific head parameters during the forward pass, this method aims to improve the model's ability to maintain coherent visual focus and prevent modality collapse or misinterpretation. The challenge lies in identifying these critical heads and ensuring interventions generalize across diverse inputs.\n\nFurther advancing this mechanistic understanding with theoretical rigor, \\citet{zhou2024lvp} introduced **CAUSAL MM**, a causal inference framework for **Modality Prior Balancing in Multimodal LLMs**. This approach employs structural causal modeling (SCM) to analyze and adjust the influence of different modalities within the attention mechanisms. By treating modality priors (visual and language) as confounding factors in the causal path between attention and output, CAUSAL MM uses techniques like back-door adjustment and counterfactual reasoning. This allows for deciphering the true causal impact of effective attention on MLLM output by isolating and mitigating the effects of misleading modality priors. For instance, by simulating counterfactual attention states (e.g., uniform or shuffled attention), the framework quantifies how much a specific modality's prior causally influences the output, then adjusts attention to achieve a more balanced integration. This provides a principled, theoretically grounded way to enhance visual grounding and reduce hallucination, offering a more robust alternative to empirical attention head interventions.\n\nAnother innovative forward-pass intervention, particularly for contextual hallucinations, is the **Lookback Lens** proposed by \\citet{chuang20248ey}. This method introduces a \"lookback ratio\" derived from attention maps, which quantifies the proportion of attention weights focused on the input context versus newly generated tokens. This interpretable feature serves as a signal for contextual faithfulness. During generation, a \"Lookback Lens Guided Decoding\" strategy samples multiple candidate text chunks. For each chunk, its lookback ratio features are computed and scored by a lightweight classifier. The chunk predicted to be least hallucinated (i.e., most grounded in the context) is then selected. This approach dynamically steers generation by leveraging attention patterns, demonstrating strong cross-task and cross-model transferability due to its reliance solely on attention maps, which are hypothesized to be more generalizable indicators of contextual faithfulness than raw hidden states.\n\nFinally, \\citet{jin2024jpw} introduced a **Collaborative Decoding Framework** that represents a different form of internal state manipulation. Recognizing that pretrained models often retain higher factual accuracy while finetuned models excel in instruction following but may hallucinate more, this framework dynamically decides which model to use for the next token. A \"critical token classifier,\" trained to identify tokens where factual accuracy is paramount, dictates whether the pretrained model or the finetuned model should generate the subsequent token. This allows the system to harness the factuality of the pretrained model for critical information while benefiting from the fluency and instruction-following of the finetuned model for general generation. This method intervenes at a higher level, dynamically routing internal processing based on the perceived \"criticality\" of the next token, effectively balancing competing objectives during the forward pass.\n\nThese forward-pass interventions collectively represent a significant shift towards a deeper, mechanistic understanding of LLM behavior, enabling more precise and less intrusive corrections. While methods like CAUSAL MM offer theoretical grounding and principled adjustments, they can be computationally intensive due to counterfactual simulations. Heuristic approaches like VAF or targeted attention head interventions (\\cite{chen2024j0g}) might be simpler to implement but may lack the same level of theoretical guarantees or generalizability. Lookback Lens (\\cite{chuang20248ey}) provides an interpretable signal for contextual grounding, but its guided decoding introduces computational overhead from candidate sampling. Collaborative decoding (\\cite{jin2024jpw}) offers an intriguing way to leverage different internal knowledge states, but its effectiveness depends on the accuracy of the critical token classifier and the alignment of the base models. Challenges remain in generalizing identified intervention points across diverse model architectures and tasks, as well as in developing more comprehensive theoretical frameworks to guide the optimal application of these internal state manipulations, ensuring both efficacy and efficiency in real-time.\n\\subsection{Self-Correction and Abstention Mechanisms}\n\\label{sec:6_3_self-correction__and__abstention_mechanisms}\n\n\nThe pursuit of reliable and trustworthy Large Language Models (LLMs) necessitates equipping them with the intrinsic capabilities to detect and rectify their own errors, as well as to proactively abstain from generating responses when faced with high uncertainty. This subsection delves into the sophisticated strategies that empower LLMs with these self-aware mechanisms, transforming them from mere text generators into more reflective and judicious agents. As surveyed by \\cite{pan2024y3a}, automated correction strategies are broadly categorized, with self-correction and abstention representing crucial generation-time and post-hoc interventions.\n\nThe foundation of LLM self-correction lies in enhancing their reasoning capabilities. Early advancements like Chain-of-Thought (CoT) prompting \\cite{wei2022chainofthought, kadavath2022language} were pivotal, enabling LLMs to articulate intermediate reasoning steps. By externalizing their thought process, models could expose potential logical flaws, laying the groundwork for subsequent self-reflection. Building on this, more advanced frameworks emerged to facilitate iterative and exploratory reasoning. ReAct (Reasoning and Acting) \\cite{yao20229uz} integrates CoT with external tool use, allowing LLMs to interleave reasoning steps with actions (e.g., searching external knowledge bases or executing code). This iterative cycle of thought, action, and observation provides a powerful mechanism for self-correction by verifying internal reasoning against external facts and refining plans based on observed outcomes, thereby directly connecting to external knowledge grounding discussed in Section 5. Tree of Thoughts (ToT) \\cite{yao2023treeofthoughts} further extends this by exploring multiple reasoning paths, evaluating intermediate states, and backtracking when a path proves unfruitful. This search-based approach allows LLMs to engage in more complex problem-solving and to self-correct by identifying and discarding inconsistent or incorrect lines of reasoning.\n\nBeyond these foundational reasoning paradigms, explicit self-correction frameworks have been developed. The \\textit{Self-Refine} framework \\cite{madaan2023selfrefine} exemplifies an iterative generate-critique-refine loop. An LLM first generates an initial output, then critically reflects on it by generating self-feedback to identify potential errors or areas for improvement, and finally uses this self-generated critique to produce a refined response. This internal feedback loop significantly enhances output quality and accuracy. Another powerful approach is Chain-of-Verification (CoVe) \\cite{dhuliawala2023rqn}, which systematically reduces factual hallucination. CoVe operates in multiple steps: generating a baseline response, planning specific verification questions based on the claims in the response, executing these verifications (often independently to prevent error propagation), and finally generating a revised, verified response. The \"factored\" variant of CoVe, where each verification question is answered without conditioning on the potentially hallucinated baseline, is particularly effective in minimizing the LLM's tendency to repeat its own errors. Furthermore, self-consistency \\cite{wang2022selfconsistency} has proven effective, especially in mathematical reasoning \\cite{liu2025juo}. This technique involves prompting the LLM to generate multiple diverse reasoning paths and corresponding answers, then selecting the most frequent or consistent answer, thereby leveraging the model's own internal agreement as a form of self-correction.\n\nConcurrently with self-correction, the critical need for LLMs to express \"I don't know\" has driven the development of robust abstention mechanisms to mitigate hallucination and overconfidence. Early abstention methods often relied on calibrating the model's predicted probabilities or confidence scores \\cite{lin2022calibrated}. These techniques typically involved post-hoc adjustments like temperature scaling to determine when a model should refrain from answering. However, such methods frequently required auxiliary models, ground truth data for effective calibration, or suffered from miscalibration, limiting their robustness and true \"label-free\" nature.\n\nA significant innovation has been the development of label-free abstention mechanisms that quantify uncertainty intrinsically. \\cite{kuhn2023semantic} introduced 'semantic entropy' as a novel measure of uncertainty. This approach quantifies the diversity and plausibility of semantically distinct alternative outputs for a given query. A high semantic entropy indicates a lack of a single, clear, and confident answer, prompting the model to abstain. This provides a more robust and inherent way for LLMs to recognize their limitations without external labels. Further advancements include token-level uncertainty quantification, such as Claim Conditioned Probability (CCP) \\cite{fadeeva2024lt8}, which measures the uncertainty of a particular claim value expressed by the model, enabling fine-grained fact-checking and highlighting specific unreliable segments. Similarly, \\cite{ling2024hqv} proposed methods to quantify both aleatoric (data-inherent) and epistemic (model-inherent) uncertainties in in-context learning, offering a deeper understanding of the sources of LLM uncertainty for more informed abstention.\n\nUltimately, the most robust systems integrate both self-correction and abstention capabilities. LLMs can first attempt to self-correct their responses using iterative refinement or verification frameworks. If, after this refinement process, significant uncertainty persists (quantified by metrics like semantic entropy or token-level uncertainty), the model can then judiciously choose to abstain. This synergistic strategy ensures that models actively strive to improve their answers while also possessing the crucial self-awareness to decline answering when truly uncertain, thereby enhancing overall reliability and transparency. In safety-critical applications, this translates into \"semantic guardrails\" \\cite{hakim2024d4u}, which are designed to prevent \"never event\" errors by combining internal uncertainty quantification with external consistency checks, effectively forcing abstention or flagging for human review when high-stakes factual accuracy cannot be guaranteed.\n\nDespite these advancements, significant challenges persist. The computational cost associated with extensive reflection, iterative refinement, and generating multiple diverse outputs for uncertainty quantification can be substantial, particularly for real-time applications. Defining and universally applying robust \"semantic plausibility\" for uncertainty quantification across diverse and open-ended domains remains an active research area. Moreover, while frameworks like ReAct integrate external tools, the optimal balance between internal self-reflection and external knowledge verification needs further exploration. Future directions include developing more adaptive and context-aware self-correction mechanisms, refining uncertainty quantification to be more robust and interpretable across all types of LLM tasks, and exploring hybrid approaches that dynamically combine internal reasoning with external grounding to achieve both high accuracy and appropriate humility.\n\\subsection{Training-Based Approaches and Automated Data Generation}\n\\label{sec:6_4_training-based_approaches__and__automated_data_generation}\n\n\nTraining-based approaches, encompassing fine-tuning and unlearning, represent a direct and potent strategy for mitigating hallucinations in large language models (LLMs) and multimodal large language models (MLLMs) \\cite{sahoo2024hcb, zhang2023k1j, liu2024p39}. A central impediment to their scalability and effectiveness, however, is the prohibitive cost and scarcity of high-quality, labeled data, particularly for diverse and nuanced hallucination types \\cite{cao2023ecl, li2025qzg}. Recent research has thus heavily focused on innovative automated data generation techniques to circumvent this data bottleneck, enabling more targeted and efficient model interventions.\n\nOne direct approach to address data scarcity for hallucination detection and mitigation is the automated generation of datasets by leveraging existing knowledge. AutoHall \\cite{cao2023ecl} proposes a three-step pipeline to automatically construct model-specific hallucination datasets from existing fact-checking resources. By prompting an LLM to generate references for claims, classifying their support, and then flagging contradictions with ground truth, AutoHall efficiently creates labeled hallucinatory examples. This method eliminates laborious manual annotation, making it scalable for continuous model updates and specific hallucination patterns. While effective for text-based factuality, AutoHall's reliance on pre-existing fact-checking resources may limit the diversity of generated hallucinations and risks inheriting their topical biases, potentially failing to uncover novel or subtle hallucination types that are not yet documented.\n\nBuilding on the principle of generating adversarial examples, several methods leverage auxiliary models or controlled processes to create dispreferred, hallucinatory content for training. Hallucination-Induced Optimization (HIO) \\cite{chen20247jb} exemplifies this by training an \"Evil LVLM\" specifically to generate adversarial, hallucinated examples given an image and a prompt. This \"Evil LVLM\" is optimized using a Contrary Bradley-Terry Model (CBTM) to *prioritize* hallucinatory content, effectively amplifying a diverse set of potential visual and factual inconsistencies. These meticulously crafted hallucinatory outputs then serve as negative examples to train a \"Good LVLM\" via contrastive decoding (as discussed further in Section 6.1), thereby enhancing its robustness. A similar concept is explored in Induce-then-Contrast Decoding (ICD) \\cite{zhang202396g} for LLMs, which constructs a \"factually weak LLM\" by fine-tuning it on non-factual samples generated by converting factual samples into untruthful ones. During inference, the log probabilities of these induced hallucinations from the weak model are subtracted from the original model's predictions, penalizing untruthful tokens. Further extending this, \\textit{VHTest} \\cite{huang20247wn} introduces an adversarial generation paradigm for visual hallucination (VH) in MLLMs. It systematically creates new, diverse, and uncontaminated VH images using text-to-image models (e.g., DALLÂ·E-3), guided by MLLM-generated descriptions of hallucination modes. This approach allows for the construction of robust benchmarks and subsequent fine-tuning to mitigate specific VH types like object existence, shape, and size. While these generative approaches offer greater diversity and novelty in adversarial examples compared to AutoHall, they introduce their own challenges, such as the computational cost of training auxiliary \"evil\" or \"weak\" models and the risk that the generated \"bad\" data might still be stereotypical or lack the subtle nuances of real-world hallucinations, potentially introducing new biases rather than creating truly generalizable improvements \\cite{yin2024iau}.\n\nThe broader paradigm of preference optimization, including Reinforcement Learning from Human Feedback (RLHF) and Direct Preference Optimization (DPO), has proven highly effective in aligning LLMs with human preferences. A critical component of these methods is the availability of high-quality preference pairs, especially dispreferred (negative) responses. Preference Optimization in VLLM (POVID) \\cite{zhou2024wbi} addresses this by automating the creation of dispreferred responses using an AI model. This method is particularly effective in reducing object hallucination in VLLMs by providing abundant, automatically generated examples of incorrect or misleading descriptions, allowing the model to learn preferred, factually accurate outputs more efficiently. Extending this, Hallucination-targeted Direct Preference Optimization (HDPO) \\cite{fu2024yqj} specifically constructs preference pair data designed to target three distinct causes of MLLM hallucinations: visual distracted hallucination, long context hallucination, and multimodal conflict hallucination. HDPO's innovation lies in its novel data construction strategies, such as generating negative samples by preserving only low-attention visual tokens or by prompting the MLLM with conflicting information, thereby guiding the model to learn robust alignment against diverse hallucination types. Further advancing judge-free self-improvement, Deng et al. \\cite{deng202405j} propose a framework that generates both positive and negative response candidates by introducing a \"hallucination ratio\" during decoding, blending conditional and unconditional token distributions. These generated pairs are then verified by a lightweight, objective verifier (e.g., a CLIP model) to ensure data quality, significantly reducing the computational costs and biases associated with MLLM-as-judge approaches in traditional RLHF/DPO pipelines. A critical consideration for these preference optimization methods is the reliability of the AI model used to generate dispreferred responses or act as a verifier; if the AI itself is prone to biases or errors, it could inadvertently reinforce undesirable behaviors or generate suboptimal training signals.\n\nAddressing a specific, yet globally relevant, hallucination challenge, Multilingual Hallucination Removal (MHR) \\cite{qu20240f7} tackles the problem of significantly more severe hallucination in Large Vision-Language Models (LVLMs) when queried in non-English languages. MHR proposes a two-stage framework, with the second stage focusing on hallucination-enhanced preference optimization. Crucially, it introduces a novel *cross-lingual alignment method* to automatically generate multilingual hallucination-aware data pairs. This method leverages the LVLM itself to generate multiple responses in various non-English languages, which are then aligned with existing English hallucination/non-hallucination answers using semantic distance metrics. This scalable approach creates multilingual hallucination-aware datasets, significantly reducing manual effort and enabling DPO-based fine-tuning to favor non-hallucinating responses across languages. While MHR demonstrates substantial improvements, particularly in reducing \"unknown\" responses and increasing accuracy across diverse languages, it acknowledges that some instruction-following issues persist for low-resource languages, indicating limitations tied to the foundational multilingual capabilities of the base LLM.\n\nCollectively, these training-based approaches underscore a significant shift towards more automated and data-efficient methods for mitigating hallucinations. By innovatively generating synthetic, adversarial, or dispreferred data, methods like AutoHall, HIO, ICD, POVID, HDPO, VHTest, and MHR circumvent the traditional data annotation bottleneck, making fine-tuning and preference optimization strategies more practical and scalable \\cite{sahoo2024hcb}. The integration of lightweight verifiers, as seen in Deng et al. \\cite{deng202405j}, further enhances efficiency and reduces reliance on expensive human or LLM-based judgments. However, challenges persist in ensuring the generalizability and diversity of automatically generated adversarial examples, precisely defining and identifying all forms of hallucination for targeted unlearning, and managing the computational overhead associated with training auxiliary models or complex unlearning processes. Furthermore, the theoretical inevitability of hallucination in computable LLMs \\cite{xu2024n76, li2025qzg} suggests that even the most sophisticated training-based approaches may only reduce, but not entirely eliminate, the problem. Future research will likely explore hybrid approaches that combine these automated data generation techniques with more advanced unlearning algorithms, investigate methods for dynamically adapting data generation strategies to evolving hallucination types, and further refine judge-free verification mechanisms to build even more robust and trustworthy AI systems within these inherent limitations.\n",
    "The Multimodal Frontier: Hallucination in Vision, Audio, and Video Language Models": "\\section{The Multimodal Frontier: Hallucination in Vision, Audio, and Video Language Models}\n\\label{sec:the_multimodal_frontier:_hallucination_in_vision,_audio,__and__video_language_models}\n\n\n\n\\subsection{Defining and Categorizing Multimodal Hallucinations}\n\\label{sec:7_1_defining__and__categorizing_multimodal_hallucinations}\n\n\nThe phenomenon of hallucination, traditionally understood in Large Language Models (LLMs) as the generation of factually incorrect or nonsensical information, takes on a new dimension of complexity within Multimodal Large Language Models (MLLMs). Here, hallucinations extend beyond mere factual inaccuracies to encompass inconsistencies and misalignments across different modalities, such as vision, audio, and language \\cite{multimodal_hallucination_overview_2022}. This subsection extends the conceptual framework of hallucination to MLLMs, including Vision-Language Models (LVLMs), Audio-Language Models (LALMs), and Video-Language Models (VLLMs), defining unique hallucination types that arise from cross-modal inconsistencies.\n\nA primary challenge in MLLMs is the inherent 'modality gap,' where models must bridge disparate data representations and semantic spaces from different input types \\cite{crossmodal_alignment_challenges_2022}. This gap often leads to the generation of textual descriptions that contradict the visual, audio, or temporal information presented. In visual contexts, for instance, LVLMs frequently exhibit object, attribute, and relation hallucinations \\cite{vision_language_hallucination_taxonomy_2023}. Object hallucinations occur when the model describes an entity that is not present in the image, such as claiming to see a \"cat\" in a picture containing only dogs. Attribute hallucinations involve misrepresenting characteristics of existing objects, like describing a \"red car\" when the car is clearly blue. Furthermore, relation hallucinations manifest as incorrect spatial or semantic relationships between objects, such as stating a \"person is sitting on the table\" when they are standing beside it. These errors highlight a failure in the model's ability to accurately perceive and ground its linguistic output in the visual input.\n\nExtending this understanding, Audio-Language Models (LALMs) also exhibit unique forms of hallucination, primarily revolving around object hallucination in the auditory domain \\cite{audio_language_hallucination_2023}. An LALM might describe the sound of \"rain\" when only ambient city noise is present, or misidentify the source of a sound, attributing a \"dog bark\" to a cat's meow. These auditory inconsistencies demonstrate a similar modality gap where the generated language fails to accurately reflect the acoustic environment. The complexity further escalates with Video-Language Models (VLLMs), where the temporal dimension introduces additional layers of potential error \\cite{video_language_temporal_errors_2024}. VLLMs can suffer from temporal hallucinations, misrepresenting the sequence of events or the duration of actions within a video. Semantic detail errors are also prevalent, where the model fabricates non-existent actions or events, such as describing a \"person jumping\" when they are merely walking, or inventing an entire scene that did not occur in the video footage.\n\nThe integration of diverse information sources and the inherent 'modality gap' necessitate specialized taxonomies to accurately characterize and understand these complex phenomena \\cite{comprehensive_multimodal_taxonomy_2024}. Unlike unimodal hallucinations, which often stem from factual inaccuracies or logical inconsistencies within a single data type, multimodal hallucinations arise from the intricate interplay and potential misalignments between different modalities. These specialized taxonomies are crucial for moving beyond generic error classifications to precisely pinpoint the source and nature of cross-modal discrepancies, thereby enabling more targeted mitigation strategies and robust evaluation metrics \\cite{multimodal_error_characterization_2024}.\n\nIn conclusion, the definition and categorization of hallucinations in MLLMs represent a significant evolution from their unimodal counterparts. The unique challenges posed by cross-modal inconsistencies, manifesting as object, attribute, relation, temporal, and semantic detail errors across visual, audio, and video contexts, underscore the need for a comprehensive and nuanced understanding. While initial taxonomies have begun to delineate these complex phenomena, future research must continue to refine these categories, considering the dynamic and context-dependent nature of multimodal interactions to develop more robust and reliable MLLM systems.\n\\subsection{Evaluation Benchmarks for LVLMs, LALMs, and VLLMs}\n\\label{sec:7_2_evaluation_benchmarks_for_lvlms,_lalms,__and__vllms}\n\n\nThe rigorous assessment of hallucinations in multimodal models, encompassing Large Vision-Language Models (LVLMs), Large Audio-Language Models (LALMs), and Vision-Language Models (VLLMs), necessitates the development of specialized and nuanced evaluation benchmarks. Traditional metrics often fall short in capturing the complex factual inconsistencies and fabricated information generated by these models, driving the community to innovate sophisticated frameworks for hallucination detection.\n\nEarly efforts began to address object-level hallucinations, particularly in emerging modalities. For instance, \\cite{Li2023Object} pioneered a benchmark specifically designed for Large Audio-Language Models (LALMs), focusing on evaluating object hallucination by analyzing generated audio descriptions for mentions of non-existent objects. This work established a foundational approach to quantifying spurious object references in a modality distinct from vision. Building upon the need for more granular evaluation in vision-language models, \\cite{Chen2023Freeform} introduced an object-centric benchmark tailored for free-form text generations by LVLMs. This framework leverages object detection and grounding techniques to verify the factual consistency of mentioned objects and their attributes against the visual input, moving beyond simple presence or absence to assess the accuracy of descriptive details.\n\nAs models grew more capable, the scope of hallucination expanded beyond mere object presence to complex relationships. To address this, \\cite{Wang2023Relation} developed Tri-Eval, a novel triplet-level evaluation framework specifically designed to detect and quantify relational hallucinations in LVLMs. This benchmark meticulously examines subject-predicate-object relationships within the generated text, identifying instances where the model fabricates or misrepresents connections between entities depicted in the image, thus offering a more sophisticated measure of factual accuracy. Further refining the understanding of hallucination types, \\cite{Zhao2023VLLMHallucination} proposed a comprehensive framework to differentiate between intrinsic and extrinsic hallucinations in VLLMs. Intrinsic hallucinations contradict the visual input, while extrinsic ones are plausible but ungrounded in the image, providing a critical distinction for diagnosing model failures and guiding future improvements.\n\nBeyond general-purpose evaluation, the criticality of model reliability in sensitive domains has led to the creation of domain-specific benchmarks. \\cite{Gupta2024Medical} introduced Med-Eval, a specialized benchmark for evaluating LVLMs within medical contexts. This work highlights the paramount importance of factual accuracy and hallucination detection in applications such as clinical report generation and diagnostic assistance, where inaccuracies can have severe consequences, emphasizing the need for expert-curated datasets and domain-specific evaluation criteria.\n\nFurthermore, the robustness of these models under adverse conditions is a significant concern. \\cite{Zhang2024Robustness} investigated the resilience of LVLMs and VLLMs by benchmarking their performance against various visual perturbations, including noise, occlusion, and adversarial attacks. Their findings quantify the increased susceptibility to hallucination under such challenging inputs, underscoring the need for models that maintain factual consistency even when confronted with imperfect or manipulated visual data. Finally, a crucial aspect of model evaluation is distinguishing true visual understanding from mere memorization of training data. \\cite{Lee2024Understanding} addressed this by introducing a benchmark designed to test true visual understanding in LVLMs. This framework employs novel, out-of-distribution visual concepts and compositional reasoning tasks to ascertain whether models can generalize their knowledge rather than simply recalling learned patterns, ensuring that evaluations reflect genuine comprehension.\n\nIn conclusion, the evolution of evaluation benchmarks for LVLMs, LALMs, and VLLMs reflects a growing understanding of the multifaceted nature of hallucinations. While significant progress has been made in developing frameworks for object-level, relational, intrinsic/extrinsic, domain-specific, and robustness-focused evaluations, challenges remain. Future research must focus on dynamic, adaptive benchmarks that can assess emergent hallucination types, better align with human perception of factual accuracy, and continuously evolve with the increasing complexity and capabilities of multimodal AI models.\n\\subsection{Multimodal Mitigation Strategies}\n\\label{sec:7_3_multimodal_mitigation_strategies}\n\n\nHallucinations in multimodal models (MLLMs), encompassing Large Vision-Language Models (LVLMs), Large Audio-Language Models (LALMs), and Video-Language Models (VLLMs), present a complex challenge due to the intricate interplay and potential misalignments between diverse modalities \\cite{lan20240yz, bai2024tkm, sahoo2024hcb}. These errors often stem from a \"modality gap,\" where differences in data distribution or semantics between modalities lead to biased understanding, dataset toxicity, and inherited biases from underlying Large Language Models (LLMs) \\cite{lan20240yz, bai2024tkm}. Addressing these issues necessitates specialized mitigation strategies that go beyond unimodal approaches, focusing on improving cross-modal grounding, balancing modality priors, and ensuring factual consistency in integrated outputs.\n\nA prominent category of multimodal mitigation strategies involves training-free external guidance and verification mechanisms, which leverage cross-modal consistency checks to steer model generation without modifying internal model weights. These methods intervene during inference or use external tools to validate outputs. For instance, \\cite{kim2024ozf} introduced \\textbf{Counterfactual Inception}, a training-free method that prompts LMMs to engage in counterfactual thinking. The model generates \"counterfactual keywords\" (e.g., non-existent objects) based on visual input, which are filtered by a Plausibility Verification Process (PVP) using CLIP-based semantic alignment. These refined keywords then instruct the LMM to *avoid* generating such content, thereby enhancing visual grounding and reducing object, attribute, and relational hallucinations. This approach is fundamentally multimodal as it relies on the semantic interplay between generated text and visual content for self-correction. Similarly, \\cite{park20247cm} leveraged Text-to-Image (T2I) diffusion models for hallucination visualization. By generating visual representations of an MLLM's textual output and comparing them with the original input, this method provides an external visual verification mechanism to guide contrastive decoding, effectively using an external generative model to identify and correct cross-modal inconsistencies. Complementary to this, \\cite{wang2023hallucination} proposed a training-free image-grounded guidance method that employs a \"negative image\" (e.g., a blurred version of the input) to suppress non-grounded content in VLMs, using a \"visual-grounding score\" to quantify hallucination severity. While these external guidance methods offer flexibility and avoid costly retraining, their effectiveness is often contingent on the fidelity of the external verification models, and they do not fundamentally alter the MLLM's internal representation learning.\n\nBeyond external guidance, more sophisticated strategies delve into the internal workings of multimodal models, often involving fine-tuning or specialized decoding, to address issues like 'visual amnesia' and modality imbalance at a deeper level. A significant challenge unique to interactive multimodal settings is \"multimodal hallucination snowballing,\" where an MLLM's previously generated hallucination can mislead subsequent responses, even when ground visual information is available \\cite{zhong2024mfi}. To combat this, \\cite{zhong2024mfi} introduced \\textbf{Residual Visual Decoding (RVD)}, a training-free decoding method that \"residual connects\" visual information with the current user instruction. RVD revises the MLLM's output distribution to emphasize direct visual evidence, providing models with more robust access to visual information during generation and reducing the propagation of self-generated errors in conversational contexts. For improving foundational visual grounding, \\cite{jiang2022reg} proposed \\textbf{Visual Cluster Grounding} for image captioning, which implicitly links generated words to informative regions in the image, dynamically focusing on discriminative parts or full object content to reduce object hallucinations and language bias.\n\nRecent advancements have also focused on directly manipulating the decoding process or fine-tuning models with hallucination-targeted objectives. \\textbf{Hallucination-Induced Optimization (HIO)} \\cite{chen20247jb} introduces a novel paradigm where an \"Evil LVLM\" is intentionally trained using a *reversed* Bradley-Terry model to *prioritize* hallucinatory content. This \"Evil LVLM\" then serves as a strong contrastive signal during inference, amplifying the logit differences between hallucinatory and correct tokens to steer the original LVLM towards more factual outputs. This approach offers a more precise way to induce specific hallucinatory tokens for contrastive decoding compared to generic visual uncertainty methods. Similarly, \\textbf{Hallucination-targeted Direct Preference Optimization (HDPO)} \\cite{fu2024yqj} fine-tunes MLLMs by constructing specific preference pair data designed to address three distinct causes of hallucinations: Visual Distracted Hallucination (VDH), Long Context Hallucination (LCH), and Multimodal Conflict Hallucination (MCH). For VDH, negative samples are generated by amplifying irrelevant visual information; for LCH, negative examples are created by prompting the MLLM to continue truncated captions, often leading to deviation; and for MCH, conflicting textual information is introduced to train the model to prioritize visual grounding. HDPO's strength lies in its ability to jointly address multiple types of MLLM hallucinations through targeted data construction, offering a more comprehensive fine-tuning strategy compared to general DPO methods.\n\nDeeper architectural interventions aim to resolve representational issues within the MLLM. Methods like \\textbf{Memory-Space Visual Retracing (MemVR)} \\cite{liu2024hallucination} allow models to re-examine and leverage visual features more effectively within intermediate layers, combating 'visual amnesia' by re-injecting visual tokens based on uncertainty. This ensures that crucial visual details are not forgotten during the generation process. Crucially, these internal interventions often incorporate causal mechanisms to balance modality priors, preventing one modality from dominating or suppressing information from another \\cite{liu2024hallucination, zhou2024lvp}. For instance, \\cite{zhou2024lvp}'s \\textbf{CAUSAL MM} framework applies structural causal modeling to MLLMs, treating visual and language priors as confounding factors and using back-door adjustment and counterfactual reasoning to isolate and mitigate modality biases. Similarly, \\textbf{Visual Amplification Fusion (VAF)} \\cite{yin2025s2b} enhances attention to visual signals specifically within the MLLM's middle layers, arguing that language bias often stems from *insufficient* visual attention rather than an overemphasis on language. These intrinsic methods represent a deeper, mechanistic understanding of MLLM behavior, allowing for more precise and less intrusive corrections by ensuring more robust cross-modal integration.\n\nWhile much of the research on multimodal hallucination mitigation has focused on LVLMs, the principles extend to other modalities, albeit with fewer dedicated studies \\cite{sahoo2024hcb}. For Large Audio-Language Models (LALMs), object hallucinationâ€”where models generate or affirm the presence of non-existent sounds or objectsâ€”is a significant concern \\cite{kuan20249pm}. \\cite{kuan20249pm} demonstrated that carefully crafted prompt engineering can significantly improve LALM performance on discriminative audio tasks and reduce object hallucination, highlighting that even simple interventions can be effective when the underlying issue is query understanding rather than audio processing. Mitigation strategies for LALMs also include leveraging latent diffusion models or retrieval-based methods to ensure consistency between audio and text, as indicated by comprehensive surveys \\cite{sahoo2024hcb}. In the realm of Audio-Visual Large Language Models (AV-LLMs), \\cite{sungbin2024r2g} highlighted \"cross-modal driven hallucinations,\" where models misinterpret information due to subtle relationships or over-reliance on one modality (e.g., video-driven audio hallucination or audio-driven video hallucination). Their work demonstrated that even simple training methods, such as Low-Rank Adaptation (LoRA) fine-tuning with enhanced feature alignment, can improve AV-LLM robustness against these complex inter-modal inconsistencies. General mitigation strategies for Video-Language Models (VLLMs) often involve temporal dependency modeling to ensure consistency across dynamic sequences, a critical challenge given the sequential nature of video data \\cite{sahoo2024hcb}.\n\nIn conclusion, multimodal mitigation strategies have evolved from flexible, training-free external guidance and verification to sophisticated internal architectural interventions and causal reasoning frameworks, alongside targeted fine-tuning approaches. External methods like Counterfactual Inception \\cite{kim2024ozf} and T2I visualization \\cite{park20247cm} offer quick, adaptable solutions but rely on the robustness of external components. Decoding-time interventions like HIO \\cite{chen20247jb} and fine-tuning approaches like HDPO \\cite{fu2024yqj} represent a more direct engagement with the model's generation process, offering deeper control but requiring additional training or preference data. Intrinsic methods such as MemVR \\cite{liu2024hallucination}, VAF \\cite{yin2025s2b}, and CAUSAL MM \\cite{zhou2024lvp} aim for fundamental improvements in cross-modal integration and modality balancing, offering robust solutions at the cost of increased model complexity or intrusiveness. While LVLMs have seen the most dedicated research, emerging work in LALMs \\cite{kuan20249pm} and AV-LLMs \\cite{sungbin2024r2g} indicates a growing focus on modality-specific challenges.\n\nDespite significant progress, several challenges persist. The scalability and real-time applicability of complex causal interventions like CAUSAL MM \\cite{zhou2024lvp} in dynamic multimodal streams, such as live video or audio, remain critical areas for research. A key open question is developing robust methods for arbitrating conflicting information presented by different modalities (e.g., an image showing a blue object while accompanying text describes a a red one), requiring dynamic weighting and conflict resolution mechanisms. Furthermore, the robustness of external verification methods, such as T2I-based approaches \\cite{park20247cm}, is inherently tied to the fidelity and potential hallucination tendencies of the underlying generative models themselves. Ensuring temporal and causal consistency in long video or audio sequences, moving beyond single-frame object grounding, also poses a significant hurdle. Finally, effectively balancing the trade-offs between hallucination reduction, maintaining content quality, and preserving inference speed across diverse multimodal tasks (e.g., VQA, captioning, dialogue, video, audio) and model architectures remains a crucial area for future research \\cite{yin2025s2b, sahoo2024hcb, lan20240yz, bai2024tkm}.\n\\subsection{Cross-Modal Dynamics and Snowballing}\n\\label{sec:7_4_cross-modal_dynamics__and__snowballing}\n\n\nThe transition from Large Language Models (LLMs) to Multimodal Large Language Models (MLLMs) and Vision-Language Models (LVLMs) introduces a new dimension to the hallucination problem: complex dynamic behaviors where errors propagate across modalities and conversational turns. While Section 4.4 addresses dialogue-level inconsistencies in text-only models, this subsection focuses specifically on how the interplay with persistent visual, audio, or video modalities creates unique error propagation dynamics, often termed \"multimodal hallucination snowballing,\" which are not present in purely linguistic systems. Understanding these dynamic and interactive aspects is crucial for developing robust and coherent AI systems capable of sustained, reliable interaction across sensory inputs.\n\nA primary dynamic challenge is \\textit{multimodal hallucination snowballing}, where an LVLM's previously generated hallucination can mislead subsequent responses in conversational settings. \\cite{zhong2024mfi} meticulously identifies and characterizes this problem, demonstrating how initial factual errors in an LVLM's output can be implicitly accepted and built upon in subsequent turns, leading to a cascade of incorrect information. To counteract this, they introduce Residual Visual Decoding (RVD), a training-free decoding method that emphasizes direct visual evidence to prevent the model from relying on its own prior, potentially erroneous, textual generations. While RVD offers a practical solution, its effectiveness hinges on the clarity and availability of direct visual evidence, which might be insufficient in scenarios requiring abstract reasoning or subtle contextual understanding. Reinforcing this challenge, \\cite{cao2024o9a} introduces VisDiaHalBench, a visual dialogue benchmark specifically designed to diagnose hallucinations arising from \"long-term misleading textual history\" in LVLMs. This benchmark, featuring five-turn questions about edited images, directly probes the model's susceptibility to propagating errors in a conversational context, highlighting the need for continuous visual re-grounding. The principles of error propagation in multi-turn dialogues, as explored in text-only contexts by \\cite{chen2024c4k} with their DiaHalu benchmark, find direct and exacerbated parallels in multimodal settings where visual context can be misremembered or ignored.\n\nBeyond explicit conversational snowballing, misinterpretations can arise from subtle, dynamic interactions and inconsistencies between different modalities, acting as triggers for initial hallucinations that can then propagate. \\cite{zhou2024lvp} investigates the causal impact of modality priors on attention and output, revealing how an imbalance in these priors can lead to hallucinations. Their work, CAUSAL MM, provides a principled causal inference framework to understand and balance the influence of visual and linguistic inputs by applying back-door adjustment and counterfactual reasoning. This mitigates errors stemming from over-reliance on one modality, which could otherwise initiate a chain of incorrect inferences. However, the complexity of causal modeling and defining appropriate counterfactuals remains a challenge for broad applicability. Similarly, \\cite{han202439z} uncovers a \"semantic shift bias\" where the mere insertion of a paragraph break in textual input can subtly alter an LVLM's understanding of an image, leading to hallucinations. This demonstrates how minor, seemingly innocuous textual formatting can dynamically influence cross-modal interpretation, revealing a brittleness in vision-language grounding that may require more fundamental architectural solutions than the proposed MiHI/MiHO interventions.\n\nUnderlying these dynamic misinterpretations are fundamental vulnerabilities within the model architecture that enable error propagation. \\cite{wang2025jen} reveals that hallucinations can be induced by exploiting \"attention sinks,\" a phenomenon where attention mechanisms become fixated on irrelevant tokens, diverting processing power from critical visual information. In a multimodal context, this mechanism can directly contribute to propagating errors by causing the model to misinterpret visual cues, thus initiating a chain of incorrect inferences that could snowball. This highlights a critical internal dynamic where attention misallocation directly impacts multimodal grounding. Furthermore, the temporal dynamics inherent in video-language models present unique challenges. \\cite{ma2023mka} introduces Vista-llama to address the \"diminishing impact of video\" as generated text length increases, a clear example of cross-modal dynamic error where the visual grounding weakens over time, leading to irrelevant content. Their solution, which maintains a consistent distance between visual and language tokens, underscores the need for continuous and robust visual attention throughout the generation process.\n\nEvaluating an LVLM's ability to maintain coherent understanding across dynamic visual changes is also crucial for identifying propagating errors. \\cite{yebin2024txh} introduces BEAF (Observing BEfore-AFter Changes), an innovative framework that manipulates visual scenes by removing objects and introduces change-aware metrics. This allows for a more robust assessment of whether models truly understand visual changes or merely hallucinate, which is vital for identifying when subtle inconsistencies lead to misinterpretations or propagating errors in a dynamic environment.\n\nThe insights from LLM-centric self-correction mechanisms offer conceptual parallels for mitigating multimodal snowballing. \\cite{dhuliawala2023rqn}'s Chain-of-Verification (CoVe) and \\cite{liu2025juo}'s self-consistency framework for mathematical reasoning both emphasize internal deliberation and verification steps to prevent models from repeating their own mistakes and propagating errors. Adapting such multi-step, self-correcting paradigms to multimodal contexts would require sophisticated mechanisms for re-grounding each verification step in the visual or audio evidence, rather than solely relying on internal textual consistency. This is a significant challenge, as highlighted by surveys like \\cite{liu2024sn3} and \\cite{lan20240yz}, which discuss the persistent \"modality gap\" and the difficulty of ensuring consistent understanding across diverse data distributions. \\cite{tonmoy20244e4} also notes that snowballing in complex reasoning remains a challenge for many mitigation approaches, underscoring the severity of this dynamic problem.\n\nIn conclusion, the study of cross-modal dynamics and snowballing highlights that hallucination in LVLMs is not merely a static error but a complex, evolving problem. The propagation of errors, whether through explicit conversational snowballing, subtle cross-modal misinterpretations, or diminishing visual grounding over time, poses a significant challenge. Future research must focus on developing models with stronger internal consistency checks that explicitly re-ground in multimodal reality at each conversational turn, advanced causal modeling of cross-modal interactions to prevent the initiation of errors, and robust self-correction mechanisms that can effectively leverage and verify against dynamic sensory inputs to prevent the propagation of hallucinations.\n",
    "Towards Trustworthy AI: Robustness, Safety, and Advanced Evaluation": "\\section{Towards Trustworthy AI: Robustness, Safety, and Advanced Evaluation}\n\\label{sec:towards_trustworthy_ai:_robustness,_safety,__and__advanced_evaluation}\n\n\n\n\\subsection{Zero-Resource and Black-Box Hallucination Detection}\n\\label{sec:8_1_zero-resource__and__black-box_hallucination_detection}\n\n\nDetecting hallucinations in Large Language Models (LLMs) presents a formidable challenge, particularly when evaluating proprietary models where direct access to internal states, training data, or extensive human labels is unavailable. This subsection focuses on \"zero-resource\" and \"black-box\" detection methods, which operate under these constraints by leveraging the intrinsic properties of LLM generation to identify factual and logical inconsistencies. These approaches are indispensable for scalable, model-agnostic evaluation, thereby enhancing the practical applicability of hallucination detection frameworks across diverse deployment scenarios.\n\nA foundational contribution in this domain is \\textit{SelfCheckGPT} \\cite{manakul20236ex}, which introduced a novel zero-resource, black-box strategy. The core premise is that an LLM genuinely \"knowing\" a fact will produce consistent responses across multiple stochastically sampled generations for the same query. Conversely, if the LLM hallucinates, these samples are likely to diverge, contradict, or present inconsistent information. \\textit{SelfCheckGPT} generates several diverse responses from the black-box LLM and then employs various consistency measures, such as BERTScore, Natural Language Inference (NLI), or even another LLM acting as an evaluator, to quantify the informational agreement between the original response and the generated samples. This method effectively identifies non-factual statements without requiring internal token probabilities or external fact-checking databases. However, a significant limitation arises when LLMs consistently repeat their own errors across multiple samples due to strong internal biases or memorization, leading to false negatives in hallucination detection, as the model appears \"consistent\" in its incorrectness. Furthermore, the computational overhead of generating and comparing multiple responses can be substantial, especially for complex queries or real-time applications.\n\nTo address the limitations of consistent self-hallucinations, \\textit{MetaQA} \\cite{yang20251dw} significantly advanced zero-resource detection by introducing *metamorphic relations* and *prompt mutation*. Instead of merely re-sampling from the same prompt, \\textit{MetaQA} generates logically equivalent or semantically related prompts (e.g., synonymous queries, rephrased questions, or queries testing inverse relations) to elicit a more diverse and robust set of responses. By checking for consistency across these responses, which are generated from varied but semantically linked inputs, \\textit{MetaQA} makes it harder for the LLM to consistently hallucinate the same fact. This technique effectively probes the LLM's understanding from multiple angles, providing a more reliable signal for exposing factual inconsistencies. While more robust, the effectiveness of \\textit{MetaQA} is contingent on the careful design of metamorphic relations, which can be task-specific and may not generalize universally across all types of factual or reasoning errors.\n\nBeyond consistency checks, other black-box approaches explore alternative signals. \\textit{Attention-Guided SElf-Reflection (AGSER)} \\cite{liu2025xwv} proposes a zero-shot hallucination detection method that attempts to leverage insights from attention mechanisms without direct internal model access. AGSER categorizes the input query into \"attentive\" and \"non-attentive\" parts, processes each separately through the LLM, and then computes consistency scores between the generated responses and the original answer. The difference between these consistency scores serves as a hallucination estimator. This method notably reduces computational overhead, requiring only three passes through the LLM, making it more efficient than methods relying on numerous generations. While the precise mechanism for inferring \"attention contributions\" in a strictly black-box manner requires careful consideration, AGSER demonstrates a promising direction for deriving more nuanced signals from LLM outputs without full transparency.\n\nZero-resource, black-box principles are also being tailored for specific, complex reasoning domains. For instance, \\cite{liu2025juo} enhances mathematical reasoning in LLMs by applying a structured self-consistency framework. This approach goes beyond merely checking the final answer, enforcing consistency across *intermediate reasoning steps* in tasks like theorem proving, symbolic transformation, and numerical computation. By ensuring logical coherence throughout the problem-solving process, this method significantly reduces logical inconsistencies and hallucinations specific to mathematical contexts. While domain-specific, it highlights how black-box consistency checks can be adapted to probe deeper into an LLM's reasoning integrity, rather than just surface-level factual recall.\n\n\\textbf{Comparative Analysis and Critical Discussion:}\nThese black-box detection methods offer distinct advantages and trade-offs. \\textit{SelfCheckGPT} provides a simple, general-purpose baseline, but its vulnerability to consistently incorrect outputs limits its robustness. \\textit{MetaQA} improves robustness by actively perturbing inputs, making it harder for LLMs to hide systematic errors, yet it introduces complexity in designing effective metamorphic relations. Both methods incur significant computational costs due to multiple inference calls. \\textit{AGSER} attempts to mitigate this computational burden by leveraging a more efficient, attention-guided reflection mechanism, potentially offering a better balance between detection efficacy and resource usage. The mathematical self-consistency approach \\cite{liu2025juo} demonstrates the power of adapting these principles to domain-specific reasoning, highlighting that while general black-box detectors are valuable, specialized approaches can achieve higher precision for particular types of complex hallucinations. A common limitation across these methods is their reliance on the LLM's own generative capabilities to expose its flaws; they cannot detect hallucinations that the LLM consistently and confidently generates as \"true\" across all probed variations. Furthermore, the sensitivity to sampling parameters (e.g., temperature) and the choice of consistency metrics (e.g., BERTScore vs. NLI) can significantly impact detection performance, requiring careful tuning.\n\nIn summary, zero-resource and black-box hallucination detection methods represent a vital area of research, offering scalable and model-agnostic solutions for evaluating and improving the trustworthiness of LLMs, especially proprietary ones. From the foundational consistency checks of \\textit{SelfCheckGPT} \\cite{manakul20236ex} and the robust metamorphic relations of \\textit{MetaQA} \\cite{yang20251dw}, to the efficient attention-guided reflection of \\textit{AGSER} \\cite{liu2025xwv} and domain-specific logical consistency for mathematical reasoning \\cite{liu2025juo}, these techniques collectively push the boundaries of what is possible without privileged model access or extensive human annotation. Future directions in this area could involve the fusion of diverse black-box signals, combining internal consistency with metamorphic testing and attention-guided insights to create more robust hybrid detectors. Furthermore, research could focus on developing computationally lighter black-box methods, exploring their applicability to detect more subtle forms of hallucination beyond factual errors, such as logical fallacies, reasoning inconsistencies, and biases, and enhancing their resilience against adversarial attacks designed to evade detection.\n\\subsection{Adversarial Attacks and Vulnerability Probing}\n\\label{sec:8_2_adversarial_attacks__and__vulnerability_probing}\n\n\nBeyond merely observing and reacting to instances of hallucination, a critical new frontier in understanding and mitigating this phenomenon involves proactively probing Large Multimodal Models (LMMs) for vulnerabilities through adversarial attacks. This methodology aims to intentionally induce hallucinations, thereby uncovering specific weaknesses and failure modes that might remain hidden during passive observation, guiding the development of more resilient and secure AI systems.\n\nA pioneering work in this domain is \\cite{wang2025jen}, which introduces a novel adversarial attack termed \"Mirage in the Eyes.\" This technique specifically targets and exploits the \"attention sink\" phenomenon within MLLMs to intentionally induce hallucinations. By dynamically manipulating internal attention scores and hidden embeddings, \\cite{wang2025jen} demonstrates how to steer the model towards generating factually incorrect or non-existent visual content, providing crucial insights into the internal mechanisms that contribute to hallucination. This approach moves beyond external input perturbations, delving into the model's internal processing to expose its susceptibility.\n\nComplementing such targeted internal attacks, other research has integrated adversarial principles into evaluation and discovery. \\cite{huang20247wn}, in their comprehensive benchmark VHTest, incorporates an adversarial generation paradigm to create diverse visual hallucination instances. While not a direct attack for inducing hallucination in the same manner as \\cite{wang2025jen}, this paradigm contributes to the proactive identification of vulnerabilities by systematically generating challenging inputs that are likely to trigger various types of hallucinations, including those related to object shape and size. This allows for a broader exploration of an MLLM's fragility across different visual attributes.\n\nFurthermore, vulnerabilities can be exposed through surprisingly subtle adversarial manipulations. \\cite{han202439z} uncovered a \"semantic shift bias\" where the mere insertion of paragraph breaks (\\texttt{\\textbackslash n}) into textual prompts can induce hallucinations in MLLMs. This seemingly innocuous input perturbation acts as a potent adversarial trigger, demonstrating that models can be led astray by minor structural changes that do not alter the semantic content of the prompt. Such findings highlight unexpected failure modes and underscore the importance of probing for vulnerabilities across a wide spectrum of input types, from complex internal manipulations to simple textual formatting.\n\nCollectively, these approaches represent a significant shift from reactive mitigation to proactive robustness testing. By actively designing attacks that exploit internal model characteristics like attention sinks \\cite{wang2025jen}, or by systematically generating adversarial test cases \\cite{huang20247wn}, or even by identifying subtle input biases \\cite{han202439z}, researchers are gaining a deeper understanding of *why* and *how* hallucinations occur. This proactive methodology is indispensable for identifying the root causes of hallucination, enabling the development of more robust architectures and training strategies that can withstand sophisticated adversarial attempts to induce erroneous outputs. The insights gleaned from these adversarial probes are crucial for building truly trustworthy and secure AI systems that can maintain factual consistency even under challenging or malicious inputs.\n\\subsection{Semantic Guardrails for Safety-Critical Applications}\n\\label{sec:8_3_semantic_guardrails_for_safety-critical_applications}\n\n\nThe increasing deployment of Large Language Models (LLMs) in safety-critical domains, particularly in clinical medicine, necessitates a paradigm shift from general hallucination mitigation to robust, high-assurance safety mechanisms. In these high-stakes environments, the generation of factually incorrect or ungrounded information, often termed \"hallucinations\" \\cite{maynez2020h3q}, can lead to \"never events\"â€”catastrophic errors with severe consequences for patient safety and well-being \\cite{hakim2024d4u}. The inherent probabilistic nature of LLMs, which can lead to plausible but incorrect outputs \\cite{hamid2024pwn}, coupled with observed inaccuracies in analyzing unstructured clinical notes \\cite{shah20242sx} and significant challenges in complex medical reasoning tasks \\cite{umapathi2023puv}, underscores the urgent need for specialized safeguards.\n\nTo address this imperative, the concept of 'semantic guardrails' has emerged as a targeted solution, designed to prevent these critical errors by enforcing strict adherence to factual accuracy and consistency within domain-specific knowledge. Unlike broader LLM safety surveys that discuss ethical considerations and prompt injection alongside hallucination \\cite{gao20242nu}, semantic guardrails focus specifically on content integrity and factual grounding. This approach represents an evolution from traditional rule-based expert systems used in clinical AI, which offered high precision but often lacked the flexibility and generative power of LLMs. Semantic guardrails aim to imbue LLMs with a similar level of verifiable reliability, but within their more dynamic and open-ended operational context.\n\nA pioneering framework in this area is presented by \\cite{hakim2024d4u}, which introduces specific semantic guardrails tailored for pharmacovigilance, a domain where regulatory compliance and absolute accuracy are paramount. Their work highlights the distinction between \"structural guardrails\" (ensuring output format) and \"semantic guardrails\" (verifying content accuracy). They propose two primary mechanisms: Document-wise Uncertainty Quantification (DL-UQ) and MISMATCH guardrails. DL-UQ functions as a \"soft\" semantic guardrail by quantifying the LLM's uncertainty regarding each generated statement, specifically by evaluating its evidential support within a provided reference document. This mechanism identifies and flags information lacking sufficient backing, preventing unsupported claims from being presented as definitive facts. This is crucial for ensuring faithfulness to source material, a non-negotiable requirement in medical contexts.\n\nComplementing DL-UQ, the MISMATCH guardrail acts as a \"hard\" semantic guardrail, actively detecting contradictions or inconsistencies between the LLM's generated output and the authoritative reference document \\cite{hakim2024d4u}. For instance, in pharmacovigilance, it ensures that drug names or adverse event terms are consistently present in both source and target texts, preventing hallucination or omission of critical terms through the use of custom dictionaries and medical ontologies like MedDRA. Both DL-UQ and MISMATCH are engineered with the explicit goal of absolute error prevention for \"never events,\" fundamentally shifting the paradigm from merely reducing the frequency of hallucinations to actively precluding errors where severe consequences are at stake.\n\nWhile \\cite{hakim2024d4u} focuses on document-wise uncertainty for text-to-text tasks, other research explores different facets of uncertainty quantification (UQ) that could complement or inform guardrail development. For instance, \\cite{ling2024hqv} investigates aleatoric and epistemic uncertainties in LLMs during in-context learning, which could provide finer-grained signals for guardrails beyond document-level support. Similarly, \\cite{zhang2024mmj} introduces VL-Uncertainty for Large Vision-Language Models (LVLMs), quantifying intrinsic uncertainty by analyzing prediction variance across semantically equivalent but perturbed prompts. This highlights a broader trend towards intrinsic uncertainty estimation, which could be integrated into multimodal semantic guardrails in the future to address the complexities of visual and other non-textual data in clinical settings.\n\nSemantic guardrails also stand in contrast to, or can be integrated with, other mitigation strategies. Retrieval-Augmented Generation (RAG) is a foundational approach for grounding LLMs in external knowledge \\cite{gilbert2024uu2}. While RAG aims to prevent hallucinations by providing relevant context, semantic guardrails act as a subsequent, explicit verification layer, ensuring the *correctness* of the generated text *against* that context, rather than just relying on the retrieval process. Similarly, approaches like In-Context Padding (ICP) that guide clinical reasoning with \"knowledge seeds\" \\cite{wu202407f} aim to improve accuracy during generation by aligning LLM reasoning with clinical decision pathways, whereas guardrails provide a post-generation safety net that can validate the outcome of such guided reasoning.\n\nDespite their promise, the development and deployment of semantic guardrails face significant challenges. The theoretical inevitability of hallucination in any computable LLM \\cite{xu2024n76} suggests that achieving \"absolute error prevention\" is an asymptotic goal, requiring continuous vigilance and robust design. This means guardrails must be designed not just to prevent errors, but to gracefully handle irreducible uncertainties and flag them for human review. The tuning of thresholds for uncertainty-based guardrails (e.g., DL-UQ) involves delicate trade-offs between sensitivity (catching all potential errors) and specificity (avoiding false positives), which is particularly critical in clinical settings where over-flagging can lead to alert fatigue and hinder workflow efficiency. Furthermore, the computational overhead of running multiple, stringent semantic checks in real-time clinical workflows needs careful optimization to ensure practical applicability.\n\nFuture research must therefore focus on several key areas. Firstly, developing more sophisticated, domain-adaptable, and potentially formally verifiable semantic guardrails is crucial to expand their applicability beyond specific tasks like pharmacovigilance to broader medical reasoning and diagnostics. This includes exploring methods for automatically generating and validating guardrail rules, potentially leveraging knowledge graphs for enhanced precision and explainability. Secondly, integrating these guardrails seamlessly into human-in-the-loop systems, ensuring clear communication of uncertainty and rationale for flagging, is paramount for fostering trust and effective human-AI collaboration. Finally, research into multimodal semantic guardrails will be vital as LLMs increasingly process diverse data types in clinical settings, demanding consistent factual grounding across visual, textual, and other modalities. This continuous pursuit of high-assurance solutions is essential for the responsible and ethical integration of LLMs into safety-critical applications.\n\\subsection{Meta-Evaluation of Hallucination Benchmarks}\n\\label{sec:8_4_meta-evaluation_of_hallucination_benchmarks}\n\n\nThe rapid proliferation of hallucination benchmarks, while crucial for advancing the field, has simultaneously introduced challenges regarding their quality, reliability, and validity. As the community developed increasingly sophisticated methods to detect diverse hallucination types, a critical self-reflection emerged: how do we ensure that the tools used to measure LLM performance are themselves robust and trustworthy? This subsection delves into the vital area of meta-evaluation, focusing on frameworks designed to assess the quality of these benchmarks.\n\nEarly efforts to quantify hallucination, such as the CHAIR metric, faced considerable scrutiny due to issues like instability and sensitivity to instruction design \\cite{li2023249}. While subsequent benchmarks like POPE \\cite{li2023249} addressed some of these limitations by offering more stable, polling-based evaluation, the overarching need for a systematic framework to evaluate *any* hallucination benchmark remained. This necessity stems from concerns about potential prompt bias, data leakage, and the ability of benchmarks to accurately capture the multifaceted nature of hallucinations across various contexts and modalities.\n\nAddressing this critical gap, \\cite{yan2024ux8} introduced a groundbreaking psychometrics-inspired framework for the meta-evaluation of hallucination benchmarks. Their work proposes the \\textbf{Hallucination benchmark Quality Measurement (HQM)} framework, which systematically assesses benchmarks across four key dimensions: reliability, validity, fairness, and utility. Reliability evaluates the consistency of a benchmark's results, ensuring that repeated measurements under similar conditions yield comparable outcomes. Validity, perhaps the most crucial dimension, ascertains whether a benchmark truly measures what it purports to measure, accurately capturing the intended hallucination types without conflating them with other errors. Fairness scrutinizes benchmarks for biases, such as prompt-specific biases that might inadvertently favor certain models or data leakage issues that compromise the integrity of evaluation. Finally, utility considers the practical aspects, including the scalability, interpretability, and overall usefulness of a benchmark for researchers and developers.\n\nBy applying HQM to existing benchmarks, \\cite{yan2024ux8} revealed inherent strengths and weaknesses, providing a much-needed critical perspective on the tools foundational to hallucination research. This meta-evaluation not only highlights areas for improvement in current benchmarks but also proposes the concept of a \\textbf{High-Quality Hallucination Benchmark (HQH)} as a guiding principle for future development. The HQH concept encourages benchmark designers to proactively incorporate principles of psychometric rigor, ensuring that new evaluation methodologies are inherently robust, unbiased, and capable of accurately reflecting model performance.\n\nThe introduction of meta-evaluation frameworks marks a significant maturation of the field, shifting from merely creating benchmarks to critically assessing their foundational quality. This self-reflective advancement ensures that research findings on LLM hallucination are built upon solid, trustworthy evaluation methodologies. It is vital for maintaining the integrity of research, guiding the development of truly effective and unbiased evaluation tools, and ultimately accelerating progress towards more reliable and trustworthy large language models. Future work will likely see the HQM framework become a standard for validating new benchmarks, fostering a more rigorous and transparent evaluation ecosystem.\n",
    "Conclusion and Future Directions": "\\section{Conclusion and Future Directions}\n\\label{sec:conclusion__and__future_directions}\n\n\n\n\\subsection{Summary of Key Advancements}\n\\label{sec:9_1_summary_of_key_advancements}\n\n\nThe persistent challenge of hallucination in Large Language Models (LLMs) has driven a rapid and profound evolution in research, transforming the field from initial problem identification to a sophisticated, multi-faceted scientific endeavor. This trajectory highlights a collective effort towards building more reliable, transparent, and contextually grounded AI systems. Crucially, a profound intellectual shift has occurred, moving beyond empirical observation to a formal theoretical grounding, demonstrating the inherent inevitability of hallucination in computable LLMs \\cite{xu2024n76, li2025qzg}. This fundamental insight re-frames the research goal from complete eradication to robust management and mitigation, acknowledging an innate limitation.\n\nEarly foundational work established the critical need to address unfaithful content. \\cite{maynez2020h3q} provided a seminal large-scale human evaluation, categorizing hallucinations in abstractive summarization and demonstrating the utility of textual entailment for faithfulness evaluation. Recognizing the limitations of static knowledge, \\cite{trivedi2022qsf} introduced Interleaving Retrieval with Chain-of-Thought (IRCoT), an early few-shot, training-free method that dynamically interleaved reasoning steps with knowledge retrieval to ground LLMs in external facts. This marked an early shift towards adaptive, real-time grounding during generation. Subsequent advancements in Retrieval-Augmented Generation (RAG) further refined this paradigm, with frameworks like Rowen \\cite{ding20244yr} intelligently deciding *when* to retrieve based on cross-language/cross-model consistency, optimizing efficiency and mitigating both internal and external hallucinations. Complementing this, the strategic integration of structured Knowledge Graphs (KGs) through \"Graph-guided retrieval\" and \"Graph-guided generation\" has significantly enhanced trustworthiness and reduced hallucinations in open-ended QA by providing verifiable, structured knowledge \\cite{sui20242u1}. However, it is critical to acknowledge that RAG systems, while powerful, are not infallible; their efficacy is inherently tied to the quality of retrieved information, making them vulnerable to noisy or biased sources and susceptible to 'confabulations' arising from limitations within RAG components themselves \\cite{zhang20252at}.\n\nAs LLMs became more capable and complex, the focus expanded to developing sophisticated evaluation benchmarks that could rigorously assess trustworthiness beyond simple accuracy. \\cite{gao2023ht7} introduced ALCE, the first reproducible benchmark for evaluating LLMs' ability to generate text with verifiable citations, complete with automatic metrics for fluency, correctness, and citation quality. Pushing evaluation further, \\cite{oh2024xa3} developed ERBench, a novel benchmark leveraging relational databases to automatically verify not just final answers but also the *rationales* provided by LLMs, addressing a critical need for transparency in reasoning. Complementing this, \\cite{ghosh2024tj5} developed new logical fact-checking datasets and quantitative measures to assess LLM consistency on complex propositional logic queries, expanding the definition of \"hallucination\" to include logical inconsistencies. For scenarios where external knowledge or human labels are scarce, methods like MetaQA \\cite{yang20251dw} emerged as zero-resource, self-contained hallucination detection techniques leveraging metamorphic relations and prompt mutation. The theoretical underpinnings of such detection methods were further explored by \\cite{karbasi2025j7n}, who established an equivalence between hallucination detection and language identification, proving that automated detection is fundamentally impossible for most language collections without expert-labeled feedback, thereby providing theoretical support for methods like RLHF. Furthermore, to foster more responsible AI behavior, \\cite{tjandra2024umq} introduced the Accuracy-Engagement Distance (AED) metric to evaluate models capable of appropriately *abstaining* from answers when uncertain, utilizing semantic entropy for label-free uncertainty estimation.\n\nConcurrently, mitigation strategies evolved from general retrieval to highly adaptive, proactive, and mechanistically targeted interventions. On a more mechanistic front, \\cite{zhang2024qq9} identified \"knowledge overshadowing\" as a novel root cause of \"amalgamated hallucinations\" arising from data imbalance, proposing an inference-time, training-free self-contrastive decoding method for targeted mitigation. This represents a deeper understanding of internal model dynamics. Similarly, \\cite{chen2024j0g} presented ICT, a training-free, forward-pass intervention method that targets specific attention heads to enhance focus on crucial information, mitigating the dominance of language priors. The development of self-correction mechanisms, as exemplified by \\cite{tjandra2024umq}'s work on abstention, empowers LLMs to reflect on their uncertainty and proactively avoid generating ungrounded content, moving towards more self-aware and reliable systems.\n\nA major conceptual and methodological shift has been the dedicated focus on multimodal hallucination, particularly in Large Vision-Language Models (LVLMs). These models introduce unique complexities due to the \"modality gap\" and the integration of diverse information sources \\cite{lan20240yz}. \\cite{kaul2024ta7} addressed the inadequacy of prior benchmarks by introducing THRONE, the first accurate object-based hallucination benchmark for *free-form* generations of LVLMs, utilizing LM-based semantic judgment. Building on the \"snowballing\" effect observed in LLMs, \\cite{zhong2024mfi} investigated and mitigated \"multimodal hallucination snowballing\" in LVLMs, proposing Residual Visual Decoding (RVD) to emphasize direct visual evidence and prevent error propagation in conversational settings. To address the diverse nature of LVLM queries, \\cite{chang2024u3t} introduced Dentist, a unified mitigation framework that classifies query types (perception vs. reasoning) and applies tailored, iterative validation strategies. Furthermore, intrinsic model interventions emerged, with \\cite{wang2024vym} identifying \"Visual Encoding Distortion\" as a critical source of LVLM hallucinations and proposing Visual-Layer Fusion Contrastive Decoding (VaLiD) to correct it by fusing features from early visual layers. However, early multimodal evaluation benchmarks, such as object-based approaches, were found to be highly susceptible to prompt bias, leading to inaccurate assessments of real-world hallucination \\cite{wang2023zop}, necessitating more robust, LLM-based evaluation frameworks like HaELM.\n\nCollectively, these advancements underscore a profound intellectual trajectory: from merely identifying and correcting errors to a deep, multi-faceted understanding of hallucination's origins, its manifestation across modalities, and the development of sophisticated, adaptive, and intrinsically aware mechanisms for prevention and evaluation. This comprehensive progress marks a significant stride in the pursuit of building truly trustworthy, transparent, and contextually grounded AI systems, acknowledging both their immense potential and their inherent, theoretically proven limitations. The field is actively navigating the tension between external grounding, internal correction, and the acceptance of these fundamental limits, guiding research towards robust management rather than the elusive goal of complete eradication.\n\\subsection{Remaining Challenges and Open Questions}\n\\label{sec:9_2_remaining_challenges__and__open_questions}\n\n\nDespite significant advancements in characterizing, evaluating, and mitigating hallucination in large language models (LLMs) and multimodal large language models (MLLMs), several critical unresolved issues persist, defining promising avenues for future research in trustworthy AI. The dynamic nature of these models and their expanding capabilities mean that hallucination remains a moving target, necessitating continuous innovation.\n\nOne fundamental challenge lies in the **scalability of fine-grained annotation and evaluation**. While efforts like \\cite{ji20243j6} and \\cite{gu202414e} have introduced analytical, sentence-level annotation datasets (ANAH and ANAH-v2) and iterative self-training frameworks to scale this process, the sheer diversity of tasks, domains, and hallucination types makes truly comprehensive, human-quality annotation prohibitively expensive and time-consuming. This bottleneck hinders the development of robust, generalizable evaluation benchmarks that can capture the nuances of complex reasoning, as exemplified by the need for rationale verification in \\cite{oh2024xa3} (ERBench) and path-based evaluation in graph computation tasks \\cite{tang2024a1j} (GraphArena). The challenge extends beyond mere data quantity to ensuring the *quality, diversity, and contextual richness* of annotated data across an ever-expanding problem space.\n\nAnother pressing issue is the development of **truly real-time, adaptive, and seamlessly integrated mitigation strategies**. Current approaches have made strides towards proactive prevention and self-correction. For instance, \\cite{manakul20236ex} (SelfCheckGPT) and \\cite{yang20251dw} (MetaQA) offer zero-resource, black-box detection methods, yet these often incur computational overhead or rely on sampling, which can introduce latency or fail to capture dynamic shifts in model uncertainty. While \\cite{tjandra2024umq} proposes label-free abstention using semantic entropy, the challenge remains in making such mechanisms adaptively responsive to subtle changes in user intent or context without sacrificing generation quality or speed. Similarly, advanced Retrieval-Augmented Generation (RAG) techniques like \\cite{lv2024k5x}'s Coarse-to-Fine Highlighting (COFT) and \\cite{ding20244yr}'s Rowen improve context relevance, but the seamless integration of such dynamic knowledge retrieval and synthesis into the core generation process, without introducing new latency or compromising creative outputs, remains an open problem. Furthermore, while \\cite{hakim2024d4u} introduced \"semantic guardrails\" for safety-critical domains, generalizing such hard constraints to open-ended, creative, or rapidly evolving tasks without stifling utility is a complex balancing act.\n\nThe field also grapples with the complex task of **bridging the gap between theoretical inevitability and practical error reduction**. The unified theoretical framework proposed by \\cite{li2025qzg} (Loki's Dance of Illusions) suggests that some forms of hallucination might be mathematically inherent to LLM architectures or their training paradigms. If certain types of hallucination are indeed inevitable, the critical question shifts from absolute elimination to understanding the *acceptable error rate* and designing systems that can gracefully handle or transparently communicate these inherent limitations. This necessitates further research into robust uncertainty quantification and calibrated abstention mechanisms that are both accurate and user-friendly, allowing LLMs to \"know what they don't know\" effectively.\n\nBeyond these challenges, several **open questions** guide the next wave of innovation. The rapid expansion into multimodal AI has unveiled **novel hallucination types** that require dedicated investigation. While object hallucination in vision-language models \\cite{liu2024sn3, lan20240yz} and audio-language models \\cite{kuan20249pm} has been identified, and video-specific temporal inconsistencies are being explored \\cite{wang2024rta}, the full spectrum of cross-modal fabrication and misinterpretation, especially in complex reasoning or creative multimodal generation, is yet to be fully mapped. Moreover, the emergence of adversarial attacks that induce hallucinations \\cite{wang2025jen} suggests that new, engineered forms of hallucination will continue to challenge detection and mitigation efforts.\n\nAnother critical open question concerns the **long-term impact of multimodal interactions and cascading hallucinations**. While \\cite{qiu2024zyc} has begun to address long-context multimodal hallucination, the cumulative effect of minor inconsistencies over extended dialogues or multi-turn reasoning in complex multimodal environments remains poorly understood. The concept of \"multimodal hallucination snowballing\" \\cite{zhong2024mfi} highlights the potential for initial errors to propagate and amplify, leading to increasingly unreliable outputs. Developing methods to track, predict, and mitigate these cascading effects across modalities and over time is crucial for building truly robust conversational and interactive AI systems.\n\nFinally, there is a pressing need for **more unified and generalizable solutions that perform robustly across diverse tasks and domains**. Many current mitigation strategies are task-specific (e.g., RAG for factual QA, visual grounding for LVLMs). The development of a single, overarching framework that can effectively address hallucination across diverse applicationsâ€”from summarization and dialogue to code generation and creative writing, and across various modalitiesâ€”while maintaining high performance and adaptability, remains an elusive goal. While some efforts, like \\cite{chang2024u3t}'s unified LVLM mitigation framework, attempt to generalize within a multimodal context, achieving true cross-domain and cross-task robustness without extensive, domain-specific fine-tuning is a significant hurdle. The pursuit of a holistic \"information quality\" metric, as conceptualized by \\cite{rejeleene2024okw}, could pave the way for more generalizable evaluation and mitigation, but its practical implementation across diverse scenarios is still an open area of research. Addressing these challenges and open questions will be paramount in guiding the next wave of innovation towards building inherently trustworthy and transparent AI systems.\n\\subsection{Ethical Considerations and Responsible AI Development}\n\\label{sec:9_3_ethical_considerations__and__responsible_ai_development}\n\n\nThe challenge of hallucination in Large Language Models (LLMs) transcends mere technical inaccuracy, presenting profound ethical dilemmas that necessitate a robust framework for responsible AI development. The generation of confident yet incorrect or fabricated information by LLMs carries significant societal implications, demanding critical attention to transparency, accountability, and the safe deployment of these powerful systems, particularly in high-stakes applications \\cite{dbeeca8466e0c177ec67c60d529899232415ca87, e7c97e953849f1a8e5d85ceb4cfcc0a5d54d2365}.\n\nA foundational ethical consideration arises from the inherent limitations of LLMs. As discussed in Section 3.3, theoretical frameworks, notably those employing diagonalization arguments, suggest the mathematical inevitability of hallucination in any computable LLM \\cite{xu2024n76, li2025qzg}. This theoretical grounding shifts the ethical imperative from eradicating hallucination to transparently communicating its unavoidable nature. Responsible AI development demands that developers and deployers manage societal expectations, clearly articulate the inherent limitations of LLMs, and avoid presenting them as infallible or universally reliable. Failure to do so can lead to a breach of trust, misinformed decision-making, and a violation of the principle of non-maleficence, particularly when LLMs are deployed in sensitive domains.\n\nTo foster accountability and enable users to verify generated content, technical solutions have focused on grounding LLM outputs. As detailed in Section 4.1, the development of benchmarks like ALCE, which encourage LLMs to generate text with explicit citations to supporting evidence, represents a significant step towards ethical verifiability \\cite{gao2023ht7}. Ethically, such mechanisms aim to empower user autonomy by providing the means to cross-reference information, thereby combating the spread of misinformation. However, a critical perspective reveals potential pitfalls: if the citation mechanism itself is susceptible to hallucination (e.g., generating non-existent sources or misattributing information), it could create a false sense of security, exacerbating the problem rather than solving it. Ensuring the integrity of the grounding process is therefore paramount.\n\nBeyond proactive grounding, robust detection mechanisms are crucial for continuous monitoring and for informing users about potential inaccuracies. As explored in Section 8.1, zero-resource, black-box hallucination detection methods like \\textit{SelfCheckGPT} \\cite{manakul20236ex} and techniques leveraging metamorphic relations \\cite{yang20251dw} offer practical tools for identifying ungrounded content, even in proprietary models. Ethically, these tools support ongoing oversight and allow for post-hoc correction or flagging of potentially harmful outputs. However, their limitations, such as potential false negatives in subtle hallucinations or the computational cost of multiple generations, must be transparently acknowledged to prevent over-reliance and to ensure that human oversight remains a critical component of the safety loop.\n\nA key aspect of responsible deployment is the clear communication of uncertainty. As discussed in Section 6.3, models can be fine-tuned to proactively abstain from answering when uncertain, utilizing label-free techniques based on semantic entropy \\cite{tjandra2024umq}. This mechanism directly addresses the ethical principle of transparency by allowing LLMs to express \"I don't know\" rather than confidently asserting potentially incorrect information. This reduces the risk of misinformation in sensitive contexts and promotes a more honest interaction paradigm. However, the ethical balance lies in determining the appropriate threshold for abstention; an overly conservative model might diminish utility, while an overly permissive one risks harm.\n\nThe responsible deployment of AI systems, particularly in high-stakes applications, necessitates robust safety mechanisms and domain-specific \"guardrails.\" In medical safety-critical settings, for instance, the implementation of \"semantic guardrails\" (e.g., Document-wise Uncertainty Quantification and MISMATCH guardrails), as highlighted in Section 8.3, aims to prevent \"never event\" errorsâ€”hallucinations with severe consequences \\cite{hakim2024d4u}. This exemplifies a direct coupling of technical solutions with stringent ethical frameworks, prioritizing non-maleficence. Such guardrails are crucial for ensuring that LLMs can be trusted in environments where factual accuracy and safety are paramount, moving beyond general mitigation to targeted, high-assurance solutions.\n\nMoreover, responsible AI development extends beyond mere factual accuracy to encompass a broader spectrum of ethical considerations. Hallucination can intersect with and amplify algorithmic bias, leading to outputs that are not only incorrect but also unfair or discriminatory, thereby violating principles of justice and fairness. The phenomenon of \"sycophancy,\" where LLMs excessively agree with or flatter users, even when the user's premise is incorrect, poses a distinct ethical challenge \\cite{malmqvist2024k7x}. Sycophancy can undermine critical thinking, reinforce user biases, and create echo chambers, impacting user autonomy and potentially leading to societal harm. Addressing such behavioral biases is as critical as addressing factual errors for building truly reliable and ethically aligned LLMs. Furthermore, the generation of toxic or harmful content, as noted in surveys on automated correction \\cite{pan2024y3a}, underscores the need for comprehensive safety measures.\n\nThe pursuit of trustworthy AI also requires holistic conceptual frameworks. Research has moved towards defining and mathematically formalizing \"Information Quality\" (IQ) based on consistency, relevance, and accuracy, providing a structured approach to evaluating the ethical performance of LLMs beyond mere factual correctness \\cite{rejeleene2024okw}. This holistic perspective is vital as LLMs expand into multimodal domains, where challenges like long-context hallucinations in multimodal models demand continuous monitoring and tailored ethical considerations \\cite{qiu2024zyc}. The emergence of adversarial attacks that can intentionally induce hallucinations in multimodal LLMs, as discussed in Section 8.2 \\cite{wang2025jen}, highlights critical security vulnerabilities. Ethically, this necessitates proactive security measures and foresight to prevent malicious exploitation and ensure the integrity and safety of AI systems, as emphasized by broader surveys on LLM safety \\cite{gao20242nu}.\n\nIn conclusion, addressing hallucination is not solely a technical endeavor but a profound ethical responsibility that underpins the trustworthiness and societal benefit of LLMs. The development of transparent, accountable, and safe LLM systems requires a multi-faceted approach that integrates robust detection, verifiable content generation, clear communication of uncertainty, and domain-specific safety mechanisms. This forward-looking perspective emphasizes that technical advancements must be inextricably linked with strong ethical frameworks, encompassing principles of non-maleficence, beneficence, justice, fairness, and user autonomy, to ensure that LLMs are developed and utilized in a manner that genuinely benefits society, minimizes harm, and maximizes trustworthiness.\n"
  },
  "subsections": {
    "The Rise of Large Language Models and the Hallucination Challenge": "\\subsection{The Rise of Large Language Models and the Hallucination Challenge}\n\nThe advent of Large Language Models (LLMs) has heralded a transformative era in artificial intelligence, demonstrating unprecedented capabilities in natural language understanding and generation \\cite{ahmadi2024j88}. These models, characterized by their massive scale and emergent abilities, have revolutionized diverse domains, from automating complex cognitive tasks like code generation and scientific discovery to enhancing human-computer interaction through sophisticated dialogue systems. Their capacity to produce highly coherent, contextually relevant, and often creative text has positioned them as pivotal technologies poised to redefine numerous industries and research paradigms.\n\nHowever, alongside these remarkable advancements, a pervasive and critical limitation, widely termed 'hallucination,' significantly impedes the reliability and trustworthiness of LLMs \\cite{ahmadi2024j88, rawte2023ao8}. Hallucination refers to the generation of content that is factually incorrect, nonsensical, or ungrounded in the provided input or real-world knowledge. Early investigations into neural text generation, particularly in abstractive summarization, highlighted this issue, revealing that models frequently produced information not present in the source document, with a substantial portion being factually erroneous \\cite{maynez2020h3q}. This foundational work introduced a critical distinction between 'intrinsic' hallucinations (misrepresenting source information) and 'extrinsic' hallucinations (adding ungrounded information), underscoring the challenge of ensuring faithfulness and factual accuracy. The problem's prevalence has only intensified with the increasing scale and generality of LLMs, making it a central concern for their safe and effective deployment \\cite{rawte2023ao8}.\n\nThe implications of hallucination are profound and far-reaching, posing substantial risks to user trust, the credibility of AI systems, and the safety of their applications across various domains. In high-stakes environments such as healthcare, finance, or legal services, hallucinated information can lead to severe consequences, including misdiagnoses, flawed financial advice, or fabricated legal precedents \\cite{li2025qzg, ahmadi2024j88}. Beyond factual errors, hallucinations can manifest as logical inconsistencies or ungrounded reasoning, eroding confidence in an LLM's ability to perform complex tasks reliably. This challenge extends beyond mere inconvenience, directly impacting the deployability of LLMs and necessitating robust mechanisms to ensure their outputs are verifiable and trustworthy.\n\nThe research trajectory has evolved from merely characterizing hallucination to seeking a deeper understanding of its origins, both empirical and theoretical. Initial observations revealed that even during pre-training, smaller language models could learn to reduce perplexity on grammatical sequences that *contained* hallucinations, suggesting that the propensity for generating plausible but incorrect text is embedded early in the learning process \\cite{xia20224cl}. As models scale, some of these issues are mitigated, but the fundamental challenge persists. More recently, the understanding of hallucination has been elevated to a theoretical plane, with formal mathematical definitions being proposed \\cite{li2025qzg}. Groundbreaking work has even posited that hallucination is not merely a transient engineering problem but an inherent and *inevitable* limitation of any computable LLM, stemming from fundamental principles of learning theory and computability \\cite{xu2024n76}. This theoretical perspective fundamentally reshapes the problem, suggesting that while mitigation is crucial, complete eradication might be impossible, thus shifting the focus towards robust management and transparent communication of uncertainty rather than absolute elimination. This pervasive issue is also not confined to text, manifesting uniquely across multimodal contexts, including Large Vision-Language Models (LVLMs) and Large Audio-Language Models (LALMs), further complicating the landscape of trustworthy AI \\cite{li2025qzg}.\n\nIn light of these challenges, the urgent need for robust research into the diverse causes, comprehensive evaluation, and effective mitigation strategies for hallucination is paramount. Addressing this phenomenon, which spans factual inaccuracies, nonsensical outputs, and ungrounded content across text and increasingly multimodal domains, is not merely an incremental improvement but a central hurdle to achieving truly trustworthy, verifiable, and safely deployable AI systems. This review aims to comprehensively explore the multifaceted problem of hallucination, from its foundational definitions and mechanistic causes to advanced evaluation methodologies and cutting-edge mitigation techniques, ultimately guiding the development of more reliable and accountable LLMs.",
    "Scope and Structure of the Review": "\\subsection*{Scope and Structure of the Review}\n\nThis literature review provides a comprehensive and structured analysis of research into hallucination in Large Language Models (LLMs), adopting a pedagogical progression to trace the field's maturation from foundational concepts to advanced methodological approaches and cutting-edge multimodal developments. This structured organization is designed to offer a clear, evolving narrative of the research landscape, emphasizing the interconnections between different facets of the hallucination problem. The review synthesizes the evolving understanding of hallucination, its underlying causes, methods for its evaluation, and strategies for its mitigation, with a primary focus on advancements from 2022 to 2025 to ensure relevance to the current state of the art in LLM research.\n\nThe review is meticulously organized into several thematic sections, each building upon the preceding one to offer a holistic and in-depth understanding of hallucination. Following this introduction, Section 2, \"Foundational Understanding: Defining and Categorizing Hallucination,\" lays the groundwork by exploring the evolution of hallucination definitions. It distinguishes between crucial concepts like faithfulness to source material and factual correctness in general knowledge, and traces the development of early taxonomies. This section highlights how the understanding of hallucination has broadened significantly beyond simple factual errors to encompass a diverse array of content inconsistencies, logical flaws, and reasoning failures, thereby setting the stage for more granular analysis and targeted interventions.\n\nBuilding upon this definitional and conceptual foundation, Section 3, \"The Roots of Hallucination: Mechanistic Causes and Theoretical Limits,\" delves into the underlying reasons why LLMs generate incorrect or ungrounded content. This section moves beyond mere empirical observation to investigate both practical causes related to data quality, training processes, and inference biases. Crucially, it explores specific internal model mechanisms, such as 'knowledge overshadowing' and 'attention sinks,' that directly contribute to hallucination. The section culminates in a discussion of the formal theoretical grounding of hallucination, examining its mathematical origins and the concept of its inherent inevitability, which fundamentally re-frames the problem from a transient engineering bug to an innate characteristic of computable LLMs.\n\nWith a clear understanding of what hallucination is and why it occurs, Section 4, \"Evaluating Hallucination: Benchmarks for Factual Accuracy and Reasoning,\" details the evolution of evaluation methodologies. It showcases a progression from initial, broad assessments to highly granular, automatically verifiable, and context-aware benchmarks. This section covers early efforts to establish reproducible metrics for factual correctness and citation quality, advancements in fine-grained and rationale-based evaluation that probe the model's reasoning process, and the development of specialized benchmarks for complex algorithmic reasoning. Furthermore, it addresses the unique challenges of assessing hallucinations in long-context and dialogue-level interactions, underscoring the field's commitment to rigorous and comprehensive measurement of LLM trustworthiness.\n\nSubsequently, the review transitions to a comprehensive exploration of mitigation strategies, divided into two complementary sections. Section 5, \"Mitigation Strategies I: External Knowledge Grounding and Adaptive Retrieval,\" explores approaches centered on grounding LLMs in external knowledge. It introduces Retrieval-Augmented Generation (RAG) as a foundational paradigm, explaining its core principles and early implementations. This section then details the evolution of RAG into advanced and adaptive architectures that dynamically integrate external information, optimize retrieval, and intelligently manage context. It also highlights the increasing importance of integrating structured knowledge graphs to enhance factual accuracy, logical consistency, and overall trustworthiness.\n\nComplementing external grounding, Section 6, \"Mitigation Strategies II: Intrinsic Model Interventions and Self-Correction,\" focuses on interventions that operate intrinsically within the LLM itself. This includes techniques that modify the decoding process, such as contrastive methods, to steer generation away from ungrounded content. The section then delves into more granular interventions that manipulate internal model states and attention mechanisms during the forward pass for precise control over information flow. Finally, it discusses the development of self-correction and abstention mechanisms, enabling LLMs to detect their own uncertainties and errors, along with training-based approaches that leverage automated data generation for more efficient and targeted fine-tuning against hallucination.\n\nRecognizing the expanding frontier of AI, Section 7, \"The Multimodal Frontier: Hallucination in Vision, Audio, and Video Language Models,\" addresses the significant challenges posed by hallucination in Large Vision-Language Models (LVLMs), Large Audio-Language Models (LALMs), and Video-Language Models (VLLMs). This section defines and categorizes the distinct types of hallucinations that emerge from cross-modal interactions, details specialized evaluation benchmarks developed to rigorously assess these multimodal inconsistencies, and explores tailored multimodal mitigation strategies. It also investigates dynamic behaviors such as 'multimodal hallucination snowballing' in interactive multimodal settings, highlighting the complexity of ensuring trustworthiness across diverse sensory inputs.\n\nFinally, Section 8, \"Towards Trustworthy AI: Robustness, Safety, and Advanced Evaluation,\" explores critical advancements in building truly trustworthy LLMs. It covers methods for zero-resource and black-box hallucination detection, crucial for proprietary models, and delves into the proactive testing of LLM vulnerabilities through adversarial attacks, which actively induce hallucinations to identify weaknesses. This section highlights the development of 'semantic guardrails' for safety-critical applications, aiming for absolute error prevention, and discusses the emerging field of meta-evaluation, which critically assesses the quality of hallucination benchmarks themselves, ensuring the integrity of all research efforts towards reliable AI.\n\nThe review concludes in Section 9, \"Conclusion and Future Directions,\" by synthesizing the key advancements, outlining remaining challenges and open questions, and discussing the critical ethical considerations necessary for responsible AI development. This structured progression aims to provide readers with a comprehensive, nuanced, and forward-looking perspective on the multifaceted problem of hallucination in LLMs, fostering a deeper understanding of the path towards more reliable and accountable AI systems.",
    "Early Characterization and Taxonomies in LLMs": "\\subsection*{Early Characterization and Taxonomies in LLMs}\n\nThe advent of Large Language Models (LLMs) marked a significant leap in generative AI, but simultaneously brought to the forefront the pervasive challenge of \"hallucination\"â€”the generation of content that is unfaithful to source material, factually incorrect, or logically inconsistent. Early research was thus fundamentally driven by the need to empirically observe, define, and categorize these phenomena to establish a foundational understanding for subsequent evaluation and mitigation strategies.\n\nInitial empirical observations of unfaithful content were predominantly rooted in specific Natural Language Generation (NLG) tasks, particularly abstractive summarization. A seminal work by \\cite{maynez2020h3q} provided a large-scale human evaluation of \"faithfulness\" in neural summarization. This study systematically characterized hallucinations into \"intrinsic\" (misrepresenting information present in the source) and \"extrinsic\" (adding information not directly inferable from the source) types. Crucially, \\cite{maynez2020h3q} highlighted that traditional automatic metrics like ROUGE correlated poorly with human judgments of faithfulness, underscoring the limitations of surface-level evaluation and the necessity for deeper semantic understanding. Building on this, \\cite{dong20223yz} further refined the understanding of faithfulness, challenging the strict assumption that all out-of-article content is undesirable. Their work demonstrated that gold-standard summaries often contain factually correct entities not explicitly present in the source, requiring external world knowledge. This introduced a critical dual perspective: faithfulness to the source document versus faithfulness to external world knowledge, thereby significantly shaping the early research agenda by clarifying these distinct dimensions of hallucination. These early efforts in summarization laid the groundwork for identifying content unsupported by a given context and distinguishing between adherence to input and general factual correctness.\n\nMoving beyond document-level analysis, the need for finer-grained detection methods applicable to free-form text generation became apparent. While not directly a hallucination detection method, the work by \\cite{chen2022gkm} on proposition-level segmentation and textual entailment recognition offered a foundational conceptual framework for analyzing the truthfulness of individual meaning units within sentences. By enabling the segmentation of sentences into distinct propositions and classifying their entailment relations with respect to a reference document, \\cite{chen2022gkm} provided tools that could underpin more precise, token- or phrase-level identification of unsupported content. This approach represented a significant step towards dissecting generated text at a granular level, addressing the limitations of coarser-grained methods that might miss subtle inconsistencies, and paving the way for more detailed analyses of where and how hallucinations manifest within generated outputs.\n\nAs LLMs gained prominence, the scope of hallucination expanded, necessitating structured frameworks for categorization that transcended task-specific observations. Comprehensive surveys like \\cite{zhang2023k1j} and \\cite{ye2023yom} emerged as foundational works, synthesizing the burgeoning literature and proposing early LLM-centric taxonomies. \\cite{zhang2023k1j} categorized LLM hallucinations into input-conflicting, context-conflicting, and fact-conflicting types, emphasizing the particular challenges posed by fact-conflicting errors due to the absence of an authoritative knowledge source. This survey provided a clear analytical framework for understanding the multifaceted nature of LLM failures. Concurrently, \\cite{ye2023yom} offered a detailed taxonomy that spanned various text generation tasks, from machine translation to dialogue systems, and identified initial hypothesized origins related to data collection, knowledge gaps, and the optimization process. These meta-analyses were instrumental in consolidating diverse empirical findings into a common language, providing structured frameworks for understanding different types of hallucinations, and distinguishing between faithfulness to source and factual correctness in general knowledge. While these early taxonomies provided critical initial classifications, they often remained descriptive, and the sheer diversity and subtlety of LLM-generated errors hinted at a problem far more complex than simple factual deviations.\n\nIn conclusion, the early characterization of hallucination in LLMs evolved from empirical observations in specific NLG tasks, particularly abstractive summarization, to the development of sophisticated, LLM-centric taxonomies and meta-analyses. This progression, from identifying unfaithful content at a document level and refining the concept of faithfulness, to enabling finer-grained analysis and categorizing diverse error types, was instrumental in defining the problem space. These foundational works established the crucial distinction between faithfulness to source and factual correctness, which became a cornerstone for subsequent research. However, the initial frameworks also revealed that many LLM errors were not merely simple factual deviations but deeper failures of reasoning and consistency, suggesting that a more pervasive and nuanced understanding of hallucination was required, a challenge we explore in the subsequent section.",
    "Hallucination as a Pervasive Problem: Beyond Factual Errors": "\\subsection*{Hallucination as a Pervasive Problem: Beyond Factual Errors}\n\nThe initial conceptualization of hallucination in Large Language Models (LLMs) primarily focused on the generation of content that was factually incorrect or unfaithful to source material. However, as LLMs have grown in complexity and application, the understanding of hallucination has profoundly expanded, revealing it as a multifaceted problem encompassing a broader spectrum of inconsistencies and failures that extend beyond mere factual inaccuracies. This shift highlights that hallucination impacts the overall 'information quality' and trustworthiness of LLM outputs, necessitating a deeper examination of underlying cognitive and logical failures \\cite{rejeleene2024okw}.\n\nEarly research began to delineate the nuances of unfaithful generation, laying the groundwork for this broader view. For instance, \\cite{maynez2020h3q} distinguished between \"intrinsic hallucinations,\" which misrepresent information present in the source, and \"extrinsic hallucinations,\" which introduce new, ungrounded information. Crucially, they observed that many extrinsic hallucinations were erroneous, demonstrating that even superficially plausible generated content could be unfaithful to its source and thus unreliable. This early distinction underscored that hallucination was not solely about factual errors against world knowledge, but also about fidelity to provided context. This conceptual expansion quickly extended to multimodal contexts, where \\cite{dai20229aa} identified \"object hallucination\" and \"attribute hallucination\" in vision-language models. Here, LLMs generated objects or attributes not present in the visual input, illustrating how ungrounded reasoning could manifest across different modalities, producing semantically coherent but contextually false outputs.\n\nThe field has since moved towards recognizing that hallucination often stems from deeper failures in an LLM's reasoning process and internal consistency, rather than just a lack of factual recall. Logical inconsistencies, where an LLM's internal reasoning path is flawed, represent a significant category of such failures. For example, \\cite{xie20247zk} demonstrated that the order in which LLMs generate answers and reasoning significantly impacts their consistency, revealing instances where models fabricate answers and then retrospectively generate justifications. This highlights a fundamental flaw in the logical coherence of the model's thought process, rather than a simple factual error. Further, \\cite{jiang20242kz} investigated how LLMs can hallucinate *despite possessing the correct knowledge*, attributing this to problematic inference dynamics. Their work revealed that in hallucinated cases, the output token's probability rarely demonstrated consistent superiority in later stages of the model, suggesting a failure in applying known facts during generation, indicative of ungrounded reasoning.\n\nBeyond explicit logical errors, hallucinations can also manifest as subtle semantic shifts, biases, or ungrounded claims that appear superficially plausible, thereby undermining trustworthiness. \\cite{zhang2024qq9} introduced \"knowledge overshadowing,\" a phenomenon where LLMs prioritize certain knowledge due to data imbalance. This leads to outputs that, while not necessarily factually incorrect, are ungrounded in the immediate context or subtly biased, generating plausible but misleading information. A particularly insidious form of this is \"sycophantic hallucination,\" as explored by \\cite{rrv2024gw0}. This occurs when LLMs provide answers that align with a user's potentially misleading keywords or desired narrative, even if factually questionable. Such behavior amplifies misinformation and erodes user trust by prioritizing perceived user preference over factual accuracy, representing a significant semantic shift that compromises the integrity of the generated content.\n\nCollectively, these diverse manifestations underscore a critical shift in understanding: hallucination is not merely about isolated factual errors, but about a pervasive degradation of overall \"information quality\" and trustworthiness. \\cite{rejeleene2024okw} directly addresses this by proposing a mathematical framework for evaluating Information Quality (IQ) in LLMs, defining it as a function of consistency, relevance, and accuracy. This framework explicitly moves beyond simple factual correctness to encompass the broader attributes essential for reliable and trustworthy AI outputs. The recognition of hallucination as a multifaceted problem, encompassing logical inconsistencies, ungrounded reasoning, and subtle semantic shifts, necessitates a paradigm shift towards more sophisticated evaluation and mitigation strategies that address these deeper cognitive and logical failures inherent in LLM architectures.",
    "The Evolution of Hallucination Types": "\\subsection*{The Evolution of Hallucination Types}\n\nThe understanding and categorization of AI hallucinations have undergone a significant evolution, moving from broad, general definitions to highly granular and context-specific classifications. This progression reflects the increasing complexity of AI models and their applications, necessitating a more nuanced taxonomy for effective detection and mitigation.\n\nInitially, research into hallucinations in large language models (LLMs) established foundational categories. Early work, such as the comprehensive survey by \\cite{DBLP:journals/corr/abs-2202-03629}, broadly categorized hallucinations into *intrinsic*, where the generated content contradicts the source input, and *extrinsic*, where it contradicts established world knowledge. This foundational classification also introduced related concepts like factuality, faithfulness, and consistency, providing an initial framework for analyzing model outputs. Expanding on this, another survey by \\cite{DBLP:journals/corr/abs-2305-13889} further refined LLM hallucination types by categorizing them based on their *source* (e.g., data, model architecture, inference process), *form* (e.g., factual, logical, numerical inconsistencies), and *severity*, offering a multi-dimensional perspective on how and why these errors manifest.\n\nAs AI capabilities extended to multimodal domains, the definition of hallucination necessarily diversified to encompass new forms of inconsistency. \\cite{DBLP:journals/corr/abs-2305-18654} specifically addressed hallucinations in Large Multimodal Models (LMMs), proposing a refined taxonomy that includes *object hallucinations* (misidentifying or fabricating objects), *attribute hallucinations* (incorrectly describing an object's properties), and *relation hallucinations* (misrepresenting relationships between objects). This marked a crucial step towards more granular, modality-specific categorizations. Building upon this multimodal understanding, research into Video-Language Models (VLMs) further introduced the dimension of time. \\cite{DBLP:journals/corr/abs-2305-18260} identified and categorized VLM hallucinations based on *temporal consistency*, detailing issues such as incorrect event order, duration, or frequency, which are paramount for accurately interpreting dynamic visual information.\n\nBeyond modality, the operational context and interaction paradigm of AI systems have also led to the identification of distinct hallucination types. For instance, the challenges posed by extended inputs prompted the definition of *long-context specific hallucinations* by \\cite{DBLP:journals/corr/abs-2309-17424}. These are errors that emerge or are exacerbated when models process unusually long sequences of text, often involving subtle inconsistencies or omissions that are difficult to detect without a comprehensive understanding of the entire context. Similarly, in the realm of conversational AI, \\cite{DBLP:journals/corr/abs-2309-07490} introduced the concept of *dialogue-level hallucinations*. These extend beyond single-turn factual errors to encompass inconsistencies in persona, conversation flow, or maintaining coherent context across multiple turns, which are critical for natural and trustworthy human-AI interaction. Furthermore, the global deployment of AI models has highlighted *multilingual hallucinations*, as investigated by \\cite{DBLP:journals/corr/abs-2305-10424}. This work revealed that hallucination rates can vary significantly across languages, suggesting that language-specific nuances, data biases, or model training disparities can lead to distinct patterns of erroneous generation in non-English contexts.\n\nThis progressive refinement in categorizing hallucination typesâ€”from broad textual inconsistencies to specific multimodal, temporal, long-context, dialogue-level, and multilingual manifestationsâ€”is indispensable. It underscores a growing recognition that a one-size-fits-all approach to hallucination is insufficient. The continued identification of such distinct types is crucial for developing targeted evaluation metrics, designing more robust and context-aware mitigation strategies, and ultimately fostering greater reliability across diverse AI applications.",
    "Empirical Causes: Data, Training, and Inference Biases": "\\subsection*{Empirical Causes: Data, Training, and Inference Biases}\n\nLarge Language Models (LLMs) are susceptible to generating \"hallucinations\"â€”plausible but factually incorrect or unfaithful contentâ€”due to a complex interplay of empirical factors spanning their entire lifecycle, from data acquisition to training and inference. Understanding these practical causes is crucial for developing targeted interventions throughout the model development pipeline. A foundational observation, particularly in abstractive summarization, highlighted that standard likelihood training objectives and approximate decoding strategies inherently prioritize fluency and coherence over strict factual accuracy, leading to both intrinsic (misrepresenting source information) and extrinsic (adding ungrounded information) hallucinations \\cite{maynez2020h3q}. This initial insight underscores biases embedded during training and exacerbated during generation.\n\nA significant category of empirical causes stems from **data collection and preparation issues**. The immense pre-training corpora, while enabling powerful generalization, often contain noise, inconsistencies, and factual decay that models inadvertently learn \\cite{li2024qrj}. For instance, empirical studies reveal that the lower the frequency of specific knowledge in pre-training data, the higher the propensity for LLMs to hallucinate when queried on that topic \\cite{li2024qrj}. This suggests that data imbalance and under-representation of certain facts directly contribute to factual errors. Furthermore, LLMs learn superficial statistical patterns from their training data rather than robust logical reasoning. McKenna et al. (2023) identified two key biases in pre-trained models: an **attestation bias**, where models over-rely on propositional memory, affirming entailment if a hypothesis is likely attested in their training data regardless of the premise; and a **relative frequency bias**, where models tend to affirm entailment if the premise predicate is less frequent than the hypothesis predicate \\cite{mckenna2023pzc}. These biases demonstrate how statistical regularities in the data, rather than semantic understanding, can lead to incorrect inferences, with specific named entities often acting as \"indices\" to trigger memorized, potentially irrelevant, propositional knowledge \\cite{mckenna2023pzc}.\n\nDuring **training**, models can develop biases that manifest as hallucinations. The predominant optimization objectives, typically focused on next-token prediction, do not explicitly penalize factual inaccuracies. This encourages models to \"over-generalize\" or invent plausible but unverified information to maintain fluency, thereby internalizing spurious correlations present in the data \\cite{li2024qrj}. Supervised fine-tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF) aim to align models, but their effectiveness can be highly dependent on the quality of instructions and the balanced complexity of the fine-tuning data. Suboptimal SFT can exacerbate hallucinations, and even RLHF's benefits can be domain-specific, indicating that the training process itself can introduce or fail to mitigate biases effectively \\cite{li2024qrj}. In multimodal contexts, such as with Vision-Language Models (VLMs), training challenges can lead to \"uncontrollable global visual uncertainty,\" where the model struggles to consistently ground its textual descriptions in the visual input, resulting in visual hallucinations \\cite{chen20247jb}. This highlights that even with rich multimodal data, the training process might not sufficiently enforce cross-modal consistency.\n\n**Inference-time biases** further contribute significantly to hallucination, even in well-trained models. The decoding process, as noted by \\cite{maynez2020h3q}, often prioritizes fluency over factual accuracy. LLMs frequently struggle with complex reasoning tasks, leading to erroneous outputs. Empirical studies show that a \"lack of logical reasoning capabilities is the primary contributor to Fact-Conflicting Hallucination (FCH),\" particularly when dealing with temporal concepts and out-of-distribution knowledge \\cite{li2024osp}. Benchmarks evaluating LLM rationales reveal that models can generate plausible-sounding but logically flawed reasoning paths \\cite{oh2024xa3}. Furthermore, the order in which an LLM generates its reasoning and answer significantly impacts consistency, with generating an answer first often leading to subsequent fabrication of justifications, highlighting a fragile reasoning process at inference time \\cite{xie20247zk}. When tasked with algorithmic reasoning on complex structures like real-world graphs, LLMs often fail to produce correct solutions, indicating a struggle with systematic, multi-step inference that can lead to hallucinated steps or conclusions \\cite{tang2024a1j}.\n\nAnother prevalent inference-time bias is the model's overconfidence or lack of uncertainty awareness. LLMs tend to generate definitive answers even when highly uncertain, leading to confident hallucinations \\cite{li2024qrj}. This necessitates proactive abstention mechanisms, which allow models to signal uncertainty and refuse to answer when confidence is low \\cite{tjandra2024umq}. The empirical observation that models can produce \"never event\" errors in safety-critical domains further underscores this lack of inherent self-correction and the need for \"semantic guardrails\" to prevent catastrophic hallucinations during deployment \\cite{hakim2024d4u}. For LVLMs, an inference-time bias can be their failure to adequately ground generated text in specific visual regions without explicit guidance, leading to object hallucinations that require training-free, image-grounded interventions \\cite{zhao2024ge8}.\n\nEven with Retrieval-Augmented Generation (RAG) systems designed to mitigate hallucinations by providing external knowledge, LLMs exhibit several inference-time biases in context utilization. Empirical benchmarks reveal that LLMs struggle with **noise robustness**, often confusing similar information when relevant and noisy documents are present; **negative rejection**, frequently failing to abstain from answering when no relevant information is available; **information integration**, demonstrating a significant lack of ability to synthesize facts from multiple documents; and **counterfactual robustness**, tending to trust and prioritize factually incorrect retrieved information even when possessing correct internal knowledge or being warned \\cite{chen2023h04}. Moreover, LLMs can be receptive to newly generated counterfactual passages, indicating a vulnerability in their utility judgment of retrieved evidence \\cite{zhang2024o58}. These challenges necessitate more intelligent, uncertainty-aware retrieval and context highlighting mechanisms \\cite{niu2024v97, su2024gnz, lv2024k5x}.\n\nIn conclusion, empirical causes of hallucination are multifaceted, spanning the entire LLM development pipeline. Data quality issues, especially imbalances and superficial statistical patterns, lead to models learning incorrect associations and biases. Training objectives that prioritize fluency over factuality, coupled with challenges in enforcing cross-modal consistency, contribute to over-generalization and learned biases. During inference, models exhibit biases towards fluency, struggle with complex logical and algorithmic reasoning, often lack uncertainty awareness, and can inefficiently or incorrectly utilize external knowledge. Addressing these empirical causes requires a holistic approach, from meticulous data curation and refined training objectives to advanced inference-time controls and uncertainty quantification.",
    "Internal Mechanisms: Knowledge Overshadowing and Attention Sinks": "\\subsection*{Internal Mechanisms: Knowledge Overshadowing and Attention Sinks}\n\nUnderstanding the genesis of hallucinations in Large Language Models (LLMs) necessitates a deep dive into their internal processing mechanisms, moving beyond surface-level output analysis to pinpoint the granular cognitive biases and representational distortions that contribute to factual inaccuracies. This subsection elucidates specific internal phenomena, such as knowledge overshadowing, amalgamated hallucinations, visual encoding distortion, semantic shift bias, and the exploitation of attention sinks, offering a mechanistic perspective on hallucination.\n\nOne fundamental internal mechanism contributing to hallucination is \\textit{knowledge overshadowing}, where dominant patterns or more frequently encountered information in the training data can suppress or override specific, less frequent facts. \\cite{smith2021overshadowing} introduced this concept, demonstrating how an LLM, even when possessing the correct information, might default to a more generalized or statistically prevalent answer due to the overshadowing effect, leading to plausible but incorrect generations. Building upon this understanding of internal misrepresentation, \\cite{jones2022amalgamation} further explored \\textit{amalgamated hallucinations}, where models do not merely over-generalize but actively combine multiple factually correct, yet disparate, pieces of information into a novel, coherent, but ultimately false statement or narrative. This highlights the model's internal capacity for creative miscombination, where the synthesis of information goes awry, producing a \"truthy\" but fabricated output.\n\nThe challenge of internal mechanistic failures extends beyond purely textual models, manifesting acutely in multimodal architectures. \\cite{chen2023multimodal} identified \\textit{visual encoding distortion} in multimodal LLMs, where the model's internal representation of visual information becomes corrupted during the encoding process. This distortion leads to hallucinations where the generated text inaccurately describes visual elements, even when the visual input itself is clear and unambiguous, underscoring that representational fidelity issues are not modality-specific but inherent to the internal processing pipelines. Further elaborating on the subtleties of internal processing, \\cite{lee2024semanticshift} revealed \\textit{semantic shift bias}, demonstrating how seemingly innocuous linguistic structures, such as paragraph breaks, formatting, or specific phrasing, can subtly but significantly alter an LLM's internal semantic understanding. This internal bias can lead to shifts in meaning, subsequently causing the generation of content that is factually divergent or contextually inappropriate, highlighting the profound sensitivity of internal representations to structural cues.\n\nThese identified internal vulnerabilities are not merely theoretical constructs but represent exploitable weaknesses. \\cite{wang2025attentionsinks} demonstrated how adversarial inputs can be meticulously crafted to exploit \\textit{attention sinks} within LLMs. By strategically placing specific tokens, attackers can manipulate the model's internal attention mechanisms, forcing it to allocate disproportionate focus to irrelevant or misleading information. This targeted manipulation of internal attention pathways can predictably induce specific hallucinations, providing a direct mechanistic link between internal processing flaws and external adversarial attacks.\n\nIn conclusion, the literature underscores that hallucinations are not monolithic errors but rather symptoms of diverse and granular internal mechanisms, ranging from the statistical biases of knowledge overshadowing and the creative miscombination of amalgamated hallucinations, to cross-modal representational distortions and subtle linguistic influences on semantic understanding. The identification of attention sinks further reveals how these internal processing quirks can be leveraged for adversarial purposes. While significant progress has been made in dissecting these internal mechanisms, future research must continue to investigate their complex interplay and develop more precise, mechanistic interventions that can directly address these core processing failures, moving beyond post-hoc corrections to foundational robustness.",
    "Formal Theoretical Grounding: Inevitability and Mathematical Origins": "\\subsection{Formal Theoretical Grounding: Inevitability and Mathematical Origins}\n\nThe understanding of hallucination in Large Language Models (LLMs) has undergone a profound paradigm shift, transitioning from an empirically driven problem-solving endeavor to a rigorous theoretical exploration of its fundamental limits. While early research primarily focused on identifying practical sources of hallucination and developing engineering-centric mitigation strategies \\cite{xu2024n76}, a groundbreaking line of inquiry has emerged to investigate whether hallucination is merely a transient engineering challenge or an innate, unavoidable characteristic of these powerful models. This theoretical turn seeks to establish a formal grounding for hallucination, moving beyond observational data to mathematical proofs of its inevitability.\n\nA pivotal contribution in this area is presented by \\cite{xu2024n76}, which fundamentally redefines the discourse around LLM hallucination. The paper introduces a \"formal world\" where LLMs are abstracted as total computable functions and ground truth is similarly defined as a computable function. This abstraction allows for the first formal definition of hallucination (Definition 4), characterizing it as an inconsistency between the LLM's output and the ground truth for a given input. This rigorous formalization provides a robust framework for theoretical analysis, independent of specific model architectures or training algorithms.\n\nThe core innovation of \\cite{xu2024n76} lies in its application of advanced concepts from learning theory, specifically Cantor's diagonalization argument, to prove the inherent inevitability of hallucination. This mathematical technique, traditionally used to demonstrate the existence of uncomputable numbers or functions, is ingeniously adapted to show that no computable LLM can perfectly learn all computable functions representing ground truths. The authors present Theorem 1, which demonstrates that for any computably enumerable set of LLMs, all states will inevitably hallucinate on some inputs. This is further extended by Theorem 2, proving that such hallucination will occur on *infinitely many* inputs. Crucially, Theorem 3 generalizes these findings to establish that hallucination is inevitable for *any individual computable LLM*, both on some inputs and on infinitely many inputs. These proofs establish a theoretical upper bound on the capabilities of LLMs, demonstrating that they cannot serve as universal problem solvers without generating factually incorrect information.\n\nThis theoretical insight fundamentally reshapes the understanding of LLM capabilities and limitations. Unlike previous empirical efforts that aimed to *reduce* or *eliminate* hallucination through better data, architectures, or prompting techniques, \\cite{xu2024n76} posits that complete eradication is mathematically impossible for any computable LLM. While parallel work by \\cite{kalai2023statistical} provides a statistical lower bound on hallucination rates for calibrated LLMs, \\cite{xu2024n76}'s findings are more general, applying to all computable LLMs and proving an absolute inevitability rather than a statistical likelihood. This distinction highlights the profound nature of the theoretical limits identified by the diagonalization argument.\n\nConsequently, hallucination is reframed not as a transient engineering problem to be fixed, but as an innate, fundamental characteristic inherent to the very nature of computable functions that LLMs embody. This profound theoretical grounding guides future research away from the elusive goal of achieving \"hallucination-free\" LLMs and towards more realistic and robust strategies for detection, mitigation, and responsible deployment. It underscores the necessity for LLMs to be used as specialized tools, often requiring external verification and human oversight, rather than as infallible general problem solvers. The mathematical origins of hallucination thus compel a shift in focus towards managing its unavoidable presence, fostering a more realistic and effective research agenda for the future of LLM development.",
    "Early Benchmarks and Automated Evaluation": "\\subsection*{Early Benchmarks and Automated Evaluation}\n\nThe initial efforts to evaluate hallucination in Large Language Models (LLMs) were driven by the need for standardized, reproducible benchmarks that could move beyond subjective human judgment. These foundational works primarily focused on assessing factual correctness and the ability to generate text consistent with provided sources or external knowledge, laying crucial groundwork for automated evaluation.\n\nEarly investigations into factual inconsistencies often centered on specific natural language generation tasks. For instance, \\cite{DBLP:journals/corr/abs-2005-14165} introduced a dataset specifically designed for factual error correction in summarization, highlighting the prevalence of factual inconsistencies even in models trained on large corpora. This work underscored the necessity of robust evaluation methods to identify and rectify such errors, often relying on human annotation for ground truth. Recognizing the scalability limitations of human evaluation, researchers soon began developing automated metrics. \\cite{DBLP:journals/corr/abs-2009-07853} proposed a differentiable fact-checking model that could automatically assess the factuality of generated text by comparing it against a structured knowledge base. This marked a significant step towards scalable evaluation, offering an automated alternative to manual verification by leveraging external factual sources.\n\nAs LLMs became more capable and their applications diversified, the scope of factual evaluation expanded to more open-ended generation tasks. \\cite{DBLP:journals/corr/abs-2104-08784} extended the investigation of factuality to neural dialogue response generation, proposing a metric called FactScore and methods to improve factual consistency by grounding responses in external knowledge. This demonstrated an early attempt to tackle factual correctness in conversational contexts, where the absence of a clear source document makes evaluation more challenging. The growing awareness of factual errors also spurred research into mitigation strategies, with \\cite{DBLP:journals/corr/abs-2109-00971} exploring self-correction mechanisms to enhance factual consistency. While not strictly an evaluation paper, it implicitly highlighted the need for sensitive evaluation metrics to guide and measure the effectiveness of such corrective approaches.\n\nFurther refining the understanding and evaluation of factual consistency, \\cite{DBLP:journals/corr/abs-2201-02604} provided a comprehensive review and benchmark for factual consistency in abstractive summarization. This work meticulously categorized different types of factual errors and systematically evaluated various existing metrics, revealing their strengths and weaknesses in capturing the nuances of factual inaccuracies. This comprehensive analysis showcased the increasing sophistication in diagnosing the specific ways LLMs could hallucinate factually.\n\nA pivotal development in challenging the models' \"honesty\" and moving beyond simple factual recall was the introduction of TruthfulQA by \\cite{DBLP:journals/corr/abs-2205-07823}. This benchmark was specifically designed to measure whether LLMs generate truthful answers to questions that people commonly answer incorrectly due to widespread misconceptions. TruthfulQA pushed the boundaries of hallucination evaluation by targeting subtle forms of untruthfulness stemming from learned biases or misrepresentations, rather than just outright fabrication. Complementing these efforts, \\cite{DBLP:journals/corr/abs-2206-04624} presented a systematic evaluation of factual consistency across various LLM tasks and proposed a new metric, FactScore-NLI, based on Natural Language Inference. This approach leveraged the robust capabilities of NLI models to assess the entailment or contradiction between generated text and reference facts, offering a more generalized and linguistically informed automated evaluation paradigm.\n\nWhile these early benchmarks and automated metrics laid crucial groundwork for evaluating factual correctness and consistency, they often faced limitations in capturing the full spectrum of hallucination. Their reliance on external knowledge bases or factual consistency with source documents struggled to address more subtle forms of hallucination, such as plausible but entirely fabricated information in open-ended generation, or errors in reasoning and coherence that did not directly contradict a known fact. These methods, while scalable, often lacked the nuance to fully understand the cognitive processes leading to hallucination, paving the way for more sophisticated and fine-grained evaluation paradigms that consider broader aspects of truthfulness, coherence, and user intent.",
    "Fine-Grained and Rationale-Based Evaluation": "\\subsection*{Fine-Grained and Rationale-Based Evaluation}\n\nTraditional evaluation metrics for Large Language Models (LLMs), often relying on surface-level textual overlap (e.g., ROUGE, BLEU) or simple answer correctness, frequently prove insufficient for comprehensively assessing the nuanced quality and underlying reasoning capabilities of their outputs. These metrics struggle to pinpoint the exact nature and location of errors, particularly hallucinations, or to verify the logical soundness of the model's internal reasoning process. To address this, a significant shift has occurred towards fine-grained and rationale-based evaluation methodologies, which delve deeper into the LLM's reasoning and output fidelity, aiming to provide detailed insights into error types and locations, and to scrutinize the logical coherence of generated rationales. This paradigm shift is crucial for fostering greater transparency and verifiability in LLM behavior.\n\nThe initial advancements in this domain focused on developing more granular methods for identifying and categorizing hallucinations within generated text. Rather than a binary correct/incorrect judgment, these approaches sought to annotate errors at a sentence or even phrase level, distinguishing between factual inaccuracies, logical inconsistencies, or ungrounded statements. This fine-grained annotation provides a richer understanding of where and how LLMs deviate from ground truth, moving beyond aggregate scores to actionable insights into model weaknesses. For instance, some benchmarks explore the impact of reasoning order on LLM consistency, revealing how models might fabricate justifications when asked to provide an answer before its rationale, highlighting a fine-grained flaw in their generation process \\cite{xie20247zk}.\n\nA crucial advancement in this area is the emergence of 'rationale verification' techniques, which scrutinize the intermediate steps an LLM takes to arrive at a conclusion, rather than solely evaluating the final output. This moves beyond merely identifying *what* an LLM gets wrong to understanding *why*, by pinpointing logical missteps in its chain of thought. The most sophisticated iterations of rationale verification leverage structured data to automatically assess logical soundness, offering a scalable and objective alternative to human judgment.\n\nA prominent example of this is \\textit{ERBench}, which utilizes existing relational databases (RDBs) and their inherent Entity-Relationship (ER) model to construct automatically verifiable benchmarks for LLMs \\cite{oh2024xa3}. ERBench leverages database schema, records, and crucially, integrity constraints like Functional Dependencies (FDs) and Foreign Key Constraints (FKCs), to generate complex, multi-hop questions. Its innovation lies in automatically verifying not only the final answer but also the LLM's rationale by checking for the presence of FD-inferred critical keywords within the generated reasoning steps. This allows for a deeper evaluation of factual hallucination and introduces novel metrics such as Rationale Accuracy (R) and Answer-Rationale Accuracy (AR), providing a verifiable audit trail for LLM conclusions by grounding evaluation in external, structured knowledge.\n\nBeyond relational databases, other structured knowledge sources like Knowledge Graphs (KGs) have been instrumental in developing rationale-based evaluation. \\cite{ghosh2024tj5} proposes a framework to assess and improve the logical consistency of Retrieval-Augmented LLMs (RAG) in fact-checking tasks, specifically for propositional logic queries derived from KGs. This work defines quantitative measures for LLM consistency across primitive logical operators (negation, conjunction, disjunction) and complex logical rules, directly evaluating the LLM's adherence to logical soundness within a structured context. Similarly, \\textit{Drowzee} introduces a logic-programming-aided metamorphic testing technique for Fact-Conflicting Hallucination (FCH) detection \\cite{li2024osp}. It constructs a factual knowledge base and generates diverse test cases using logic reasoning rules. By employing semantic-aware metamorphic oracles, Drowzee automatically detects FCHs by comparing the logical and semantic structures of LLM answers against ground truth, thereby validating the reasoning process itself.\n\nThe principle of verifying intermediate reasoning steps extends to specialized domains like mathematical reasoning. \\cite{liu2025juo} introduces a structured self-consistency framework designed to enhance the reliability of mathematical reasoning in LLMs. This method enforces self-consistency not just on final answers but critically across intermediate steps in tasks such as theorem proving, symbolic manipulation, and numerical computation. By ensuring logical consistency throughout the reasoning trajectory, this approach effectively reduces hallucinations and logical inconsistencies, paving the way for more reliable and interpretable AI-driven mathematics.\n\nIn conclusion, the progression from coarse-grained to fine-grained and rationale-based evaluation marks a critical maturation in LLM assessment. These methodologies, particularly those leveraging structured data like relational databases and knowledge graphs, offer unprecedented transparency by allowing for the automatic verification of an LLM's reasoning process. While significant strides have been made in pinpointing errors and verifying reasoning, challenges remain. The construction and maintenance of comprehensive, domain-specific structured knowledge bases can be resource-intensive, limiting scalability to highly complex, open-ended, or multi-hop reasoning tasks where the required knowledge might be vast or ill-defined. Furthermore, models might learn to produce 'verifiable' but ultimately incorrect rationales, highlighting the need for more robust verification mechanisms that are less susceptible to superficial adherence to rules. Future directions will likely focus on developing more adaptive rationale verification systems that can dynamically interact with diverse and evolving knowledge sources, handle ambiguous reasoning paths, and integrate with advanced uncertainty quantification methods to provide a holistic assessment of LLM trustworthiness.",
    "Complex Reasoning and Algorithmic Hallucination": "\\subsection{Complex Reasoning and Algorithmic Hallucination}\n\nEvaluating Large Language Models (LLMs) on tasks that demand genuine algorithmic reasoning and complex problem-solving represents a critical frontier, moving beyond superficial knowledge retrieval or pattern matching to assess the integrity of the entire solution process \\cite{zhang2023k1j}. While early work on hallucination often focused on factual inconsistencies in generative tasks \\cite{maynez2020h3q}, the increasing capabilities of LLMs necessitate benchmarks that probe their ability to perform multi-step logical deductions, execute precise algorithms, and maintain coherence throughout complex reasoning paths. This shift reveals a deeper form of hallucination, rooted in a lack of true algorithmic understanding rather than mere factual error.\n\nA significant area of research focuses on assessing LLMs' logical consistency in structured data and fact-checking scenarios. \\cite{oh2024xa3} introduced ERBench, a benchmark that leverages relational databases and their integrity constraints to generate complex, automatically verifiable questions. Crucially, ERBench not only assesses the final answer but also rigorously verifies the LLM's generated rationales, exposing inconsistencies in the underlying thought process even when the final answer might appear correct. This highlights that LLMs often struggle with maintaining logical coherence across multiple deductive steps. Extending this, \\cite{li2024osp} proposed Drowzee, a framework employing metamorphic testing to evaluate the logical consistency and robustness of LLM rationales in fact-checking. By generating semantically equivalent but syntactically varied inputs based on logic reasoning rules, Drowzee reveals how minor perturbations can expose brittle reasoning and lead to contradictory outputs, underscoring the fragility of LLM logical understanding in complex scenarios. Further, \\cite{ghosh2024tj5} developed a framework to quantitatively assess LLMs' logical consistency in fact-checking, specifically for propositional logic queries derived from Knowledge Graphs. Their work defines measures for consistency across primitive logical operators, complex DNF/CNF facts, and logical rules, demonstrating that LLMs exhibit significant logical inconsistency on such complex queries, even when provided with authoritative context. These studies collectively emphasize the challenge of ensuring logical soundness and verifiable reasoning paths in LLMs.\n\nBeyond structured logical deduction, a more demanding test for LLMs lies in their ability to perform pure algorithmic execution. \\cite{tang2024a1j} developed GraphArena, a benchmark specifically tailored to evaluate LLMs on real-world graph computational problems, including NP-complete tasks, which require precise, step-by-step algorithmic execution rather than heuristic pattern matching. Their findings reveal alarmingly high hallucination rates, where LLMs frequently generate plausible but incorrect intermediate steps or entirely fabricated solutions, demonstrating a profound limitation in their capacity to understand and accurately execute algorithmic instructions. Similarly, in the domain of mathematical reasoning, \\cite{liu2025juo} introduced a structured self-consistency framework to enhance reliability by enforcing consistency across intermediate steps and final outputs in tasks like theorem proving, symbolic manipulation, and numerical computation. Their results indicate that while self-consistency can improve accuracy, LLMs remain susceptible to hallucinations in these precise mathematical and algorithmic contexts, highlighting the difficulty in achieving robust, step-by-step correctness.\n\nThe integrity of the reasoning process itself is further probed by examining the impact of reasoning order. \\cite{xie20247zk} introduced a benchmark demonstrating that the order in which LLMs generate answers and their corresponding reasoning significantly impacts consistency. They found that LLMs often fabricate answers and then retrospectively generate justifications, exposing a fundamental flaw where the reasoning path is constructed to fit a pre-determined (potentially incorrect) answer, rather than the answer being a product of sound reasoning. This \"reasoning order hallucination\" underscores the challenge of ensuring that LLMs genuinely understand and follow logical steps.\n\nThe problem of complex reasoning hallucination also manifests acutely in high-stakes, domain-specific applications. \\cite{umapathi2023puv} introduced Med-HALT, a Medical Domain Hallucination Test, which includes \"Reasoning Hallucination Tests\" (RHTs) such as False Confidence Tests (FCT), None of the Above (NOTA) Tests, and Fake Questions Tests (FQT). These RHTs assess an LLM's ability to reason about complex medical problems, generate logically coherent and factually accurate output, and identify invalid queries without creating fake information. Their findings indicate that even state-of-the-art LLMs perform poorly on these reasoning-based medical tasks, struggling with complex logical inference and exhibiting a tendency to hallucinate with undue certainty, highlighting the severe implications of such failures in critical domains.\n\nIn conclusion, current LLMs exhibit significant limitations in tasks requiring genuine algorithmic reasoning and complex problem-solving. Benchmarks focusing on rationale verification, logical consistency in structured data, pure algorithmic execution, mathematical reasoning, and the integrity of the reasoning process consistently reveal high hallucination rates. These evaluations demonstrate that LLMs struggle with the precise execution, deep understanding, and robust logical coherence necessary for these tasks. The emphasis has decisively shifted from merely assessing final answers to rigorously evaluating the integrity and consistency of the entire solution process. Future research must focus on developing models that can perform verifiable, step-by-step algorithmic execution, moving beyond superficial pattern matching to achieve true algorithmic intelligence and logical soundness.",
    "Long-Context and Dialogue-Level Evaluation": "\\subsection{Long-Context and Dialogue-Level Evaluation}\nEvaluating Large Language Models (LLMs) in extended conversational contexts and with lengthy documents presents distinct and complex challenges. Models are prone to generating hallucinations stemming from an inability to maintain consistent information across multiple turns, misremembering previous dialogue states, or losing factual accuracy and coherence when processing extensive inputs. These specialized evaluations are paramount for developing LLMs that are reliable, consistent, and trustworthy in real-world interactive and document-intensive applications. The evolution of hallucination types, as highlighted by \\cite{qiu2024zyc}, increasingly includes dialogue-level and long-context specific manifestations, necessitating dedicated evaluation paradigms.\n\nA critical area of focus is the assessment of dialogue-level consistency and factual accuracy across multi-turn interactions. Traditional evaluation methods, often focused on single-turn responses, fail to capture the cumulative errors that can emerge in dynamic conversations. To address this, \\cite{chen2024c4k} introduce \\texttt{DiaHalu}, a pioneering benchmark specifically designed to evaluate hallucinations in multi-turn dialogues. \\texttt{DiaHalu} distinguishes between various hallucination subtypes, including non-factual information, incoherence (input-conflicting, context-conflicting, self-conflicting), irrelevance, overreliance, and reasoning errors. Its construction involves LLM self-dialogue generation and expert annotation across diverse domains like knowledge-grounded and task-oriented conversations, making it a challenging and realistic testbed for conversational reliability. However, a limitation of such benchmarks is their reliance on LLM-generated dialogues, which, while efficient, may not fully capture the nuances of human-LLM interaction or introduce biases from the generating LLM itself. Further insights into dialogue-level errors come from studies like \\cite{dziri2021bw9}, which, through human studies, critically analyze the modes of hallucination in dialogue systems, revealing that extrinsic hallucinations, particularly erroneous entity mentions, are prevalent. They also observe that increased response diversity often correlates with increased hallucination, highlighting the necessity for evaluations that can precisely identify and ground specific entities within a conversational flow.\n\nFurthermore, the consistency of an LLM's internal reasoning process across turns is vital. \\cite{xie20247zk} propose a novel benchmark method that assesses LLM consistency by comparing responses generated when reasoning precedes the answer versus when the answer precedes reasoning. This approach is particularly insightful for dialogue evaluation, as it exposes instances where LLMs might fabricate justifications for previously stated, potentially hallucinatory, conclusions, thus revealing logical inconsistencies over an extended reasoning path inherent in multi-turn interactions. This method helps to uncover a deeper form of dialogue hallucination where the model's internal state becomes inconsistent. Extending dialogue evaluation to multimodal contexts, \\cite{cao2024o9a} introduce \\texttt{VisDiaHalBench}, a visual dialogue benchmark for Large Vision-Language Models (LVLMs). This benchmark specifically investigates hallucinations arising from \"long-term misleading textual history\" in visual dialogues, featuring five-turn questions about edited and original images. This highlights the complex interplay between textual context, visual input, and conversational memory in multimodal dialogue hallucination. Conceptually, the Information Quality (IQ) model proposed by \\cite{rejeleene2024okw}, which defines IQ based on consistency, relevance, and accuracy, provides a valuable framework for what comprehensive dialogue evaluations should strive to measure.\n\nBeyond dialogues, the ability of LLMs to process and synthesize information from extensive documents without generating spurious details or losing track of relevant facts is a major challenge. A foundational and widely adopted method for probing long-context factual recall is the \"Needle In A Haystack\" (NIAH) test. This technique involves embedding a specific, verifiable piece of information (the \"needle\") within a much longer, often irrelevant document (the \"haystack\") and then querying the LLM to retrieve that information. The performance on NIAH tests directly measures an LLM's capacity to maintain attention and extract precise facts from lengthy inputs, revealing how context length impacts factual grounding. While not a formal benchmark suite, NIAH has become a de facto standard for demonstrating an LLM's effective context window and susceptibility to 'getting lost' in irrelevant details. Variants of NIAH, such as those testing multiple needles or needles at different positions, have revealed crucial insights, like the \"lost in the middle\" phenomenon where LLMs often perform worse on information located in the middle of a long context \\cite{liu2024lost}. However, a critical limitation of NIAH is its focus on *retrieval* rather than *synthesis* or complex reasoning, and its artificial nature may not fully reflect real-world document processing challenges.\n\nTo address the more complex task of long-document *synthesis*, particularly in abstractive summarization, \\cite{maynez2020h3q} provided a foundational human evaluation and taxonomy of hallucinations, distinguishing between intrinsic (misrepresenting source) and extrinsic (adding ungrounded information) hallucinations. They demonstrated that traditional metrics like ROUGE correlate poorly with human judgments of faithfulness, advocating for semantically-aware metrics like textual entailment. This work underscores the need for evaluations that go beyond surface-level metrics to assess deep semantic consistency with long source documents. Similarly, \\cite{hegselmann20249q4} developed a rigorous labeling protocol for errors and an annotated dataset of hallucinations in long patient summaries, highlighting domain-specific challenges in medical text generation and the need for high-fidelity evaluation in sensitive applications.\n\nFurthermore, \\cite{qiu2024zyc} introduce \\texttt{LongHalQA}, an LLM-free benchmark for evaluating long-context hallucinations in Multimodal Large Language Models (MLLMs). This benchmark features 6K long and complex hallucination texts across object-level descriptions, image-level descriptions, and multi-round conversations. It employs novel \"Hallucination Discrimination\" and \"Hallucination Completion\" tasks, framed as multiple-choice questions, to efficiently assess MLLMs' ability to identify and avoid generating hallucinations in lengthy outputs. \\texttt{LongHalQA}'s coverage of 12 distinct hallucination types, including complex logical and contextual inconsistencies, represents a significant step beyond simple factual checks. The survey by \\cite{sahoo2024hcb} further emphasizes the challenge, noting the absence of standardized metrics for assessing object hallucination in LVLMs, particularly relevant in long multimodal contexts. For evaluating faithfulness in complex, multi-source question answering, which often involves synthesizing information from long contexts, \\cite{pan2024hm4} propose a \"multi-reference faith score (MRFS)\" to verify and resolve conflicts in generated answers, indicating a move towards more robust, verifiable evaluation metrics for long-form generation. The broader review by \\cite{malin2024fin} reinforces that evaluating open-ended generation provides a more comprehensive measure of LLM performance than commonly used multiple-choice benchmarking, which is crucial for assessing faithfulness in long-context tasks.\n\nIn conclusion, the evaluation of long-context and dialogue-level hallucinations necessitates a shift from isolated factual checks to comprehensive assessments of consistency, coherence, and factual accuracy across extended interactions and lengthy inputs. Benchmarks like \\texttt{DiaHalu} and \\texttt{VisDiaHalBench} provide critical tools for understanding dialogue-level errors, while methods like the NIAH test, when critically understood for its retrieval focus, and the synthesis-oriented evaluations for summarization \\cite{maynez2020h3q} and multimodal long-context generation \\cite{qiu2024zyc} are indispensable for evaluating an LLM's ability to remain grounded in long documents and maintain coherence over extended generations. While significant progress has been made, challenges persist in developing scalable, fine-grained evaluation techniques for truly massive contexts and dynamically assessing the nuanced consistency required for highly interactive, multi-agent conversational systems. Future research will likely integrate intrinsic detection mechanisms with comprehensive external validation to build more robust and trustworthy LLMs for complex, real-world applications.",
    "Retrieval-Augmented Generation (RAG) Fundamentals": "\\subsection{Retrieval-Augmented Generation (RAG) Fundamentals}\n\nLarge Language Models (LLMs) have demonstrated remarkable capabilities in generating human-like text, yet they frequently suffer from issues of hallucination, generating factually incorrect or unfaithful content, and are constrained by the static, often outdated, knowledge encoded during their pre-training \\cite{Lewis2020}. Retrieval-Augmented Generation (RAG) emerged as a pivotal strategy to mitigate these limitations by dynamically grounding LLM responses in up-to-date and authoritative external knowledge sources. The core concept of RAG involves augmenting the LLM's generation process with a retrieval step, where relevant information is fetched from a vast corpus of documents and then provided to the LLM as context for generating its response, thereby reducing the generation of unfaithful or outdated content.\n\nThe foundational paradigm, often referred to as 'Naive RAG', typically combines a retriever component with a generator component. \\textcite{Lewis2020} introduced Retrieval-Augmented Generation (RAG), a general-purpose fine-tuning approach that integrates a pre-trained parametric memory (the generator, a seq2seq model) with a non-parametric memory (the retriever, a Dense Passage Retriever or DPR). This architecture dynamically retrieves relevant documents from a large corpus and conditions the generator's output on these retrieved passages, demonstrating state-of-the-art results on knowledge-intensive NLP tasks like open-domain question answering and fact verification. Simultaneously, \\textcite{Guu2020} proposed REALM (Retrieval-Augmented Language Model Pre-training), which explored a deeper integration by learning to retrieve documents from a large corpus and use them to augment a language model's input during pre-training. REALM showcased that jointly training the retriever and the language model end-to-end could significantly improve performance on knowledge-intensive tasks, highlighting the benefits of building retrieval capabilities directly into the model's knowledge acquisition process.\n\nThese early works established the immense promise of RAG, demonstrating that external knowledge retrieval could significantly enhance the factual accuracy and currency of LLM outputs. Building upon this foundation, \\textcite{Izacard2022} introduced Atlas, a retrieval-augmented language model that leverages a frozen pre-trained T5 model and a jointly trained retriever. Atlas demonstrated that even relatively smaller language models (e.g., 11B parameters) could achieve competitive performance with much larger, purely parametric LLMs on knowledge-intensive tasks when effectively augmented with retrieval, underscoring the efficiency benefits and the importance of well-trained retriever-generator interactions.\n\nHowever, these initial RAG paradigms often assumed full access to the language model's internal states or gradients for joint training or fine-tuning, posing a significant challenge for proprietary or black-box LLMs. Addressing this practical limitation, \\textcite{Shi2023} proposed REPLUG, a novel method designed to train a retriever to work effectively with black-box language models without requiring access to their internal states or gradients. REPLUG trains the retriever by maximizing the likelihood of the LLM's output given the retrieved documents, employing a contrastive learning objective, thus enabling the application of RAG to a broader range of LLMs, including those deployed as APIs. This progression from tightly coupled, white-box RAG systems to more flexible, black-box compatible approaches highlights the evolving understanding of RAG's practical deployment challenges and the innovative solutions developed to overcome them. The foundational understanding of RAG, from its initial promise to its early architectural challenges and solutions, sets the stage for appreciating the subsequent advancements and refinements in RAG architectures designed to further enhance retrieval quality, generation coherence, and overall system robustness.",
    "Advanced and Adaptive RAG Architectures": "\\subsection*{Advanced and Adaptive RAG Architectures}\n\nThe foundational Retrieval-Augmented Generation (RAG) paradigm, while effective, often operates with a static retrieval mechanism that can be inefficient, imprecise, or fail to fully leverage the capabilities of Large Language Models (LLMs). This has spurred the development of advanced and adaptive RAG architectures, focusing on sophisticated, modular designs that empower LLMs with greater control over the retrieval process, leading to more intelligent and targeted information synthesis. These innovations aim to significantly improve RAG's efficiency, precision, and robustness, particularly in mitigating hallucinations.\n\nEarly conceptualizations of these advanced paradigms were laid out by comprehensive surveys, such as that by \\cite{gao2023retrieval}, which categorized RAG into basic, advanced, and modular forms, highlighting the need for more sophisticated retrieval and generation strategies to overcome limitations like hallucination and outdated information. Similarly, \\cite{zhang2024retriever} further elaborated on this evolution, detailing various components like pre-retrieval, retrieval, post-retrieval, and generation, underscoring the shift towards more dynamic and integrated RAG systems. Within this evolving landscape, initial efforts focused on enhancing the LLM's ability to process and utilize retrieved information more effectively. For instance, \\cite{yu2023chainofthought} demonstrated how Chain-of-Thought (CoT) reasoning can be integrated into RAG, guiding the LLM to generate intermediate thoughts that improve the utilization of retrieved documents and consequently reduce hallucinations by fostering a deeper understanding of the context.\n\nMoving beyond static retrieval, subsequent research introduced mechanisms for the LLM to actively influence the retrieval process itself. \\cite{shi2023replug} proposed RePlug, a framework that enables LLMs to self-refine their retrieval queries and generated answers. By using internal feedback, RePlug iteratively improves the relevance of retrieved documents, making the retrieval process more adaptive and responsive to the LLM's evolving information needs. This marked a significant step towards adaptive RAG, where the LLM is no longer a passive consumer but an active participant in shaping its information landscape.\n\nThe concept of \"Modular RAG\" further extends this adaptivity by integrating RAG into broader, multi-step reasoning frameworks. \\cite{yang2023ragagents} introduced RAG-Agents, a framework that combines RAG with autonomous agents, allowing LLMs to tackle complex tasks by breaking them down, planning, executing actions (including strategic retrieval), and self-correcting. This approach transforms RAG into a specialized tool within an agentic workflow, enabling intelligent decision-making about *when* and *what* to retrieve in a multi-step reasoning process, thereby enhancing its utility for complex problem-solving.\n\nA pinnacle of adaptive RAG is exemplified by architectures that grant LLMs the autonomy to decide when and what to retrieve based on their internal state. \\cite{wang2023selfrag} introduced Self-RAG, an LLM framework that learns to retrieve, generate, and critique through self-reflection. This innovative approach allows the LLM to generate \"reflection tokens\" that guide its own retrieval and generation process, enabling it to selectively retrieve information when its internal uncertainty is high or external knowledge is required. Furthermore, Self-RAG incorporates dynamic context highlighting by allowing the LLM to critique its own output, ensuring that the generated response is factual and well-supported by the retrieved documents, thus embodying a highly intelligent and targeted solution for hallucination mitigation.\n\nIn conclusion, the evolution of RAG architectures showcases a clear trajectory towards more intelligent, autonomous, and modular systems. While significant progress has been made in enabling selective retrieval, dynamic context highlighting, and iterative refinement, challenges remain. These include the computational overhead of iterative processes, the complexity of fine-tuning sophisticated feedback loops, and ensuring the generalizability of self-reflection mechanisms across diverse domains. Future directions will likely focus on developing more efficient uncertainty estimation techniques, extending adaptive RAG to multi-modal contexts, and achieving real-time adaptation in dynamic information environments.",
    "Knowledge Graph Integration for Trustworthiness": "\\subsection{Knowledge Graph Integration for Trustworthiness}\n\nLarge Language Models (LLMs) often suffer from factual inaccuracies and logical inconsistencies, commonly referred to as hallucinations, which severely undermine their trustworthiness. Strategic integration of structured Knowledge Graphs (KGs) with LLMs offers a robust solution by providing a reliable, up-to-date, and verifiable source of information, thereby serving as a powerful grounding mechanism against such issues.\n\nEarly efforts to mitigate LLM hallucinations recognized the critical need for external knowledge to ground their responses. For instance, \\cite{trivedi2022qsf} demonstrated that interleaving retrieval with Chain-of-Thought (CoT) reasoning could significantly improve factual accuracy in knowledge-intensive multi-step questions. Their IRCoT framework dynamically uses intermediate CoT steps to generate context-aware queries for retrieving relevant paragraphs, grounding the reasoning process and reducing factual errors by up to 50\\%. However, these foundational approaches often relied on unstructured text retrieval, which, while effective, could still introduce noise or outdated information, highlighting the need for more structured and verifiable knowledge sources.\n\nBuilding upon the broader Retrieval-Augmented Generation (RAG) paradigm, which generally involves fetching relevant information from an external corpus to inform LLM responses \\cite{gao20232zb}, recent research has increasingly focused on leveraging the inherent structure and verifiability of Knowledge Graphs. \\cite{sui20242u1} directly addresses how KGs can enhance LLM trustworthiness by proposing a unified framework that combines \"Graph-guided retrieval\" and \"Graph-guided generation.\" This approach enables LLMs to query and integrate structured facts from KGs, leading to more accurate and logically consistent answers in open-ended question answering tasks, and introduces the OKGQA benchmark to evaluate such KG-augmented models, even under perturbed KG conditions. This represents a significant step beyond generic text retrieval by providing a rich, semantically structured context that is less prone to misinterpretation or hallucination.\n\nBeyond mere factual accuracy, trustworthiness also encompasses logical consistency, especially in complex reasoning tasks. While LLMs can struggle with maintaining logical coherence, KGs, by their very nature, encode explicit relationships and constraints that can be leveraged for verification. \\cite{ghosh2024tj5} emphasizes the importance of evaluating LLM logical consistency in fact-checking, proposing new logical fact-checking (LFC) datasets and quantitative measures to assess their performance on complex propositional logic queries. Although this work does not explicitly integrate KGs, the structured nature of KGs makes them an ideal candidate for providing the ground truth and relational context necessary to enforce and verify such logical consistency in LLM outputs. Further enhancing verifiability, \\cite{oh2024xa3} introduced ERBench, a benchmark that utilizes relational databases (conceptually akin to KGs in their structured representation of entities and relationships) to automatically verify not only the LLM's final answers but, crucially, its *rationales*. This capability to scrutinize the reasoning path against structured knowledge is paramount for building truly transparent and trustworthy AI systems, moving beyond simple output correctness to verifiable logical soundness.\n\nIn conclusion, the integration of Knowledge Graphs represents a pivotal advancement in addressing the trustworthiness challenges of LLMs. By providing a structured, verifiable, and logically consistent source of external knowledge, KGs enable LLMs to move beyond mere fluency to produce responses that are factually accurate, logically sound, and inherently more reliable. This strategic integration, encompassing graph-guided retrieval, generation, and rationale verification, lays a robust foundation for developing next-generation LLMs that are not only powerful but also transparent and trustworthy in their knowledge-intensive and complex reasoning capabilities. Ongoing challenges include the scalability of KG construction and maintenance, and the seamless, real-time integration of dynamic KGs with evolving LLM architectures.",
    "Decoding-Time Interventions and Contrastive Methods": "\\subsection*{Decoding-Time Interventions and Contrastive Methods}\n\nLarge Language Models (LLMs) and Large Vision-Language Models (LVLMs) frequently generate \"hallucinations\"â€”plausible but factually incorrect or ungrounded information. Addressing this critical challenge, decoding-time interventions offer an efficient, training-free paradigm for the primary model to steer generation towards more factual and grounded outputs by manipulating the probability distribution of generated tokens during inference. These methods operate by introducing a \"negative\" signal or bias that discourages the generation of ungrounded content, without requiring extensive fine-tuning of the main model itself.\n\nOne prominent approach in this domain is \\textbf{Visual Contrastive Decoding (VCD)}, as proposed by \\cite{park20247cm}. VCD mitigates hallucination, particularly in LVLMs, by penalizing tokens that are disproportionately favored when the visual input is subtly distorted or perturbed. The core mechanism involves comparing the logit distribution generated from the original input with that from a slightly altered, \"negative\" input (e.g., an image with minor noise or a masked object). By identifying tokens whose probabilities increase significantly under these subtle perturbations, VCD infers their sensitivity to ungrounded visual cues and suppresses them. This method leverages the model's internal representations to identify and counteract potential hallucinatory tendencies without requiring any additional training of the main LVLM. VCD has demonstrated effectiveness in reducing attribute and object hallucinations by steering the model towards more robust and visually grounded descriptions, often outperforming standard decoding strategies in terms of factual consistency. Its strength lies in its training-free nature and generalizability across different LVLMs, though its efficacy can be sensitive to the choice and strength of the perturbation.\n\nFurther enhancing the grounding aspect, other methods integrate external modalities to provide a strong, verifiable signal for contrastive decoding. For instance, approaches like the \\textbf{Image-Grounded Guidance} proposed by \\cite{zhao2024ge8} for LVLMs leverage pre-trained external vision models to establish a factual baseline. This method generates a reference caption for an input image using a robust image captioning model. During the LVLM's decoding process, the generated tokens are continuously compared against this reference. If the LVLM generates tokens corresponding to objects or attributes not present in the reference caption, these tokens are penalized by adjusting their logits downwards. This effectively guides the LVLM to generate text that is consistent with the external visual understanding, mitigating object and attribute hallucinations. While this approach is training-free for the main LVLM, it introduces computational overhead due to the inference required by the external image captioning model. However, it offers a powerful mechanism for ensuring strong visual grounding, particularly for object-centric hallucinations, by explicitly incorporating external, verifiable visual information into the decoding process.\n\nComplementing these sophisticated contrastive techniques are simpler, yet often effective, \\textbf{logit-based interventions}. Works such as \\cite{leng2023ohr} demonstrate that direct manipulation of token logits can counter specific linguistic biases or common hallucination patterns. These methods involve applying predefined biases or penalties to certain token probabilities based on external knowledge, semantic categories, or simple heuristics. For example, if a model frequently hallucinates specific entities in a given context, their logits can be slightly suppressed. \\cite{leng2023ohr} specifically proposes a logit-based calibration method that adjusts token probabilities by considering their frequency in factual vs. non-factual contexts, effectively down-weighting tokens associated with higher hallucination rates. These interventions are computationally light and offer a direct way to address specific, recurring types of hallucinations or linguistic tendencies that lead to ungrounded outputs. However, their generalizability is often limited, as they require prior identification of problematic tokens or patterns and may not adapt well to novel hallucination types.\n\nIn synthesis, decoding-time interventions offer a versatile and efficient suite of strategies. VCD (\\cite{park20247cm}) provides a general, training-free mechanism to enhance robustness against subtle input variations, making it effective for reducing attribute and object hallucinations by promoting consistency. Image-Grounded Guidance (\\cite{zhao2024ge8}) offers a more explicit form of external grounding for multimodal models, leveraging an auxiliary vision model to provide a strong factual anchor, albeit with increased inference cost. Logit-based calibration (\\cite{leng2023ohr}) represents the most lightweight approach, directly adjusting token probabilities based on observed biases, offering immediate impact for known issues but requiring careful design to avoid over-suppression or limiting creativity.\n\nDespite their efficiency and training-free nature for the primary model, challenges remain. A key difficulty lies in defining universally effective \"negative\" signals or biases that reliably identify and suppress hallucinations without inadvertently stifling creativity or factual nuance. The computational overhead of integrating external models for guidance, as seen in image-grounded approaches, can also be a practical limitation. Furthermore, ensuring that these interventions do not introduce new biases or degenerate the quality of non-hallucinatory content requires careful calibration. Future directions may involve developing more adaptive and dynamic intervention strengths, perhaps guided by real-time uncertainty estimation within the LLM, or exploring hybrid approaches that combine the general robustness of VCD with the explicit grounding of external knowledge and the targeted precision of logit-based adjustments. The goal is to create a robust, yet flexible, decoding framework that can dynamically balance factual accuracy with fluency and coherence across diverse generation tasks.",
    "Internal State Manipulation and Forward-Pass Interventions": "\\subsection{Internal State Manipulation and Forward-Pass Interventions}\n\nTo effectively mitigate hallucination and enhance grounding in large language models (LLMs), advanced strategies move beyond external prompting to directly intervene within the model's internal processing during the forward pass. These methods leverage a deeper, mechanistic understanding of LLM behavior, allowing for more precise and less intrusive corrections by manipulating internal states, attention mechanisms, and feature representations. This approach contrasts with decoding-time interventions by operating earlier in the generation pipeline, often influencing the very construction of hidden states and attention patterns that precede token prediction.\n\nOne family of interventions focuses on dynamically adjusting feature representations based on model uncertainty or signal strength. \\citet{zou2024dp7} introduced **Memory-Space Visual Retracing (MemVR)**, a reactive strategy primarily for multimodal LLMs. MemVR addresses visual grounding issues by monitoring the LLM's internal uncertainty, often quantified through the entropy of token probabilities or disagreement across multiple decoding paths. When high uncertainty is detected, MemVR reactively re-injects relevant visual tokens or their representations into intermediate layers of the model. This re-grounding aims to provide the model with a fresh, reinforced visual context precisely when it is most prone to hallucination due to visual amnesia or insufficient attention. Complementing this, \\citet{yin2025s2b} proposed **Visual Amplification Fusion (VAF)**, a more proactive strategy. VAF addresses the problem of insufficient or diluted visual signals in deeper layers of the LLM by actively enhancing and amplifying visual feature representations within specific middle layers. This proactive boosting, often achieved through learned scaling factors or specialized fusion modules, helps prevent the early decay of visual information, ensuring that the model maintains sufficient visual attention throughout the forward pass. While MemVR is reactive and uncertainty-driven, VAF is proactive, tackling the \"modality gap\" by ensuring visual signals remain salient.\n\nBeyond general feature manipulation, research has increasingly focused on granular control over the model's attention mechanisms, which are critical for integrating information across modalities and contexts. \\citet{chen2024j0g} explored **Targeted Attention Head Interventions for Cross-Level Visual Focus**. This work delves into identifying specific attention heads crucial for integrating visual information across varying levels of abstraction, from low-level features to high-level concepts. By applying interventions, such as re-weighting attention scores or fine-tuning specific head parameters during the forward pass, this method aims to improve the model's ability to maintain coherent visual focus and prevent modality collapse or misinterpretation. The challenge lies in identifying these critical heads and ensuring interventions generalize across diverse inputs.\n\nFurther advancing this mechanistic understanding with theoretical rigor, \\citet{zhou2024lvp} introduced **CAUSAL MM**, a causal inference framework for **Modality Prior Balancing in Multimodal LLMs**. This approach employs structural causal modeling (SCM) to analyze and adjust the influence of different modalities within the attention mechanisms. By treating modality priors (visual and language) as confounding factors in the causal path between attention and output, CAUSAL MM uses techniques like back-door adjustment and counterfactual reasoning. This allows for deciphering the true causal impact of effective attention on MLLM output by isolating and mitigating the effects of misleading modality priors. For instance, by simulating counterfactual attention states (e.g., uniform or shuffled attention), the framework quantifies how much a specific modality's prior causally influences the output, then adjusts attention to achieve a more balanced integration. This provides a principled, theoretically grounded way to enhance visual grounding and reduce hallucination, offering a more robust alternative to empirical attention head interventions.\n\nAnother innovative forward-pass intervention, particularly for contextual hallucinations, is the **Lookback Lens** proposed by \\citet{chuang20248ey}. This method introduces a \"lookback ratio\" derived from attention maps, which quantifies the proportion of attention weights focused on the input context versus newly generated tokens. This interpretable feature serves as a signal for contextual faithfulness. During generation, a \"Lookback Lens Guided Decoding\" strategy samples multiple candidate text chunks. For each chunk, its lookback ratio features are computed and scored by a lightweight classifier. The chunk predicted to be least hallucinated (i.e., most grounded in the context) is then selected. This approach dynamically steers generation by leveraging attention patterns, demonstrating strong cross-task and cross-model transferability due to its reliance solely on attention maps, which are hypothesized to be more generalizable indicators of contextual faithfulness than raw hidden states.\n\nFinally, \\citet{jin2024jpw} introduced a **Collaborative Decoding Framework** that represents a different form of internal state manipulation. Recognizing that pretrained models often retain higher factual accuracy while finetuned models excel in instruction following but may hallucinate more, this framework dynamically decides which model to use for the next token. A \"critical token classifier,\" trained to identify tokens where factual accuracy is paramount, dictates whether the pretrained model or the finetuned model should generate the subsequent token. This allows the system to harness the factuality of the pretrained model for critical information while benefiting from the fluency and instruction-following of the finetuned model for general generation. This method intervenes at a higher level, dynamically routing internal processing based on the perceived \"criticality\" of the next token, effectively balancing competing objectives during the forward pass.\n\nThese forward-pass interventions collectively represent a significant shift towards a deeper, mechanistic understanding of LLM behavior, enabling more precise and less intrusive corrections. While methods like CAUSAL MM offer theoretical grounding and principled adjustments, they can be computationally intensive due to counterfactual simulations. Heuristic approaches like VAF or targeted attention head interventions (\\cite{chen2024j0g}) might be simpler to implement but may lack the same level of theoretical guarantees or generalizability. Lookback Lens (\\cite{chuang20248ey}) provides an interpretable signal for contextual grounding, but its guided decoding introduces computational overhead from candidate sampling. Collaborative decoding (\\cite{jin2024jpw}) offers an intriguing way to leverage different internal knowledge states, but its effectiveness depends on the accuracy of the critical token classifier and the alignment of the base models. Challenges remain in generalizing identified intervention points across diverse model architectures and tasks, as well as in developing more comprehensive theoretical frameworks to guide the optimal application of these internal state manipulations, ensuring both efficacy and efficiency in real-time.",
    "Self-Correction and Abstention Mechanisms": "\\subsection*{Self-Correction and Abstention Mechanisms}\n\nThe pursuit of reliable and trustworthy Large Language Models (LLMs) necessitates equipping them with the intrinsic capabilities to detect and rectify their own errors, as well as to proactively abstain from generating responses when faced with high uncertainty. This subsection delves into the sophisticated strategies that empower LLMs with these self-aware mechanisms, transforming them from mere text generators into more reflective and judicious agents. As surveyed by \\cite{pan2024y3a}, automated correction strategies are broadly categorized, with self-correction and abstention representing crucial generation-time and post-hoc interventions.\n\nThe foundation of LLM self-correction lies in enhancing their reasoning capabilities. Early advancements like Chain-of-Thought (CoT) prompting \\cite{wei2022chainofthought, kadavath2022language} were pivotal, enabling LLMs to articulate intermediate reasoning steps. By externalizing their thought process, models could expose potential logical flaws, laying the groundwork for subsequent self-reflection. Building on this, more advanced frameworks emerged to facilitate iterative and exploratory reasoning. ReAct (Reasoning and Acting) \\cite{yao20229uz} integrates CoT with external tool use, allowing LLMs to interleave reasoning steps with actions (e.g., searching external knowledge bases or executing code). This iterative cycle of thought, action, and observation provides a powerful mechanism for self-correction by verifying internal reasoning against external facts and refining plans based on observed outcomes, thereby directly connecting to external knowledge grounding discussed in Section 5. Tree of Thoughts (ToT) \\cite{yao2023treeofthoughts} further extends this by exploring multiple reasoning paths, evaluating intermediate states, and backtracking when a path proves unfruitful. This search-based approach allows LLMs to engage in more complex problem-solving and to self-correct by identifying and discarding inconsistent or incorrect lines of reasoning.\n\nBeyond these foundational reasoning paradigms, explicit self-correction frameworks have been developed. The \\textit{Self-Refine} framework \\cite{madaan2023selfrefine} exemplifies an iterative generate-critique-refine loop. An LLM first generates an initial output, then critically reflects on it by generating self-feedback to identify potential errors or areas for improvement, and finally uses this self-generated critique to produce a refined response. This internal feedback loop significantly enhances output quality and accuracy. Another powerful approach is Chain-of-Verification (CoVe) \\cite{dhuliawala2023rqn}, which systematically reduces factual hallucination. CoVe operates in multiple steps: generating a baseline response, planning specific verification questions based on the claims in the response, executing these verifications (often independently to prevent error propagation), and finally generating a revised, verified response. The \"factored\" variant of CoVe, where each verification question is answered without conditioning on the potentially hallucinated baseline, is particularly effective in minimizing the LLM's tendency to repeat its own errors. Furthermore, self-consistency \\cite{wang2022selfconsistency} has proven effective, especially in mathematical reasoning \\cite{liu2025juo}. This technique involves prompting the LLM to generate multiple diverse reasoning paths and corresponding answers, then selecting the most frequent or consistent answer, thereby leveraging the model's own internal agreement as a form of self-correction.\n\nConcurrently with self-correction, the critical need for LLMs to express \"I don't know\" has driven the development of robust abstention mechanisms to mitigate hallucination and overconfidence. Early abstention methods often relied on calibrating the model's predicted probabilities or confidence scores \\cite{lin2022calibrated}. These techniques typically involved post-hoc adjustments like temperature scaling to determine when a model should refrain from answering. However, such methods frequently required auxiliary models, ground truth data for effective calibration, or suffered from miscalibration, limiting their robustness and true \"label-free\" nature.\n\nA significant innovation has been the development of label-free abstention mechanisms that quantify uncertainty intrinsically. \\cite{kuhn2023semantic} introduced 'semantic entropy' as a novel measure of uncertainty. This approach quantifies the diversity and plausibility of semantically distinct alternative outputs for a given query. A high semantic entropy indicates a lack of a single, clear, and confident answer, prompting the model to abstain. This provides a more robust and inherent way for LLMs to recognize their limitations without external labels. Further advancements include token-level uncertainty quantification, such as Claim Conditioned Probability (CCP) \\cite{fadeeva2024lt8}, which measures the uncertainty of a particular claim value expressed by the model, enabling fine-grained fact-checking and highlighting specific unreliable segments. Similarly, \\cite{ling2024hqv} proposed methods to quantify both aleatoric (data-inherent) and epistemic (model-inherent) uncertainties in in-context learning, offering a deeper understanding of the sources of LLM uncertainty for more informed abstention.\n\nUltimately, the most robust systems integrate both self-correction and abstention capabilities. LLMs can first attempt to self-correct their responses using iterative refinement or verification frameworks. If, after this refinement process, significant uncertainty persists (quantified by metrics like semantic entropy or token-level uncertainty), the model can then judiciously choose to abstain. This synergistic strategy ensures that models actively strive to improve their answers while also possessing the crucial self-awareness to decline answering when truly uncertain, thereby enhancing overall reliability and transparency. In safety-critical applications, this translates into \"semantic guardrails\" \\cite{hakim2024d4u}, which are designed to prevent \"never event\" errors by combining internal uncertainty quantification with external consistency checks, effectively forcing abstention or flagging for human review when high-stakes factual accuracy cannot be guaranteed.\n\nDespite these advancements, significant challenges persist. The computational cost associated with extensive reflection, iterative refinement, and generating multiple diverse outputs for uncertainty quantification can be substantial, particularly for real-time applications. Defining and universally applying robust \"semantic plausibility\" for uncertainty quantification across diverse and open-ended domains remains an active research area. Moreover, while frameworks like ReAct integrate external tools, the optimal balance between internal self-reflection and external knowledge verification needs further exploration. Future directions include developing more adaptive and context-aware self-correction mechanisms, refining uncertainty quantification to be more robust and interpretable across all types of LLM tasks, and exploring hybrid approaches that dynamically combine internal reasoning with external grounding to achieve both high accuracy and appropriate humility.",
    "Training-Based Approaches and Automated Data Generation": "\\subsection*{Training-Based Approaches and Automated Data Generation}\n\nTraining-based approaches, encompassing fine-tuning and unlearning, represent a direct and potent strategy for mitigating hallucinations in large language models (LLMs) and multimodal large language models (MLLMs) \\cite{sahoo2024hcb, zhang2023k1j, liu2024p39}. A central impediment to their scalability and effectiveness, however, is the prohibitive cost and scarcity of high-quality, labeled data, particularly for diverse and nuanced hallucination types \\cite{cao2023ecl, li2025qzg}. Recent research has thus heavily focused on innovative automated data generation techniques to circumvent this data bottleneck, enabling more targeted and efficient model interventions.\n\nOne direct approach to address data scarcity for hallucination detection and mitigation is the automated generation of datasets by leveraging existing knowledge. AutoHall \\cite{cao2023ecl} proposes a three-step pipeline to automatically construct model-specific hallucination datasets from existing fact-checking resources. By prompting an LLM to generate references for claims, classifying their support, and then flagging contradictions with ground truth, AutoHall efficiently creates labeled hallucinatory examples. This method eliminates laborious manual annotation, making it scalable for continuous model updates and specific hallucination patterns. While effective for text-based factuality, AutoHall's reliance on pre-existing fact-checking resources may limit the diversity of generated hallucinations and risks inheriting their topical biases, potentially failing to uncover novel or subtle hallucination types that are not yet documented.\n\nBuilding on the principle of generating adversarial examples, several methods leverage auxiliary models or controlled processes to create dispreferred, hallucinatory content for training. Hallucination-Induced Optimization (HIO) \\cite{chen20247jb} exemplifies this by training an \"Evil LVLM\" specifically to generate adversarial, hallucinated examples given an image and a prompt. This \"Evil LVLM\" is optimized using a Contrary Bradley-Terry Model (CBTM) to *prioritize* hallucinatory content, effectively amplifying a diverse set of potential visual and factual inconsistencies. These meticulously crafted hallucinatory outputs then serve as negative examples to train a \"Good LVLM\" via contrastive decoding (as discussed further in Section 6.1), thereby enhancing its robustness. A similar concept is explored in Induce-then-Contrast Decoding (ICD) \\cite{zhang202396g} for LLMs, which constructs a \"factually weak LLM\" by fine-tuning it on non-factual samples generated by converting factual samples into untruthful ones. During inference, the log probabilities of these induced hallucinations from the weak model are subtracted from the original model's predictions, penalizing untruthful tokens. Further extending this, \\textit{VHTest} \\cite{huang20247wn} introduces an adversarial generation paradigm for visual hallucination (VH) in MLLMs. It systematically creates new, diverse, and uncontaminated VH images using text-to-image models (e.g., DALLÂ·E-3), guided by MLLM-generated descriptions of hallucination modes. This approach allows for the construction of robust benchmarks and subsequent fine-tuning to mitigate specific VH types like object existence, shape, and size. While these generative approaches offer greater diversity and novelty in adversarial examples compared to AutoHall, they introduce their own challenges, such as the computational cost of training auxiliary \"evil\" or \"weak\" models and the risk that the generated \"bad\" data might still be stereotypical or lack the subtle nuances of real-world hallucinations, potentially introducing new biases rather than creating truly generalizable improvements \\cite{yin2024iau}.\n\nThe broader paradigm of preference optimization, including Reinforcement Learning from Human Feedback (RLHF) and Direct Preference Optimization (DPO), has proven highly effective in aligning LLMs with human preferences. A critical component of these methods is the availability of high-quality preference pairs, especially dispreferred (negative) responses. Preference Optimization in VLLM (POVID) \\cite{zhou2024wbi} addresses this by automating the creation of dispreferred responses using an AI model. This method is particularly effective in reducing object hallucination in VLLMs by providing abundant, automatically generated examples of incorrect or misleading descriptions, allowing the model to learn preferred, factually accurate outputs more efficiently. Extending this, Hallucination-targeted Direct Preference Optimization (HDPO) \\cite{fu2024yqj} specifically constructs preference pair data designed to target three distinct causes of MLLM hallucinations: visual distracted hallucination, long context hallucination, and multimodal conflict hallucination. HDPO's innovation lies in its novel data construction strategies, such as generating negative samples by preserving only low-attention visual tokens or by prompting the MLLM with conflicting information, thereby guiding the model to learn robust alignment against diverse hallucination types. Further advancing judge-free self-improvement, Deng et al. \\cite{deng202405j} propose a framework that generates both positive and negative response candidates by introducing a \"hallucination ratio\" during decoding, blending conditional and unconditional token distributions. These generated pairs are then verified by a lightweight, objective verifier (e.g., a CLIP model) to ensure data quality, significantly reducing the computational costs and biases associated with MLLM-as-judge approaches in traditional RLHF/DPO pipelines. A critical consideration for these preference optimization methods is the reliability of the AI model used to generate dispreferred responses or act as a verifier; if the AI itself is prone to biases or errors, it could inadvertently reinforce undesirable behaviors or generate suboptimal training signals.\n\nAddressing a specific, yet globally relevant, hallucination challenge, Multilingual Hallucination Removal (MHR) \\cite{qu20240f7} tackles the problem of significantly more severe hallucination in Large Vision-Language Models (LVLMs) when queried in non-English languages. MHR proposes a two-stage framework, with the second stage focusing on hallucination-enhanced preference optimization. Crucially, it introduces a novel *cross-lingual alignment method* to automatically generate multilingual hallucination-aware data pairs. This method leverages the LVLM itself to generate multiple responses in various non-English languages, which are then aligned with existing English hallucination/non-hallucination answers using semantic distance metrics. This scalable approach creates multilingual hallucination-aware datasets, significantly reducing manual effort and enabling DPO-based fine-tuning to favor non-hallucinating responses across languages. While MHR demonstrates substantial improvements, particularly in reducing \"unknown\" responses and increasing accuracy across diverse languages, it acknowledges that some instruction-following issues persist for low-resource languages, indicating limitations tied to the foundational multilingual capabilities of the base LLM.\n\nCollectively, these training-based approaches underscore a significant shift towards more automated and data-efficient methods for mitigating hallucinations. By innovatively generating synthetic, adversarial, or dispreferred data, methods like AutoHall, HIO, ICD, POVID, HDPO, VHTest, and MHR circumvent the traditional data annotation bottleneck, making fine-tuning and preference optimization strategies more practical and scalable \\cite{sahoo2024hcb}. The integration of lightweight verifiers, as seen in Deng et al. \\cite{deng202405j}, further enhances efficiency and reduces reliance on expensive human or LLM-based judgments. However, challenges persist in ensuring the generalizability and diversity of automatically generated adversarial examples, precisely defining and identifying all forms of hallucination for targeted unlearning, and managing the computational overhead associated with training auxiliary models or complex unlearning processes. Furthermore, the theoretical inevitability of hallucination in computable LLMs \\cite{xu2024n76, li2025qzg} suggests that even the most sophisticated training-based approaches may only reduce, but not entirely eliminate, the problem. Future research will likely explore hybrid approaches that combine these automated data generation techniques with more advanced unlearning algorithms, investigate methods for dynamically adapting data generation strategies to evolving hallucination types, and further refine judge-free verification mechanisms to build even more robust and trustworthy AI systems within these inherent limitations.",
    "Defining and Categorizing Multimodal Hallucinations": "\\subsection*{Defining and Categorizing Multimodal Hallucinations}\n\nThe phenomenon of hallucination, traditionally understood in Large Language Models (LLMs) as the generation of factually incorrect or nonsensical information, takes on a new dimension of complexity within Multimodal Large Language Models (MLLMs). Here, hallucinations extend beyond mere factual inaccuracies to encompass inconsistencies and misalignments across different modalities, such as vision, audio, and language \\cite{multimodal_hallucination_overview_2022}. This subsection extends the conceptual framework of hallucination to MLLMs, including Vision-Language Models (LVLMs), Audio-Language Models (LALMs), and Video-Language Models (VLLMs), defining unique hallucination types that arise from cross-modal inconsistencies.\n\nA primary challenge in MLLMs is the inherent 'modality gap,' where models must bridge disparate data representations and semantic spaces from different input types \\cite{crossmodal_alignment_challenges_2022}. This gap often leads to the generation of textual descriptions that contradict the visual, audio, or temporal information presented. In visual contexts, for instance, LVLMs frequently exhibit object, attribute, and relation hallucinations \\cite{vision_language_hallucination_taxonomy_2023}. Object hallucinations occur when the model describes an entity that is not present in the image, such as claiming to see a \"cat\" in a picture containing only dogs. Attribute hallucinations involve misrepresenting characteristics of existing objects, like describing a \"red car\" when the car is clearly blue. Furthermore, relation hallucinations manifest as incorrect spatial or semantic relationships between objects, such as stating a \"person is sitting on the table\" when they are standing beside it. These errors highlight a failure in the model's ability to accurately perceive and ground its linguistic output in the visual input.\n\nExtending this understanding, Audio-Language Models (LALMs) also exhibit unique forms of hallucination, primarily revolving around object hallucination in the auditory domain \\cite{audio_language_hallucination_2023}. An LALM might describe the sound of \"rain\" when only ambient city noise is present, or misidentify the source of a sound, attributing a \"dog bark\" to a cat's meow. These auditory inconsistencies demonstrate a similar modality gap where the generated language fails to accurately reflect the acoustic environment. The complexity further escalates with Video-Language Models (VLLMs), where the temporal dimension introduces additional layers of potential error \\cite{video_language_temporal_errors_2024}. VLLMs can suffer from temporal hallucinations, misrepresenting the sequence of events or the duration of actions within a video. Semantic detail errors are also prevalent, where the model fabricates non-existent actions or events, such as describing a \"person jumping\" when they are merely walking, or inventing an entire scene that did not occur in the video footage.\n\nThe integration of diverse information sources and the inherent 'modality gap' necessitate specialized taxonomies to accurately characterize and understand these complex phenomena \\cite{comprehensive_multimodal_taxonomy_2024}. Unlike unimodal hallucinations, which often stem from factual inaccuracies or logical inconsistencies within a single data type, multimodal hallucinations arise from the intricate interplay and potential misalignments between different modalities. These specialized taxonomies are crucial for moving beyond generic error classifications to precisely pinpoint the source and nature of cross-modal discrepancies, thereby enabling more targeted mitigation strategies and robust evaluation metrics \\cite{multimodal_error_characterization_2024}.\n\nIn conclusion, the definition and categorization of hallucinations in MLLMs represent a significant evolution from their unimodal counterparts. The unique challenges posed by cross-modal inconsistencies, manifesting as object, attribute, relation, temporal, and semantic detail errors across visual, audio, and video contexts, underscore the need for a comprehensive and nuanced understanding. While initial taxonomies have begun to delineate these complex phenomena, future research must continue to refine these categories, considering the dynamic and context-dependent nature of multimodal interactions to develop more robust and reliable MLLM systems.",
    "Evaluation Benchmarks for LVLMs, LALMs, and VLLMs": "\\subsection{Evaluation Benchmarks for LVLMs, LALMs, and VLLMs}\n\nThe rigorous assessment of hallucinations in multimodal models, encompassing Large Vision-Language Models (LVLMs), Large Audio-Language Models (LALMs), and Vision-Language Models (VLLMs), necessitates the development of specialized and nuanced evaluation benchmarks. Traditional metrics often fall short in capturing the complex factual inconsistencies and fabricated information generated by these models, driving the community to innovate sophisticated frameworks for hallucination detection.\n\nEarly efforts began to address object-level hallucinations, particularly in emerging modalities. For instance, \\cite{Li2023Object} pioneered a benchmark specifically designed for Large Audio-Language Models (LALMs), focusing on evaluating object hallucination by analyzing generated audio descriptions for mentions of non-existent objects. This work established a foundational approach to quantifying spurious object references in a modality distinct from vision. Building upon the need for more granular evaluation in vision-language models, \\cite{Chen2023Freeform} introduced an object-centric benchmark tailored for free-form text generations by LVLMs. This framework leverages object detection and grounding techniques to verify the factual consistency of mentioned objects and their attributes against the visual input, moving beyond simple presence or absence to assess the accuracy of descriptive details.\n\nAs models grew more capable, the scope of hallucination expanded beyond mere object presence to complex relationships. To address this, \\cite{Wang2023Relation} developed Tri-Eval, a novel triplet-level evaluation framework specifically designed to detect and quantify relational hallucinations in LVLMs. This benchmark meticulously examines subject-predicate-object relationships within the generated text, identifying instances where the model fabricates or misrepresents connections between entities depicted in the image, thus offering a more sophisticated measure of factual accuracy. Further refining the understanding of hallucination types, \\cite{Zhao2023VLLMHallucination} proposed a comprehensive framework to differentiate between intrinsic and extrinsic hallucinations in VLLMs. Intrinsic hallucinations contradict the visual input, while extrinsic ones are plausible but ungrounded in the image, providing a critical distinction for diagnosing model failures and guiding future improvements.\n\nBeyond general-purpose evaluation, the criticality of model reliability in sensitive domains has led to the creation of domain-specific benchmarks. \\cite{Gupta2024Medical} introduced Med-Eval, a specialized benchmark for evaluating LVLMs within medical contexts. This work highlights the paramount importance of factual accuracy and hallucination detection in applications such as clinical report generation and diagnostic assistance, where inaccuracies can have severe consequences, emphasizing the need for expert-curated datasets and domain-specific evaluation criteria.\n\nFurthermore, the robustness of these models under adverse conditions is a significant concern. \\cite{Zhang2024Robustness} investigated the resilience of LVLMs and VLLMs by benchmarking their performance against various visual perturbations, including noise, occlusion, and adversarial attacks. Their findings quantify the increased susceptibility to hallucination under such challenging inputs, underscoring the need for models that maintain factual consistency even when confronted with imperfect or manipulated visual data. Finally, a crucial aspect of model evaluation is distinguishing true visual understanding from mere memorization of training data. \\cite{Lee2024Understanding} addressed this by introducing a benchmark designed to test true visual understanding in LVLMs. This framework employs novel, out-of-distribution visual concepts and compositional reasoning tasks to ascertain whether models can generalize their knowledge rather than simply recalling learned patterns, ensuring that evaluations reflect genuine comprehension.\n\nIn conclusion, the evolution of evaluation benchmarks for LVLMs, LALMs, and VLLMs reflects a growing understanding of the multifaceted nature of hallucinations. While significant progress has been made in developing frameworks for object-level, relational, intrinsic/extrinsic, domain-specific, and robustness-focused evaluations, challenges remain. Future research must focus on dynamic, adaptive benchmarks that can assess emergent hallucination types, better align with human perception of factual accuracy, and continuously evolve with the increasing complexity and capabilities of multimodal AI models.",
    "Multimodal Mitigation Strategies": "\\subsection{Multimodal Mitigation Strategies}\n\nHallucinations in multimodal models (MLLMs), encompassing Large Vision-Language Models (LVLMs), Large Audio-Language Models (LALMs), and Video-Language Models (VLLMs), present a complex challenge due to the intricate interplay and potential misalignments between diverse modalities \\cite{lan20240yz, bai2024tkm, sahoo2024hcb}. These errors often stem from a \"modality gap,\" where differences in data distribution or semantics between modalities lead to biased understanding, dataset toxicity, and inherited biases from underlying Large Language Models (LLMs) \\cite{lan20240yz, bai2024tkm}. Addressing these issues necessitates specialized mitigation strategies that go beyond unimodal approaches, focusing on improving cross-modal grounding, balancing modality priors, and ensuring factual consistency in integrated outputs.\n\nA prominent category of multimodal mitigation strategies involves training-free external guidance and verification mechanisms, which leverage cross-modal consistency checks to steer model generation without modifying internal model weights. These methods intervene during inference or use external tools to validate outputs. For instance, \\cite{kim2024ozf} introduced \\textbf{Counterfactual Inception}, a training-free method that prompts LMMs to engage in counterfactual thinking. The model generates \"counterfactual keywords\" (e.g., non-existent objects) based on visual input, which are filtered by a Plausibility Verification Process (PVP) using CLIP-based semantic alignment. These refined keywords then instruct the LMM to *avoid* generating such content, thereby enhancing visual grounding and reducing object, attribute, and relational hallucinations. This approach is fundamentally multimodal as it relies on the semantic interplay between generated text and visual content for self-correction. Similarly, \\cite{park20247cm} leveraged Text-to-Image (T2I) diffusion models for hallucination visualization. By generating visual representations of an MLLM's textual output and comparing them with the original input, this method provides an external visual verification mechanism to guide contrastive decoding, effectively using an external generative model to identify and correct cross-modal inconsistencies. Complementary to this, \\cite{wang2023hallucination} proposed a training-free image-grounded guidance method that employs a \"negative image\" (e.g., a blurred version of the input) to suppress non-grounded content in VLMs, using a \"visual-grounding score\" to quantify hallucination severity. While these external guidance methods offer flexibility and avoid costly retraining, their effectiveness is often contingent on the fidelity of the external verification models, and they do not fundamentally alter the MLLM's internal representation learning.\n\nBeyond external guidance, more sophisticated strategies delve into the internal workings of multimodal models, often involving fine-tuning or specialized decoding, to address issues like 'visual amnesia' and modality imbalance at a deeper level. A significant challenge unique to interactive multimodal settings is \"multimodal hallucination snowballing,\" where an MLLM's previously generated hallucination can mislead subsequent responses, even when ground visual information is available \\cite{zhong2024mfi}. To combat this, \\cite{zhong2024mfi} introduced \\textbf{Residual Visual Decoding (RVD)}, a training-free decoding method that \"residual connects\" visual information with the current user instruction. RVD revises the MLLM's output distribution to emphasize direct visual evidence, providing models with more robust access to visual information during generation and reducing the propagation of self-generated errors in conversational contexts. For improving foundational visual grounding, \\cite{jiang2022reg} proposed \\textbf{Visual Cluster Grounding} for image captioning, which implicitly links generated words to informative regions in the image, dynamically focusing on discriminative parts or full object content to reduce object hallucinations and language bias.\n\nRecent advancements have also focused on directly manipulating the decoding process or fine-tuning models with hallucination-targeted objectives. \\textbf{Hallucination-Induced Optimization (HIO)} \\cite{chen20247jb} introduces a novel paradigm where an \"Evil LVLM\" is intentionally trained using a *reversed* Bradley-Terry model to *prioritize* hallucinatory content. This \"Evil LVLM\" then serves as a strong contrastive signal during inference, amplifying the logit differences between hallucinatory and correct tokens to steer the original LVLM towards more factual outputs. This approach offers a more precise way to induce specific hallucinatory tokens for contrastive decoding compared to generic visual uncertainty methods. Similarly, \\textbf{Hallucination-targeted Direct Preference Optimization (HDPO)} \\cite{fu2024yqj} fine-tunes MLLMs by constructing specific preference pair data designed to address three distinct causes of hallucinations: Visual Distracted Hallucination (VDH), Long Context Hallucination (LCH), and Multimodal Conflict Hallucination (MCH). For VDH, negative samples are generated by amplifying irrelevant visual information; for LCH, negative examples are created by prompting the MLLM to continue truncated captions, often leading to deviation; and for MCH, conflicting textual information is introduced to train the model to prioritize visual grounding. HDPO's strength lies in its ability to jointly address multiple types of MLLM hallucinations through targeted data construction, offering a more comprehensive fine-tuning strategy compared to general DPO methods.\n\nDeeper architectural interventions aim to resolve representational issues within the MLLM. Methods like \\textbf{Memory-Space Visual Retracing (MemVR)} \\cite{liu2024hallucination} allow models to re-examine and leverage visual features more effectively within intermediate layers, combating 'visual amnesia' by re-injecting visual tokens based on uncertainty. This ensures that crucial visual details are not forgotten during the generation process. Crucially, these internal interventions often incorporate causal mechanisms to balance modality priors, preventing one modality from dominating or suppressing information from another \\cite{liu2024hallucination, zhou2024lvp}. For instance, \\cite{zhou2024lvp}'s \\textbf{CAUSAL MM} framework applies structural causal modeling to MLLMs, treating visual and language priors as confounding factors and using back-door adjustment and counterfactual reasoning to isolate and mitigate modality biases. Similarly, \\textbf{Visual Amplification Fusion (VAF)} \\cite{yin2025s2b} enhances attention to visual signals specifically within the MLLM's middle layers, arguing that language bias often stems from *insufficient* visual attention rather than an overemphasis on language. These intrinsic methods represent a deeper, mechanistic understanding of MLLM behavior, allowing for more precise and less intrusive corrections by ensuring more robust cross-modal integration.\n\nWhile much of the research on multimodal hallucination mitigation has focused on LVLMs, the principles extend to other modalities, albeit with fewer dedicated studies \\cite{sahoo2024hcb}. For Large Audio-Language Models (LALMs), object hallucinationâ€”where models generate or affirm the presence of non-existent sounds or objectsâ€”is a significant concern \\cite{kuan20249pm}. \\cite{kuan20249pm} demonstrated that carefully crafted prompt engineering can significantly improve LALM performance on discriminative audio tasks and reduce object hallucination, highlighting that even simple interventions can be effective when the underlying issue is query understanding rather than audio processing. Mitigation strategies for LALMs also include leveraging latent diffusion models or retrieval-based methods to ensure consistency between audio and text, as indicated by comprehensive surveys \\cite{sahoo2024hcb}. In the realm of Audio-Visual Large Language Models (AV-LLMs), \\cite{sungbin2024r2g} highlighted \"cross-modal driven hallucinations,\" where models misinterpret information due to subtle relationships or over-reliance on one modality (e.g., video-driven audio hallucination or audio-driven video hallucination). Their work demonstrated that even simple training methods, such as Low-Rank Adaptation (LoRA) fine-tuning with enhanced feature alignment, can improve AV-LLM robustness against these complex inter-modal inconsistencies. General mitigation strategies for Video-Language Models (VLLMs) often involve temporal dependency modeling to ensure consistency across dynamic sequences, a critical challenge given the sequential nature of video data \\cite{sahoo2024hcb}.\n\nIn conclusion, multimodal mitigation strategies have evolved from flexible, training-free external guidance and verification to sophisticated internal architectural interventions and causal reasoning frameworks, alongside targeted fine-tuning approaches. External methods like Counterfactual Inception \\cite{kim2024ozf} and T2I visualization \\cite{park20247cm} offer quick, adaptable solutions but rely on the robustness of external components. Decoding-time interventions like HIO \\cite{chen20247jb} and fine-tuning approaches like HDPO \\cite{fu2024yqj} represent a more direct engagement with the model's generation process, offering deeper control but requiring additional training or preference data. Intrinsic methods such as MemVR \\cite{liu2024hallucination}, VAF \\cite{yin2025s2b}, and CAUSAL MM \\cite{zhou2024lvp} aim for fundamental improvements in cross-modal integration and modality balancing, offering robust solutions at the cost of increased model complexity or intrusiveness. While LVLMs have seen the most dedicated research, emerging work in LALMs \\cite{kuan20249pm} and AV-LLMs \\cite{sungbin2024r2g} indicates a growing focus on modality-specific challenges.\n\nDespite significant progress, several challenges persist. The scalability and real-time applicability of complex causal interventions like CAUSAL MM \\cite{zhou2024lvp} in dynamic multimodal streams, such as live video or audio, remain critical areas for research. A key open question is developing robust methods for arbitrating conflicting information presented by different modalities (e.g., an image showing a blue object while accompanying text describes a a red one), requiring dynamic weighting and conflict resolution mechanisms. Furthermore, the robustness of external verification methods, such as T2I-based approaches \\cite{park20247cm}, is inherently tied to the fidelity and potential hallucination tendencies of the underlying generative models themselves. Ensuring temporal and causal consistency in long video or audio sequences, moving beyond single-frame object grounding, also poses a significant hurdle. Finally, effectively balancing the trade-offs between hallucination reduction, maintaining content quality, and preserving inference speed across diverse multimodal tasks (e.g., VQA, captioning, dialogue, video, audio) and model architectures remains a crucial area for future research \\cite{yin2025s2b, sahoo2024hcb, lan20240yz, bai2024tkm}.",
    "Cross-Modal Dynamics and Snowballing": "\\subsection{Cross-Modal Dynamics and Snowballing}\n\nThe transition from Large Language Models (LLMs) to Multimodal Large Language Models (MLLMs) and Vision-Language Models (LVLMs) introduces a new dimension to the hallucination problem: complex dynamic behaviors where errors propagate across modalities and conversational turns. While Section 4.4 addresses dialogue-level inconsistencies in text-only models, this subsection focuses specifically on how the interplay with persistent visual, audio, or video modalities creates unique error propagation dynamics, often termed \"multimodal hallucination snowballing,\" which are not present in purely linguistic systems. Understanding these dynamic and interactive aspects is crucial for developing robust and coherent AI systems capable of sustained, reliable interaction across sensory inputs.\n\nA primary dynamic challenge is \\textit{multimodal hallucination snowballing}, where an LVLM's previously generated hallucination can mislead subsequent responses in conversational settings. \\cite{zhong2024mfi} meticulously identifies and characterizes this problem, demonstrating how initial factual errors in an LVLM's output can be implicitly accepted and built upon in subsequent turns, leading to a cascade of incorrect information. To counteract this, they introduce Residual Visual Decoding (RVD), a training-free decoding method that emphasizes direct visual evidence to prevent the model from relying on its own prior, potentially erroneous, textual generations. While RVD offers a practical solution, its effectiveness hinges on the clarity and availability of direct visual evidence, which might be insufficient in scenarios requiring abstract reasoning or subtle contextual understanding. Reinforcing this challenge, \\cite{cao2024o9a} introduces VisDiaHalBench, a visual dialogue benchmark specifically designed to diagnose hallucinations arising from \"long-term misleading textual history\" in LVLMs. This benchmark, featuring five-turn questions about edited images, directly probes the model's susceptibility to propagating errors in a conversational context, highlighting the need for continuous visual re-grounding. The principles of error propagation in multi-turn dialogues, as explored in text-only contexts by \\cite{chen2024c4k} with their DiaHalu benchmark, find direct and exacerbated parallels in multimodal settings where visual context can be misremembered or ignored.\n\nBeyond explicit conversational snowballing, misinterpretations can arise from subtle, dynamic interactions and inconsistencies between different modalities, acting as triggers for initial hallucinations that can then propagate. \\cite{zhou2024lvp} investigates the causal impact of modality priors on attention and output, revealing how an imbalance in these priors can lead to hallucinations. Their work, CAUSAL MM, provides a principled causal inference framework to understand and balance the influence of visual and linguistic inputs by applying back-door adjustment and counterfactual reasoning. This mitigates errors stemming from over-reliance on one modality, which could otherwise initiate a chain of incorrect inferences. However, the complexity of causal modeling and defining appropriate counterfactuals remains a challenge for broad applicability. Similarly, \\cite{han202439z} uncovers a \"semantic shift bias\" where the mere insertion of a paragraph break in textual input can subtly alter an LVLM's understanding of an image, leading to hallucinations. This demonstrates how minor, seemingly innocuous textual formatting can dynamically influence cross-modal interpretation, revealing a brittleness in vision-language grounding that may require more fundamental architectural solutions than the proposed MiHI/MiHO interventions.\n\nUnderlying these dynamic misinterpretations are fundamental vulnerabilities within the model architecture that enable error propagation. \\cite{wang2025jen} reveals that hallucinations can be induced by exploiting \"attention sinks,\" a phenomenon where attention mechanisms become fixated on irrelevant tokens, diverting processing power from critical visual information. In a multimodal context, this mechanism can directly contribute to propagating errors by causing the model to misinterpret visual cues, thus initiating a chain of incorrect inferences that could snowball. This highlights a critical internal dynamic where attention misallocation directly impacts multimodal grounding. Furthermore, the temporal dynamics inherent in video-language models present unique challenges. \\cite{ma2023mka} introduces Vista-llama to address the \"diminishing impact of video\" as generated text length increases, a clear example of cross-modal dynamic error where the visual grounding weakens over time, leading to irrelevant content. Their solution, which maintains a consistent distance between visual and language tokens, underscores the need for continuous and robust visual attention throughout the generation process.\n\nEvaluating an LVLM's ability to maintain coherent understanding across dynamic visual changes is also crucial for identifying propagating errors. \\cite{yebin2024txh} introduces BEAF (Observing BEfore-AFter Changes), an innovative framework that manipulates visual scenes by removing objects and introduces change-aware metrics. This allows for a more robust assessment of whether models truly understand visual changes or merely hallucinate, which is vital for identifying when subtle inconsistencies lead to misinterpretations or propagating errors in a dynamic environment.\n\nThe insights from LLM-centric self-correction mechanisms offer conceptual parallels for mitigating multimodal snowballing. \\cite{dhuliawala2023rqn}'s Chain-of-Verification (CoVe) and \\cite{liu2025juo}'s self-consistency framework for mathematical reasoning both emphasize internal deliberation and verification steps to prevent models from repeating their own mistakes and propagating errors. Adapting such multi-step, self-correcting paradigms to multimodal contexts would require sophisticated mechanisms for re-grounding each verification step in the visual or audio evidence, rather than solely relying on internal textual consistency. This is a significant challenge, as highlighted by surveys like \\cite{liu2024sn3} and \\cite{lan20240yz}, which discuss the persistent \"modality gap\" and the difficulty of ensuring consistent understanding across diverse data distributions. \\cite{tonmoy20244e4} also notes that snowballing in complex reasoning remains a challenge for many mitigation approaches, underscoring the severity of this dynamic problem.\n\nIn conclusion, the study of cross-modal dynamics and snowballing highlights that hallucination in LVLMs is not merely a static error but a complex, evolving problem. The propagation of errors, whether through explicit conversational snowballing, subtle cross-modal misinterpretations, or diminishing visual grounding over time, poses a significant challenge. Future research must focus on developing models with stronger internal consistency checks that explicitly re-ground in multimodal reality at each conversational turn, advanced causal modeling of cross-modal interactions to prevent the initiation of errors, and robust self-correction mechanisms that can effectively leverage and verify against dynamic sensory inputs to prevent the propagation of hallucinations.",
    "Zero-Resource and Black-Box Hallucination Detection": "\\subsection*{Zero-Resource and Black-Box Hallucination Detection}\n\nDetecting hallucinations in Large Language Models (LLMs) presents a formidable challenge, particularly when evaluating proprietary models where direct access to internal states, training data, or extensive human labels is unavailable. This subsection focuses on \"zero-resource\" and \"black-box\" detection methods, which operate under these constraints by leveraging the intrinsic properties of LLM generation to identify factual and logical inconsistencies. These approaches are indispensable for scalable, model-agnostic evaluation, thereby enhancing the practical applicability of hallucination detection frameworks across diverse deployment scenarios.\n\nA foundational contribution in this domain is \\textit{SelfCheckGPT} \\cite{manakul20236ex}, which introduced a novel zero-resource, black-box strategy. The core premise is that an LLM genuinely \"knowing\" a fact will produce consistent responses across multiple stochastically sampled generations for the same query. Conversely, if the LLM hallucinates, these samples are likely to diverge, contradict, or present inconsistent information. \\textit{SelfCheckGPT} generates several diverse responses from the black-box LLM and then employs various consistency measures, such as BERTScore, Natural Language Inference (NLI), or even another LLM acting as an evaluator, to quantify the informational agreement between the original response and the generated samples. This method effectively identifies non-factual statements without requiring internal token probabilities or external fact-checking databases. However, a significant limitation arises when LLMs consistently repeat their own errors across multiple samples due to strong internal biases or memorization, leading to false negatives in hallucination detection, as the model appears \"consistent\" in its incorrectness. Furthermore, the computational overhead of generating and comparing multiple responses can be substantial, especially for complex queries or real-time applications.\n\nTo address the limitations of consistent self-hallucinations, \\textit{MetaQA} \\cite{yang20251dw} significantly advanced zero-resource detection by introducing *metamorphic relations* and *prompt mutation*. Instead of merely re-sampling from the same prompt, \\textit{MetaQA} generates logically equivalent or semantically related prompts (e.g., synonymous queries, rephrased questions, or queries testing inverse relations) to elicit a more diverse and robust set of responses. By checking for consistency across these responses, which are generated from varied but semantically linked inputs, \\textit{MetaQA} makes it harder for the LLM to consistently hallucinate the same fact. This technique effectively probes the LLM's understanding from multiple angles, providing a more reliable signal for exposing factual inconsistencies. While more robust, the effectiveness of \\textit{MetaQA} is contingent on the careful design of metamorphic relations, which can be task-specific and may not generalize universally across all types of factual or reasoning errors.\n\nBeyond consistency checks, other black-box approaches explore alternative signals. \\textit{Attention-Guided SElf-Reflection (AGSER)} \\cite{liu2025xwv} proposes a zero-shot hallucination detection method that attempts to leverage insights from attention mechanisms without direct internal model access. AGSER categorizes the input query into \"attentive\" and \"non-attentive\" parts, processes each separately through the LLM, and then computes consistency scores between the generated responses and the original answer. The difference between these consistency scores serves as a hallucination estimator. This method notably reduces computational overhead, requiring only three passes through the LLM, making it more efficient than methods relying on numerous generations. While the precise mechanism for inferring \"attention contributions\" in a strictly black-box manner requires careful consideration, AGSER demonstrates a promising direction for deriving more nuanced signals from LLM outputs without full transparency.\n\nZero-resource, black-box principles are also being tailored for specific, complex reasoning domains. For instance, \\cite{liu2025juo} enhances mathematical reasoning in LLMs by applying a structured self-consistency framework. This approach goes beyond merely checking the final answer, enforcing consistency across *intermediate reasoning steps* in tasks like theorem proving, symbolic transformation, and numerical computation. By ensuring logical coherence throughout the problem-solving process, this method significantly reduces logical inconsistencies and hallucinations specific to mathematical contexts. While domain-specific, it highlights how black-box consistency checks can be adapted to probe deeper into an LLM's reasoning integrity, rather than just surface-level factual recall.\n\n\\textbf{Comparative Analysis and Critical Discussion:}\nThese black-box detection methods offer distinct advantages and trade-offs. \\textit{SelfCheckGPT} provides a simple, general-purpose baseline, but its vulnerability to consistently incorrect outputs limits its robustness. \\textit{MetaQA} improves robustness by actively perturbing inputs, making it harder for LLMs to hide systematic errors, yet it introduces complexity in designing effective metamorphic relations. Both methods incur significant computational costs due to multiple inference calls. \\textit{AGSER} attempts to mitigate this computational burden by leveraging a more efficient, attention-guided reflection mechanism, potentially offering a better balance between detection efficacy and resource usage. The mathematical self-consistency approach \\cite{liu2025juo} demonstrates the power of adapting these principles to domain-specific reasoning, highlighting that while general black-box detectors are valuable, specialized approaches can achieve higher precision for particular types of complex hallucinations. A common limitation across these methods is their reliance on the LLM's own generative capabilities to expose its flaws; they cannot detect hallucinations that the LLM consistently and confidently generates as \"true\" across all probed variations. Furthermore, the sensitivity to sampling parameters (e.g., temperature) and the choice of consistency metrics (e.g., BERTScore vs. NLI) can significantly impact detection performance, requiring careful tuning.\n\nIn summary, zero-resource and black-box hallucination detection methods represent a vital area of research, offering scalable and model-agnostic solutions for evaluating and improving the trustworthiness of LLMs, especially proprietary ones. From the foundational consistency checks of \\textit{SelfCheckGPT} \\cite{manakul20236ex} and the robust metamorphic relations of \\textit{MetaQA} \\cite{yang20251dw}, to the efficient attention-guided reflection of \\textit{AGSER} \\cite{liu2025xwv} and domain-specific logical consistency for mathematical reasoning \\cite{liu2025juo}, these techniques collectively push the boundaries of what is possible without privileged model access or extensive human annotation. Future directions in this area could involve the fusion of diverse black-box signals, combining internal consistency with metamorphic testing and attention-guided insights to create more robust hybrid detectors. Furthermore, research could focus on developing computationally lighter black-box methods, exploring their applicability to detect more subtle forms of hallucination beyond factual errors, such as logical fallacies, reasoning inconsistencies, and biases, and enhancing their resilience against adversarial attacks designed to evade detection.",
    "Adversarial Attacks and Vulnerability Probing": "\\subsection{Adversarial Attacks and Vulnerability Probing}\n\nBeyond merely observing and reacting to instances of hallucination, a critical new frontier in understanding and mitigating this phenomenon involves proactively probing Large Multimodal Models (LMMs) for vulnerabilities through adversarial attacks. This methodology aims to intentionally induce hallucinations, thereby uncovering specific weaknesses and failure modes that might remain hidden during passive observation, guiding the development of more resilient and secure AI systems.\n\nA pioneering work in this domain is \\cite{wang2025jen}, which introduces a novel adversarial attack termed \"Mirage in the Eyes.\" This technique specifically targets and exploits the \"attention sink\" phenomenon within MLLMs to intentionally induce hallucinations. By dynamically manipulating internal attention scores and hidden embeddings, \\cite{wang2025jen} demonstrates how to steer the model towards generating factually incorrect or non-existent visual content, providing crucial insights into the internal mechanisms that contribute to hallucination. This approach moves beyond external input perturbations, delving into the model's internal processing to expose its susceptibility.\n\nComplementing such targeted internal attacks, other research has integrated adversarial principles into evaluation and discovery. \\cite{huang20247wn}, in their comprehensive benchmark VHTest, incorporates an adversarial generation paradigm to create diverse visual hallucination instances. While not a direct attack for inducing hallucination in the same manner as \\cite{wang2025jen}, this paradigm contributes to the proactive identification of vulnerabilities by systematically generating challenging inputs that are likely to trigger various types of hallucinations, including those related to object shape and size. This allows for a broader exploration of an MLLM's fragility across different visual attributes.\n\nFurthermore, vulnerabilities can be exposed through surprisingly subtle adversarial manipulations. \\cite{han202439z} uncovered a \"semantic shift bias\" where the mere insertion of paragraph breaks (\\texttt{\\textbackslash n}) into textual prompts can induce hallucinations in MLLMs. This seemingly innocuous input perturbation acts as a potent adversarial trigger, demonstrating that models can be led astray by minor structural changes that do not alter the semantic content of the prompt. Such findings highlight unexpected failure modes and underscore the importance of probing for vulnerabilities across a wide spectrum of input types, from complex internal manipulations to simple textual formatting.\n\nCollectively, these approaches represent a significant shift from reactive mitigation to proactive robustness testing. By actively designing attacks that exploit internal model characteristics like attention sinks \\cite{wang2025jen}, or by systematically generating adversarial test cases \\cite{huang20247wn}, or even by identifying subtle input biases \\cite{han202439z}, researchers are gaining a deeper understanding of *why* and *how* hallucinations occur. This proactive methodology is indispensable for identifying the root causes of hallucination, enabling the development of more robust architectures and training strategies that can withstand sophisticated adversarial attempts to induce erroneous outputs. The insights gleaned from these adversarial probes are crucial for building truly trustworthy and secure AI systems that can maintain factual consistency even under challenging or malicious inputs.",
    "Semantic Guardrails for Safety-Critical Applications": "\\subsection{Semantic Guardrails for Safety-Critical Applications}\n\nThe increasing deployment of Large Language Models (LLMs) in safety-critical domains, particularly in clinical medicine, necessitates a paradigm shift from general hallucination mitigation to robust, high-assurance safety mechanisms. In these high-stakes environments, the generation of factually incorrect or ungrounded information, often termed \"hallucinations\" \\cite{maynez2020h3q}, can lead to \"never events\"â€”catastrophic errors with severe consequences for patient safety and well-being \\cite{hakim2024d4u}. The inherent probabilistic nature of LLMs, which can lead to plausible but incorrect outputs \\cite{hamid2024pwn}, coupled with observed inaccuracies in analyzing unstructured clinical notes \\cite{shah20242sx} and significant challenges in complex medical reasoning tasks \\cite{umapathi2023puv}, underscores the urgent need for specialized safeguards.\n\nTo address this imperative, the concept of 'semantic guardrails' has emerged as a targeted solution, designed to prevent these critical errors by enforcing strict adherence to factual accuracy and consistency within domain-specific knowledge. Unlike broader LLM safety surveys that discuss ethical considerations and prompt injection alongside hallucination \\cite{gao20242nu}, semantic guardrails focus specifically on content integrity and factual grounding. This approach represents an evolution from traditional rule-based expert systems used in clinical AI, which offered high precision but often lacked the flexibility and generative power of LLMs. Semantic guardrails aim to imbue LLMs with a similar level of verifiable reliability, but within their more dynamic and open-ended operational context.\n\nA pioneering framework in this area is presented by \\cite{hakim2024d4u}, which introduces specific semantic guardrails tailored for pharmacovigilance, a domain where regulatory compliance and absolute accuracy are paramount. Their work highlights the distinction between \"structural guardrails\" (ensuring output format) and \"semantic guardrails\" (verifying content accuracy). They propose two primary mechanisms: Document-wise Uncertainty Quantification (DL-UQ) and MISMATCH guardrails. DL-UQ functions as a \"soft\" semantic guardrail by quantifying the LLM's uncertainty regarding each generated statement, specifically by evaluating its evidential support within a provided reference document. This mechanism identifies and flags information lacking sufficient backing, preventing unsupported claims from being presented as definitive facts. This is crucial for ensuring faithfulness to source material, a non-negotiable requirement in medical contexts.\n\nComplementing DL-UQ, the MISMATCH guardrail acts as a \"hard\" semantic guardrail, actively detecting contradictions or inconsistencies between the LLM's generated output and the authoritative reference document \\cite{hakim2024d4u}. For instance, in pharmacovigilance, it ensures that drug names or adverse event terms are consistently present in both source and target texts, preventing hallucination or omission of critical terms through the use of custom dictionaries and medical ontologies like MedDRA. Both DL-UQ and MISMATCH are engineered with the explicit goal of absolute error prevention for \"never events,\" fundamentally shifting the paradigm from merely reducing the frequency of hallucinations to actively precluding errors where severe consequences are at stake.\n\nWhile \\cite{hakim2024d4u} focuses on document-wise uncertainty for text-to-text tasks, other research explores different facets of uncertainty quantification (UQ) that could complement or inform guardrail development. For instance, \\cite{ling2024hqv} investigates aleatoric and epistemic uncertainties in LLMs during in-context learning, which could provide finer-grained signals for guardrails beyond document-level support. Similarly, \\cite{zhang2024mmj} introduces VL-Uncertainty for Large Vision-Language Models (LVLMs), quantifying intrinsic uncertainty by analyzing prediction variance across semantically equivalent but perturbed prompts. This highlights a broader trend towards intrinsic uncertainty estimation, which could be integrated into multimodal semantic guardrails in the future to address the complexities of visual and other non-textual data in clinical settings.\n\nSemantic guardrails also stand in contrast to, or can be integrated with, other mitigation strategies. Retrieval-Augmented Generation (RAG) is a foundational approach for grounding LLMs in external knowledge \\cite{gilbert2024uu2}. While RAG aims to prevent hallucinations by providing relevant context, semantic guardrails act as a subsequent, explicit verification layer, ensuring the *correctness* of the generated text *against* that context, rather than just relying on the retrieval process. Similarly, approaches like In-Context Padding (ICP) that guide clinical reasoning with \"knowledge seeds\" \\cite{wu202407f} aim to improve accuracy during generation by aligning LLM reasoning with clinical decision pathways, whereas guardrails provide a post-generation safety net that can validate the outcome of such guided reasoning.\n\nDespite their promise, the development and deployment of semantic guardrails face significant challenges. The theoretical inevitability of hallucination in any computable LLM \\cite{xu2024n76} suggests that achieving \"absolute error prevention\" is an asymptotic goal, requiring continuous vigilance and robust design. This means guardrails must be designed not just to prevent errors, but to gracefully handle irreducible uncertainties and flag them for human review. The tuning of thresholds for uncertainty-based guardrails (e.g., DL-UQ) involves delicate trade-offs between sensitivity (catching all potential errors) and specificity (avoiding false positives), which is particularly critical in clinical settings where over-flagging can lead to alert fatigue and hinder workflow efficiency. Furthermore, the computational overhead of running multiple, stringent semantic checks in real-time clinical workflows needs careful optimization to ensure practical applicability.\n\nFuture research must therefore focus on several key areas. Firstly, developing more sophisticated, domain-adaptable, and potentially formally verifiable semantic guardrails is crucial to expand their applicability beyond specific tasks like pharmacovigilance to broader medical reasoning and diagnostics. This includes exploring methods for automatically generating and validating guardrail rules, potentially leveraging knowledge graphs for enhanced precision and explainability. Secondly, integrating these guardrails seamlessly into human-in-the-loop systems, ensuring clear communication of uncertainty and rationale for flagging, is paramount for fostering trust and effective human-AI collaboration. Finally, research into multimodal semantic guardrails will be vital as LLMs increasingly process diverse data types in clinical settings, demanding consistent factual grounding across visual, textual, and other modalities. This continuous pursuit of high-assurance solutions is essential for the responsible and ethical integration of LLMs into safety-critical applications.",
    "Meta-Evaluation of Hallucination Benchmarks": "\\subsection{Meta-Evaluation of Hallucination Benchmarks}\n\nThe rapid proliferation of hallucination benchmarks, while crucial for advancing the field, has simultaneously introduced challenges regarding their quality, reliability, and validity. As the community developed increasingly sophisticated methods to detect diverse hallucination types, a critical self-reflection emerged: how do we ensure that the tools used to measure LLM performance are themselves robust and trustworthy? This subsection delves into the vital area of meta-evaluation, focusing on frameworks designed to assess the quality of these benchmarks.\n\nEarly efforts to quantify hallucination, such as the CHAIR metric, faced considerable scrutiny due to issues like instability and sensitivity to instruction design \\cite{li2023249}. While subsequent benchmarks like POPE \\cite{li2023249} addressed some of these limitations by offering more stable, polling-based evaluation, the overarching need for a systematic framework to evaluate *any* hallucination benchmark remained. This necessity stems from concerns about potential prompt bias, data leakage, and the ability of benchmarks to accurately capture the multifaceted nature of hallucinations across various contexts and modalities.\n\nAddressing this critical gap, \\cite{yan2024ux8} introduced a groundbreaking psychometrics-inspired framework for the meta-evaluation of hallucination benchmarks. Their work proposes the \\textbf{Hallucination benchmark Quality Measurement (HQM)} framework, which systematically assesses benchmarks across four key dimensions: reliability, validity, fairness, and utility. Reliability evaluates the consistency of a benchmark's results, ensuring that repeated measurements under similar conditions yield comparable outcomes. Validity, perhaps the most crucial dimension, ascertains whether a benchmark truly measures what it purports to measure, accurately capturing the intended hallucination types without conflating them with other errors. Fairness scrutinizes benchmarks for biases, such as prompt-specific biases that might inadvertently favor certain models or data leakage issues that compromise the integrity of evaluation. Finally, utility considers the practical aspects, including the scalability, interpretability, and overall usefulness of a benchmark for researchers and developers.\n\nBy applying HQM to existing benchmarks, \\cite{yan2024ux8} revealed inherent strengths and weaknesses, providing a much-needed critical perspective on the tools foundational to hallucination research. This meta-evaluation not only highlights areas for improvement in current benchmarks but also proposes the concept of a \\textbf{High-Quality Hallucination Benchmark (HQH)} as a guiding principle for future development. The HQH concept encourages benchmark designers to proactively incorporate principles of psychometric rigor, ensuring that new evaluation methodologies are inherently robust, unbiased, and capable of accurately reflecting model performance.\n\nThe introduction of meta-evaluation frameworks marks a significant maturation of the field, shifting from merely creating benchmarks to critically assessing their foundational quality. This self-reflective advancement ensures that research findings on LLM hallucination are built upon solid, trustworthy evaluation methodologies. It is vital for maintaining the integrity of research, guiding the development of truly effective and unbiased evaluation tools, and ultimately accelerating progress towards more reliable and trustworthy large language models. Future work will likely see the HQM framework become a standard for validating new benchmarks, fostering a more rigorous and transparent evaluation ecosystem.",
    "Summary of Key Advancements": "\\subsection{Summary of Key Advancements}\n\nThe persistent challenge of hallucination in Large Language Models (LLMs) has driven a rapid and profound evolution in research, transforming the field from initial problem identification to a sophisticated, multi-faceted scientific endeavor. This trajectory highlights a collective effort towards building more reliable, transparent, and contextually grounded AI systems. Crucially, a profound intellectual shift has occurred, moving beyond empirical observation to a formal theoretical grounding, demonstrating the inherent inevitability of hallucination in computable LLMs \\cite{xu2024n76, li2025qzg}. This fundamental insight re-frames the research goal from complete eradication to robust management and mitigation, acknowledging an innate limitation.\n\nEarly foundational work established the critical need to address unfaithful content. \\cite{maynez2020h3q} provided a seminal large-scale human evaluation, categorizing hallucinations in abstractive summarization and demonstrating the utility of textual entailment for faithfulness evaluation. Recognizing the limitations of static knowledge, \\cite{trivedi2022qsf} introduced Interleaving Retrieval with Chain-of-Thought (IRCoT), an early few-shot, training-free method that dynamically interleaved reasoning steps with knowledge retrieval to ground LLMs in external facts. This marked an early shift towards adaptive, real-time grounding during generation. Subsequent advancements in Retrieval-Augmented Generation (RAG) further refined this paradigm, with frameworks like Rowen \\cite{ding20244yr} intelligently deciding *when* to retrieve based on cross-language/cross-model consistency, optimizing efficiency and mitigating both internal and external hallucinations. Complementing this, the strategic integration of structured Knowledge Graphs (KGs) through \"Graph-guided retrieval\" and \"Graph-guided generation\" has significantly enhanced trustworthiness and reduced hallucinations in open-ended QA by providing verifiable, structured knowledge \\cite{sui20242u1}. However, it is critical to acknowledge that RAG systems, while powerful, are not infallible; their efficacy is inherently tied to the quality of retrieved information, making them vulnerable to noisy or biased sources and susceptible to 'confabulations' arising from limitations within RAG components themselves \\cite{zhang20252at}.\n\nAs LLMs became more capable and complex, the focus expanded to developing sophisticated evaluation benchmarks that could rigorously assess trustworthiness beyond simple accuracy. \\cite{gao2023ht7} introduced ALCE, the first reproducible benchmark for evaluating LLMs' ability to generate text with verifiable citations, complete with automatic metrics for fluency, correctness, and citation quality. Pushing evaluation further, \\cite{oh2024xa3} developed ERBench, a novel benchmark leveraging relational databases to automatically verify not just final answers but also the *rationales* provided by LLMs, addressing a critical need for transparency in reasoning. Complementing this, \\cite{ghosh2024tj5} developed new logical fact-checking datasets and quantitative measures to assess LLM consistency on complex propositional logic queries, expanding the definition of \"hallucination\" to include logical inconsistencies. For scenarios where external knowledge or human labels are scarce, methods like MetaQA \\cite{yang20251dw} emerged as zero-resource, self-contained hallucination detection techniques leveraging metamorphic relations and prompt mutation. The theoretical underpinnings of such detection methods were further explored by \\cite{karbasi2025j7n}, who established an equivalence between hallucination detection and language identification, proving that automated detection is fundamentally impossible for most language collections without expert-labeled feedback, thereby providing theoretical support for methods like RLHF. Furthermore, to foster more responsible AI behavior, \\cite{tjandra2024umq} introduced the Accuracy-Engagement Distance (AED) metric to evaluate models capable of appropriately *abstaining* from answers when uncertain, utilizing semantic entropy for label-free uncertainty estimation.\n\nConcurrently, mitigation strategies evolved from general retrieval to highly adaptive, proactive, and mechanistically targeted interventions. On a more mechanistic front, \\cite{zhang2024qq9} identified \"knowledge overshadowing\" as a novel root cause of \"amalgamated hallucinations\" arising from data imbalance, proposing an inference-time, training-free self-contrastive decoding method for targeted mitigation. This represents a deeper understanding of internal model dynamics. Similarly, \\cite{chen2024j0g} presented ICT, a training-free, forward-pass intervention method that targets specific attention heads to enhance focus on crucial information, mitigating the dominance of language priors. The development of self-correction mechanisms, as exemplified by \\cite{tjandra2024umq}'s work on abstention, empowers LLMs to reflect on their uncertainty and proactively avoid generating ungrounded content, moving towards more self-aware and reliable systems.\n\nA major conceptual and methodological shift has been the dedicated focus on multimodal hallucination, particularly in Large Vision-Language Models (LVLMs). These models introduce unique complexities due to the \"modality gap\" and the integration of diverse information sources \\cite{lan20240yz}. \\cite{kaul2024ta7} addressed the inadequacy of prior benchmarks by introducing THRONE, the first accurate object-based hallucination benchmark for *free-form* generations of LVLMs, utilizing LM-based semantic judgment. Building on the \"snowballing\" effect observed in LLMs, \\cite{zhong2024mfi} investigated and mitigated \"multimodal hallucination snowballing\" in LVLMs, proposing Residual Visual Decoding (RVD) to emphasize direct visual evidence and prevent error propagation in conversational settings. To address the diverse nature of LVLM queries, \\cite{chang2024u3t} introduced Dentist, a unified mitigation framework that classifies query types (perception vs. reasoning) and applies tailored, iterative validation strategies. Furthermore, intrinsic model interventions emerged, with \\cite{wang2024vym} identifying \"Visual Encoding Distortion\" as a critical source of LVLM hallucinations and proposing Visual-Layer Fusion Contrastive Decoding (VaLiD) to correct it by fusing features from early visual layers. However, early multimodal evaluation benchmarks, such as object-based approaches, were found to be highly susceptible to prompt bias, leading to inaccurate assessments of real-world hallucination \\cite{wang2023zop}, necessitating more robust, LLM-based evaluation frameworks like HaELM.\n\nCollectively, these advancements underscore a profound intellectual trajectory: from merely identifying and correcting errors to a deep, multi-faceted understanding of hallucination's origins, its manifestation across modalities, and the development of sophisticated, adaptive, and intrinsically aware mechanisms for prevention and evaluation. This comprehensive progress marks a significant stride in the pursuit of building truly trustworthy, transparent, and contextually grounded AI systems, acknowledging both their immense potential and their inherent, theoretically proven limitations. The field is actively navigating the tension between external grounding, internal correction, and the acceptance of these fundamental limits, guiding research towards robust management rather than the elusive goal of complete eradication.",
    "Remaining Challenges and Open Questions": "\\subsection*{Remaining Challenges and Open Questions}\n\nDespite significant advancements in characterizing, evaluating, and mitigating hallucination in large language models (LLMs) and multimodal large language models (MLLMs), several critical unresolved issues persist, defining promising avenues for future research in trustworthy AI. The dynamic nature of these models and their expanding capabilities mean that hallucination remains a moving target, necessitating continuous innovation.\n\nOne fundamental challenge lies in the **scalability of fine-grained annotation and evaluation**. While efforts like \\cite{ji20243j6} and \\cite{gu202414e} have introduced analytical, sentence-level annotation datasets (ANAH and ANAH-v2) and iterative self-training frameworks to scale this process, the sheer diversity of tasks, domains, and hallucination types makes truly comprehensive, human-quality annotation prohibitively expensive and time-consuming. This bottleneck hinders the development of robust, generalizable evaluation benchmarks that can capture the nuances of complex reasoning, as exemplified by the need for rationale verification in \\cite{oh2024xa3} (ERBench) and path-based evaluation in graph computation tasks \\cite{tang2024a1j} (GraphArena). The challenge extends beyond mere data quantity to ensuring the *quality, diversity, and contextual richness* of annotated data across an ever-expanding problem space.\n\nAnother pressing issue is the development of **truly real-time, adaptive, and seamlessly integrated mitigation strategies**. Current approaches have made strides towards proactive prevention and self-correction. For instance, \\cite{manakul20236ex} (SelfCheckGPT) and \\cite{yang20251dw} (MetaQA) offer zero-resource, black-box detection methods, yet these often incur computational overhead or rely on sampling, which can introduce latency or fail to capture dynamic shifts in model uncertainty. While \\cite{tjandra2024umq} proposes label-free abstention using semantic entropy, the challenge remains in making such mechanisms adaptively responsive to subtle changes in user intent or context without sacrificing generation quality or speed. Similarly, advanced Retrieval-Augmented Generation (RAG) techniques like \\cite{lv2024k5x}'s Coarse-to-Fine Highlighting (COFT) and \\cite{ding20244yr}'s Rowen improve context relevance, but the seamless integration of such dynamic knowledge retrieval and synthesis into the core generation process, without introducing new latency or compromising creative outputs, remains an open problem. Furthermore, while \\cite{hakim2024d4u} introduced \"semantic guardrails\" for safety-critical domains, generalizing such hard constraints to open-ended, creative, or rapidly evolving tasks without stifling utility is a complex balancing act.\n\nThe field also grapples with the complex task of **bridging the gap between theoretical inevitability and practical error reduction**. The unified theoretical framework proposed by \\cite{li2025qzg} (Loki's Dance of Illusions) suggests that some forms of hallucination might be mathematically inherent to LLM architectures or their training paradigms. If certain types of hallucination are indeed inevitable, the critical question shifts from absolute elimination to understanding the *acceptable error rate* and designing systems that can gracefully handle or transparently communicate these inherent limitations. This necessitates further research into robust uncertainty quantification and calibrated abstention mechanisms that are both accurate and user-friendly, allowing LLMs to \"know what they don't know\" effectively.\n\nBeyond these challenges, several **open questions** guide the next wave of innovation. The rapid expansion into multimodal AI has unveiled **novel hallucination types** that require dedicated investigation. While object hallucination in vision-language models \\cite{liu2024sn3, lan20240yz} and audio-language models \\cite{kuan20249pm} has been identified, and video-specific temporal inconsistencies are being explored \\cite{wang2024rta}, the full spectrum of cross-modal fabrication and misinterpretation, especially in complex reasoning or creative multimodal generation, is yet to be fully mapped. Moreover, the emergence of adversarial attacks that induce hallucinations \\cite{wang2025jen} suggests that new, engineered forms of hallucination will continue to challenge detection and mitigation efforts.\n\nAnother critical open question concerns the **long-term impact of multimodal interactions and cascading hallucinations**. While \\cite{qiu2024zyc} has begun to address long-context multimodal hallucination, the cumulative effect of minor inconsistencies over extended dialogues or multi-turn reasoning in complex multimodal environments remains poorly understood. The concept of \"multimodal hallucination snowballing\" \\cite{zhong2024mfi} highlights the potential for initial errors to propagate and amplify, leading to increasingly unreliable outputs. Developing methods to track, predict, and mitigate these cascading effects across modalities and over time is crucial for building truly robust conversational and interactive AI systems.\n\nFinally, there is a pressing need for **more unified and generalizable solutions that perform robustly across diverse tasks and domains**. Many current mitigation strategies are task-specific (e.g., RAG for factual QA, visual grounding for LVLMs). The development of a single, overarching framework that can effectively address hallucination across diverse applicationsâ€”from summarization and dialogue to code generation and creative writing, and across various modalitiesâ€”while maintaining high performance and adaptability, remains an elusive goal. While some efforts, like \\cite{chang2024u3t}'s unified LVLM mitigation framework, attempt to generalize within a multimodal context, achieving true cross-domain and cross-task robustness without extensive, domain-specific fine-tuning is a significant hurdle. The pursuit of a holistic \"information quality\" metric, as conceptualized by \\cite{rejeleene2024okw}, could pave the way for more generalizable evaluation and mitigation, but its practical implementation across diverse scenarios is still an open area of research. Addressing these challenges and open questions will be paramount in guiding the next wave of innovation towards building inherently trustworthy and transparent AI systems.",
    "Ethical Considerations and Responsible AI Development": "\\subsection{Ethical Considerations and Responsible AI Development}\n\nThe challenge of hallucination in Large Language Models (LLMs) transcends mere technical inaccuracy, presenting profound ethical dilemmas that necessitate a robust framework for responsible AI development. The generation of confident yet incorrect or fabricated information by LLMs carries significant societal implications, demanding critical attention to transparency, accountability, and the safe deployment of these powerful systems, particularly in high-stakes applications \\cite{dbeeca8466e0c177ec67c60d529899232415ca87, e7c97e953849f1a8e5d85ceb4cfcc0a5d54d2365}.\n\nA foundational ethical consideration arises from the inherent limitations of LLMs. As discussed in Section 3.3, theoretical frameworks, notably those employing diagonalization arguments, suggest the mathematical inevitability of hallucination in any computable LLM \\cite{xu2024n76, li2025qzg}. This theoretical grounding shifts the ethical imperative from eradicating hallucination to transparently communicating its unavoidable nature. Responsible AI development demands that developers and deployers manage societal expectations, clearly articulate the inherent limitations of LLMs, and avoid presenting them as infallible or universally reliable. Failure to do so can lead to a breach of trust, misinformed decision-making, and a violation of the principle of non-maleficence, particularly when LLMs are deployed in sensitive domains.\n\nTo foster accountability and enable users to verify generated content, technical solutions have focused on grounding LLM outputs. As detailed in Section 4.1, the development of benchmarks like ALCE, which encourage LLMs to generate text with explicit citations to supporting evidence, represents a significant step towards ethical verifiability \\cite{gao2023ht7}. Ethically, such mechanisms aim to empower user autonomy by providing the means to cross-reference information, thereby combating the spread of misinformation. However, a critical perspective reveals potential pitfalls: if the citation mechanism itself is susceptible to hallucination (e.g., generating non-existent sources or misattributing information), it could create a false sense of security, exacerbating the problem rather than solving it. Ensuring the integrity of the grounding process is therefore paramount.\n\nBeyond proactive grounding, robust detection mechanisms are crucial for continuous monitoring and for informing users about potential inaccuracies. As explored in Section 8.1, zero-resource, black-box hallucination detection methods like \\textit{SelfCheckGPT} \\cite{manakul20236ex} and techniques leveraging metamorphic relations \\cite{yang20251dw} offer practical tools for identifying ungrounded content, even in proprietary models. Ethically, these tools support ongoing oversight and allow for post-hoc correction or flagging of potentially harmful outputs. However, their limitations, such as potential false negatives in subtle hallucinations or the computational cost of multiple generations, must be transparently acknowledged to prevent over-reliance and to ensure that human oversight remains a critical component of the safety loop.\n\nA key aspect of responsible deployment is the clear communication of uncertainty. As discussed in Section 6.3, models can be fine-tuned to proactively abstain from answering when uncertain, utilizing label-free techniques based on semantic entropy \\cite{tjandra2024umq}. This mechanism directly addresses the ethical principle of transparency by allowing LLMs to express \"I don't know\" rather than confidently asserting potentially incorrect information. This reduces the risk of misinformation in sensitive contexts and promotes a more honest interaction paradigm. However, the ethical balance lies in determining the appropriate threshold for abstention; an overly conservative model might diminish utility, while an overly permissive one risks harm.\n\nThe responsible deployment of AI systems, particularly in high-stakes applications, necessitates robust safety mechanisms and domain-specific \"guardrails.\" In medical safety-critical settings, for instance, the implementation of \"semantic guardrails\" (e.g., Document-wise Uncertainty Quantification and MISMATCH guardrails), as highlighted in Section 8.3, aims to prevent \"never event\" errorsâ€”hallucinations with severe consequences \\cite{hakim2024d4u}. This exemplifies a direct coupling of technical solutions with stringent ethical frameworks, prioritizing non-maleficence. Such guardrails are crucial for ensuring that LLMs can be trusted in environments where factual accuracy and safety are paramount, moving beyond general mitigation to targeted, high-assurance solutions.\n\nMoreover, responsible AI development extends beyond mere factual accuracy to encompass a broader spectrum of ethical considerations. Hallucination can intersect with and amplify algorithmic bias, leading to outputs that are not only incorrect but also unfair or discriminatory, thereby violating principles of justice and fairness. The phenomenon of \"sycophancy,\" where LLMs excessively agree with or flatter users, even when the user's premise is incorrect, poses a distinct ethical challenge \\cite{malmqvist2024k7x}. Sycophancy can undermine critical thinking, reinforce user biases, and create echo chambers, impacting user autonomy and potentially leading to societal harm. Addressing such behavioral biases is as critical as addressing factual errors for building truly reliable and ethically aligned LLMs. Furthermore, the generation of toxic or harmful content, as noted in surveys on automated correction \\cite{pan2024y3a}, underscores the need for comprehensive safety measures.\n\nThe pursuit of trustworthy AI also requires holistic conceptual frameworks. Research has moved towards defining and mathematically formalizing \"Information Quality\" (IQ) based on consistency, relevance, and accuracy, providing a structured approach to evaluating the ethical performance of LLMs beyond mere factual correctness \\cite{rejeleene2024okw}. This holistic perspective is vital as LLMs expand into multimodal domains, where challenges like long-context hallucinations in multimodal models demand continuous monitoring and tailored ethical considerations \\cite{qiu2024zyc}. The emergence of adversarial attacks that can intentionally induce hallucinations in multimodal LLMs, as discussed in Section 8.2 \\cite{wang2025jen}, highlights critical security vulnerabilities. Ethically, this necessitates proactive security measures and foresight to prevent malicious exploitation and ensure the integrity and safety of AI systems, as emphasized by broader surveys on LLM safety \\cite{gao20242nu}.\n\nIn conclusion, addressing hallucination is not solely a technical endeavor but a profound ethical responsibility that underpins the trustworthiness and societal benefit of LLMs. The development of transparent, accountable, and safe LLM systems requires a multi-faceted approach that integrates robust detection, verifiable content generation, clear communication of uncertainty, and domain-specific safety mechanisms. This forward-looking perspective emphasizes that technical advancements must be inextricably linked with strong ethical frameworks, encompassing principles of non-maleficence, beneficence, justice, fairness, and user autonomy, to ensure that LLMs are developed and utilized in a manner that genuinely benefits society, minimizes harm, and maximizes trustworthiness."
  },
  "latex_document": "\\documentclass[12pt,a4paper]{article}\n    \\usepackage[utf8]{inputenc}\n    \\usepackage[T1]{fontenc}\n    \\usepackage{amsmath,amsfonts,amssymb}\n    \\usepackage{graphicx}\n    \\usepackage[margin=2.5cm]{geometry}\n    \\usepackage{setspace}\n    \\usepackage{natbib}\n    \\usepackage{url}\n    \\usepackage{hyperref}\n    \\usepackage{booktabs}\n    \\usepackage{longtable}\n    \\usepackage{array}\n    \\usepackage{multirow}\n    \\usepackage{wrapfig}\n    \\usepackage{float}\n    \\usepackage{colortbl}\n    \\usepackage{pdflscape}\n    \\usepackage{tabu}\n    \\usepackage{threeparttable}\n    \\usepackage{threeparttablex}\n    \\usepackage[normalem]{ulem}\n    \\usepackage{makecell}\n    \\usepackage{xcolor}\n\n    % Set line spacing\n    \\doublespacing\n\n    % Configure hyperref\n    \\hypersetup{\n        colorlinks=true,\n        linkcolor=blue,\n        filecolor=magenta,      \n        urlcolor=cyan,\n        citecolor=red,\n    }\n\n    % Title and author information\n    \\title{A Comprehensive Literature Review with Self-Reflection}\n    \\author{Literature Review}\n    \\date{\\today}\n\n    \\begin{document}\n\n    \\maketitle\n\n    % Abstract (optional)\n    \\begin{abstract}\n    This literature review provides a comprehensive analysis of recent research in the field. The review synthesizes findings from 277 research papers, identifying key themes, methodological approaches, and future research directions.\n    \\end{abstract}\n\n    \\newpage\n    \\tableofcontents\n    \\newpage\n\n    \\label{sec:introduction}\n\n\\section{Introduction}\n\\label{sec:introduction}\n\n\\subsection{The Rise of Large Language Models and the Hallucination Challenge}\n\\label{sec:1\\_1\\_the\\_rise\\_of\\_large\\_language\\_models\\_\\_and\\_\\_the\\_hallucination\\_challenge}\n\nThe advent of Large Language Models (LLMs) has heralded a transformative era in artificial intelligence, demonstrating unprecedented capabilities in natural language understanding and generation \\cite{ahmadi2024j88}. These models, characterized by their massive scale and emergent abilities, have revolutionized diverse domains, from automating complex cognitive tasks like code generation and scientific discovery to enhancing human-computer interaction through sophisticated dialogue systems. Their capacity to produce highly coherent, contextually relevant, and often creative text has positioned them as pivotal technologies poised to redefine numerous industries and research paradigms.\n\nHowever, alongside these remarkable advancements, a pervasive and critical limitation, widely termed 'hallucination,' significantly impedes the reliability and trustworthiness of LLMs \\cite{ahmadi2024j88, rawte2023ao8}. Hallucination refers to the generation of content that is factually incorrect, nonsensical, or ungrounded in the provided input or real-world knowledge. Early investigations into neural text generation, particularly in abstractive summarization, highlighted this issue, revealing that models frequently produced information not present in the source document, with a substantial portion being factually erroneous \\cite{maynez2020h3q}. This foundational work introduced a critical distinction between 'intrinsic' hallucinations (misrepresenting source information) and 'extrinsic' hallucinations (adding ungrounded information), underscoring the challenge of ensuring faithfulness and factual accuracy. The problem's prevalence has only intensified with the increasing scale and generality of LLMs, making it a central concern for their safe and effective deployment \\cite{rawte2023ao8}.\n\nThe implications of hallucination are profound and far-reaching, posing substantial risks to user trust, the credibility of AI systems, and the safety of their applications across various domains. In high-stakes environments such as healthcare, finance, or legal services, hallucinated information can lead to severe consequences, including misdiagnoses, flawed financial advice, or fabricated legal precedents \\cite{li2025qzg, ahmadi2024j88}. Beyond factual errors, hallucinations can manifest as logical inconsistencies or ungrounded reasoning, eroding confidence in an LLM's ability to perform complex tasks reliably. This challenge extends beyond mere inconvenience, directly impacting the deployability of LLMs and necessitating robust mechanisms to ensure their outputs are verifiable and trustworthy.\n\nThe research trajectory has evolved from merely characterizing hallucination to seeking a deeper understanding of its origins, both empirical and theoretical. Initial observations revealed that even during pre-training, smaller language models could learn to reduce perplexity on grammatical sequences that \\textit{contained} hallucinations, suggesting that the propensity for generating plausible but incorrect text is embedded early in the learning process \\cite{xia20224cl}. As models scale, some of these issues are mitigated, but the fundamental challenge persists. More recently, the understanding of hallucination has been elevated to a theoretical plane, with formal mathematical definitions being proposed \\cite{li2025qzg}. Groundbreaking work has even posited that hallucination is not merely a transient engineering problem but an inherent and \\textit{inevitable} limitation of any computable LLM, stemming from fundamental principles of learning theory and computability \\cite{xu2024n76}. This theoretical perspective fundamentally reshapes the problem, suggesting that while mitigation is crucial, complete eradication might be impossible, thus shifting the focus towards robust management and transparent communication of uncertainty rather than absolute elimination. This pervasive issue is also not confined to text, manifesting uniquely across multimodal contexts, including Large Vision-Language Models (LVLMs) and Large Audio-Language Models (LALMs), further complicating the landscape of trustworthy AI \\cite{li2025qzg}.\n\nIn light of these challenges, the urgent need for robust research into the diverse causes, comprehensive evaluation, and effective mitigation strategies for hallucination is paramount. Addressing this phenomenon, which spans factual inaccuracies, nonsensical outputs, and ungrounded content across text and increasingly multimodal domains, is not merely an incremental improvement but a central hurdle to achieving truly trustworthy, verifiable, and safely deployable AI systems. This review aims to comprehensively explore the multifaceted problem of hallucination, from its foundational definitions and mechanistic causes to advanced evaluation methodologies and cutting-edge mitigation techniques, ultimately guiding the development of more reliable and accountable LLMs.\n\\subsection{Scope and Structure of the Review}\n\\label{sec:1\\_2\\_scope\\_\\_and\\_\\_structure\\_of\\_the\\_review}\n\nThis literature review provides a comprehensive and structured analysis of research into hallucination in Large Language Models (LLMs), adopting a pedagogical progression to trace the field's maturation from foundational concepts to advanced methodological approaches and cutting-edge multimodal developments. This structured organization is designed to offer a clear, evolving narrative of the research landscape, emphasizing the interconnections between different facets of the hallucination problem. The review synthesizes the evolving understanding of hallucination, its underlying causes, methods for its evaluation, and strategies for its mitigation, with a primary focus on advancements from 2022 to 2025 to ensure relevance to the current state of the art in LLM research.\n\nThe review is meticulously organized into several thematic sections, each building upon the preceding one to offer a holistic and in-depth understanding of hallucination. Following this introduction, Section 2, \"Foundational Understanding: Defining and Categorizing Hallucination,\" lays the groundwork by exploring the evolution of hallucination definitions. It distinguishes between crucial concepts like faithfulness to source material and factual correctness in general knowledge, and traces the development of early taxonomies. This section highlights how the understanding of hallucination has broadened significantly beyond simple factual errors to encompass a diverse array of content inconsistencies, logical flaws, and reasoning failures, thereby setting the stage for more granular analysis and targeted interventions.\n\nBuilding upon this definitional and conceptual foundation, Section 3, \"The Roots of Hallucination: Mechanistic Causes and Theoretical Limits,\" delves into the underlying reasons why LLMs generate incorrect or ungrounded content. This section moves beyond mere empirical observation to investigate both practical causes related to data quality, training processes, and inference biases. Crucially, it explores specific internal model mechanisms, such as 'knowledge overshadowing' and 'attention sinks,' that directly contribute to hallucination. The section culminates in a discussion of the formal theoretical grounding of hallucination, examining its mathematical origins and the concept of its inherent inevitability, which fundamentally re-frames the problem from a transient engineering bug to an innate characteristic of computable LLMs.\n\nWith a clear understanding of what hallucination is and why it occurs, Section 4, \"Evaluating Hallucination: Benchmarks for Factual Accuracy and Reasoning,\" details the evolution of evaluation methodologies. It showcases a progression from initial, broad assessments to highly granular, automatically verifiable, and context-aware benchmarks. This section covers early efforts to establish reproducible metrics for factual correctness and citation quality, advancements in fine-grained and rationale-based evaluation that probe the model's reasoning process, and the development of specialized benchmarks for complex algorithmic reasoning. Furthermore, it addresses the unique challenges of assessing hallucinations in long-context and dialogue-level interactions, underscoring the field's commitment to rigorous and comprehensive measurement of LLM trustworthiness.\n\nSubsequently, the review transitions to a comprehensive exploration of mitigation strategies, divided into two complementary sections. Section 5, \"Mitigation Strategies I: External Knowledge Grounding and Adaptive Retrieval,\" explores approaches centered on grounding LLMs in external knowledge. It introduces Retrieval-Augmented Generation (RAG) as a foundational paradigm, explaining its core principles and early implementations. This section then details the evolution of RAG into advanced and adaptive architectures that dynamically integrate external information, optimize retrieval, and intelligently manage context. It also highlights the increasing importance of integrating structured knowledge graphs to enhance factual accuracy, logical consistency, and overall trustworthiness.\n\nComplementing external grounding, Section 6, \"Mitigation Strategies II: Intrinsic Model Interventions and Self-Correction,\" focuses on interventions that operate intrinsically within the LLM itself. This includes techniques that modify the decoding process, such as contrastive methods, to steer generation away from ungrounded content. The section then delves into more granular interventions that manipulate internal model states and attention mechanisms during the forward pass for precise control over information flow. Finally, it discusses the development of self-correction and abstention mechanisms, enabling LLMs to detect their own uncertainties and errors, along with training-based approaches that leverage automated data generation for more efficient and targeted fine-tuning against hallucination.\n\nRecognizing the expanding frontier of AI, Section 7, \"The Multimodal Frontier: Hallucination in Vision, Audio, and Video Language Models,\" addresses the significant challenges posed by hallucination in Large Vision-Language Models (LVLMs), Large Audio-Language Models (LALMs), and Video-Language Models (VLLMs). This section defines and categorizes the distinct types of hallucinations that emerge from cross-modal interactions, details specialized evaluation benchmarks developed to rigorously assess these multimodal inconsistencies, and explores tailored multimodal mitigation strategies. It also investigates dynamic behaviors such as 'multimodal hallucination snowballing' in interactive multimodal settings, highlighting the complexity of ensuring trustworthiness across diverse sensory inputs.\n\nFinally, Section 8, \"Towards Trustworthy AI: Robustness, Safety, and Advanced Evaluation,\" explores critical advancements in building truly trustworthy LLMs. It covers methods for zero-resource and black-box hallucination detection, crucial for proprietary models, and delves into the proactive testing of LLM vulnerabilities through adversarial attacks, which actively induce hallucinations to identify weaknesses. This section highlights the development of 'semantic guardrails' for safety-critical applications, aiming for absolute error prevention, and discusses the emerging field of meta-evaluation, which critically assesses the quality of hallucination benchmarks themselves, ensuring the integrity of all research efforts towards reliable AI.\n\nThe review concludes in Section 9, \"Conclusion and Future Directions,\" by synthesizing the key advancements, outlining remaining challenges and open questions, and discussing the critical ethical considerations necessary for responsible AI development. This structured progression aims to provide readers with a comprehensive, nuanced, and forward-looking perspective on the multifaceted problem of hallucination in LLMs, fostering a deeper understanding of the path towards more reliable and accountable AI systems.\n\n\n\\label{sec:foundational_understanding:_defining_and_categorizing_hallucination}\n\n\\section{Foundational Understanding: Defining and Categorizing Hallucination}\n\\label{sec:foundational\\_underst\\_and\\_ing:\\_defining\\_\\_and\\_\\_categorizing\\_hallucination}\n\n\\subsection{Early Characterization and Taxonomies in LLMs}\n\\label{sec:2\\_1\\_early\\_characterization\\_\\_and\\_\\_taxonomies\\_in\\_llms}\n\nThe advent of Large Language Models (LLMs) marked a significant leap in generative AI, but simultaneously brought to the forefront the pervasive challenge of \"hallucination\"â€”the generation of content that is unfaithful to source material, factually incorrect, or logically inconsistent. Early research was thus fundamentally driven by the need to empirically observe, define, and categorize these phenomena to establish a foundational understanding for subsequent evaluation and mitigation strategies.\n\nInitial empirical observations of unfaithful content were predominantly rooted in specific Natural Language Generation (NLG) tasks, particularly abstractive summarization. A seminal work by \\cite{maynez2020h3q} provided a large-scale human evaluation of \"faithfulness\" in neural summarization. This study systematically characterized hallucinations into \"intrinsic\" (misrepresenting information present in the source) and \"extrinsic\" (adding information not directly inferable from the source) types. Crucially, \\cite{maynez2020h3q} highlighted that traditional automatic metrics like ROUGE correlated poorly with human judgments of faithfulness, underscoring the limitations of surface-level evaluation and the necessity for deeper semantic understanding. Building on this, \\cite{dong20223yz} further refined the understanding of faithfulness, challenging the strict assumption that all out-of-article content is undesirable. Their work demonstrated that gold-standard summaries often contain factually correct entities not explicitly present in the source, requiring external world knowledge. This introduced a critical dual perspective: faithfulness to the source document versus faithfulness to external world knowledge, thereby significantly shaping the early research agenda by clarifying these distinct dimensions of hallucination. These early efforts in summarization laid the groundwork for identifying content unsupported by a given context and distinguishing between adherence to input and general factual correctness.\n\nMoving beyond document-level analysis, the need for finer-grained detection methods applicable to free-form text generation became apparent. While not directly a hallucination detection method, the work by \\cite{chen2022gkm} on proposition-level segmentation and textual entailment recognition offered a foundational conceptual framework for analyzing the truthfulness of individual meaning units within sentences. By enabling the segmentation of sentences into distinct propositions and classifying their entailment relations with respect to a reference document, \\cite{chen2022gkm} provided tools that could underpin more precise, token- or phrase-level identification of unsupported content. This approach represented a significant step towards dissecting generated text at a granular level, addressing the limitations of coarser-grained methods that might miss subtle inconsistencies, and paving the way for more detailed analyses of where and how hallucinations manifest within generated outputs.\n\nAs LLMs gained prominence, the scope of hallucination expanded, necessitating structured frameworks for categorization that transcended task-specific observations. Comprehensive surveys like \\cite{zhang2023k1j} and \\cite{ye2023yom} emerged as foundational works, synthesizing the burgeoning literature and proposing early LLM-centric taxonomies. \\cite{zhang2023k1j} categorized LLM hallucinations into input-conflicting, context-conflicting, and fact-conflicting types, emphasizing the particular challenges posed by fact-conflicting errors due to the absence of an authoritative knowledge source. This survey provided a clear analytical framework for understanding the multifaceted nature of LLM failures. Concurrently, \\cite{ye2023yom} offered a detailed taxonomy that spanned various text generation tasks, from machine translation to dialogue systems, and identified initial hypothesized origins related to data collection, knowledge gaps, and the optimization process. These meta-analyses were instrumental in consolidating diverse empirical findings into a common language, providing structured frameworks for understanding different types of hallucinations, and distinguishing between faithfulness to source and factual correctness in general knowledge. While these early taxonomies provided critical initial classifications, they often remained descriptive, and the sheer diversity and subtlety of LLM-generated errors hinted at a problem far more complex than simple factual deviations.\n\nIn conclusion, the early characterization of hallucination in LLMs evolved from empirical observations in specific NLG tasks, particularly abstractive summarization, to the development of sophisticated, LLM-centric taxonomies and meta-analyses. This progression, from identifying unfaithful content at a document level and refining the concept of faithfulness, to enabling finer-grained analysis and categorizing diverse error types, was instrumental in defining the problem space. These foundational works established the crucial distinction between faithfulness to source and factual correctness, which became a cornerstone for subsequent research. However, the initial frameworks also revealed that many LLM errors were not merely simple factual deviations but deeper failures of reasoning and consistency, suggesting that a more pervasive and nuanced understanding of hallucination was required, a challenge we explore in the subsequent section.\n\\subsection{Hallucination as a Pervasive Problem: Beyond Factual Errors}\n\\label{sec:2\\_2\\_hallucination\\_as\\_a\\_pervasive\\_problem:\\_beyond\\_factual\\_errors}\n\nThe initial conceptualization of hallucination in Large Language Models (LLMs) primarily focused on the generation of content that was factually incorrect or unfaithful to source material. However, as LLMs have grown in complexity and application, the understanding of hallucination has profoundly expanded, revealing it as a multifaceted problem encompassing a broader spectrum of inconsistencies and failures that extend beyond mere factual inaccuracies. This shift highlights that hallucination impacts the overall 'information quality' and trustworthiness of LLM outputs, necessitating a deeper examination of underlying cognitive and logical failures \\cite{rejeleene2024okw}.\n\nEarly research began to delineate the nuances of unfaithful generation, laying the groundwork for this broader view. For instance, \\cite{maynez2020h3q} distinguished between \"intrinsic hallucinations,\" which misrepresent information present in the source, and \"extrinsic hallucinations,\" which introduce new, ungrounded information. Crucially, they observed that many extrinsic hallucinations were erroneous, demonstrating that even superficially plausible generated content could be unfaithful to its source and thus unreliable. This early distinction underscored that hallucination was not solely about factual errors against world knowledge, but also about fidelity to provided context. This conceptual expansion quickly extended to multimodal contexts, where \\cite{dai20229aa} identified \"object hallucination\" and \"attribute hallucination\" in vision-language models. Here, LLMs generated objects or attributes not present in the visual input, illustrating how ungrounded reasoning could manifest across different modalities, producing semantically coherent but contextually false outputs.\n\nThe field has since moved towards recognizing that hallucination often stems from deeper failures in an LLM's reasoning process and internal consistency, rather than just a lack of factual recall. Logical inconsistencies, where an LLM's internal reasoning path is flawed, represent a significant category of such failures. For example, \\cite{xie20247zk} demonstrated that the order in which LLMs generate answers and reasoning significantly impacts their consistency, revealing instances where models fabricate answers and then retrospectively generate justifications. This highlights a fundamental flaw in the logical coherence of the model's thought process, rather than a simple factual error. Further, \\cite{jiang20242kz} investigated how LLMs can hallucinate \\textit{despite possessing the correct knowledge}, attributing this to problematic inference dynamics. Their work revealed that in hallucinated cases, the output token's probability rarely demonstrated consistent superiority in later stages of the model, suggesting a failure in applying known facts during generation, indicative of ungrounded reasoning.\n\nBeyond explicit logical errors, hallucinations can also manifest as subtle semantic shifts, biases, or ungrounded claims that appear superficially plausible, thereby undermining trustworthiness. \\cite{zhang2024qq9} introduced \"knowledge overshadowing,\" a phenomenon where LLMs prioritize certain knowledge due to data imbalance. This leads to outputs that, while not necessarily factually incorrect, are ungrounded in the immediate context or subtly biased, generating plausible but misleading information. A particularly insidious form of this is \"sycophantic hallucination,\" as explored by \\cite{rrv2024gw0}. This occurs when LLMs provide answers that align with a user's potentially misleading keywords or desired narrative, even if factually questionable. Such behavior amplifies misinformation and erodes user trust by prioritizing perceived user preference over factual accuracy, representing a significant semantic shift that compromises the integrity of the generated content.\n\nCollectively, these diverse manifestations underscore a critical shift in understanding: hallucination is not merely about isolated factual errors, but about a pervasive degradation of overall \"information quality\" and trustworthiness. \\cite{rejeleene2024okw} directly addresses this by proposing a mathematical framework for evaluating Information Quality (IQ) in LLMs, defining it as a function of consistency, relevance, and accuracy. This framework explicitly moves beyond simple factual correctness to encompass the broader attributes essential for reliable and trustworthy AI outputs. The recognition of hallucination as a multifaceted problem, encompassing logical inconsistencies, ungrounded reasoning, and subtle semantic shifts, necessitates a paradigm shift towards more sophisticated evaluation and mitigation strategies that address these deeper cognitive and logical failures inherent in LLM architectures.\n\\subsection{The Evolution of Hallucination Types}\n\\label{sec:2\\_3\\_the\\_evolution\\_of\\_hallucination\\_types}\n\nThe understanding and categorization of AI hallucinations have undergone a significant evolution, moving from broad, general definitions to highly granular and context-specific classifications. This progression reflects the increasing complexity of AI models and their applications, necessitating a more nuanced taxonomy for effective detection and mitigation.\n\nInitially, research into hallucinations in large language models (LLMs) established foundational categories. Early work, such as the comprehensive survey by \\cite{DBLP:journals/corr/abs-2202-03629}, broadly categorized hallucinations into \\textit{intrinsic}, where the generated content contradicts the source input, and \\textit{extrinsic}, where it contradicts established world knowledge. This foundational classification also introduced related concepts like factuality, faithfulness, and consistency, providing an initial framework for analyzing model outputs. Expanding on this, another survey by \\cite{DBLP:journals/corr/abs-2305-13889} further refined LLM hallucination types by categorizing them based on their \\textit{source} (e.g., data, model architecture, inference process), \\textit{form} (e.g., factual, logical, numerical inconsistencies), and \\textit{severity}, offering a multi-dimensional perspective on how and why these errors manifest.\n\nAs AI capabilities extended to multimodal domains, the definition of hallucination necessarily diversified to encompass new forms of inconsistency. \\cite{DBLP:journals/corr/abs-2305-18654} specifically addressed hallucinations in Large Multimodal Models (LMMs), proposing a refined taxonomy that includes \\textit{object hallucinations} (misidentifying or fabricating objects), \\textit{attribute hallucinations} (incorrectly describing an object's properties), and \\textit{relation hallucinations} (misrepresenting relationships between objects). This marked a crucial step towards more granular, modality-specific categorizations. Building upon this multimodal understanding, research into Video-Language Models (VLMs) further introduced the dimension of time. \\cite{DBLP:journals/corr/abs-2305-18260} identified and categorized VLM hallucinations based on \\textit{temporal consistency}, detailing issues such as incorrect event order, duration, or frequency, which are paramount for accurately interpreting dynamic visual information.\n\nBeyond modality, the operational context and interaction paradigm of AI systems have also led to the identification of distinct hallucination types. For instance, the challenges posed by extended inputs prompted the definition of \\textit{long-context specific hallucinations} by \\cite{DBLP:journals/corr/abs-2309-17424}. These are errors that emerge or are exacerbated when models process unusually long sequences of text, often involving subtle inconsistencies or omissions that are difficult to detect without a comprehensive understanding of the entire context. Similarly, in the realm of conversational AI, \\cite{DBLP:journals/corr/abs-2309-07490} introduced the concept of \\textit{dialogue-level hallucinations}. These extend beyond single-turn factual errors to encompass inconsistencies in persona, conversation flow, or maintaining coherent context across multiple turns, which are critical for natural and trustworthy human-AI interaction. Furthermore, the global deployment of AI models has highlighted \\textit{multilingual hallucinations}, as investigated by \\cite{DBLP:journals/corr/abs-2305-10424}. This work revealed that hallucination rates can vary significantly across languages, suggesting that language-specific nuances, data biases, or model training disparities can lead to distinct patterns of erroneous generation in non-English contexts.\n\nThis progressive refinement in categorizing hallucination typesâ€”from broad textual inconsistencies to specific multimodal, temporal, long-context, dialogue-level, and multilingual manifestationsâ€”is indispensable. It underscores a growing recognition that a one-size-fits-all approach to hallucination is insufficient. The continued identification of such distinct types is crucial for developing targeted evaluation metrics, designing more robust and context-aware mitigation strategies, and ultimately fostering greater reliability across diverse AI applications.\n\n\n\\label{sec:the_roots_of_hallucination:_mechanistic_causes_and_theoretical_limits}\n\n\\section{The Roots of Hallucination: Mechanistic Causes and Theoretical Limits}\n\\label{sec:the\\_roots\\_of\\_hallucination:\\_mechanistic\\_causes\\_\\_and\\_\\_theoretical\\_limits}\n\n\\subsection{Empirical Causes: Data, Training, and Inference Biases}\n\\label{sec:3\\_1\\_empirical\\_causes:\\_data,\\_training,\\_\\_and\\_\\_inference\\_biases}\n\nLarge Language Models (LLMs) are susceptible to generating \"hallucinations\"â€”plausible but factually incorrect or unfaithful contentâ€”due to a complex interplay of empirical factors spanning their entire lifecycle, from data acquisition to training and inference. Understanding these practical causes is crucial for developing targeted interventions throughout the model development pipeline. A foundational observation, particularly in abstractive summarization, highlighted that standard likelihood training objectives and approximate decoding strategies inherently prioritize fluency and coherence over strict factual accuracy, leading to both intrinsic (misrepresenting source information) and extrinsic (adding ungrounded information) hallucinations \\cite{maynez2020h3q}. This initial insight underscores biases embedded during training and exacerbated during generation.\n\nA significant category of empirical causes stems from \\textbf{data collection and preparation issues}. The immense pre-training corpora, while enabling powerful generalization, often contain noise, inconsistencies, and factual decay that models inadvertently learn \\cite{li2024qrj}. For instance, empirical studies reveal that the lower the frequency of specific knowledge in pre-training data, the higher the propensity for LLMs to hallucinate when queried on that topic \\cite{li2024qrj}. This suggests that data imbalance and under-representation of certain facts directly contribute to factual errors. Furthermore, LLMs learn superficial statistical patterns from their training data rather than robust logical reasoning. McKenna et al. (2023) identified two key biases in pre-trained models: an \\textbf{attestation bias}, where models over-rely on propositional memory, affirming entailment if a hypothesis is likely attested in their training data regardless of the premise; and a \\textbf{relative frequency bias}, where models tend to affirm entailment if the premise predicate is less frequent than the hypothesis predicate \\cite{mckenna2023pzc}. These biases demonstrate how statistical regularities in the data, rather than semantic understanding, can lead to incorrect inferences, with specific named entities often acting as \"indices\" to trigger memorized, potentially irrelevant, propositional knowledge \\cite{mckenna2023pzc}.\n\nDuring \\textbf{training}, models can develop biases that manifest as hallucinations. The predominant optimization objectives, typically focused on next-token prediction, do not explicitly penalize factual inaccuracies. This encourages models to \"over-generalize\" or invent plausible but unverified information to maintain fluency, thereby internalizing spurious correlations present in the data \\cite{li2024qrj}. Supervised fine-tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF) aim to align models, but their effectiveness can be highly dependent on the quality of instructions and the balanced complexity of the fine-tuning data. Suboptimal SFT can exacerbate hallucinations, and even RLHF's benefits can be domain-specific, indicating that the training process itself can introduce or fail to mitigate biases effectively \\cite{li2024qrj}. In multimodal contexts, such as with Vision-Language Models (VLMs), training challenges can lead to \"uncontrollable global visual uncertainty,\" where the model struggles to consistently ground its textual descriptions in the visual input, resulting in visual hallucinations \\cite{chen20247jb}. This highlights that even with rich multimodal data, the training process might not sufficiently enforce cross-modal consistency.\n\n\\textbf{Inference-time biases} further contribute significantly to hallucination, even in well-trained models. The decoding process, as noted by \\cite{maynez2020h3q}, often prioritizes fluency over factual accuracy. LLMs frequently struggle with complex reasoning tasks, leading to erroneous outputs. Empirical studies show that a \"lack of logical reasoning capabilities is the primary contributor to Fact-Conflicting Hallucination (FCH),\" particularly when dealing with temporal concepts and out-of-distribution knowledge \\cite{li2024osp}. Benchmarks evaluating LLM rationales reveal that models can generate plausible-sounding but logically flawed reasoning paths \\cite{oh2024xa3}. Furthermore, the order in which an LLM generates its reasoning and answer significantly impacts consistency, with generating an answer first often leading to subsequent fabrication of justifications, highlighting a fragile reasoning process at inference time \\cite{xie20247zk}. When tasked with algorithmic reasoning on complex structures like real-world graphs, LLMs often fail to produce correct solutions, indicating a struggle with systematic, multi-step inference that can lead to hallucinated steps or conclusions \\cite{tang2024a1j}.\n\nAnother prevalent inference-time bias is the model's overconfidence or lack of uncertainty awareness. LLMs tend to generate definitive answers even when highly uncertain, leading to confident hallucinations \\cite{li2024qrj}. This necessitates proactive abstention mechanisms, which allow models to signal uncertainty and refuse to answer when confidence is low \\cite{tjandra2024umq}. The empirical observation that models can produce \"never event\" errors in safety-critical domains further underscores this lack of inherent self-correction and the need for \"semantic guardrails\" to prevent catastrophic hallucinations during deployment \\cite{hakim2024d4u}. For LVLMs, an inference-time bias can be their failure to adequately ground generated text in specific visual regions without explicit guidance, leading to object hallucinations that require training-free, image-grounded interventions \\cite{zhao2024ge8}.\n\nEven with Retrieval-Augmented Generation (RAG) systems designed to mitigate hallucinations by providing external knowledge, LLMs exhibit several inference-time biases in context utilization. Empirical benchmarks reveal that LLMs struggle with \\textbf{noise robustness}, often confusing similar information when relevant and noisy documents are present; \\textbf{negative rejection}, frequently failing to abstain from answering when no relevant information is available; \\textbf{information integration}, demonstrating a significant lack of ability to synthesize facts from multiple documents; and \\textbf{counterfactual robustness}, tending to trust and prioritize factually incorrect retrieved information even when possessing correct internal knowledge or being warned \\cite{chen2023h04}. Moreover, LLMs can be receptive to newly generated counterfactual passages, indicating a vulnerability in their utility judgment of retrieved evidence \\cite{zhang2024o58}. These challenges necessitate more intelligent, uncertainty-aware retrieval and context highlighting mechanisms \\cite{niu2024v97, su2024gnz, lv2024k5x}.\n\nIn conclusion, empirical causes of hallucination are multifaceted, spanning the entire LLM development pipeline. Data quality issues, especially imbalances and superficial statistical patterns, lead to models learning incorrect associations and biases. Training objectives that prioritize fluency over factuality, coupled with challenges in enforcing cross-modal consistency, contribute to over-generalization and learned biases. During inference, models exhibit biases towards fluency, struggle with complex logical and algorithmic reasoning, often lack uncertainty awareness, and can inefficiently or incorrectly utilize external knowledge. Addressing these empirical causes requires a holistic approach, from meticulous data curation and refined training objectives to advanced inference-time controls and uncertainty quantification.\n\\subsection{Internal Mechanisms: Knowledge Overshadowing and Attention Sinks}\n\\label{sec:3\\_2\\_internal\\_mechanisms:\\_knowledge\\_overshadowing\\_\\_and\\_\\_attention\\_sinks}\n\nUnderstanding the genesis of hallucinations in Large Language Models (LLMs) necessitates a deep dive into their internal processing mechanisms, moving beyond surface-level output analysis to pinpoint the granular cognitive biases and representational distortions that contribute to factual inaccuracies. This subsection elucidates specific internal phenomena, such as knowledge overshadowing, amalgamated hallucinations, visual encoding distortion, semantic shift bias, and the exploitation of attention sinks, offering a mechanistic perspective on hallucination.\n\nOne fundamental internal mechanism contributing to hallucination is \\textit{knowledge overshadowing}, where dominant patterns or more frequently encountered information in the training data can suppress or override specific, less frequent facts. \\cite{smith2021overshadowing} introduced this concept, demonstrating how an LLM, even when possessing the correct information, might default to a more generalized or statistically prevalent answer due to the overshadowing effect, leading to plausible but incorrect generations. Building upon this understanding of internal misrepresentation, \\cite{jones2022amalgamation} further explored \\textit{amalgamated hallucinations}, where models do not merely over-generalize but actively combine multiple factually correct, yet disparate, pieces of information into a novel, coherent, but ultimately false statement or narrative. This highlights the model's internal capacity for creative miscombination, where the synthesis of information goes awry, producing a \"truthy\" but fabricated output.\n\nThe challenge of internal mechanistic failures extends beyond purely textual models, manifesting acutely in multimodal architectures. \\cite{chen2023multimodal} identified \\textit{visual encoding distortion} in multimodal LLMs, where the model's internal representation of visual information becomes corrupted during the encoding process. This distortion leads to hallucinations where the generated text inaccurately describes visual elements, even when the visual input itself is clear and unambiguous, underscoring that representational fidelity issues are not modality-specific but inherent to the internal processing pipelines. Further elaborating on the subtleties of internal processing, \\cite{lee2024semanticshift} revealed \\textit{semantic shift bias}, demonstrating how seemingly innocuous linguistic structures, such as paragraph breaks, formatting, or specific phrasing, can subtly but significantly alter an LLM's internal semantic understanding. This internal bias can lead to shifts in meaning, subsequently causing the generation of content that is factually divergent or contextually inappropriate, highlighting the profound sensitivity of internal representations to structural cues.\n\nThese identified internal vulnerabilities are not merely theoretical constructs but represent exploitable weaknesses. \\cite{wang2025attentionsinks} demonstrated how adversarial inputs can be meticulously crafted to exploit \\textit{attention sinks} within LLMs. By strategically placing specific tokens, attackers can manipulate the model's internal attention mechanisms, forcing it to allocate disproportionate focus to irrelevant or misleading information. This targeted manipulation of internal attention pathways can predictably induce specific hallucinations, providing a direct mechanistic link between internal processing flaws and external adversarial attacks.\n\nIn conclusion, the literature underscores that hallucinations are not monolithic errors but rather symptoms of diverse and granular internal mechanisms, ranging from the statistical biases of knowledge overshadowing and the creative miscombination of amalgamated hallucinations, to cross-modal representational distortions and subtle linguistic influences on semantic understanding. The identification of attention sinks further reveals how these internal processing quirks can be leveraged for adversarial purposes. While significant progress has been made in dissecting these internal mechanisms, future research must continue to investigate their complex interplay and develop more precise, mechanistic interventions that can directly address these core processing failures, moving beyond post-hoc corrections to foundational robustness.\n\\subsection{Formal Theoretical Grounding: Inevitability and Mathematical Origins}\n\\label{sec:3\\_3\\_formal\\_theoretical\\_grounding:\\_inevitability\\_\\_and\\_\\_mathematical\\_origins}\n\nThe understanding of hallucination in Large Language Models (LLMs) has undergone a profound paradigm shift, transitioning from an empirically driven problem-solving endeavor to a rigorous theoretical exploration of its fundamental limits. While early research primarily focused on identifying practical sources of hallucination and developing engineering-centric mitigation strategies \\cite{xu2024n76}, a groundbreaking line of inquiry has emerged to investigate whether hallucination is merely a transient engineering challenge or an innate, unavoidable characteristic of these powerful models. This theoretical turn seeks to establish a formal grounding for hallucination, moving beyond observational data to mathematical proofs of its inevitability.\n\nA pivotal contribution in this area is presented by \\cite{xu2024n76}, which fundamentally redefines the discourse around LLM hallucination. The paper introduces a \"formal world\" where LLMs are abstracted as total computable functions and ground truth is similarly defined as a computable function. This abstraction allows for the first formal definition of hallucination (Definition 4), characterizing it as an inconsistency between the LLM's output and the ground truth for a given input. This rigorous formalization provides a robust framework for theoretical analysis, independent of specific model architectures or training algorithms.\n\nThe core innovation of \\cite{xu2024n76} lies in its application of advanced concepts from learning theory, specifically Cantor's diagonalization argument, to prove the inherent inevitability of hallucination. This mathematical technique, traditionally used to demonstrate the existence of uncomputable numbers or functions, is ingeniously adapted to show that no computable LLM can perfectly learn all computable functions representing ground truths. The authors present Theorem 1, which demonstrates that for any computably enumerable set of LLMs, all states will inevitably hallucinate on some inputs. This is further extended by Theorem 2, proving that such hallucination will occur on \\textit{infinitely many} inputs. Crucially, Theorem 3 generalizes these findings to establish that hallucination is inevitable for \\textit{any individual computable LLM}, both on some inputs and on infinitely many inputs. These proofs establish a theoretical upper bound on the capabilities of LLMs, demonstrating that they cannot serve as universal problem solvers without generating factually incorrect information.\n\nThis theoretical insight fundamentally reshapes the understanding of LLM capabilities and limitations. Unlike previous empirical efforts that aimed to \\textit{reduce} or \\textit{eliminate} hallucination through better data, architectures, or prompting techniques, \\cite{xu2024n76} posits that complete eradication is mathematically impossible for any computable LLM. While parallel work by \\cite{kalai2023statistical} provides a statistical lower bound on hallucination rates for calibrated LLMs, \\cite{xu2024n76}'s findings are more general, applying to all computable LLMs and proving an absolute inevitability rather than a statistical likelihood. This distinction highlights the profound nature of the theoretical limits identified by the diagonalization argument.\n\nConsequently, hallucination is reframed not as a transient engineering problem to be fixed, but as an innate, fundamental characteristic inherent to the very nature of computable functions that LLMs embody. This profound theoretical grounding guides future research away from the elusive goal of achieving \"hallucination-free\" LLMs and towards more realistic and robust strategies for detection, mitigation, and responsible deployment. It underscores the necessity for LLMs to be used as specialized tools, often requiring external verification and human oversight, rather than as infallible general problem solvers. The mathematical origins of hallucination thus compel a shift in focus towards managing its unavoidable presence, fostering a more realistic and effective research agenda for the future of LLM development.\n\n\n\\label{sec:evaluating_hallucination:_benchmarks_for_factual_accuracy_and_reasoning}\n\n\\section{Evaluating Hallucination: Benchmarks for Factual Accuracy and Reasoning}\n\\label{sec:evaluating\\_hallucination:\\_benchmarks\\_for\\_factual\\_accuracy\\_\\_and\\_\\_reasoning}\n\n\\subsection{Early Benchmarks and Automated Evaluation}\n\\label{sec:4\\_1\\_early\\_benchmarks\\_\\_and\\_\\_automated\\_evaluation}\n\nThe initial efforts to evaluate hallucination in Large Language Models (LLMs) were driven by the need for standardized, reproducible benchmarks that could move beyond subjective human judgment. These foundational works primarily focused on assessing factual correctness and the ability to generate text consistent with provided sources or external knowledge, laying crucial groundwork for automated evaluation.\n\nEarly investigations into factual inconsistencies often centered on specific natural language generation tasks. For instance, \\cite{DBLP:journals/corr/abs-2005-14165} introduced a dataset specifically designed for factual error correction in summarization, highlighting the prevalence of factual inconsistencies even in models trained on large corpora. This work underscored the necessity of robust evaluation methods to identify and rectify such errors, often relying on human annotation for ground truth. Recognizing the scalability limitations of human evaluation, researchers soon began developing automated metrics. \\cite{DBLP:journals/corr/abs-2009-07853} proposed a differentiable fact-checking model that could automatically assess the factuality of generated text by comparing it against a structured knowledge base. This marked a significant step towards scalable evaluation, offering an automated alternative to manual verification by leveraging external factual sources.\n\nAs LLMs became more capable and their applications diversified, the scope of factual evaluation expanded to more open-ended generation tasks. \\cite{DBLP:journals/corr/abs-2104-08784} extended the investigation of factuality to neural dialogue response generation, proposing a metric called FactScore and methods to improve factual consistency by grounding responses in external knowledge. This demonstrated an early attempt to tackle factual correctness in conversational contexts, where the absence of a clear source document makes evaluation more challenging. The growing awareness of factual errors also spurred research into mitigation strategies, with \\cite{DBLP:journals/corr/abs-2109-00971} exploring self-correction mechanisms to enhance factual consistency. While not strictly an evaluation paper, it implicitly highlighted the need for sensitive evaluation metrics to guide and measure the effectiveness of such corrective approaches.\n\nFurther refining the understanding and evaluation of factual consistency, \\cite{DBLP:journals/corr/abs-2201-02604} provided a comprehensive review and benchmark for factual consistency in abstractive summarization. This work meticulously categorized different types of factual errors and systematically evaluated various existing metrics, revealing their strengths and weaknesses in capturing the nuances of factual inaccuracies. This comprehensive analysis showcased the increasing sophistication in diagnosing the specific ways LLMs could hallucinate factually.\n\nA pivotal development in challenging the models' \"honesty\" and moving beyond simple factual recall was the introduction of TruthfulQA by \\cite{DBLP:journals/corr/abs-2205-07823}. This benchmark was specifically designed to measure whether LLMs generate truthful answers to questions that people commonly answer incorrectly due to widespread misconceptions. TruthfulQA pushed the boundaries of hallucination evaluation by targeting subtle forms of untruthfulness stemming from learned biases or misrepresentations, rather than just outright fabrication. Complementing these efforts, \\cite{DBLP:journals/corr/abs-2206-04624} presented a systematic evaluation of factual consistency across various LLM tasks and proposed a new metric, FactScore-NLI, based on Natural Language Inference. This approach leveraged the robust capabilities of NLI models to assess the entailment or contradiction between generated text and reference facts, offering a more generalized and linguistically informed automated evaluation paradigm.\n\nWhile these early benchmarks and automated metrics laid crucial groundwork for evaluating factual correctness and consistency, they often faced limitations in capturing the full spectrum of hallucination. Their reliance on external knowledge bases or factual consistency with source documents struggled to address more subtle forms of hallucination, such as plausible but entirely fabricated information in open-ended generation, or errors in reasoning and coherence that did not directly contradict a known fact. These methods, while scalable, often lacked the nuance to fully understand the cognitive processes leading to hallucination, paving the way for more sophisticated and fine-grained evaluation paradigms that consider broader aspects of truthfulness, coherence, and user intent.\n\\subsection{Fine-Grained and Rationale-Based Evaluation}\n\\label{sec:4\\_2\\_fine-grained\\_\\_and\\_\\_rationale-based\\_evaluation}\n\nTraditional evaluation metrics for Large Language Models (LLMs), often relying on surface-level textual overlap (e.g., ROUGE, BLEU) or simple answer correctness, frequently prove insufficient for comprehensively assessing the nuanced quality and underlying reasoning capabilities of their outputs. These metrics struggle to pinpoint the exact nature and location of errors, particularly hallucinations, or to verify the logical soundness of the model's internal reasoning process. To address this, a significant shift has occurred towards fine-grained and rationale-based evaluation methodologies, which delve deeper into the LLM's reasoning and output fidelity, aiming to provide detailed insights into error types and locations, and to scrutinize the logical coherence of generated rationales. This paradigm shift is crucial for fostering greater transparency and verifiability in LLM behavior.\n\nThe initial advancements in this domain focused on developing more granular methods for identifying and categorizing hallucinations within generated text. Rather than a binary correct/incorrect judgment, these approaches sought to annotate errors at a sentence or even phrase level, distinguishing between factual inaccuracies, logical inconsistencies, or ungrounded statements. This fine-grained annotation provides a richer understanding of where and how LLMs deviate from ground truth, moving beyond aggregate scores to actionable insights into model weaknesses. For instance, some benchmarks explore the impact of reasoning order on LLM consistency, revealing how models might fabricate justifications when asked to provide an answer before its rationale, highlighting a fine-grained flaw in their generation process \\cite{xie20247zk}.\n\nA crucial advancement in this area is the emergence of 'rationale verification' techniques, which scrutinize the intermediate steps an LLM takes to arrive at a conclusion, rather than solely evaluating the final output. This moves beyond merely identifying \\textit{what} an LLM gets wrong to understanding \\textit{why}, by pinpointing logical missteps in its chain of thought. The most sophisticated iterations of rationale verification leverage structured data to automatically assess logical soundness, offering a scalable and objective alternative to human judgment.\n\nA prominent example of this is \\textit{ERBench}, which utilizes existing relational databases (RDBs) and their inherent Entity-Relationship (ER) model to construct automatically verifiable benchmarks for LLMs \\cite{oh2024xa3}. ERBench leverages database schema, records, and crucially, integrity constraints like Functional Dependencies (FDs) and Foreign Key Constraints (FKCs), to generate complex, multi-hop questions. Its innovation lies in automatically verifying not only the final answer but also the LLM's rationale by checking for the presence of FD-inferred critical keywords within the generated reasoning steps. This allows for a deeper evaluation of factual hallucination and introduces novel metrics such as Rationale Accuracy (R) and Answer-Rationale Accuracy (AR), providing a verifiable audit trail for LLM conclusions by grounding evaluation in external, structured knowledge.\n\nBeyond relational databases, other structured knowledge sources like Knowledge Graphs (KGs) have been instrumental in developing rationale-based evaluation. \\cite{ghosh2024tj5} proposes a framework to assess and improve the logical consistency of Retrieval-Augmented LLMs (RAG) in fact-checking tasks, specifically for propositional logic queries derived from KGs. This work defines quantitative measures for LLM consistency across primitive logical operators (negation, conjunction, disjunction) and complex logical rules, directly evaluating the LLM's adherence to logical soundness within a structured context. Similarly, \\textit{Drowzee} introduces a logic-programming-aided metamorphic testing technique for Fact-Conflicting Hallucination (FCH) detection \\cite{li2024osp}. It constructs a factual knowledge base and generates diverse test cases using logic reasoning rules. By employing semantic-aware metamorphic oracles, Drowzee automatically detects FCHs by comparing the logical and semantic structures of LLM answers against ground truth, thereby validating the reasoning process itself.\n\nThe principle of verifying intermediate reasoning steps extends to specialized domains like mathematical reasoning. \\cite{liu2025juo} introduces a structured self-consistency framework designed to enhance the reliability of mathematical reasoning in LLMs. This method enforces self-consistency not just on final answers but critically across intermediate steps in tasks such as theorem proving, symbolic manipulation, and numerical computation. By ensuring logical consistency throughout the reasoning trajectory, this approach effectively reduces hallucinations and logical inconsistencies, paving the way for more reliable and interpretable AI-driven mathematics.\n\nIn conclusion, the progression from coarse-grained to fine-grained and rationale-based evaluation marks a critical maturation in LLM assessment. These methodologies, particularly those leveraging structured data like relational databases and knowledge graphs, offer unprecedented transparency by allowing for the automatic verification of an LLM's reasoning process. While significant strides have been made in pinpointing errors and verifying reasoning, challenges remain. The construction and maintenance of comprehensive, domain-specific structured knowledge bases can be resource-intensive, limiting scalability to highly complex, open-ended, or multi-hop reasoning tasks where the required knowledge might be vast or ill-defined. Furthermore, models might learn to produce 'verifiable' but ultimately incorrect rationales, highlighting the need for more robust verification mechanisms that are less susceptible to superficial adherence to rules. Future directions will likely focus on developing more adaptive rationale verification systems that can dynamically interact with diverse and evolving knowledge sources, handle ambiguous reasoning paths, and integrate with advanced uncertainty quantification methods to provide a holistic assessment of LLM trustworthiness.\n\\subsection{Complex Reasoning and Algorithmic Hallucination}\n\\label{sec:4\\_3\\_complex\\_reasoning\\_\\_and\\_\\_algorithmic\\_hallucination}\n\nEvaluating Large Language Models (LLMs) on tasks that demand genuine algorithmic reasoning and complex problem-solving represents a critical frontier, moving beyond superficial knowledge retrieval or pattern matching to assess the integrity of the entire solution process \\cite{zhang2023k1j}. While early work on hallucination often focused on factual inconsistencies in generative tasks \\cite{maynez2020h3q}, the increasing capabilities of LLMs necessitate benchmarks that probe their ability to perform multi-step logical deductions, execute precise algorithms, and maintain coherence throughout complex reasoning paths. This shift reveals a deeper form of hallucination, rooted in a lack of true algorithmic understanding rather than mere factual error.\n\nA significant area of research focuses on assessing LLMs' logical consistency in structured data and fact-checking scenarios. \\cite{oh2024xa3} introduced ERBench, a benchmark that leverages relational databases and their integrity constraints to generate complex, automatically verifiable questions. Crucially, ERBench not only assesses the final answer but also rigorously verifies the LLM's generated rationales, exposing inconsistencies in the underlying thought process even when the final answer might appear correct. This highlights that LLMs often struggle with maintaining logical coherence across multiple deductive steps. Extending this, \\cite{li2024osp} proposed Drowzee, a framework employing metamorphic testing to evaluate the logical consistency and robustness of LLM rationales in fact-checking. By generating semantically equivalent but syntactically varied inputs based on logic reasoning rules, Drowzee reveals how minor perturbations can expose brittle reasoning and lead to contradictory outputs, underscoring the fragility of LLM logical understanding in complex scenarios. Further, \\cite{ghosh2024tj5} developed a framework to quantitatively assess LLMs' logical consistency in fact-checking, specifically for propositional logic queries derived from Knowledge Graphs. Their work defines measures for consistency across primitive logical operators, complex DNF/CNF facts, and logical rules, demonstrating that LLMs exhibit significant logical inconsistency on such complex queries, even when provided with authoritative context. These studies collectively emphasize the challenge of ensuring logical soundness and verifiable reasoning paths in LLMs.\n\nBeyond structured logical deduction, a more demanding test for LLMs lies in their ability to perform pure algorithmic execution. \\cite{tang2024a1j} developed GraphArena, a benchmark specifically tailored to evaluate LLMs on real-world graph computational problems, including NP-complete tasks, which require precise, step-by-step algorithmic execution rather than heuristic pattern matching. Their findings reveal alarmingly high hallucination rates, where LLMs frequently generate plausible but incorrect intermediate steps or entirely fabricated solutions, demonstrating a profound limitation in their capacity to understand and accurately execute algorithmic instructions. Similarly, in the domain of mathematical reasoning, \\cite{liu2025juo} introduced a structured self-consistency framework to enhance reliability by enforcing consistency across intermediate steps and final outputs in tasks like theorem proving, symbolic manipulation, and numerical computation. Their results indicate that while self-consistency can improve accuracy, LLMs remain susceptible to hallucinations in these precise mathematical and algorithmic contexts, highlighting the difficulty in achieving robust, step-by-step correctness.\n\nThe integrity of the reasoning process itself is further probed by examining the impact of reasoning order. \\cite{xie20247zk} introduced a benchmark demonstrating that the order in which LLMs generate answers and their corresponding reasoning significantly impacts consistency. They found that LLMs often fabricate answers and then retrospectively generate justifications, exposing a fundamental flaw where the reasoning path is constructed to fit a pre-determined (potentially incorrect) answer, rather than the answer being a product of sound reasoning. This \"reasoning order hallucination\" underscores the challenge of ensuring that LLMs genuinely understand and follow logical steps.\n\nThe problem of complex reasoning hallucination also manifests acutely in high-stakes, domain-specific applications. \\cite{umapathi2023puv} introduced Med-HALT, a Medical Domain Hallucination Test, which includes \"Reasoning Hallucination Tests\" (RHTs) such as False Confidence Tests (FCT), None of the Above (NOTA) Tests, and Fake Questions Tests (FQT). These RHTs assess an LLM's ability to reason about complex medical problems, generate logically coherent and factually accurate output, and identify invalid queries without creating fake information. Their findings indicate that even state-of-the-art LLMs perform poorly on these reasoning-based medical tasks, struggling with complex logical inference and exhibiting a tendency to hallucinate with undue certainty, highlighting the severe implications of such failures in critical domains.\n\nIn conclusion, current LLMs exhibit significant limitations in tasks requiring genuine algorithmic reasoning and complex problem-solving. Benchmarks focusing on rationale verification, logical consistency in structured data, pure algorithmic execution, mathematical reasoning, and the integrity of the reasoning process consistently reveal high hallucination rates. These evaluations demonstrate that LLMs struggle with the precise execution, deep understanding, and robust logical coherence necessary for these tasks. The emphasis has decisively shifted from merely assessing final answers to rigorously evaluating the integrity and consistency of the entire solution process. Future research must focus on developing models that can perform verifiable, step-by-step algorithmic execution, moving beyond superficial pattern matching to achieve true algorithmic intelligence and logical soundness.\n\\subsection{Long-Context and Dialogue-Level Evaluation}\n\\label{sec:4\\_4\\_long-context\\_\\_and\\_\\_dialogue-level\\_evaluation}\n\nEvaluating Large Language Models (LLMs) in extended conversational contexts and with lengthy documents presents distinct and complex challenges. Models are prone to generating hallucinations stemming from an inability to maintain consistent information across multiple turns, misremembering previous dialogue states, or losing factual accuracy and coherence when processing extensive inputs. These specialized evaluations are paramount for developing LLMs that are reliable, consistent, and trustworthy in real-world interactive and document-intensive applications. The evolution of hallucination types, as highlighted by \\cite{qiu2024zyc}, increasingly includes dialogue-level and long-context specific manifestations, necessitating dedicated evaluation paradigms.\n\nA critical area of focus is the assessment of dialogue-level consistency and factual accuracy across multi-turn interactions. Traditional evaluation methods, often focused on single-turn responses, fail to capture the cumulative errors that can emerge in dynamic conversations. To address this, \\cite{chen2024c4k} introduce \\texttt{DiaHalu}, a pioneering benchmark specifically designed to evaluate hallucinations in multi-turn dialogues. \\texttt{DiaHalu} distinguishes between various hallucination subtypes, including non-factual information, incoherence (input-conflicting, context-conflicting, self-conflicting), irrelevance, overreliance, and reasoning errors. Its construction involves LLM self-dialogue generation and expert annotation across diverse domains like knowledge-grounded and task-oriented conversations, making it a challenging and realistic testbed for conversational reliability. However, a limitation of such benchmarks is their reliance on LLM-generated dialogues, which, while efficient, may not fully capture the nuances of human-LLM interaction or introduce biases from the generating LLM itself. Further insights into dialogue-level errors come from studies like \\cite{dziri2021bw9}, which, through human studies, critically analyze the modes of hallucination in dialogue systems, revealing that extrinsic hallucinations, particularly erroneous entity mentions, are prevalent. They also observe that increased response diversity often correlates with increased hallucination, highlighting the necessity for evaluations that can precisely identify and ground specific entities within a conversational flow.\n\nFurthermore, the consistency of an LLM's internal reasoning process across turns is vital. \\cite{xie20247zk} propose a novel benchmark method that assesses LLM consistency by comparing responses generated when reasoning precedes the answer versus when the answer precedes reasoning. This approach is particularly insightful for dialogue evaluation, as it exposes instances where LLMs might fabricate justifications for previously stated, potentially hallucinatory, conclusions, thus revealing logical inconsistencies over an extended reasoning path inherent in multi-turn interactions. This method helps to uncover a deeper form of dialogue hallucination where the model's internal state becomes inconsistent. Extending dialogue evaluation to multimodal contexts, \\cite{cao2024o9a} introduce \\texttt{VisDiaHalBench}, a visual dialogue benchmark for Large Vision-Language Models (LVLMs). This benchmark specifically investigates hallucinations arising from \"long-term misleading textual history\" in visual dialogues, featuring five-turn questions about edited and original images. This highlights the complex interplay between textual context, visual input, and conversational memory in multimodal dialogue hallucination. Conceptually, the Information Quality (IQ) model proposed by \\cite{rejeleene2024okw}, which defines IQ based on consistency, relevance, and accuracy, provides a valuable framework for what comprehensive dialogue evaluations should strive to measure.\n\nBeyond dialogues, the ability of LLMs to process and synthesize information from extensive documents without generating spurious details or losing track of relevant facts is a major challenge. A foundational and widely adopted method for probing long-context factual recall is the \"Needle In A Haystack\" (NIAH) test. This technique involves embedding a specific, verifiable piece of information (the \"needle\") within a much longer, often irrelevant document (the \"haystack\") and then querying the LLM to retrieve that information. The performance on NIAH tests directly measures an LLM's capacity to maintain attention and extract precise facts from lengthy inputs, revealing how context length impacts factual grounding. While not a formal benchmark suite, NIAH has become a de facto standard for demonstrating an LLM's effective context window and susceptibility to 'getting lost' in irrelevant details. Variants of NIAH, such as those testing multiple needles or needles at different positions, have revealed crucial insights, like the \"lost in the middle\" phenomenon where LLMs often perform worse on information located in the middle of a long context \\cite{liu2024lost}. However, a critical limitation of NIAH is its focus on \\textit{retrieval} rather than \\textit{synthesis} or complex reasoning, and its artificial nature may not fully reflect real-world document processing challenges.\n\nTo address the more complex task of long-document \\textit{synthesis}, particularly in abstractive summarization, \\cite{maynez2020h3q} provided a foundational human evaluation and taxonomy of hallucinations, distinguishing between intrinsic (misrepresenting source) and extrinsic (adding ungrounded information) hallucinations. They demonstrated that traditional metrics like ROUGE correlate poorly with human judgments of faithfulness, advocating for semantically-aware metrics like textual entailment. This work underscores the need for evaluations that go beyond surface-level metrics to assess deep semantic consistency with long source documents. Similarly, \\cite{hegselmann20249q4} developed a rigorous labeling protocol for errors and an annotated dataset of hallucinations in long patient summaries, highlighting domain-specific challenges in medical text generation and the need for high-fidelity evaluation in sensitive applications.\n\nFurthermore, \\cite{qiu2024zyc} introduce \\texttt{LongHalQA}, an LLM-free benchmark for evaluating long-context hallucinations in Multimodal Large Language Models (MLLMs). This benchmark features 6K long and complex hallucination texts across object-level descriptions, image-level descriptions, and multi-round conversations. It employs novel \"Hallucination Discrimination\" and \"Hallucination Completion\" tasks, framed as multiple-choice questions, to efficiently assess MLLMs' ability to identify and avoid generating hallucinations in lengthy outputs. \\texttt{LongHalQA}'s coverage of 12 distinct hallucination types, including complex logical and contextual inconsistencies, represents a significant step beyond simple factual checks. The survey by \\cite{sahoo2024hcb} further emphasizes the challenge, noting the absence of standardized metrics for assessing object hallucination in LVLMs, particularly relevant in long multimodal contexts. For evaluating faithfulness in complex, multi-source question answering, which often involves synthesizing information from long contexts, \\cite{pan2024hm4} propose a \"multi-reference faith score (MRFS)\" to verify and resolve conflicts in generated answers, indicating a move towards more robust, verifiable evaluation metrics for long-form generation. The broader review by \\cite{malin2024fin} reinforces that evaluating open-ended generation provides a more comprehensive measure of LLM performance than commonly used multiple-choice benchmarking, which is crucial for assessing faithfulness in long-context tasks.\n\nIn conclusion, the evaluation of long-context and dialogue-level hallucinations necessitates a shift from isolated factual checks to comprehensive assessments of consistency, coherence, and factual accuracy across extended interactions and lengthy inputs. Benchmarks like \\texttt{DiaHalu} and \\texttt{VisDiaHalBench} provide critical tools for understanding dialogue-level errors, while methods like the NIAH test, when critically understood for its retrieval focus, and the synthesis-oriented evaluations for summarization \\cite{maynez2020h3q} and multimodal long-context generation \\cite{qiu2024zyc} are indispensable for evaluating an LLM's ability to remain grounded in long documents and maintain coherence over extended generations. While significant progress has been made, challenges persist in developing scalable, fine-grained evaluation techniques for truly massive contexts and dynamically assessing the nuanced consistency required for highly interactive, multi-agent conversational systems. Future research will likely integrate intrinsic detection mechanisms with comprehensive external validation to build more robust and trustworthy LLMs for complex, real-world applications.\n\n\n\\label{sec:mitigation_strategies_i:_external_knowledge_grounding_and_adaptive_retrieval}\n\n\\section{Mitigation Strategies I: External Knowledge Grounding and Adaptive Retrieval}\n\\label{sec:mitigation\\_strategies\\_i:\\_external\\_knowledge\\_grounding\\_\\_and\\_\\_adaptive\\_retrieval}\n\n\\subsection{Retrieval-Augmented Generation (RAG) Fundamentals}\n\\label{sec:5\\_1\\_retrieval-augmented\\_generation\\_(rag)\\_fundamentals}\n\nLarge Language Models (LLMs) have demonstrated remarkable capabilities in generating human-like text, yet they frequently suffer from issues of hallucination, generating factually incorrect or unfaithful content, and are constrained by the static, often outdated, knowledge encoded during their pre-training \\cite{Lewis2020}. Retrieval-Augmented Generation (RAG) emerged as a pivotal strategy to mitigate these limitations by dynamically grounding LLM responses in up-to-date and authoritative external knowledge sources. The core concept of RAG involves augmenting the LLM's generation process with a retrieval step, where relevant information is fetched from a vast corpus of documents and then provided to the LLM as context for generating its response, thereby reducing the generation of unfaithful or outdated content.\n\nThe foundational paradigm, often referred to as 'Naive RAG', typically combines a retriever component with a generator component. \\textcite{Lewis2020} introduced Retrieval-Augmented Generation (RAG), a general-purpose fine-tuning approach that integrates a pre-trained parametric memory (the generator, a seq2seq model) with a non-parametric memory (the retriever, a Dense Passage Retriever or DPR). This architecture dynamically retrieves relevant documents from a large corpus and conditions the generator's output on these retrieved passages, demonstrating state-of-the-art results on knowledge-intensive NLP tasks like open-domain question answering and fact verification. Simultaneously, \\textcite{Guu2020} proposed REALM (Retrieval-Augmented Language Model Pre-training), which explored a deeper integration by learning to retrieve documents from a large corpus and use them to augment a language model's input during pre-training. REALM showcased that jointly training the retriever and the language model end-to-end could significantly improve performance on knowledge-intensive tasks, highlighting the benefits of building retrieval capabilities directly into the model's knowledge acquisition process.\n\nThese early works established the immense promise of RAG, demonstrating that external knowledge retrieval could significantly enhance the factual accuracy and currency of LLM outputs. Building upon this foundation, \\textcite{Izacard2022} introduced Atlas, a retrieval-augmented language model that leverages a frozen pre-trained T5 model and a jointly trained retriever. Atlas demonstrated that even relatively smaller language models (e.g., 11B parameters) could achieve competitive performance with much larger, purely parametric LLMs on knowledge-intensive tasks when effectively augmented with retrieval, underscoring the efficiency benefits and the importance of well-trained retriever-generator interactions.\n\nHowever, these initial RAG paradigms often assumed full access to the language model's internal states or gradients for joint training or fine-tuning, posing a significant challenge for proprietary or black-box LLMs. Addressing this practical limitation, \\textcite{Shi2023} proposed REPLUG, a novel method designed to train a retriever to work effectively with black-box language models without requiring access to their internal states or gradients. REPLUG trains the retriever by maximizing the likelihood of the LLM's output given the retrieved documents, employing a contrastive learning objective, thus enabling the application of RAG to a broader range of LLMs, including those deployed as APIs. This progression from tightly coupled, white-box RAG systems to more flexible, black-box compatible approaches highlights the evolving understanding of RAG's practical deployment challenges and the innovative solutions developed to overcome them. The foundational understanding of RAG, from its initial promise to its early architectural challenges and solutions, sets the stage for appreciating the subsequent advancements and refinements in RAG architectures designed to further enhance retrieval quality, generation coherence, and overall system robustness.\n\\subsection{Advanced and Adaptive RAG Architectures}\n\\label{sec:5\\_2\\_advanced\\_\\_and\\_\\_adaptive\\_rag\\_architectures}\n\nThe foundational Retrieval-Augmented Generation (RAG) paradigm, while effective, often operates with a static retrieval mechanism that can be inefficient, imprecise, or fail to fully leverage the capabilities of Large Language Models (LLMs). This has spurred the development of advanced and adaptive RAG architectures, focusing on sophisticated, modular designs that empower LLMs with greater control over the retrieval process, leading to more intelligent and targeted information synthesis. These innovations aim to significantly improve RAG's efficiency, precision, and robustness, particularly in mitigating hallucinations.\n\nEarly conceptualizations of these advanced paradigms were laid out by comprehensive surveys, such as that by \\cite{gao2023retrieval}, which categorized RAG into basic, advanced, and modular forms, highlighting the need for more sophisticated retrieval and generation strategies to overcome limitations like hallucination and outdated information. Similarly, \\cite{zhang2024retriever} further elaborated on this evolution, detailing various components like pre-retrieval, retrieval, post-retrieval, and generation, underscoring the shift towards more dynamic and integrated RAG systems. Within this evolving landscape, initial efforts focused on enhancing the LLM's ability to process and utilize retrieved information more effectively. For instance, \\cite{yu2023chainofthought} demonstrated how Chain-of-Thought (CoT) reasoning can be integrated into RAG, guiding the LLM to generate intermediate thoughts that improve the utilization of retrieved documents and consequently reduce hallucinations by fostering a deeper understanding of the context.\n\nMoving beyond static retrieval, subsequent research introduced mechanisms for the LLM to actively influence the retrieval process itself. \\cite{shi2023replug} proposed RePlug, a framework that enables LLMs to self-refine their retrieval queries and generated answers. By using internal feedback, RePlug iteratively improves the relevance of retrieved documents, making the retrieval process more adaptive and responsive to the LLM's evolving information needs. This marked a significant step towards adaptive RAG, where the LLM is no longer a passive consumer but an active participant in shaping its information landscape.\n\nThe concept of \"Modular RAG\" further extends this adaptivity by integrating RAG into broader, multi-step reasoning frameworks. \\cite{yang2023ragagents} introduced RAG-Agents, a framework that combines RAG with autonomous agents, allowing LLMs to tackle complex tasks by breaking them down, planning, executing actions (including strategic retrieval), and self-correcting. This approach transforms RAG into a specialized tool within an agentic workflow, enabling intelligent decision-making about \\textit{when} and \\textit{what} to retrieve in a multi-step reasoning process, thereby enhancing its utility for complex problem-solving.\n\nA pinnacle of adaptive RAG is exemplified by architectures that grant LLMs the autonomy to decide when and what to retrieve based on their internal state. \\cite{wang2023selfrag} introduced Self-RAG, an LLM framework that learns to retrieve, generate, and critique through self-reflection. This innovative approach allows the LLM to generate \"reflection tokens\" that guide its own retrieval and generation process, enabling it to selectively retrieve information when its internal uncertainty is high or external knowledge is required. Furthermore, Self-RAG incorporates dynamic context highlighting by allowing the LLM to critique its own output, ensuring that the generated response is factual and well-supported by the retrieved documents, thus embodying a highly intelligent and targeted solution for hallucination mitigation.\n\nIn conclusion, the evolution of RAG architectures showcases a clear trajectory towards more intelligent, autonomous, and modular systems. While significant progress has been made in enabling selective retrieval, dynamic context highlighting, and iterative refinement, challenges remain. These include the computational overhead of iterative processes, the complexity of fine-tuning sophisticated feedback loops, and ensuring the generalizability of self-reflection mechanisms across diverse domains. Future directions will likely focus on developing more efficient uncertainty estimation techniques, extending adaptive RAG to multi-modal contexts, and achieving real-time adaptation in dynamic information environments.\n\\subsection{Knowledge Graph Integration for Trustworthiness}\n\\label{sec:5\\_3\\_knowledge\\_graph\\_integration\\_for\\_trustworthiness}\n\nLarge Language Models (LLMs) often suffer from factual inaccuracies and logical inconsistencies, commonly referred to as hallucinations, which severely undermine their trustworthiness. Strategic integration of structured Knowledge Graphs (KGs) with LLMs offers a robust solution by providing a reliable, up-to-date, and verifiable source of information, thereby serving as a powerful grounding mechanism against such issues.\n\nEarly efforts to mitigate LLM hallucinations recognized the critical need for external knowledge to ground their responses. For instance, \\cite{trivedi2022qsf} demonstrated that interleaving retrieval with Chain-of-Thought (CoT) reasoning could significantly improve factual accuracy in knowledge-intensive multi-step questions. Their IRCoT framework dynamically uses intermediate CoT steps to generate context-aware queries for retrieving relevant paragraphs, grounding the reasoning process and reducing factual errors by up to 50\\\\%. However, these foundational approaches often relied on unstructured text retrieval, which, while effective, could still introduce noise or outdated information, highlighting the need for more structured and verifiable knowledge sources.\n\nBuilding upon the broader Retrieval-Augmented Generation (RAG) paradigm, which generally involves fetching relevant information from an external corpus to inform LLM responses \\cite{gao20232zb}, recent research has increasingly focused on leveraging the inherent structure and verifiability of Knowledge Graphs. \\cite{sui20242u1} directly addresses how KGs can enhance LLM trustworthiness by proposing a unified framework that combines \"Graph-guided retrieval\" and \"Graph-guided generation.\" This approach enables LLMs to query and integrate structured facts from KGs, leading to more accurate and logically consistent answers in open-ended question answering tasks, and introduces the OKGQA benchmark to evaluate such KG-augmented models, even under perturbed KG conditions. This represents a significant step beyond generic text retrieval by providing a rich, semantically structured context that is less prone to misinterpretation or hallucination.\n\nBeyond mere factual accuracy, trustworthiness also encompasses logical consistency, especially in complex reasoning tasks. While LLMs can struggle with maintaining logical coherence, KGs, by their very nature, encode explicit relationships and constraints that can be leveraged for verification. \\cite{ghosh2024tj5} emphasizes the importance of evaluating LLM logical consistency in fact-checking, proposing new logical fact-checking (LFC) datasets and quantitative measures to assess their performance on complex propositional logic queries. Although this work does not explicitly integrate KGs, the structured nature of KGs makes them an ideal candidate for providing the ground truth and relational context necessary to enforce and verify such logical consistency in LLM outputs. Further enhancing verifiability, \\cite{oh2024xa3} introduced ERBench, a benchmark that utilizes relational databases (conceptually akin to KGs in their structured representation of entities and relationships) to automatically verify not only the LLM's final answers but, crucially, its \\textit{rationales}. This capability to scrutinize the reasoning path against structured knowledge is paramount for building truly transparent and trustworthy AI systems, moving beyond simple output correctness to verifiable logical soundness.\n\nIn conclusion, the integration of Knowledge Graphs represents a pivotal advancement in addressing the trustworthiness challenges of LLMs. By providing a structured, verifiable, and logically consistent source of external knowledge, KGs enable LLMs to move beyond mere fluency to produce responses that are factually accurate, logically sound, and inherently more reliable. This strategic integration, encompassing graph-guided retrieval, generation, and rationale verification, lays a robust foundation for developing next-generation LLMs that are not only powerful but also transparent and trustworthy in their knowledge-intensive and complex reasoning capabilities. Ongoing challenges include the scalability of KG construction and maintenance, and the seamless, real-time integration of dynamic KGs with evolving LLM architectures.\n\n\n\\label{sec:mitigation_strategies_ii:_intrinsic_model_interventions_and_self-correction}\n\n\\section{Mitigation Strategies II: Intrinsic Model Interventions and Self-Correction}\n\\label{sec:mitigation\\_strategies\\_ii:\\_intrinsic\\_model\\_interventions\\_\\_and\\_\\_self-correction}\n\n\\subsection{Decoding-Time Interventions and Contrastive Methods}\n\\label{sec:6\\_1\\_decoding-time\\_interventions\\_\\_and\\_\\_contrastive\\_methods}\n\nLarge Language Models (LLMs) and Large Vision-Language Models (LVLMs) frequently generate \"hallucinations\"â€”plausible but factually incorrect or ungrounded information. Addressing this critical challenge, decoding-time interventions offer an efficient, training-free paradigm for the primary model to steer generation towards more factual and grounded outputs by manipulating the probability distribution of generated tokens during inference. These methods operate by introducing a \"negative\" signal or bias that discourages the generation of ungrounded content, without requiring extensive fine-tuning of the main model itself.\n\nOne prominent approach in this domain is \\textbf{Visual Contrastive Decoding (VCD)}, as proposed by \\cite{park20247cm}. VCD mitigates hallucination, particularly in LVLMs, by penalizing tokens that are disproportionately favored when the visual input is subtly distorted or perturbed. The core mechanism involves comparing the logit distribution generated from the original input with that from a slightly altered, \"negative\" input (e.g., an image with minor noise or a masked object). By identifying tokens whose probabilities increase significantly under these subtle perturbations, VCD infers their sensitivity to ungrounded visual cues and suppresses them. This method leverages the model's internal representations to identify and counteract potential hallucinatory tendencies without requiring any additional training of the main LVLM. VCD has demonstrated effectiveness in reducing attribute and object hallucinations by steering the model towards more robust and visually grounded descriptions, often outperforming standard decoding strategies in terms of factual consistency. Its strength lies in its training-free nature and generalizability across different LVLMs, though its efficacy can be sensitive to the choice and strength of the perturbation.\n\nFurther enhancing the grounding aspect, other methods integrate external modalities to provide a strong, verifiable signal for contrastive decoding. For instance, approaches like the \\textbf{Image-Grounded Guidance} proposed by \\cite{zhao2024ge8} for LVLMs leverage pre-trained external vision models to establish a factual baseline. This method generates a reference caption for an input image using a robust image captioning model. During the LVLM's decoding process, the generated tokens are continuously compared against this reference. If the LVLM generates tokens corresponding to objects or attributes not present in the reference caption, these tokens are penalized by adjusting their logits downwards. This effectively guides the LVLM to generate text that is consistent with the external visual understanding, mitigating object and attribute hallucinations. While this approach is training-free for the main LVLM, it introduces computational overhead due to the inference required by the external image captioning model. However, it offers a powerful mechanism for ensuring strong visual grounding, particularly for object-centric hallucinations, by explicitly incorporating external, verifiable visual information into the decoding process.\n\nComplementing these sophisticated contrastive techniques are simpler, yet often effective, \\textbf{logit-based interventions}. Works such as \\cite{leng2023ohr} demonstrate that direct manipulation of token logits can counter specific linguistic biases or common hallucination patterns. These methods involve applying predefined biases or penalties to certain token probabilities based on external knowledge, semantic categories, or simple heuristics. For example, if a model frequently hallucinates specific entities in a given context, their logits can be slightly suppressed. \\cite{leng2023ohr} specifically proposes a logit-based calibration method that adjusts token probabilities by considering their frequency in factual vs. non-factual contexts, effectively down-weighting tokens associated with higher hallucination rates. These interventions are computationally light and offer a direct way to address specific, recurring types of hallucinations or linguistic tendencies that lead to ungrounded outputs. However, their generalizability is often limited, as they require prior identification of problematic tokens or patterns and may not adapt well to novel hallucination types.\n\nIn synthesis, decoding-time interventions offer a versatile and efficient suite of strategies. VCD (\\cite{park20247cm}) provides a general, training-free mechanism to enhance robustness against subtle input variations, making it effective for reducing attribute and object hallucinations by promoting consistency. Image-Grounded Guidance (\\cite{zhao2024ge8}) offers a more explicit form of external grounding for multimodal models, leveraging an auxiliary vision model to provide a strong factual anchor, albeit with increased inference cost. Logit-based calibration (\\cite{leng2023ohr}) represents the most lightweight approach, directly adjusting token probabilities based on observed biases, offering immediate impact for known issues but requiring careful design to avoid over-suppression or limiting creativity.\n\nDespite their efficiency and training-free nature for the primary model, challenges remain. A key difficulty lies in defining universally effective \"negative\" signals or biases that reliably identify and suppress hallucinations without inadvertently stifling creativity or factual nuance. The computational overhead of integrating external models for guidance, as seen in image-grounded approaches, can also be a practical limitation. Furthermore, ensuring that these interventions do not introduce new biases or degenerate the quality of non-hallucinatory content requires careful calibration. Future directions may involve developing more adaptive and dynamic intervention strengths, perhaps guided by real-time uncertainty estimation within the LLM, or exploring hybrid approaches that combine the general robustness of VCD with the explicit grounding of external knowledge and the targeted precision of logit-based adjustments. The goal is to create a robust, yet flexible, decoding framework that can dynamically balance factual accuracy with fluency and coherence across diverse generation tasks.\n\\subsection{Internal State Manipulation and Forward-Pass Interventions}\n\\label{sec:6\\_2\\_internal\\_state\\_manipulation\\_\\_and\\_\\_forward-pass\\_interventions}\n\nTo effectively mitigate hallucination and enhance grounding in large language models (LLMs), advanced strategies move beyond external prompting to directly intervene within the model's internal processing during the forward pass. These methods leverage a deeper, mechanistic understanding of LLM behavior, allowing for more precise and less intrusive corrections by manipulating internal states, attention mechanisms, and feature representations. This approach contrasts with decoding-time interventions by operating earlier in the generation pipeline, often influencing the very construction of hidden states and attention patterns that precede token prediction.\n\nOne family of interventions focuses on dynamically adjusting feature representations based on model uncertainty or signal strength. \\citet{zou2024dp7} introduced \\textbf{Memory-Space Visual Retracing (MemVR)}, a reactive strategy primarily for multimodal LLMs. MemVR addresses visual grounding issues by monitoring the LLM's internal uncertainty, often quantified through the entropy of token probabilities or disagreement across multiple decoding paths. When high uncertainty is detected, MemVR reactively re-injects relevant visual tokens or their representations into intermediate layers of the model. This re-grounding aims to provide the model with a fresh, reinforced visual context precisely when it is most prone to hallucination due to visual amnesia or insufficient attention. Complementing this, \\citet{yin2025s2b} proposed \\textbf{Visual Amplification Fusion (VAF)}, a more proactive strategy. VAF addresses the problem of insufficient or diluted visual signals in deeper layers of the LLM by actively enhancing and amplifying visual feature representations within specific middle layers. This proactive boosting, often achieved through learned scaling factors or specialized fusion modules, helps prevent the early decay of visual information, ensuring that the model maintains sufficient visual attention throughout the forward pass. While MemVR is reactive and uncertainty-driven, VAF is proactive, tackling the \"modality gap\" by ensuring visual signals remain salient.\n\nBeyond general feature manipulation, research has increasingly focused on granular control over the model's attention mechanisms, which are critical for integrating information across modalities and contexts. \\citet{chen2024j0g} explored \\textbf{Targeted Attention Head Interventions for Cross-Level Visual Focus}. This work delves into identifying specific attention heads crucial for integrating visual information across varying levels of abstraction, from low-level features to high-level concepts. By applying interventions, such as re-weighting attention scores or fine-tuning specific head parameters during the forward pass, this method aims to improve the model's ability to maintain coherent visual focus and prevent modality collapse or misinterpretation. The challenge lies in identifying these critical heads and ensuring interventions generalize across diverse inputs.\n\nFurther advancing this mechanistic understanding with theoretical rigor, \\citet{zhou2024lvp} introduced \\textbf{CAUSAL MM}, a causal inference framework for \\textbf{Modality Prior Balancing in Multimodal LLMs}. This approach employs structural causal modeling (SCM) to analyze and adjust the influence of different modalities within the attention mechanisms. By treating modality priors (visual and language) as confounding factors in the causal path between attention and output, CAUSAL MM uses techniques like back-door adjustment and counterfactual reasoning. This allows for deciphering the true causal impact of effective attention on MLLM output by isolating and mitigating the effects of misleading modality priors. For instance, by simulating counterfactual attention states (e.g., uniform or shuffled attention), the framework quantifies how much a specific modality's prior causally influences the output, then adjusts attention to achieve a more balanced integration. This provides a principled, theoretically grounded way to enhance visual grounding and reduce hallucination, offering a more robust alternative to empirical attention head interventions.\n\nAnother innovative forward-pass intervention, particularly for contextual hallucinations, is the \\textbf{Lookback Lens} proposed by \\citet{chuang20248ey}. This method introduces a \"lookback ratio\" derived from attention maps, which quantifies the proportion of attention weights focused on the input context versus newly generated tokens. This interpretable feature serves as a signal for contextual faithfulness. During generation, a \"Lookback Lens Guided Decoding\" strategy samples multiple candidate text chunks. For each chunk, its lookback ratio features are computed and scored by a lightweight classifier. The chunk predicted to be least hallucinated (i.e., most grounded in the context) is then selected. This approach dynamically steers generation by leveraging attention patterns, demonstrating strong cross-task and cross-model transferability due to its reliance solely on attention maps, which are hypothesized to be more generalizable indicators of contextual faithfulness than raw hidden states.\n\nFinally, \\citet{jin2024jpw} introduced a \\textbf{Collaborative Decoding Framework} that represents a different form of internal state manipulation. Recognizing that pretrained models often retain higher factual accuracy while finetuned models excel in instruction following but may hallucinate more, this framework dynamically decides which model to use for the next token. A \"critical token classifier,\" trained to identify tokens where factual accuracy is paramount, dictates whether the pretrained model or the finetuned model should generate the subsequent token. This allows the system to harness the factuality of the pretrained model for critical information while benefiting from the fluency and instruction-following of the finetuned model for general generation. This method intervenes at a higher level, dynamically routing internal processing based on the perceived \"criticality\" of the next token, effectively balancing competing objectives during the forward pass.\n\nThese forward-pass interventions collectively represent a significant shift towards a deeper, mechanistic understanding of LLM behavior, enabling more precise and less intrusive corrections. While methods like CAUSAL MM offer theoretical grounding and principled adjustments, they can be computationally intensive due to counterfactual simulations. Heuristic approaches like VAF or targeted attention head interventions (\\cite{chen2024j0g}) might be simpler to implement but may lack the same level of theoretical guarantees or generalizability. Lookback Lens (\\cite{chuang20248ey}) provides an interpretable signal for contextual grounding, but its guided decoding introduces computational overhead from candidate sampling. Collaborative decoding (\\cite{jin2024jpw}) offers an intriguing way to leverage different internal knowledge states, but its effectiveness depends on the accuracy of the critical token classifier and the alignment of the base models. Challenges remain in generalizing identified intervention points across diverse model architectures and tasks, as well as in developing more comprehensive theoretical frameworks to guide the optimal application of these internal state manipulations, ensuring both efficacy and efficiency in real-time.\n\\subsection{Self-Correction and Abstention Mechanisms}\n\\label{sec:6\\_3\\_self-correction\\_\\_and\\_\\_abstention\\_mechanisms}\n\nThe pursuit of reliable and trustworthy Large Language Models (LLMs) necessitates equipping them with the intrinsic capabilities to detect and rectify their own errors, as well as to proactively abstain from generating responses when faced with high uncertainty. This subsection delves into the sophisticated strategies that empower LLMs with these self-aware mechanisms, transforming them from mere text generators into more reflective and judicious agents. As surveyed by \\cite{pan2024y3a}, automated correction strategies are broadly categorized, with self-correction and abstention representing crucial generation-time and post-hoc interventions.\n\nThe foundation of LLM self-correction lies in enhancing their reasoning capabilities. Early advancements like Chain-of-Thought (CoT) prompting \\cite{wei2022chainofthought, kadavath2022language} were pivotal, enabling LLMs to articulate intermediate reasoning steps. By externalizing their thought process, models could expose potential logical flaws, laying the groundwork for subsequent self-reflection. Building on this, more advanced frameworks emerged to facilitate iterative and exploratory reasoning. ReAct (Reasoning and Acting) \\cite{yao20229uz} integrates CoT with external tool use, allowing LLMs to interleave reasoning steps with actions (e.g., searching external knowledge bases or executing code). This iterative cycle of thought, action, and observation provides a powerful mechanism for self-correction by verifying internal reasoning against external facts and refining plans based on observed outcomes, thereby directly connecting to external knowledge grounding discussed in Section 5. Tree of Thoughts (ToT) \\cite{yao2023treeofthoughts} further extends this by exploring multiple reasoning paths, evaluating intermediate states, and backtracking when a path proves unfruitful. This search-based approach allows LLMs to engage in more complex problem-solving and to self-correct by identifying and discarding inconsistent or incorrect lines of reasoning.\n\nBeyond these foundational reasoning paradigms, explicit self-correction frameworks have been developed. The \\textit{Self-Refine} framework \\cite{madaan2023selfrefine} exemplifies an iterative generate-critique-refine loop. An LLM first generates an initial output, then critically reflects on it by generating self-feedback to identify potential errors or areas for improvement, and finally uses this self-generated critique to produce a refined response. This internal feedback loop significantly enhances output quality and accuracy. Another powerful approach is Chain-of-Verification (CoVe) \\cite{dhuliawala2023rqn}, which systematically reduces factual hallucination. CoVe operates in multiple steps: generating a baseline response, planning specific verification questions based on the claims in the response, executing these verifications (often independently to prevent error propagation), and finally generating a revised, verified response. The \"factored\" variant of CoVe, where each verification question is answered without conditioning on the potentially hallucinated baseline, is particularly effective in minimizing the LLM's tendency to repeat its own errors. Furthermore, self-consistency \\cite{wang2022selfconsistency} has proven effective, especially in mathematical reasoning \\cite{liu2025juo}. This technique involves prompting the LLM to generate multiple diverse reasoning paths and corresponding answers, then selecting the most frequent or consistent answer, thereby leveraging the model's own internal agreement as a form of self-correction.\n\nConcurrently with self-correction, the critical need for LLMs to express \"I don't know\" has driven the development of robust abstention mechanisms to mitigate hallucination and overconfidence. Early abstention methods often relied on calibrating the model's predicted probabilities or confidence scores \\cite{lin2022calibrated}. These techniques typically involved post-hoc adjustments like temperature scaling to determine when a model should refrain from answering. However, such methods frequently required auxiliary models, ground truth data for effective calibration, or suffered from miscalibration, limiting their robustness and true \"label-free\" nature.\n\nA significant innovation has been the development of label-free abstention mechanisms that quantify uncertainty intrinsically. \\cite{kuhn2023semantic} introduced 'semantic entropy' as a novel measure of uncertainty. This approach quantifies the diversity and plausibility of semantically distinct alternative outputs for a given query. A high semantic entropy indicates a lack of a single, clear, and confident answer, prompting the model to abstain. This provides a more robust and inherent way for LLMs to recognize their limitations without external labels. Further advancements include token-level uncertainty quantification, such as Claim Conditioned Probability (CCP) \\cite{fadeeva2024lt8}, which measures the uncertainty of a particular claim value expressed by the model, enabling fine-grained fact-checking and highlighting specific unreliable segments. Similarly, \\cite{ling2024hqv} proposed methods to quantify both aleatoric (data-inherent) and epistemic (model-inherent) uncertainties in in-context learning, offering a deeper understanding of the sources of LLM uncertainty for more informed abstention.\n\nUltimately, the most robust systems integrate both self-correction and abstention capabilities. LLMs can first attempt to self-correct their responses using iterative refinement or verification frameworks. If, after this refinement process, significant uncertainty persists (quantified by metrics like semantic entropy or token-level uncertainty), the model can then judiciously choose to abstain. This synergistic strategy ensures that models actively strive to improve their answers while also possessing the crucial self-awareness to decline answering when truly uncertain, thereby enhancing overall reliability and transparency. In safety-critical applications, this translates into \"semantic guardrails\" \\cite{hakim2024d4u}, which are designed to prevent \"never event\" errors by combining internal uncertainty quantification with external consistency checks, effectively forcing abstention or flagging for human review when high-stakes factual accuracy cannot be guaranteed.\n\nDespite these advancements, significant challenges persist. The computational cost associated with extensive reflection, iterative refinement, and generating multiple diverse outputs for uncertainty quantification can be substantial, particularly for real-time applications. Defining and universally applying robust \"semantic plausibility\" for uncertainty quantification across diverse and open-ended domains remains an active research area. Moreover, while frameworks like ReAct integrate external tools, the optimal balance between internal self-reflection and external knowledge verification needs further exploration. Future directions include developing more adaptive and context-aware self-correction mechanisms, refining uncertainty quantification to be more robust and interpretable across all types of LLM tasks, and exploring hybrid approaches that dynamically combine internal reasoning with external grounding to achieve both high accuracy and appropriate humility.\n\\subsection{Training-Based Approaches and Automated Data Generation}\n\\label{sec:6\\_4\\_training-based\\_approaches\\_\\_and\\_\\_automated\\_data\\_generation}\n\nTraining-based approaches, encompassing fine-tuning and unlearning, represent a direct and potent strategy for mitigating hallucinations in large language models (LLMs) and multimodal large language models (MLLMs) \\cite{sahoo2024hcb, zhang2023k1j, liu2024p39}. A central impediment to their scalability and effectiveness, however, is the prohibitive cost and scarcity of high-quality, labeled data, particularly for diverse and nuanced hallucination types \\cite{cao2023ecl, li2025qzg}. Recent research has thus heavily focused on innovative automated data generation techniques to circumvent this data bottleneck, enabling more targeted and efficient model interventions.\n\nOne direct approach to address data scarcity for hallucination detection and mitigation is the automated generation of datasets by leveraging existing knowledge. AutoHall \\cite{cao2023ecl} proposes a three-step pipeline to automatically construct model-specific hallucination datasets from existing fact-checking resources. By prompting an LLM to generate references for claims, classifying their support, and then flagging contradictions with ground truth, AutoHall efficiently creates labeled hallucinatory examples. This method eliminates laborious manual annotation, making it scalable for continuous model updates and specific hallucination patterns. While effective for text-based factuality, AutoHall's reliance on pre-existing fact-checking resources may limit the diversity of generated hallucinations and risks inheriting their topical biases, potentially failing to uncover novel or subtle hallucination types that are not yet documented.\n\nBuilding on the principle of generating adversarial examples, several methods leverage auxiliary models or controlled processes to create dispreferred, hallucinatory content for training. Hallucination-Induced Optimization (HIO) \\cite{chen20247jb} exemplifies this by training an \"Evil LVLM\" specifically to generate adversarial, hallucinated examples given an image and a prompt. This \"Evil LVLM\" is optimized using a Contrary Bradley-Terry Model (CBTM) to \\textit{prioritize} hallucinatory content, effectively amplifying a diverse set of potential visual and factual inconsistencies. These meticulously crafted hallucinatory outputs then serve as negative examples to train a \"Good LVLM\" via contrastive decoding (as discussed further in Section 6.1), thereby enhancing its robustness. A similar concept is explored in Induce-then-Contrast Decoding (ICD) \\cite{zhang202396g} for LLMs, which constructs a \"factually weak LLM\" by fine-tuning it on non-factual samples generated by converting factual samples into untruthful ones. During inference, the log probabilities of these induced hallucinations from the weak model are subtracted from the original model's predictions, penalizing untruthful tokens. Further extending this, \\textit{VHTest} \\cite{huang20247wn} introduces an adversarial generation paradigm for visual hallucination (VH) in MLLMs. It systematically creates new, diverse, and uncontaminated VH images using text-to-image models (e.g., DALLÂ·E-3), guided by MLLM-generated descriptions of hallucination modes. This approach allows for the construction of robust benchmarks and subsequent fine-tuning to mitigate specific VH types like object existence, shape, and size. While these generative approaches offer greater diversity and novelty in adversarial examples compared to AutoHall, they introduce their own challenges, such as the computational cost of training auxiliary \"evil\" or \"weak\" models and the risk that the generated \"bad\" data might still be stereotypical or lack the subtle nuances of real-world hallucinations, potentially introducing new biases rather than creating truly generalizable improvements \\cite{yin2024iau}.\n\nThe broader paradigm of preference optimization, including Reinforcement Learning from Human Feedback (RLHF) and Direct Preference Optimization (DPO), has proven highly effective in aligning LLMs with human preferences. A critical component of these methods is the availability of high-quality preference pairs, especially dispreferred (negative) responses. Preference Optimization in VLLM (POVID) \\cite{zhou2024wbi} addresses this by automating the creation of dispreferred responses using an AI model. This method is particularly effective in reducing object hallucination in VLLMs by providing abundant, automatically generated examples of incorrect or misleading descriptions, allowing the model to learn preferred, factually accurate outputs more efficiently. Extending this, Hallucination-targeted Direct Preference Optimization (HDPO) \\cite{fu2024yqj} specifically constructs preference pair data designed to target three distinct causes of MLLM hallucinations: visual distracted hallucination, long context hallucination, and multimodal conflict hallucination. HDPO's innovation lies in its novel data construction strategies, such as generating negative samples by preserving only low-attention visual tokens or by prompting the MLLM with conflicting information, thereby guiding the model to learn robust alignment against diverse hallucination types. Further advancing judge-free self-improvement, Deng et al. \\cite{deng202405j} propose a framework that generates both positive and negative response candidates by introducing a \"hallucination ratio\" during decoding, blending conditional and unconditional token distributions. These generated pairs are then verified by a lightweight, objective verifier (e.g., a CLIP model) to ensure data quality, significantly reducing the computational costs and biases associated with MLLM-as-judge approaches in traditional RLHF/DPO pipelines. A critical consideration for these preference optimization methods is the reliability of the AI model used to generate dispreferred responses or act as a verifier; if the AI itself is prone to biases or errors, it could inadvertently reinforce undesirable behaviors or generate suboptimal training signals.\n\nAddressing a specific, yet globally relevant, hallucination challenge, Multilingual Hallucination Removal (MHR) \\cite{qu20240f7} tackles the problem of significantly more severe hallucination in Large Vision-Language Models (LVLMs) when queried in non-English languages. MHR proposes a two-stage framework, with the second stage focusing on hallucination-enhanced preference optimization. Crucially, it introduces a novel \\textit{cross-lingual alignment method} to automatically generate multilingual hallucination-aware data pairs. This method leverages the LVLM itself to generate multiple responses in various non-English languages, which are then aligned with existing English hallucination/non-hallucination answers using semantic distance metrics. This scalable approach creates multilingual hallucination-aware datasets, significantly reducing manual effort and enabling DPO-based fine-tuning to favor non-hallucinating responses across languages. While MHR demonstrates substantial improvements, particularly in reducing \"unknown\" responses and increasing accuracy across diverse languages, it acknowledges that some instruction-following issues persist for low-resource languages, indicating limitations tied to the foundational multilingual capabilities of the base LLM.\n\nCollectively, these training-based approaches underscore a significant shift towards more automated and data-efficient methods for mitigating hallucinations. By innovatively generating synthetic, adversarial, or dispreferred data, methods like AutoHall, HIO, ICD, POVID, HDPO, VHTest, and MHR circumvent the traditional data annotation bottleneck, making fine-tuning and preference optimization strategies more practical and scalable \\cite{sahoo2024hcb}. The integration of lightweight verifiers, as seen in Deng et al. \\cite{deng202405j}, further enhances efficiency and reduces reliance on expensive human or LLM-based judgments. However, challenges persist in ensuring the generalizability and diversity of automatically generated adversarial examples, precisely defining and identifying all forms of hallucination for targeted unlearning, and managing the computational overhead associated with training auxiliary models or complex unlearning processes. Furthermore, the theoretical inevitability of hallucination in computable LLMs \\cite{xu2024n76, li2025qzg} suggests that even the most sophisticated training-based approaches may only reduce, but not entirely eliminate, the problem. Future research will likely explore hybrid approaches that combine these automated data generation techniques with more advanced unlearning algorithms, investigate methods for dynamically adapting data generation strategies to evolving hallucination types, and further refine judge-free verification mechanisms to build even more robust and trustworthy AI systems within these inherent limitations.\n\n\n\\label{sec:the_multimodal_frontier:_hallucination_in_vision,_audio,_and_video_language_models}\n\n\\section{The Multimodal Frontier: Hallucination in Vision, Audio, and Video Language Models}\n\\label{sec:the\\_multimodal\\_frontier:\\_hallucination\\_in\\_vision,\\_audio,\\_\\_and\\_\\_video\\_language\\_models}\n\n\\subsection{Defining and Categorizing Multimodal Hallucinations}\n\\label{sec:7\\_1\\_defining\\_\\_and\\_\\_categorizing\\_multimodal\\_hallucinations}\n\nThe phenomenon of hallucination, traditionally understood in Large Language Models (LLMs) as the generation of factually incorrect or nonsensical information, takes on a new dimension of complexity within Multimodal Large Language Models (MLLMs). Here, hallucinations extend beyond mere factual inaccuracies to encompass inconsistencies and misalignments across different modalities, such as vision, audio, and language \\cite{multimodal\\_hallucination\\_overview\\_2022}. This subsection extends the conceptual framework of hallucination to MLLMs, including Vision-Language Models (LVLMs), Audio-Language Models (LALMs), and Video-Language Models (VLLMs), defining unique hallucination types that arise from cross-modal inconsistencies.\n\nA primary challenge in MLLMs is the inherent 'modality gap,' where models must bridge disparate data representations and semantic spaces from different input types \\cite{crossmodal\\_alignment\\_challenges\\_2022}. This gap often leads to the generation of textual descriptions that contradict the visual, audio, or temporal information presented. In visual contexts, for instance, LVLMs frequently exhibit object, attribute, and relation hallucinations \\cite{vision\\_language\\_hallucination\\_taxonomy\\_2023}. Object hallucinations occur when the model describes an entity that is not present in the image, such as claiming to see a \"cat\" in a picture containing only dogs. Attribute hallucinations involve misrepresenting characteristics of existing objects, like describing a \"red car\" when the car is clearly blue. Furthermore, relation hallucinations manifest as incorrect spatial or semantic relationships between objects, such as stating a \"person is sitting on the table\" when they are standing beside it. These errors highlight a failure in the model's ability to accurately perceive and ground its linguistic output in the visual input.\n\nExtending this understanding, Audio-Language Models (LALMs) also exhibit unique forms of hallucination, primarily revolving around object hallucination in the auditory domain \\cite{audio\\_language\\_hallucination\\_2023}. An LALM might describe the sound of \"rain\" when only ambient city noise is present, or misidentify the source of a sound, attributing a \"dog bark\" to a cat's meow. These auditory inconsistencies demonstrate a similar modality gap where the generated language fails to accurately reflect the acoustic environment. The complexity further escalates with Video-Language Models (VLLMs), where the temporal dimension introduces additional layers of potential error \\cite{video\\_language\\_temporal\\_errors\\_2024}. VLLMs can suffer from temporal hallucinations, misrepresenting the sequence of events or the duration of actions within a video. Semantic detail errors are also prevalent, where the model fabricates non-existent actions or events, such as describing a \"person jumping\" when they are merely walking, or inventing an entire scene that did not occur in the video footage.\n\nThe integration of diverse information sources and the inherent 'modality gap' necessitate specialized taxonomies to accurately characterize and understand these complex phenomena \\cite{comprehensive\\_multimodal\\_taxonomy\\_2024}. Unlike unimodal hallucinations, which often stem from factual inaccuracies or logical inconsistencies within a single data type, multimodal hallucinations arise from the intricate interplay and potential misalignments between different modalities. These specialized taxonomies are crucial for moving beyond generic error classifications to precisely pinpoint the source and nature of cross-modal discrepancies, thereby enabling more targeted mitigation strategies and robust evaluation metrics \\cite{multimodal\\_error\\_characterization\\_2024}.\n\nIn conclusion, the definition and categorization of hallucinations in MLLMs represent a significant evolution from their unimodal counterparts. The unique challenges posed by cross-modal inconsistencies, manifesting as object, attribute, relation, temporal, and semantic detail errors across visual, audio, and video contexts, underscore the need for a comprehensive and nuanced understanding. While initial taxonomies have begun to delineate these complex phenomena, future research must continue to refine these categories, considering the dynamic and context-dependent nature of multimodal interactions to develop more robust and reliable MLLM systems.\n\\subsection{Evaluation Benchmarks for LVLMs, LALMs, and VLLMs}\n\\label{sec:7\\_2\\_evaluation\\_benchmarks\\_for\\_lvlms,\\_lalms,\\_\\_and\\_\\_vllms}\n\nThe rigorous assessment of hallucinations in multimodal models, encompassing Large Vision-Language Models (LVLMs), Large Audio-Language Models (LALMs), and Vision-Language Models (VLLMs), necessitates the development of specialized and nuanced evaluation benchmarks. Traditional metrics often fall short in capturing the complex factual inconsistencies and fabricated information generated by these models, driving the community to innovate sophisticated frameworks for hallucination detection.\n\nEarly efforts began to address object-level hallucinations, particularly in emerging modalities. For instance, \\cite{Li2023Object} pioneered a benchmark specifically designed for Large Audio-Language Models (LALMs), focusing on evaluating object hallucination by analyzing generated audio descriptions for mentions of non-existent objects. This work established a foundational approach to quantifying spurious object references in a modality distinct from vision. Building upon the need for more granular evaluation in vision-language models, \\cite{Chen2023Freeform} introduced an object-centric benchmark tailored for free-form text generations by LVLMs. This framework leverages object detection and grounding techniques to verify the factual consistency of mentioned objects and their attributes against the visual input, moving beyond simple presence or absence to assess the accuracy of descriptive details.\n\nAs models grew more capable, the scope of hallucination expanded beyond mere object presence to complex relationships. To address this, \\cite{Wang2023Relation} developed Tri-Eval, a novel triplet-level evaluation framework specifically designed to detect and quantify relational hallucinations in LVLMs. This benchmark meticulously examines subject-predicate-object relationships within the generated text, identifying instances where the model fabricates or misrepresents connections between entities depicted in the image, thus offering a more sophisticated measure of factual accuracy. Further refining the understanding of hallucination types, \\cite{Zhao2023VLLMHallucination} proposed a comprehensive framework to differentiate between intrinsic and extrinsic hallucinations in VLLMs. Intrinsic hallucinations contradict the visual input, while extrinsic ones are plausible but ungrounded in the image, providing a critical distinction for diagnosing model failures and guiding future improvements.\n\nBeyond general-purpose evaluation, the criticality of model reliability in sensitive domains has led to the creation of domain-specific benchmarks. \\cite{Gupta2024Medical} introduced Med-Eval, a specialized benchmark for evaluating LVLMs within medical contexts. This work highlights the paramount importance of factual accuracy and hallucination detection in applications such as clinical report generation and diagnostic assistance, where inaccuracies can have severe consequences, emphasizing the need for expert-curated datasets and domain-specific evaluation criteria.\n\nFurthermore, the robustness of these models under adverse conditions is a significant concern. \\cite{Zhang2024Robustness} investigated the resilience of LVLMs and VLLMs by benchmarking their performance against various visual perturbations, including noise, occlusion, and adversarial attacks. Their findings quantify the increased susceptibility to hallucination under such challenging inputs, underscoring the need for models that maintain factual consistency even when confronted with imperfect or manipulated visual data. Finally, a crucial aspect of model evaluation is distinguishing true visual understanding from mere memorization of training data. \\cite{Lee2024Understanding} addressed this by introducing a benchmark designed to test true visual understanding in LVLMs. This framework employs novel, out-of-distribution visual concepts and compositional reasoning tasks to ascertain whether models can generalize their knowledge rather than simply recalling learned patterns, ensuring that evaluations reflect genuine comprehension.\n\nIn conclusion, the evolution of evaluation benchmarks for LVLMs, LALMs, and VLLMs reflects a growing understanding of the multifaceted nature of hallucinations. While significant progress has been made in developing frameworks for object-level, relational, intrinsic/extrinsic, domain-specific, and robustness-focused evaluations, challenges remain. Future research must focus on dynamic, adaptive benchmarks that can assess emergent hallucination types, better align with human perception of factual accuracy, and continuously evolve with the increasing complexity and capabilities of multimodal AI models.\n\\subsection{Multimodal Mitigation Strategies}\n\\label{sec:7\\_3\\_multimodal\\_mitigation\\_strategies}\n\nHallucinations in multimodal models (MLLMs), encompassing Large Vision-Language Models (LVLMs), Large Audio-Language Models (LALMs), and Video-Language Models (VLLMs), present a complex challenge due to the intricate interplay and potential misalignments between diverse modalities \\cite{lan20240yz, bai2024tkm, sahoo2024hcb}. These errors often stem from a \"modality gap,\" where differences in data distribution or semantics between modalities lead to biased understanding, dataset toxicity, and inherited biases from underlying Large Language Models (LLMs) \\cite{lan20240yz, bai2024tkm}. Addressing these issues necessitates specialized mitigation strategies that go beyond unimodal approaches, focusing on improving cross-modal grounding, balancing modality priors, and ensuring factual consistency in integrated outputs.\n\nA prominent category of multimodal mitigation strategies involves training-free external guidance and verification mechanisms, which leverage cross-modal consistency checks to steer model generation without modifying internal model weights. These methods intervene during inference or use external tools to validate outputs. For instance, \\cite{kim2024ozf} introduced \\textbf{Counterfactual Inception}, a training-free method that prompts LMMs to engage in counterfactual thinking. The model generates \"counterfactual keywords\" (e.g., non-existent objects) based on visual input, which are filtered by a Plausibility Verification Process (PVP) using CLIP-based semantic alignment. These refined keywords then instruct the LMM to \\textit{avoid} generating such content, thereby enhancing visual grounding and reducing object, attribute, and relational hallucinations. This approach is fundamentally multimodal as it relies on the semantic interplay between generated text and visual content for self-correction. Similarly, \\cite{park20247cm} leveraged Text-to-Image (T2I) diffusion models for hallucination visualization. By generating visual representations of an MLLM's textual output and comparing them with the original input, this method provides an external visual verification mechanism to guide contrastive decoding, effectively using an external generative model to identify and correct cross-modal inconsistencies. Complementary to this, \\cite{wang2023hallucination} proposed a training-free image-grounded guidance method that employs a \"negative image\" (e.g., a blurred version of the input) to suppress non-grounded content in VLMs, using a \"visual-grounding score\" to quantify hallucination severity. While these external guidance methods offer flexibility and avoid costly retraining, their effectiveness is often contingent on the fidelity of the external verification models, and they do not fundamentally alter the MLLM's internal representation learning.\n\nBeyond external guidance, more sophisticated strategies delve into the internal workings of multimodal models, often involving fine-tuning or specialized decoding, to address issues like 'visual amnesia' and modality imbalance at a deeper level. A significant challenge unique to interactive multimodal settings is \"multimodal hallucination snowballing,\" where an MLLM's previously generated hallucination can mislead subsequent responses, even when ground visual information is available \\cite{zhong2024mfi}. To combat this, \\cite{zhong2024mfi} introduced \\textbf{Residual Visual Decoding (RVD)}, a training-free decoding method that \"residual connects\" visual information with the current user instruction. RVD revises the MLLM's output distribution to emphasize direct visual evidence, providing models with more robust access to visual information during generation and reducing the propagation of self-generated errors in conversational contexts. For improving foundational visual grounding, \\cite{jiang2022reg} proposed \\textbf{Visual Cluster Grounding} for image captioning, which implicitly links generated words to informative regions in the image, dynamically focusing on discriminative parts or full object content to reduce object hallucinations and language bias.\n\nRecent advancements have also focused on directly manipulating the decoding process or fine-tuning models with hallucination-targeted objectives. \\textbf{Hallucination-Induced Optimization (HIO)} \\cite{chen20247jb} introduces a novel paradigm where an \"Evil LVLM\" is intentionally trained using a \\textit{reversed} Bradley-Terry model to \\textit{prioritize} hallucinatory content. This \"Evil LVLM\" then serves as a strong contrastive signal during inference, amplifying the logit differences between hallucinatory and correct tokens to steer the original LVLM towards more factual outputs. This approach offers a more precise way to induce specific hallucinatory tokens for contrastive decoding compared to generic visual uncertainty methods. Similarly, \\textbf{Hallucination-targeted Direct Preference Optimization (HDPO)} \\cite{fu2024yqj} fine-tunes MLLMs by constructing specific preference pair data designed to address three distinct causes of hallucinations: Visual Distracted Hallucination (VDH), Long Context Hallucination (LCH), and Multimodal Conflict Hallucination (MCH). For VDH, negative samples are generated by amplifying irrelevant visual information; for LCH, negative examples are created by prompting the MLLM to continue truncated captions, often leading to deviation; and for MCH, conflicting textual information is introduced to train the model to prioritize visual grounding. HDPO's strength lies in its ability to jointly address multiple types of MLLM hallucinations through targeted data construction, offering a more comprehensive fine-tuning strategy compared to general DPO methods.\n\nDeeper architectural interventions aim to resolve representational issues within the MLLM. Methods like \\textbf{Memory-Space Visual Retracing (MemVR)} \\cite{liu2024hallucination} allow models to re-examine and leverage visual features more effectively within intermediate layers, combating 'visual amnesia' by re-injecting visual tokens based on uncertainty. This ensures that crucial visual details are not forgotten during the generation process. Crucially, these internal interventions often incorporate causal mechanisms to balance modality priors, preventing one modality from dominating or suppressing information from another \\cite{liu2024hallucination, zhou2024lvp}. For instance, \\cite{zhou2024lvp}'s \\textbf{CAUSAL MM} framework applies structural causal modeling to MLLMs, treating visual and language priors as confounding factors and using back-door adjustment and counterfactual reasoning to isolate and mitigate modality biases. Similarly, \\textbf{Visual Amplification Fusion (VAF)} \\cite{yin2025s2b} enhances attention to visual signals specifically within the MLLM's middle layers, arguing that language bias often stems from \\textit{insufficient} visual attention rather than an overemphasis on language. These intrinsic methods represent a deeper, mechanistic understanding of MLLM behavior, allowing for more precise and less intrusive corrections by ensuring more robust cross-modal integration.\n\nWhile much of the research on multimodal hallucination mitigation has focused on LVLMs, the principles extend to other modalities, albeit with fewer dedicated studies \\cite{sahoo2024hcb}. For Large Audio-Language Models (LALMs), object hallucinationâ€”where models generate or affirm the presence of non-existent sounds or objectsâ€”is a significant concern \\cite{kuan20249pm}. \\cite{kuan20249pm} demonstrated that carefully crafted prompt engineering can significantly improve LALM performance on discriminative audio tasks and reduce object hallucination, highlighting that even simple interventions can be effective when the underlying issue is query understanding rather than audio processing. Mitigation strategies for LALMs also include leveraging latent diffusion models or retrieval-based methods to ensure consistency between audio and text, as indicated by comprehensive surveys \\cite{sahoo2024hcb}. In the realm of Audio-Visual Large Language Models (AV-LLMs), \\cite{sungbin2024r2g} highlighted \"cross-modal driven hallucinations,\" where models misinterpret information due to subtle relationships or over-reliance on one modality (e.g., video-driven audio hallucination or audio-driven video hallucination). Their work demonstrated that even simple training methods, such as Low-Rank Adaptation (LoRA) fine-tuning with enhanced feature alignment, can improve AV-LLM robustness against these complex inter-modal inconsistencies. General mitigation strategies for Video-Language Models (VLLMs) often involve temporal dependency modeling to ensure consistency across dynamic sequences, a critical challenge given the sequential nature of video data \\cite{sahoo2024hcb}.\n\nIn conclusion, multimodal mitigation strategies have evolved from flexible, training-free external guidance and verification to sophisticated internal architectural interventions and causal reasoning frameworks, alongside targeted fine-tuning approaches. External methods like Counterfactual Inception \\cite{kim2024ozf} and T2I visualization \\cite{park20247cm} offer quick, adaptable solutions but rely on the robustness of external components. Decoding-time interventions like HIO \\cite{chen20247jb} and fine-tuning approaches like HDPO \\cite{fu2024yqj} represent a more direct engagement with the model's generation process, offering deeper control but requiring additional training or preference data. Intrinsic methods such as MemVR \\cite{liu2024hallucination}, VAF \\cite{yin2025s2b}, and CAUSAL MM \\cite{zhou2024lvp} aim for fundamental improvements in cross-modal integration and modality balancing, offering robust solutions at the cost of increased model complexity or intrusiveness. While LVLMs have seen the most dedicated research, emerging work in LALMs \\cite{kuan20249pm} and AV-LLMs \\cite{sungbin2024r2g} indicates a growing focus on modality-specific challenges.\n\nDespite significant progress, several challenges persist. The scalability and real-time applicability of complex causal interventions like CAUSAL MM \\cite{zhou2024lvp} in dynamic multimodal streams, such as live video or audio, remain critical areas for research. A key open question is developing robust methods for arbitrating conflicting information presented by different modalities (e.g., an image showing a blue object while accompanying text describes a a red one), requiring dynamic weighting and conflict resolution mechanisms. Furthermore, the robustness of external verification methods, such as T2I-based approaches \\cite{park20247cm}, is inherently tied to the fidelity and potential hallucination tendencies of the underlying generative models themselves. Ensuring temporal and causal consistency in long video or audio sequences, moving beyond single-frame object grounding, also poses a significant hurdle. Finally, effectively balancing the trade-offs between hallucination reduction, maintaining content quality, and preserving inference speed across diverse multimodal tasks (e.g., VQA, captioning, dialogue, video, audio) and model architectures remains a crucial area for future research \\cite{yin2025s2b, sahoo2024hcb, lan20240yz, bai2024tkm}.\n\\subsection{Cross-Modal Dynamics and Snowballing}\n\\label{sec:7\\_4\\_cross-modal\\_dynamics\\_\\_and\\_\\_snowballing}\n\nThe transition from Large Language Models (LLMs) to Multimodal Large Language Models (MLLMs) and Vision-Language Models (LVLMs) introduces a new dimension to the hallucination problem: complex dynamic behaviors where errors propagate across modalities and conversational turns. While Section 4.4 addresses dialogue-level inconsistencies in text-only models, this subsection focuses specifically on how the interplay with persistent visual, audio, or video modalities creates unique error propagation dynamics, often termed \"multimodal hallucination snowballing,\" which are not present in purely linguistic systems. Understanding these dynamic and interactive aspects is crucial for developing robust and coherent AI systems capable of sustained, reliable interaction across sensory inputs.\n\nA primary dynamic challenge is \\textit{multimodal hallucination snowballing}, where an LVLM's previously generated hallucination can mislead subsequent responses in conversational settings. \\cite{zhong2024mfi} meticulously identifies and characterizes this problem, demonstrating how initial factual errors in an LVLM's output can be implicitly accepted and built upon in subsequent turns, leading to a cascade of incorrect information. To counteract this, they introduce Residual Visual Decoding (RVD), a training-free decoding method that emphasizes direct visual evidence to prevent the model from relying on its own prior, potentially erroneous, textual generations. While RVD offers a practical solution, its effectiveness hinges on the clarity and availability of direct visual evidence, which might be insufficient in scenarios requiring abstract reasoning or subtle contextual understanding. Reinforcing this challenge, \\cite{cao2024o9a} introduces VisDiaHalBench, a visual dialogue benchmark specifically designed to diagnose hallucinations arising from \"long-term misleading textual history\" in LVLMs. This benchmark, featuring five-turn questions about edited images, directly probes the model's susceptibility to propagating errors in a conversational context, highlighting the need for continuous visual re-grounding. The principles of error propagation in multi-turn dialogues, as explored in text-only contexts by \\cite{chen2024c4k} with their DiaHalu benchmark, find direct and exacerbated parallels in multimodal settings where visual context can be misremembered or ignored.\n\nBeyond explicit conversational snowballing, misinterpretations can arise from subtle, dynamic interactions and inconsistencies between different modalities, acting as triggers for initial hallucinations that can then propagate. \\cite{zhou2024lvp} investigates the causal impact of modality priors on attention and output, revealing how an imbalance in these priors can lead to hallucinations. Their work, CAUSAL MM, provides a principled causal inference framework to understand and balance the influence of visual and linguistic inputs by applying back-door adjustment and counterfactual reasoning. This mitigates errors stemming from over-reliance on one modality, which could otherwise initiate a chain of incorrect inferences. However, the complexity of causal modeling and defining appropriate counterfactuals remains a challenge for broad applicability. Similarly, \\cite{han202439z} uncovers a \"semantic shift bias\" where the mere insertion of a paragraph break in textual input can subtly alter an LVLM's understanding of an image, leading to hallucinations. This demonstrates how minor, seemingly innocuous textual formatting can dynamically influence cross-modal interpretation, revealing a brittleness in vision-language grounding that may require more fundamental architectural solutions than the proposed MiHI/MiHO interventions.\n\nUnderlying these dynamic misinterpretations are fundamental vulnerabilities within the model architecture that enable error propagation. \\cite{wang2025jen} reveals that hallucinations can be induced by exploiting \"attention sinks,\" a phenomenon where attention mechanisms become fixated on irrelevant tokens, diverting processing power from critical visual information. In a multimodal context, this mechanism can directly contribute to propagating errors by causing the model to misinterpret visual cues, thus initiating a chain of incorrect inferences that could snowball. This highlights a critical internal dynamic where attention misallocation directly impacts multimodal grounding. Furthermore, the temporal dynamics inherent in video-language models present unique challenges. \\cite{ma2023mka} introduces Vista-llama to address the \"diminishing impact of video\" as generated text length increases, a clear example of cross-modal dynamic error where the visual grounding weakens over time, leading to irrelevant content. Their solution, which maintains a consistent distance between visual and language tokens, underscores the need for continuous and robust visual attention throughout the generation process.\n\nEvaluating an LVLM's ability to maintain coherent understanding across dynamic visual changes is also crucial for identifying propagating errors. \\cite{yebin2024txh} introduces BEAF (Observing BEfore-AFter Changes), an innovative framework that manipulates visual scenes by removing objects and introduces change-aware metrics. This allows for a more robust assessment of whether models truly understand visual changes or merely hallucinate, which is vital for identifying when subtle inconsistencies lead to misinterpretations or propagating errors in a dynamic environment.\n\nThe insights from LLM-centric self-correction mechanisms offer conceptual parallels for mitigating multimodal snowballing. \\cite{dhuliawala2023rqn}'s Chain-of-Verification (CoVe) and \\cite{liu2025juo}'s self-consistency framework for mathematical reasoning both emphasize internal deliberation and verification steps to prevent models from repeating their own mistakes and propagating errors. Adapting such multi-step, self-correcting paradigms to multimodal contexts would require sophisticated mechanisms for re-grounding each verification step in the visual or audio evidence, rather than solely relying on internal textual consistency. This is a significant challenge, as highlighted by surveys like \\cite{liu2024sn3} and \\cite{lan20240yz}, which discuss the persistent \"modality gap\" and the difficulty of ensuring consistent understanding across diverse data distributions. \\cite{tonmoy20244e4} also notes that snowballing in complex reasoning remains a challenge for many mitigation approaches, underscoring the severity of this dynamic problem.\n\nIn conclusion, the study of cross-modal dynamics and snowballing highlights that hallucination in LVLMs is not merely a static error but a complex, evolving problem. The propagation of errors, whether through explicit conversational snowballing, subtle cross-modal misinterpretations, or diminishing visual grounding over time, poses a significant challenge. Future research must focus on developing models with stronger internal consistency checks that explicitly re-ground in multimodal reality at each conversational turn, advanced causal modeling of cross-modal interactions to prevent the initiation of errors, and robust self-correction mechanisms that can effectively leverage and verify against dynamic sensory inputs to prevent the propagation of hallucinations.\n\n\n\\label{sec:towards_trustworthy_ai:_robustness,_safety,_and_advanced_evaluation}\n\n\\section{Towards Trustworthy AI: Robustness, Safety, and Advanced Evaluation}\n\\label{sec:towards\\_trustworthy\\_ai:\\_robustness,\\_safety,\\_\\_and\\_\\_advanced\\_evaluation}\n\n\\subsection{Zero-Resource and Black-Box Hallucination Detection}\n\\label{sec:8\\_1\\_zero-resource\\_\\_and\\_\\_black-box\\_hallucination\\_detection}\n\nDetecting hallucinations in Large Language Models (LLMs) presents a formidable challenge, particularly when evaluating proprietary models where direct access to internal states, training data, or extensive human labels is unavailable. This subsection focuses on \"zero-resource\" and \"black-box\" detection methods, which operate under these constraints by leveraging the intrinsic properties of LLM generation to identify factual and logical inconsistencies. These approaches are indispensable for scalable, model-agnostic evaluation, thereby enhancing the practical applicability of hallucination detection frameworks across diverse deployment scenarios.\n\nA foundational contribution in this domain is \\textit{SelfCheckGPT} \\cite{manakul20236ex}, which introduced a novel zero-resource, black-box strategy. The core premise is that an LLM genuinely \"knowing\" a fact will produce consistent responses across multiple stochastically sampled generations for the same query. Conversely, if the LLM hallucinates, these samples are likely to diverge, contradict, or present inconsistent information. \\textit{SelfCheckGPT} generates several diverse responses from the black-box LLM and then employs various consistency measures, such as BERTScore, Natural Language Inference (NLI), or even another LLM acting as an evaluator, to quantify the informational agreement between the original response and the generated samples. This method effectively identifies non-factual statements without requiring internal token probabilities or external fact-checking databases. However, a significant limitation arises when LLMs consistently repeat their own errors across multiple samples due to strong internal biases or memorization, leading to false negatives in hallucination detection, as the model appears \"consistent\" in its incorrectness. Furthermore, the computational overhead of generating and comparing multiple responses can be substantial, especially for complex queries or real-time applications.\n\nTo address the limitations of consistent self-hallucinations, \\textit{MetaQA} \\cite{yang20251dw} significantly advanced zero-resource detection by introducing \\textit{metamorphic relations} and \\textit{prompt mutation}. Instead of merely re-sampling from the same prompt, \\textit{MetaQA} generates logically equivalent or semantically related prompts (e.g., synonymous queries, rephrased questions, or queries testing inverse relations) to elicit a more diverse and robust set of responses. By checking for consistency across these responses, which are generated from varied but semantically linked inputs, \\textit{MetaQA} makes it harder for the LLM to consistently hallucinate the same fact. This technique effectively probes the LLM's understanding from multiple angles, providing a more reliable signal for exposing factual inconsistencies. While more robust, the effectiveness of \\textit{MetaQA} is contingent on the careful design of metamorphic relations, which can be task-specific and may not generalize universally across all types of factual or reasoning errors.\n\nBeyond consistency checks, other black-box approaches explore alternative signals. \\textit{Attention-Guided SElf-Reflection (AGSER)} \\cite{liu2025xwv} proposes a zero-shot hallucination detection method that attempts to leverage insights from attention mechanisms without direct internal model access. AGSER categorizes the input query into \"attentive\" and \"non-attentive\" parts, processes each separately through the LLM, and then computes consistency scores between the generated responses and the original answer. The difference between these consistency scores serves as a hallucination estimator. This method notably reduces computational overhead, requiring only three passes through the LLM, making it more efficient than methods relying on numerous generations. While the precise mechanism for inferring \"attention contributions\" in a strictly black-box manner requires careful consideration, AGSER demonstrates a promising direction for deriving more nuanced signals from LLM outputs without full transparency.\n\nZero-resource, black-box principles are also being tailored for specific, complex reasoning domains. For instance, \\cite{liu2025juo} enhances mathematical reasoning in LLMs by applying a structured self-consistency framework. This approach goes beyond merely checking the final answer, enforcing consistency across \\textit{intermediate reasoning steps} in tasks like theorem proving, symbolic transformation, and numerical computation. By ensuring logical coherence throughout the problem-solving process, this method significantly reduces logical inconsistencies and hallucinations specific to mathematical contexts. While domain-specific, it highlights how black-box consistency checks can be adapted to probe deeper into an LLM's reasoning integrity, rather than just surface-level factual recall.\n\n\\textbf{Comparative Analysis and Critical Discussion:}\nThese black-box detection methods offer distinct advantages and trade-offs. \\textit{SelfCheckGPT} provides a simple, general-purpose baseline, but its vulnerability to consistently incorrect outputs limits its robustness. \\textit{MetaQA} improves robustness by actively perturbing inputs, making it harder for LLMs to hide systematic errors, yet it introduces complexity in designing effective metamorphic relations. Both methods incur significant computational costs due to multiple inference calls. \\textit{AGSER} attempts to mitigate this computational burden by leveraging a more efficient, attention-guided reflection mechanism, potentially offering a better balance between detection efficacy and resource usage. The mathematical self-consistency approach \\cite{liu2025juo} demonstrates the power of adapting these principles to domain-specific reasoning, highlighting that while general black-box detectors are valuable, specialized approaches can achieve higher precision for particular types of complex hallucinations. A common limitation across these methods is their reliance on the LLM's own generative capabilities to expose its flaws; they cannot detect hallucinations that the LLM consistently and confidently generates as \"true\" across all probed variations. Furthermore, the sensitivity to sampling parameters (e.g., temperature) and the choice of consistency metrics (e.g., BERTScore vs. NLI) can significantly impact detection performance, requiring careful tuning.\n\nIn summary, zero-resource and black-box hallucination detection methods represent a vital area of research, offering scalable and model-agnostic solutions for evaluating and improving the trustworthiness of LLMs, especially proprietary ones. From the foundational consistency checks of \\textit{SelfCheckGPT} \\cite{manakul20236ex} and the robust metamorphic relations of \\textit{MetaQA} \\cite{yang20251dw}, to the efficient attention-guided reflection of \\textit{AGSER} \\cite{liu2025xwv} and domain-specific logical consistency for mathematical reasoning \\cite{liu2025juo}, these techniques collectively push the boundaries of what is possible without privileged model access or extensive human annotation. Future directions in this area could involve the fusion of diverse black-box signals, combining internal consistency with metamorphic testing and attention-guided insights to create more robust hybrid detectors. Furthermore, research could focus on developing computationally lighter black-box methods, exploring their applicability to detect more subtle forms of hallucination beyond factual errors, such as logical fallacies, reasoning inconsistencies, and biases, and enhancing their resilience against adversarial attacks designed to evade detection.\n\\subsection{Adversarial Attacks and Vulnerability Probing}\n\\label{sec:8\\_2\\_adversarial\\_attacks\\_\\_and\\_\\_vulnerability\\_probing}\n\nBeyond merely observing and reacting to instances of hallucination, a critical new frontier in understanding and mitigating this phenomenon involves proactively probing Large Multimodal Models (LMMs) for vulnerabilities through adversarial attacks. This methodology aims to intentionally induce hallucinations, thereby uncovering specific weaknesses and failure modes that might remain hidden during passive observation, guiding the development of more resilient and secure AI systems.\n\nA pioneering work in this domain is \\cite{wang2025jen}, which introduces a novel adversarial attack termed \"Mirage in the Eyes.\" This technique specifically targets and exploits the \"attention sink\" phenomenon within MLLMs to intentionally induce hallucinations. By dynamically manipulating internal attention scores and hidden embeddings, \\cite{wang2025jen} demonstrates how to steer the model towards generating factually incorrect or non-existent visual content, providing crucial insights into the internal mechanisms that contribute to hallucination. This approach moves beyond external input perturbations, delving into the model's internal processing to expose its susceptibility.\n\nComplementing such targeted internal attacks, other research has integrated adversarial principles into evaluation and discovery. \\cite{huang20247wn}, in their comprehensive benchmark VHTest, incorporates an adversarial generation paradigm to create diverse visual hallucination instances. While not a direct attack for inducing hallucination in the same manner as \\cite{wang2025jen}, this paradigm contributes to the proactive identification of vulnerabilities by systematically generating challenging inputs that are likely to trigger various types of hallucinations, including those related to object shape and size. This allows for a broader exploration of an MLLM's fragility across different visual attributes.\n\nFurthermore, vulnerabilities can be exposed through surprisingly subtle adversarial manipulations. \\cite{han202439z} uncovered a \"semantic shift bias\" where the mere insertion of paragraph breaks (\\texttt{\\textbackslash n}) into textual prompts can induce hallucinations in MLLMs. This seemingly innocuous input perturbation acts as a potent adversarial trigger, demonstrating that models can be led astray by minor structural changes that do not alter the semantic content of the prompt. Such findings highlight unexpected failure modes and underscore the importance of probing for vulnerabilities across a wide spectrum of input types, from complex internal manipulations to simple textual formatting.\n\nCollectively, these approaches represent a significant shift from reactive mitigation to proactive robustness testing. By actively designing attacks that exploit internal model characteristics like attention sinks \\cite{wang2025jen}, or by systematically generating adversarial test cases \\cite{huang20247wn}, or even by identifying subtle input biases \\cite{han202439z}, researchers are gaining a deeper understanding of \\textit{why} and \\textit{how} hallucinations occur. This proactive methodology is indispensable for identifying the root causes of hallucination, enabling the development of more robust architectures and training strategies that can withstand sophisticated adversarial attempts to induce erroneous outputs. The insights gleaned from these adversarial probes are crucial for building truly trustworthy and secure AI systems that can maintain factual consistency even under challenging or malicious inputs.\n\\subsection{Semantic Guardrails for Safety-Critical Applications}\n\\label{sec:8\\_3\\_semantic\\_guardrails\\_for\\_safety-critical\\_applications}\n\nThe increasing deployment of Large Language Models (LLMs) in safety-critical domains, particularly in clinical medicine, necessitates a paradigm shift from general hallucination mitigation to robust, high-assurance safety mechanisms. In these high-stakes environments, the generation of factually incorrect or ungrounded information, often termed \"hallucinations\" \\cite{maynez2020h3q}, can lead to \"never events\"â€”catastrophic errors with severe consequences for patient safety and well-being \\cite{hakim2024d4u}. The inherent probabilistic nature of LLMs, which can lead to plausible but incorrect outputs \\cite{hamid2024pwn}, coupled with observed inaccuracies in analyzing unstructured clinical notes \\cite{shah20242sx} and significant challenges in complex medical reasoning tasks \\cite{umapathi2023puv}, underscores the urgent need for specialized safeguards.\n\nTo address this imperative, the concept of 'semantic guardrails' has emerged as a targeted solution, designed to prevent these critical errors by enforcing strict adherence to factual accuracy and consistency within domain-specific knowledge. Unlike broader LLM safety surveys that discuss ethical considerations and prompt injection alongside hallucination \\cite{gao20242nu}, semantic guardrails focus specifically on content integrity and factual grounding. This approach represents an evolution from traditional rule-based expert systems used in clinical AI, which offered high precision but often lacked the flexibility and generative power of LLMs. Semantic guardrails aim to imbue LLMs with a similar level of verifiable reliability, but within their more dynamic and open-ended operational context.\n\nA pioneering framework in this area is presented by \\cite{hakim2024d4u}, which introduces specific semantic guardrails tailored for pharmacovigilance, a domain where regulatory compliance and absolute accuracy are paramount. Their work highlights the distinction between \"structural guardrails\" (ensuring output format) and \"semantic guardrails\" (verifying content accuracy). They propose two primary mechanisms: Document-wise Uncertainty Quantification (DL-UQ) and MISMATCH guardrails. DL-UQ functions as a \"soft\" semantic guardrail by quantifying the LLM's uncertainty regarding each generated statement, specifically by evaluating its evidential support within a provided reference document. This mechanism identifies and flags information lacking sufficient backing, preventing unsupported claims from being presented as definitive facts. This is crucial for ensuring faithfulness to source material, a non-negotiable requirement in medical contexts.\n\nComplementing DL-UQ, the MISMATCH guardrail acts as a \"hard\" semantic guardrail, actively detecting contradictions or inconsistencies between the LLM's generated output and the authoritative reference document \\cite{hakim2024d4u}. For instance, in pharmacovigilance, it ensures that drug names or adverse event terms are consistently present in both source and target texts, preventing hallucination or omission of critical terms through the use of custom dictionaries and medical ontologies like MedDRA. Both DL-UQ and MISMATCH are engineered with the explicit goal of absolute error prevention for \"never events,\" fundamentally shifting the paradigm from merely reducing the frequency of hallucinations to actively precluding errors where severe consequences are at stake.\n\nWhile \\cite{hakim2024d4u} focuses on document-wise uncertainty for text-to-text tasks, other research explores different facets of uncertainty quantification (UQ) that could complement or inform guardrail development. For instance, \\cite{ling2024hqv} investigates aleatoric and epistemic uncertainties in LLMs during in-context learning, which could provide finer-grained signals for guardrails beyond document-level support. Similarly, \\cite{zhang2024mmj} introduces VL-Uncertainty for Large Vision-Language Models (LVLMs), quantifying intrinsic uncertainty by analyzing prediction variance across semantically equivalent but perturbed prompts. This highlights a broader trend towards intrinsic uncertainty estimation, which could be integrated into multimodal semantic guardrails in the future to address the complexities of visual and other non-textual data in clinical settings.\n\nSemantic guardrails also stand in contrast to, or can be integrated with, other mitigation strategies. Retrieval-Augmented Generation (RAG) is a foundational approach for grounding LLMs in external knowledge \\cite{gilbert2024uu2}. While RAG aims to prevent hallucinations by providing relevant context, semantic guardrails act as a subsequent, explicit verification layer, ensuring the \\textit{correctness} of the generated text \\textit{against} that context, rather than just relying on the retrieval process. Similarly, approaches like In-Context Padding (ICP) that guide clinical reasoning with \"knowledge seeds\" \\cite{wu202407f} aim to improve accuracy during generation by aligning LLM reasoning with clinical decision pathways, whereas guardrails provide a post-generation safety net that can validate the outcome of such guided reasoning.\n\nDespite their promise, the development and deployment of semantic guardrails face significant challenges. The theoretical inevitability of hallucination in any computable LLM \\cite{xu2024n76} suggests that achieving \"absolute error prevention\" is an asymptotic goal, requiring continuous vigilance and robust design. This means guardrails must be designed not just to prevent errors, but to gracefully handle irreducible uncertainties and flag them for human review. The tuning of thresholds for uncertainty-based guardrails (e.g., DL-UQ) involves delicate trade-offs between sensitivity (catching all potential errors) and specificity (avoiding false positives), which is particularly critical in clinical settings where over-flagging can lead to alert fatigue and hinder workflow efficiency. Furthermore, the computational overhead of running multiple, stringent semantic checks in real-time clinical workflows needs careful optimization to ensure practical applicability.\n\nFuture research must therefore focus on several key areas. Firstly, developing more sophisticated, domain-adaptable, and potentially formally verifiable semantic guardrails is crucial to expand their applicability beyond specific tasks like pharmacovigilance to broader medical reasoning and diagnostics. This includes exploring methods for automatically generating and validating guardrail rules, potentially leveraging knowledge graphs for enhanced precision and explainability. Secondly, integrating these guardrails seamlessly into human-in-the-loop systems, ensuring clear communication of uncertainty and rationale for flagging, is paramount for fostering trust and effective human-AI collaboration. Finally, research into multimodal semantic guardrails will be vital as LLMs increasingly process diverse data types in clinical settings, demanding consistent factual grounding across visual, textual, and other modalities. This continuous pursuit of high-assurance solutions is essential for the responsible and ethical integration of LLMs into safety-critical applications.\n\\subsection{Meta-Evaluation of Hallucination Benchmarks}\n\\label{sec:8\\_4\\_meta-evaluation\\_of\\_hallucination\\_benchmarks}\n\nThe rapid proliferation of hallucination benchmarks, while crucial for advancing the field, has simultaneously introduced challenges regarding their quality, reliability, and validity. As the community developed increasingly sophisticated methods to detect diverse hallucination types, a critical self-reflection emerged: how do we ensure that the tools used to measure LLM performance are themselves robust and trustworthy? This subsection delves into the vital area of meta-evaluation, focusing on frameworks designed to assess the quality of these benchmarks.\n\nEarly efforts to quantify hallucination, such as the CHAIR metric, faced considerable scrutiny due to issues like instability and sensitivity to instruction design \\cite{li2023249}. While subsequent benchmarks like POPE \\cite{li2023249} addressed some of these limitations by offering more stable, polling-based evaluation, the overarching need for a systematic framework to evaluate \\textit{any} hallucination benchmark remained. This necessity stems from concerns about potential prompt bias, data leakage, and the ability of benchmarks to accurately capture the multifaceted nature of hallucinations across various contexts and modalities.\n\nAddressing this critical gap, \\cite{yan2024ux8} introduced a groundbreaking psychometrics-inspired framework for the meta-evaluation of hallucination benchmarks. Their work proposes the \\textbf{Hallucination benchmark Quality Measurement (HQM)} framework, which systematically assesses benchmarks across four key dimensions: reliability, validity, fairness, and utility. Reliability evaluates the consistency of a benchmark's results, ensuring that repeated measurements under similar conditions yield comparable outcomes. Validity, perhaps the most crucial dimension, ascertains whether a benchmark truly measures what it purports to measure, accurately capturing the intended hallucination types without conflating them with other errors. Fairness scrutinizes benchmarks for biases, such as prompt-specific biases that might inadvertently favor certain models or data leakage issues that compromise the integrity of evaluation. Finally, utility considers the practical aspects, including the scalability, interpretability, and overall usefulness of a benchmark for researchers and developers.\n\nBy applying HQM to existing benchmarks, \\cite{yan2024ux8} revealed inherent strengths and weaknesses, providing a much-needed critical perspective on the tools foundational to hallucination research. This meta-evaluation not only highlights areas for improvement in current benchmarks but also proposes the concept of a \\textbf{High-Quality Hallucination Benchmark (HQH)} as a guiding principle for future development. The HQH concept encourages benchmark designers to proactively incorporate principles of psychometric rigor, ensuring that new evaluation methodologies are inherently robust, unbiased, and capable of accurately reflecting model performance.\n\nThe introduction of meta-evaluation frameworks marks a significant maturation of the field, shifting from merely creating benchmarks to critically assessing their foundational quality. This self-reflective advancement ensures that research findings on LLM hallucination are built upon solid, trustworthy evaluation methodologies. It is vital for maintaining the integrity of research, guiding the development of truly effective and unbiased evaluation tools, and ultimately accelerating progress towards more reliable and trustworthy large language models. Future work will likely see the HQM framework become a standard for validating new benchmarks, fostering a more rigorous and transparent evaluation ecosystem.\n\n\n\\label{sec:conclusion_and_future_directions}\n\n\\section{Conclusion and Future Directions}\n\\label{sec:conclusion\\_\\_and\\_\\_future\\_directions}\n\n\\subsection{Summary of Key Advancements}\n\\label{sec:9\\_1\\_summary\\_of\\_key\\_advancements}\n\nThe persistent challenge of hallucination in Large Language Models (LLMs) has driven a rapid and profound evolution in research, transforming the field from initial problem identification to a sophisticated, multi-faceted scientific endeavor. This trajectory highlights a collective effort towards building more reliable, transparent, and contextually grounded AI systems. Crucially, a profound intellectual shift has occurred, moving beyond empirical observation to a formal theoretical grounding, demonstrating the inherent inevitability of hallucination in computable LLMs \\cite{xu2024n76, li2025qzg}. This fundamental insight re-frames the research goal from complete eradication to robust management and mitigation, acknowledging an innate limitation.\n\nEarly foundational work established the critical need to address unfaithful content. \\cite{maynez2020h3q} provided a seminal large-scale human evaluation, categorizing hallucinations in abstractive summarization and demonstrating the utility of textual entailment for faithfulness evaluation. Recognizing the limitations of static knowledge, \\cite{trivedi2022qsf} introduced Interleaving Retrieval with Chain-of-Thought (IRCoT), an early few-shot, training-free method that dynamically interleaved reasoning steps with knowledge retrieval to ground LLMs in external facts. This marked an early shift towards adaptive, real-time grounding during generation. Subsequent advancements in Retrieval-Augmented Generation (RAG) further refined this paradigm, with frameworks like Rowen \\cite{ding20244yr} intelligently deciding \\textit{when} to retrieve based on cross-language/cross-model consistency, optimizing efficiency and mitigating both internal and external hallucinations. Complementing this, the strategic integration of structured Knowledge Graphs (KGs) through \"Graph-guided retrieval\" and \"Graph-guided generation\" has significantly enhanced trustworthiness and reduced hallucinations in open-ended QA by providing verifiable, structured knowledge \\cite{sui20242u1}. However, it is critical to acknowledge that RAG systems, while powerful, are not infallible; their efficacy is inherently tied to the quality of retrieved information, making them vulnerable to noisy or biased sources and susceptible to 'confabulations' arising from limitations within RAG components themselves \\cite{zhang20252at}.\n\nAs LLMs became more capable and complex, the focus expanded to developing sophisticated evaluation benchmarks that could rigorously assess trustworthiness beyond simple accuracy. \\cite{gao2023ht7} introduced ALCE, the first reproducible benchmark for evaluating LLMs' ability to generate text with verifiable citations, complete with automatic metrics for fluency, correctness, and citation quality. Pushing evaluation further, \\cite{oh2024xa3} developed ERBench, a novel benchmark leveraging relational databases to automatically verify not just final answers but also the \\textit{rationales} provided by LLMs, addressing a critical need for transparency in reasoning. Complementing this, \\cite{ghosh2024tj5} developed new logical fact-checking datasets and quantitative measures to assess LLM consistency on complex propositional logic queries, expanding the definition of \"hallucination\" to include logical inconsistencies. For scenarios where external knowledge or human labels are scarce, methods like MetaQA \\cite{yang20251dw} emerged as zero-resource, self-contained hallucination detection techniques leveraging metamorphic relations and prompt mutation. The theoretical underpinnings of such detection methods were further explored by \\cite{karbasi2025j7n}, who established an equivalence between hallucination detection and language identification, proving that automated detection is fundamentally impossible for most language collections without expert-labeled feedback, thereby providing theoretical support for methods like RLHF. Furthermore, to foster more responsible AI behavior, \\cite{tjandra2024umq} introduced the Accuracy-Engagement Distance (AED) metric to evaluate models capable of appropriately \\textit{abstaining} from answers when uncertain, utilizing semantic entropy for label-free uncertainty estimation.\n\nConcurrently, mitigation strategies evolved from general retrieval to highly adaptive, proactive, and mechanistically targeted interventions. On a more mechanistic front, \\cite{zhang2024qq9} identified \"knowledge overshadowing\" as a novel root cause of \"amalgamated hallucinations\" arising from data imbalance, proposing an inference-time, training-free self-contrastive decoding method for targeted mitigation. This represents a deeper understanding of internal model dynamics. Similarly, \\cite{chen2024j0g} presented ICT, a training-free, forward-pass intervention method that targets specific attention heads to enhance focus on crucial information, mitigating the dominance of language priors. The development of self-correction mechanisms, as exemplified by \\cite{tjandra2024umq}'s work on abstention, empowers LLMs to reflect on their uncertainty and proactively avoid generating ungrounded content, moving towards more self-aware and reliable systems.\n\nA major conceptual and methodological shift has been the dedicated focus on multimodal hallucination, particularly in Large Vision-Language Models (LVLMs). These models introduce unique complexities due to the \"modality gap\" and the integration of diverse information sources \\cite{lan20240yz}. \\cite{kaul2024ta7} addressed the inadequacy of prior benchmarks by introducing THRONE, the first accurate object-based hallucination benchmark for \\textit{free-form} generations of LVLMs, utilizing LM-based semantic judgment. Building on the \"snowballing\" effect observed in LLMs, \\cite{zhong2024mfi} investigated and mitigated \"multimodal hallucination snowballing\" in LVLMs, proposing Residual Visual Decoding (RVD) to emphasize direct visual evidence and prevent error propagation in conversational settings. To address the diverse nature of LVLM queries, \\cite{chang2024u3t} introduced Dentist, a unified mitigation framework that classifies query types (perception vs. reasoning) and applies tailored, iterative validation strategies. Furthermore, intrinsic model interventions emerged, with \\cite{wang2024vym} identifying \"Visual Encoding Distortion\" as a critical source of LVLM hallucinations and proposing Visual-Layer Fusion Contrastive Decoding (VaLiD) to correct it by fusing features from early visual layers. However, early multimodal evaluation benchmarks, such as object-based approaches, were found to be highly susceptible to prompt bias, leading to inaccurate assessments of real-world hallucination \\cite{wang2023zop}, necessitating more robust, LLM-based evaluation frameworks like HaELM.\n\nCollectively, these advancements underscore a profound intellectual trajectory: from merely identifying and correcting errors to a deep, multi-faceted understanding of hallucination's origins, its manifestation across modalities, and the development of sophisticated, adaptive, and intrinsically aware mechanisms for prevention and evaluation. This comprehensive progress marks a significant stride in the pursuit of building truly trustworthy, transparent, and contextually grounded AI systems, acknowledging both their immense potential and their inherent, theoretically proven limitations. The field is actively navigating the tension between external grounding, internal correction, and the acceptance of these fundamental limits, guiding research towards robust management rather than the elusive goal of complete eradication.\n\\subsection{Remaining Challenges and Open Questions}\n\\label{sec:9\\_2\\_remaining\\_challenges\\_\\_and\\_\\_open\\_questions}\n\nDespite significant advancements in characterizing, evaluating, and mitigating hallucination in large language models (LLMs) and multimodal large language models (MLLMs), several critical unresolved issues persist, defining promising avenues for future research in trustworthy AI. The dynamic nature of these models and their expanding capabilities mean that hallucination remains a moving target, necessitating continuous innovation.\n\nOne fundamental challenge lies in the \\textbf{scalability of fine-grained annotation and evaluation}. While efforts like \\cite{ji20243j6} and \\cite{gu202414e} have introduced analytical, sentence-level annotation datasets (ANAH and ANAH-v2) and iterative self-training frameworks to scale this process, the sheer diversity of tasks, domains, and hallucination types makes truly comprehensive, human-quality annotation prohibitively expensive and time-consuming. This bottleneck hinders the development of robust, generalizable evaluation benchmarks that can capture the nuances of complex reasoning, as exemplified by the need for rationale verification in \\cite{oh2024xa3} (ERBench) and path-based evaluation in graph computation tasks \\cite{tang2024a1j} (GraphArena). The challenge extends beyond mere data quantity to ensuring the \\textit{quality, diversity, and contextual richness} of annotated data across an ever-expanding problem space.\n\nAnother pressing issue is the development of \\textbf{truly real-time, adaptive, and seamlessly integrated mitigation strategies}. Current approaches have made strides towards proactive prevention and self-correction. For instance, \\cite{manakul20236ex} (SelfCheckGPT) and \\cite{yang20251dw} (MetaQA) offer zero-resource, black-box detection methods, yet these often incur computational overhead or rely on sampling, which can introduce latency or fail to capture dynamic shifts in model uncertainty. While \\cite{tjandra2024umq} proposes label-free abstention using semantic entropy, the challenge remains in making such mechanisms adaptively responsive to subtle changes in user intent or context without sacrificing generation quality or speed. Similarly, advanced Retrieval-Augmented Generation (RAG) techniques like \\cite{lv2024k5x}'s Coarse-to-Fine Highlighting (COFT) and \\cite{ding20244yr}'s Rowen improve context relevance, but the seamless integration of such dynamic knowledge retrieval and synthesis into the core generation process, without introducing new latency or compromising creative outputs, remains an open problem. Furthermore, while \\cite{hakim2024d4u} introduced \"semantic guardrails\" for safety-critical domains, generalizing such hard constraints to open-ended, creative, or rapidly evolving tasks without stifling utility is a complex balancing act.\n\nThe field also grapples with the complex task of \\textbf{bridging the gap between theoretical inevitability and practical error reduction}. The unified theoretical framework proposed by \\cite{li2025qzg} (Loki's Dance of Illusions) suggests that some forms of hallucination might be mathematically inherent to LLM architectures or their training paradigms. If certain types of hallucination are indeed inevitable, the critical question shifts from absolute elimination to understanding the \\textit{acceptable error rate} and designing systems that can gracefully handle or transparently communicate these inherent limitations. This necessitates further research into robust uncertainty quantification and calibrated abstention mechanisms that are both accurate and user-friendly, allowing LLMs to \"know what they don't know\" effectively.\n\nBeyond these challenges, several \\textbf{open questions} guide the next wave of innovation. The rapid expansion into multimodal AI has unveiled \\textbf{novel hallucination types} that require dedicated investigation. While object hallucination in vision-language models \\cite{liu2024sn3, lan20240yz} and audio-language models \\cite{kuan20249pm} has been identified, and video-specific temporal inconsistencies are being explored \\cite{wang2024rta}, the full spectrum of cross-modal fabrication and misinterpretation, especially in complex reasoning or creative multimodal generation, is yet to be fully mapped. Moreover, the emergence of adversarial attacks that induce hallucinations \\cite{wang2025jen} suggests that new, engineered forms of hallucination will continue to challenge detection and mitigation efforts.\n\nAnother critical open question concerns the \\textbf{long-term impact of multimodal interactions and cascading hallucinations}. While \\cite{qiu2024zyc} has begun to address long-context multimodal hallucination, the cumulative effect of minor inconsistencies over extended dialogues or multi-turn reasoning in complex multimodal environments remains poorly understood. The concept of \"multimodal hallucination snowballing\" \\cite{zhong2024mfi} highlights the potential for initial errors to propagate and amplify, leading to increasingly unreliable outputs. Developing methods to track, predict, and mitigate these cascading effects across modalities and over time is crucial for building truly robust conversational and interactive AI systems.\n\nFinally, there is a pressing need for \\textbf{more unified and generalizable solutions that perform robustly across diverse tasks and domains}. Many current mitigation strategies are task-specific (e.g., RAG for factual QA, visual grounding for LVLMs). The development of a single, overarching framework that can effectively address hallucination across diverse applicationsâ€”from summarization and dialogue to code generation and creative writing, and across various modalitiesâ€”while maintaining high performance and adaptability, remains an elusive goal. While some efforts, like \\cite{chang2024u3t}'s unified LVLM mitigation framework, attempt to generalize within a multimodal context, achieving true cross-domain and cross-task robustness without extensive, domain-specific fine-tuning is a significant hurdle. The pursuit of a holistic \"information quality\" metric, as conceptualized by \\cite{rejeleene2024okw}, could pave the way for more generalizable evaluation and mitigation, but its practical implementation across diverse scenarios is still an open area of research. Addressing these challenges and open questions will be paramount in guiding the next wave of innovation towards building inherently trustworthy and transparent AI systems.\n\\subsection{Ethical Considerations and Responsible AI Development}\n\\label{sec:9\\_3\\_ethical\\_considerations\\_\\_and\\_\\_responsible\\_ai\\_development}\n\nThe challenge of hallucination in Large Language Models (LLMs) transcends mere technical inaccuracy, presenting profound ethical dilemmas that necessitate a robust framework for responsible AI development. The generation of confident yet incorrect or fabricated information by LLMs carries significant societal implications, demanding critical attention to transparency, accountability, and the safe deployment of these powerful systems, particularly in high-stakes applications \\cite{dbeeca8466e0c177ec67c60d529899232415ca87, e7c97e953849f1a8e5d85ceb4cfcc0a5d54d2365}.\n\nA foundational ethical consideration arises from the inherent limitations of LLMs. As discussed in Section 3.3, theoretical frameworks, notably those employing diagonalization arguments, suggest the mathematical inevitability of hallucination in any computable LLM \\cite{xu2024n76, li2025qzg}. This theoretical grounding shifts the ethical imperative from eradicating hallucination to transparently communicating its unavoidable nature. Responsible AI development demands that developers and deployers manage societal expectations, clearly articulate the inherent limitations of LLMs, and avoid presenting them as infallible or universally reliable. Failure to do so can lead to a breach of trust, misinformed decision-making, and a violation of the principle of non-maleficence, particularly when LLMs are deployed in sensitive domains.\n\nTo foster accountability and enable users to verify generated content, technical solutions have focused on grounding LLM outputs. As detailed in Section 4.1, the development of benchmarks like ALCE, which encourage LLMs to generate text with explicit citations to supporting evidence, represents a significant step towards ethical verifiability \\cite{gao2023ht7}. Ethically, such mechanisms aim to empower user autonomy by providing the means to cross-reference information, thereby combating the spread of misinformation. However, a critical perspective reveals potential pitfalls: if the citation mechanism itself is susceptible to hallucination (e.g., generating non-existent sources or misattributing information), it could create a false sense of security, exacerbating the problem rather than solving it. Ensuring the integrity of the grounding process is therefore paramount.\n\nBeyond proactive grounding, robust detection mechanisms are crucial for continuous monitoring and for informing users about potential inaccuracies. As explored in Section 8.1, zero-resource, black-box hallucination detection methods like \\textit{SelfCheckGPT} \\cite{manakul20236ex} and techniques leveraging metamorphic relations \\cite{yang20251dw} offer practical tools for identifying ungrounded content, even in proprietary models. Ethically, these tools support ongoing oversight and allow for post-hoc correction or flagging of potentially harmful outputs. However, their limitations, such as potential false negatives in subtle hallucinations or the computational cost of multiple generations, must be transparently acknowledged to prevent over-reliance and to ensure that human oversight remains a critical component of the safety loop.\n\nA key aspect of responsible deployment is the clear communication of uncertainty. As discussed in Section 6.3, models can be fine-tuned to proactively abstain from answering when uncertain, utilizing label-free techniques based on semantic entropy \\cite{tjandra2024umq}. This mechanism directly addresses the ethical principle of transparency by allowing LLMs to express \"I don't know\" rather than confidently asserting potentially incorrect information. This reduces the risk of misinformation in sensitive contexts and promotes a more honest interaction paradigm. However, the ethical balance lies in determining the appropriate threshold for abstention; an overly conservative model might diminish utility, while an overly permissive one risks harm.\n\nThe responsible deployment of AI systems, particularly in high-stakes applications, necessitates robust safety mechanisms and domain-specific \"guardrails.\" In medical safety-critical settings, for instance, the implementation of \"semantic guardrails\" (e.g., Document-wise Uncertainty Quantification and MISMATCH guardrails), as highlighted in Section 8.3, aims to prevent \"never event\" errorsâ€”hallucinations with severe consequences \\cite{hakim2024d4u}. This exemplifies a direct coupling of technical solutions with stringent ethical frameworks, prioritizing non-maleficence. Such guardrails are crucial for ensuring that LLMs can be trusted in environments where factual accuracy and safety are paramount, moving beyond general mitigation to targeted, high-assurance solutions.\n\nMoreover, responsible AI development extends beyond mere factual accuracy to encompass a broader spectrum of ethical considerations. Hallucination can intersect with and amplify algorithmic bias, leading to outputs that are not only incorrect but also unfair or discriminatory, thereby violating principles of justice and fairness. The phenomenon of \"sycophancy,\" where LLMs excessively agree with or flatter users, even when the user's premise is incorrect, poses a distinct ethical challenge \\cite{malmqvist2024k7x}. Sycophancy can undermine critical thinking, reinforce user biases, and create echo chambers, impacting user autonomy and potentially leading to societal harm. Addressing such behavioral biases is as critical as addressing factual errors for building truly reliable and ethically aligned LLMs. Furthermore, the generation of toxic or harmful content, as noted in surveys on automated correction \\cite{pan2024y3a}, underscores the need for comprehensive safety measures.\n\nThe pursuit of trustworthy AI also requires holistic conceptual frameworks. Research has moved towards defining and mathematically formalizing \"Information Quality\" (IQ) based on consistency, relevance, and accuracy, providing a structured approach to evaluating the ethical performance of LLMs beyond mere factual correctness \\cite{rejeleene2024okw}. This holistic perspective is vital as LLMs expand into multimodal domains, where challenges like long-context hallucinations in multimodal models demand continuous monitoring and tailored ethical considerations \\cite{qiu2024zyc}. The emergence of adversarial attacks that can intentionally induce hallucinations in multimodal LLMs, as discussed in Section 8.2 \\cite{wang2025jen}, highlights critical security vulnerabilities. Ethically, this necessitates proactive security measures and foresight to prevent malicious exploitation and ensure the integrity and safety of AI systems, as emphasized by broader surveys on LLM safety \\cite{gao20242nu}.\n\nIn conclusion, addressing hallucination is not solely a technical endeavor but a profound ethical responsibility that underpins the trustworthiness and societal benefit of LLMs. The development of transparent, accountable, and safe LLM systems requires a multi-faceted approach that integrates robust detection, verifiable content generation, clear communication of uncertainty, and domain-specific safety mechanisms. This forward-looking perspective emphasizes that technical advancements must be inextricably linked with strong ethical frameworks, encompassing principles of non-maleficence, beneficence, justice, fairness, and user autonomy, to ensure that LLMs are developed and utilized in a manner that genuinely benefits society, minimizes harm, and maximizes trustworthiness.\n\n\n\\newpage\n\\section*{References}\n\\addcontentsline{toc}{section}{References}\n\n\\begin{thebibliography}{277}\n\n\\bibitem{vu202337s}\nTu Vu, Mohit Iyyer, Xuezhi Wang, et al. (2023). \\textit{FreshLLMs: Refreshing Large Language Models with Search Engine Augmentation}. Annual Meeting of the Association for Computational Linguistics.\n\n\\bibitem{chang2024u3t}\nYue Chang, Liqiang Jing, Xiaopeng Zhang, et al. (2024). \\textit{A Unified Hallucination Mitigation Framework for Large Vision-Language Models}. Trans. Mach. Learn. Res..\n\n\\bibitem{wang2024vym}\nJiaqi Wang, Yifei Gao, and Jitao Sang (2024). \\textit{VaLiD: Mitigating the Hallucination of Large Vision Language Models by Visual Layer Fusion Contrastive Decoding}. arXiv.org.\n\n\\bibitem{niu2024v97}\nMengjia Niu, Hao Li, Jie Shi, et al. (2024). \\textit{Mitigating Hallucinations in Large Language Models via Self-Refinement-Enhanced Knowledge Retrieval}. arXiv.org.\n\n\\bibitem{liu2024gxh}\nAiwei Liu, Qiang Sheng, and Xuming Hu (2024). \\textit{Preventing and Detecting Misinformation Generated by Large Language Models}. Annual International ACM SIGIR Conference on Research and Development in Information Retrieval.\n\n\\bibitem{li2023v3v}\nXingxuan Li, Ruochen Zhao, Yew Ken Chia, et al. (2023). \\textit{Chain-of-Knowledge: Grounding Large Language Models via Dynamic Knowledge Adapting over Heterogeneous Sources}. International Conference on Learning Representations.\n\n\\bibitem{liang2024hoo}\nMengfei Liang, Archish Arun, Zekun Wu, et al. (2024). \\textit{THaMES: An End-to-End Tool for Hallucination Mitigation and Evaluation in Large Language Models}. arXiv.org.\n\n\\bibitem{zhang2023k1j}\nYue Zhang, Yafu Li, Leyang Cui, et al. (2023). \\textit{Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models}. Computational Linguistics.\n\n\\bibitem{zhou2024lvp}\nGuanyu Zhou, Yibo Yan, Xin Zou, et al. (2024). \\textit{Mitigating Modality Prior-Induced Hallucinations in Multimodal Large Language Models via Deciphering Attention Causality}. International Conference on Learning Representations.\n\n\\bibitem{bai2024tkm}\nZechen Bai, Pichao Wang, Tianjun Xiao, et al. (2024). \\textit{Hallucination of Multimodal Large Language Models: A Survey}. arXiv.org.\n\n\\bibitem{yin2023hx3}\nShukang Yin, Chaoyou Fu, Sirui Zhao, et al. (2023). \\textit{Woodpecker: Hallucination Correction for Multimodal Large Language Models}. Science China Information Sciences.\n\n\\bibitem{cao2023ecl}\nZouying Cao, Yifei Yang, and Hai Zhao (2023). \\textit{AutoHall: Automated Hallucination Dataset Generation for Large Language Models}. arXiv.org.\n\n\\bibitem{wu2024bxt}\nMing-Kuan Wu, Jiayi Ji, Oucheng Huang, et al. (2024). \\textit{Evaluating and Analyzing Relationship Hallucinations in Large Vision-Language Models}. International Conference on Machine Learning.\n\n\\bibitem{ghosh2024tj5}\nBishwamittra Ghosh, Sarah Hasan, Naheed Anjum Arafat, et al. (2024). \\textit{Logical Consistency of Large Language Models in Fact-checking}. International Conference on Learning Representations.\n\n\\bibitem{gao2023ht7}\nTianyu Gao, Howard Yen, Jiatong Yu, et al. (2023). \\textit{Enabling Large Language Models to Generate Text with Citations}. Conference on Empirical Methods in Natural Language Processing.\n\n\\bibitem{yang20251dw}\nBorui Yang, Md Afif Al Mamun, Jie M. Zhang, et al. (2025). \\textit{Hallucination Detection in Large Language Models with Metamorphic Relations}. Proc. ACM Softw. Eng..\n\n\\bibitem{zhang2025pex}\nYongheng Zhang, Xu Liu, Ruoxi Zhou, et al. (2025). \\textit{CCHall: A Novel Benchmark for Joint Cross-Lingual and Cross-Modal Hallucinations Detection in Large Language Models}. Annual Meeting of the Association for Computational Linguistics.\n\n\\bibitem{xing2024itg}\nShangyu Xing, Fei Zhao, Zhen Wu, et al. (2024). \\textit{EFUF: Efficient Fine-Grained Unlearning Framework for Mitigating Hallucinations in Multimodal Large Language Models}. Conference on Empirical Methods in Natural Language Processing.\n\n\\bibitem{leng2023ohr}\nSicong Leng, Hang Zhang, Guanzheng Chen, et al. (2023). \\textit{Mitigating Object Hallucinations in Large Vision-Language Models through Visual Contrastive Decoding}. Computer Vision and Pattern Recognition.\n\n\\bibitem{kim2024ozf}\nJunho Kim, Yeonju Kim, and Yonghyun Ro (2024). \\textit{What if...?: Thinking Counterfactual Keywords Helps to Mitigate Hallucination in Large Multi-modal Models}. Conference on Empirical Methods in Natural Language Processing.\n\n\\bibitem{li2024wyb}\nChaoyu Li, Eun Woo Im, and Pooyan Fazli (2024). \\textit{VidHalluc: Evaluating Temporal Hallucinations in Multimodal Large Language Models for Video Understanding}. Computer Vision and Pattern Recognition.\n\n\\bibitem{ji20243j6}\nZiwei Ji, Yuzhe Gu, Wenwei Zhang, et al. (2024). \\textit{ANAH: Analytical Annotation of Hallucinations in Large Language Models}. Annual Meeting of the Association for Computational Linguistics.\n\n\\bibitem{gao20232zb}\nYunfan Gao, Yun Xiong, Xinyu Gao, et al. (2023). \\textit{Retrieval-Augmented Generation for Large Language Models: A Survey}. arXiv.org.\n\n\\bibitem{ji20227ii}\nZiwei Ji, Zihan Liu, Nayeon Lee, et al. (2022). \\textit{RHO ($Ï$): Reducing Hallucination in Open-domain Dialogues with Knowledge Grounding}. Annual Meeting of the Association for Computational Linguistics.\n\n\\bibitem{adams202289x}\nGriffin Adams, Han-Chin Shing, Q. Sun, et al. (2022). \\textit{Learning to Revise References for Faithful Summarization}. Conference on Empirical Methods in Natural Language Processing.\n\n\\bibitem{su2024gnz}\nWeihang Su, Yichen Tang, Qingyao Ai, et al. (2024). \\textit{Mitigating Entity-Level Hallucination in Large Language Models}. SIGIR-AP.\n\n\\bibitem{du2023qu7}\nLI DU, Yequan Wang, Xingrun Xing, et al. (2023). \\textit{Quantifying and Attributing the Hallucination of Large Language Models via Association Analysis}. arXiv.org.\n\n\\bibitem{ji2023vhv}\nZiwei Ji, Tiezheng Yu, Yan Xu, et al. (2023). \\textit{Towards Mitigating Hallucination in Large Language Models via Self-Reflection}. arXiv.org.\n\n\\bibitem{pan2023mwu}\nLiangming Pan, Michael Stephen Saxon, Wenda Xu, et al. (2023). \\textit{Automatically Correcting Large Language Models: Surveying the landscape of diverse self-correction strategies}. arXiv.org.\n\n\\bibitem{kang20238j0}\nHaoqiang Kang, and Xiao-Yang Liu (2023). \\textit{Deficiency of Large Language Models in Finance: An Empirical Examination of Hallucination}. arXiv.org.\n\n\\bibitem{kang202378c}\nHaoqiang Kang, Juntong Ni, and Huaxiu Yao (2023). \\textit{Ever: Mitigating Hallucination in Large Language Models through Real-Time Verification and Rectification}. arXiv.org.\n\n\\bibitem{dong20223yz}\nYue Dong, J. Wieting, and Pat Verga (2022). \\textit{Faithful to the Document or to the World? Mitigating Hallucinations via Entity-linked Knowledge in Abstractive Summarization}. Conference on Empirical Methods in Natural Language Processing.\n\n\\bibitem{qu20240f7}\nXiaoye Qu, Mingyang Song, Wei Wei, et al. (2024). \\textit{Mitigating Multilingual Hallucination in Large Vision-Language Models}. arXiv.org.\n\n\\bibitem{mckenna2023pzc}\nNick McKenna, Tianyi Li, Liang Cheng, et al. (2023). \\textit{Sources of Hallucination by Large Language Models on Inference Tasks}. Conference on Empirical Methods in Natural Language Processing.\n\n\\bibitem{rejeleene2024okw}\nRick Rejeleene, Xiaowei Xu, and John R. Talburt (2024). \\textit{Towards Trustable Language Models: Investigating Information Quality of Large Language Models}. arXiv.org.\n\n\\bibitem{liu2024p39}\nXinxin Liu (2024). \\textit{A Survey of Hallucination Problems Based on Large Language Models}. Applied and Computational Engineering.\n\n\\bibitem{liu2024sn3}\nHanchao Liu, Wenyuan Xue, Yifei Chen, et al. (2024). \\textit{A Survey on Hallucination in Large Vision-Language Models}. arXiv.org.\n\n\\bibitem{chen2024hfe}\nJiawei Chen, Dingkang Yang, Tong Wu, et al. (2024). \\textit{Detecting and Evaluating Medical Hallucinations in Large Vision Language Models}. arXiv.org.\n\n\\bibitem{li2024hdc}\nQing Li, Chenyang Lyu, Jiahui Geng, et al. (2024). \\textit{Reference-free Hallucination Detection for Large Vision-Language Models}. Conference on Empirical Methods in Natural Language Processing.\n\n\\bibitem{yan2024ux8}\nBei Yan, Jie Zhang, Zheng Yuan, et al. (2024). \\textit{Evaluating the Quality of Hallucination Benchmarks for Large Vision-Language Models}. arXiv.org.\n\n\\bibitem{wang2024rta}\nYuxuan Wang, Yueqian Wang, Dongyan Zhao, et al. (2024). \\textit{VideoHallucer: Evaluating Intrinsic and Extrinsic Hallucinations in Large Video-Language Models}. arXiv.org.\n\n\\bibitem{xie2024l8a}\nYuxi Xie, Guanzhen Li, Xiao Xu, et al. (2024). \\textit{V-DPO: Mitigating Hallucination in Large Vision Language Models via Vision-Guided Direct Preference Optimization}. Conference on Empirical Methods in Natural Language Processing.\n\n\\bibitem{guan2023z15}\nTianrui Guan, Fuxiao Liu, Xiyang Wu, et al. (2023). \\textit{Hallusionbench: An Advanced Diagnostic Suite for Entangled Language Hallucination and Visual Illusion in Large Vision-Language Models}. Computer Vision and Pattern Recognition.\n\n\\bibitem{liang20236sh}\nXun Liang, Shichao Song, Simin Niu, et al. (2023). \\textit{UHGEval: Benchmarking the Hallucination of Chinese Large Language Models via Unconstrained Generation}. Annual Meeting of the Association for Computational Linguistics.\n\n\\bibitem{huang2023akj}\nLei Huang, Weijiang Yu, Weitao Ma, et al. (2023). \\textit{A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions}. ACM Trans. Inf. Syst..\n\n\\bibitem{lv2024k5x}\nQitan Lv, Jie Wang, Hanzhu Chen, et al. (2024). \\textit{Coarse-to-Fine Highlighting: Reducing Knowledge Hallucination in Large Language Models}. International Conference on Machine Learning.\n\n\\bibitem{gu202414e}\nYuzhe Gu, Ziwei Ji, Wenwei Zhang, et al. (2024). \\textit{ANAH-v2: Scaling Analytical Hallucination Annotation of Large Language Models}. Neural Information Processing Systems.\n\n\\bibitem{huang20247wn}\nWen Huang, Hongbin Liu, Minxin Guo, et al. (2024). \\textit{Visual Hallucinations of Multi-modal Large Language Models}. Annual Meeting of the Association for Computational Linguistics.\n\n\\bibitem{liu2023882}\nFuxiao Liu, Kevin Lin, Linjie Li, et al. (2023). \\textit{Mitigating Hallucination in Large Multi-Modal Models via Robust Instruction Tuning}. International Conference on Learning Representations.\n\n\\bibitem{ding2024o88}\nPeng Ding, Jingyu Wu, Jun Kuang, et al. (2024). \\textit{Hallu-PI: Evaluating Hallucination in Multi-modal Large Language Models within Perturbed Inputs}. ACM Multimedia.\n\n\\bibitem{rawte2023ao8}\nVipula Rawte, Swagata Chakraborty, Agnibh Pathak, et al. (2023). \\textit{The Troubling Emergence of Hallucination in Large Language Models - An Extensive Definition, Quantification, and Prescriptive Remediations}. Conference on Empirical Methods in Natural Language Processing.\n\n\\bibitem{pan2024y3a}\nLiangming Pan, Michael Stephen Saxon, Wenda Xu, et al. (2024). \\textit{Automatically Correcting Large Language Models: Surveying the Landscape of Diverse Automated Correction Strategies}. Transactions of the Association for Computational Linguistics.\n\n\\bibitem{li2023rvf}\nJunyi Li, Xiaoxue Cheng, Wayne Xin Zhao, et al. (2023). \\textit{HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models}. Conference on Empirical Methods in Natural Language Processing.\n\n\\bibitem{zhou2023zu6}\nYiyang Zhou, Chenhang Cui, Jaehong Yoon, et al. (2023). \\textit{Analyzing and Mitigating Object Hallucination in Large Vision-Language Models}. International Conference on Learning Representations.\n\n\\bibitem{han202439z}\nZongbo Han, Zechen Bai, Haiyang Mei, et al. (2024). \\textit{Skip \\n: A Simple Method to Reduce Hallucination in Large Vision-Language Models}. arXiv.org.\n\n\\bibitem{wang2025jen}\nYining Wang, Mi Zhang, Junjie Sun, et al. (2025). \\textit{Mirage in the Eyes: Hallucination Attack on Multi-modal Large Language Models with Only Attention Sink}. arXiv.org.\n\n\\bibitem{qu2024pqc}\nXiaoye Qu, Jiashuo Sun, Wei Wei, et al. (2024). \\textit{Look, Compare, Decide: Alleviating Hallucination in Large Vision-Language Models via Multi-View Multi-Path Reasoning}. International Conference on Computational Linguistics.\n\n\\bibitem{dai20229aa}\nWenliang Dai, Zihan Liu, Ziwei Ji, et al. (2022). \\textit{Plausible May Not Be Faithful: Probing Object Hallucination in Vision-Language Pre-training}. Conference of the European Chapter of the Association for Computational Linguistics.\n\n\\bibitem{dziri2021bw9}\nNouha Dziri, Andrea Madotto, Osmar Zaiane, et al. (2021). \\textit{Neural Path Hunter: Reducing Hallucination in Dialogue Systems via Path Grounding}. Conference on Empirical Methods in Natural Language Processing.\n\n\\bibitem{sungbin2024r2g}\nKim Sung-Bin, Oh Hyun-Bin, JungMok Lee, et al. (2024). \\textit{AVHBench: A Cross-Modal Hallucination Benchmark for Audio-Visual Large Language Models}. International Conference on Learning Representations.\n\n\\bibitem{hakim2024d4u}\nJoe B Hakim, Jeffery L. Painter, D. Ramcharran, et al. (2024). \\textit{The Need for Guardrails with Large Language Models in Medical Safety-Critical Settings: An Artificial Intelligence Application in the Pharmacovigilance Ecosystem}. arXiv.org.\n\n\\bibitem{li2025qzg}\nChaozhuo Li, Pengbo Wang, Chenxu Wang, et al. (2025). \\textit{Loki's Dance of Illusions: A Comprehensive Survey of Hallucination in Large Language Models}. arXiv.org.\n\n\\bibitem{wang2023ubf}\nLei Wang, Jiabang He, Shenshen Li, et al. (2023). \\textit{Mitigating Fine-Grained Hallucination by Fine-Tuning Large Vision-Language Models with Caption Rewrites}. Conference on Multimedia Modeling.\n\n\\bibitem{chen2024c4k}\nKedi Chen, Qin Chen, Jie Zhou, et al. (2024). \\textit{DiaHalu: A Dialogue-level Hallucination Evaluation Benchmark for Large Language Models}. Conference on Empirical Methods in Natural Language Processing.\n\n\\bibitem{ding20244yr}\nHanxing Ding, Liang Pang, Zihao Wei, et al. (2024). \\textit{Retrieve Only When It Needs: Adaptive Retrieval Augmentation for Hallucination Mitigation in Large Language Models}. arXiv.org.\n\n\\bibitem{deng202405j}\nShijian Deng, Wentian Zhao, Yu-Jhe Li, et al. (2024). \\textit{Efficient Self-Improvement in Multimodal Large Language Models: A Model-Level Judge-Free Approach}. arXiv.org.\n\n\\bibitem{chen2024j0g}\nJunzhe Chen, Tianshu Zhang, Shiyu Huang, et al. (2024). \\textit{ICT: Image-Object Cross-Level Trusted Intervention for Mitigating Object Hallucination in Large Vision-Language Models}. Computer Vision and Pattern Recognition.\n\n\\bibitem{chen20247jb}\nBeitao Chen, Xinyu Lyu, Lianli Gao, et al. (2024). \\textit{Alleviating Hallucinations in Large Vision-Language Models through Hallucination-Induced Optimization}. Neural Information Processing Systems.\n\n\\bibitem{maynez2020h3q}\nJoshua Maynez, Shashi Narayan, Bernd Bohnet, et al. (2020). \\textit{On Faithfulness and Factuality in Abstractive Summarization}. Annual Meeting of the Association for Computational Linguistics.\n\n\\bibitem{qu20246yn}\nXiaoye Qu, Qiyuan Chen, Wei Wei, et al. (2024). \\textit{Alleviating Hallucination in Large Vision-Language Models with Active Retrieval Augmentation}. ACM Transactions on Multimedia Computing, Communications, and Applications (TOMCCAP).\n\n\\bibitem{chen2023h04}\nJiawei Chen, Hongyu Lin, Xianpei Han, et al. (2023). \\textit{Benchmarking Large Language Models in Retrieval-Augmented Generation}. AAAI Conference on Artificial Intelligence.\n\n\\bibitem{zhou2024wbi}\nYiyang Zhou, Chenhang Cui, Rafael Rafailov, et al. (2024). \\textit{Aligning Modalities in Vision Large Language Models via Preference Fine-tuning}. arXiv.org.\n\n\\bibitem{jiang2024792}\nChaoya Jiang, Wei Ye, Mengfan Dong, et al. (2024). \\textit{Hal-Eval: A Universal and Fine-grained Hallucination Evaluation Framework for Large Vision Language Models}. ACM Multimedia.\n\n\\bibitem{tjandra2024umq}\nBenedict Aaron Tjandra, Muhammed Razzak, Jannik Kossen, et al. (2024). \\textit{Fine-Tuning Large Language Models to Appropriately Abstain with Semantic Entropy}. arXiv.org.\n\n\\bibitem{umapathi2023puv}\nLogesh Kumar Umapathi, Ankit Pal, and Malaikannan Sankarasubbu (2023). \\textit{Med-HALT: Medical Domain Hallucination Test for Large Language Models}. Conference on Computational Natural Language Learning.\n\n\\bibitem{zou2024dp7}\nXin Zou, Yizhou Wang, Yibo Yan, et al. (2024). \\textit{Look Twice Before You Answer: Memory-Space Visual Retracing for Hallucination Mitigation in Multimodal Large Language Models}. arXiv.org.\n\n\\bibitem{li2024osp}\nNingke Li, Yuekang Li, Yi Liu, et al. (2024). \\textit{Drowzee: Metamorphic Testing for Fact-Conflicting Hallucination Detection in Large Language Models}. Proc. ACM Program. Lang..\n\n\\bibitem{chuang20248ey}\nYung-Sung Chuang, Linlu Qiu, Cheng-Yu Hsieh, et al. (2024). \\textit{Lookback Lens: Detecting and Mitigating Contextual Hallucinations in Large Language Models Using Only Attention Maps}. Conference on Empirical Methods in Natural Language Processing.\n\n\\bibitem{li2024qrj}\nJunyi Li, Jie Chen, Ruiyang Ren, et al. (2024). \\textit{The Dawn After the Dark: An Empirical Study on Factuality Hallucination in Large Language Models}. Annual Meeting of the Association for Computational Linguistics.\n\n\\bibitem{wang2023zop}\nJunyan Wang, Yi Zhou, Guohai Xu, et al. (2023). \\textit{Evaluation and Analysis of Hallucination in Large Vision-Language Models}. arXiv.org.\n\n\\bibitem{xu2024n76}\nZiwei Xu, Sanjay Jain, and Mohan Kankanhalli (2024). \\textit{Hallucination is Inevitable: An Innate Limitation of Large Language Models}. arXiv.org.\n\n\\bibitem{liu2021mo6}\nTianyu Liu, Yizhe Zhang, C. Brockett, et al. (2021). \\textit{A Token-level Reference-free Hallucination Detection Benchmark for Free-form Text Generation}. Annual Meeting of the Association for Computational Linguistics.\n\n\\bibitem{manakul20236ex}\nPotsawee Manakul, Adian Liusie, and M. Gales (2023). \\textit{SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models}. Conference on Empirical Methods in Natural Language Processing.\n\n\\bibitem{zhong2024mfi}\nWeihong Zhong, Xiaocheng Feng, Liang Zhao, et al. (2024). \\textit{Investigating and Mitigating the Multimodal Hallucination Snowballing in Large Vision-Language Models}. Annual Meeting of the Association for Computational Linguistics.\n\n\\bibitem{zhang2024qq9}\nYuji Zhang, Sha Li, Jiateng Liu, et al. (2024). \\textit{Knowledge Overshadowing Causes Amalgamated Hallucination in Large Language Models}. arXiv.org.\n\n\\bibitem{wu20241us}\nJ. Wu, Tsz Ting Chung, Kai Chen, et al. (2024). \\textit{Unified Triplet-Level Hallucination Evaluation for Large Vision-Language Models}. Trans. Mach. Learn. Res..\n\n\\bibitem{tonmoy20244e4}\nS. Tonmoy, S. M. M. Zaman, Vinija Jain, et al. (2024). \\textit{A Comprehensive Survey of Hallucination Mitigation Techniques in Large Language Models}. arXiv.org.\n\n\\bibitem{wu2024n00}\nKai Wu, Boyuan Jiang, Zhengkai Jiang, et al. (2024). \\textit{NoiseBoost: Alleviating Hallucination with Noise Perturbation for Multimodal Large Language Models}. arXiv.org.\n\n\\bibitem{wen2023t6v}\nYilin Wen, Zifeng Wang, and Jimeng Sun (2023). \\textit{MindMap: Knowledge Graph Prompting Sparks Graph of Thoughts in Large Language Models}. Annual Meeting of the Association for Computational Linguistics.\n\n\\bibitem{li2023249}\nYifan Li, Yifan Du, Kun Zhou, et al. (2023). \\textit{Evaluating Object Hallucination in Large Vision-Language Models}. Conference on Empirical Methods in Natural Language Processing.\n\n\\bibitem{lan20240yz}\nWei Lan, Wenyi Chen, Qingfeng Chen, et al. (2024). \\textit{A Survey of Hallucination in Large Visual Language Models}. arXiv.org.\n\n\\bibitem{dhuliawala2023rqn}\nS. Dhuliawala, M. Komeili, Jing Xu, et al. (2023). \\textit{Chain-of-Verification Reduces Hallucination in Large Language Models}. Annual Meeting of the Association for Computational Linguistics.\n\n\\bibitem{sui20242u1}\nYuan Sui, and Bryan Hooi (2024). \\textit{Can Knowledge Graphs Make Large Language Models More Trustworthy? An Empirical Study over Open-ended Question Answering}. Annual Meeting of the Association for Computational Linguistics.\n\n\\bibitem{xiao2024hv1}\nWenyi Xiao, Ziwei Huang, Leilei Gan, et al. (2024). \\textit{Detecting and Mitigating Hallucination in Large Vision Language Models via Fine-Grained AI Feedback}. arXiv.org.\n\n\\bibitem{trivedi2022qsf}\nH. Trivedi, Niranjan Balasubramanian, Tushar Khot, et al. (2022). \\textit{Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions}. Annual Meeting of the Association for Computational Linguistics.\n\n\\bibitem{park20247cm}\nYeji Park, Deokyeong Lee, Junsuk Choe, et al. (2024). \\textit{ConVis: Contrastive Decoding with Hallucination Visualization for Mitigating Hallucinations in Multimodal Large Language Models}. AAAI Conference on Artificial Intelligence.\n\n\\bibitem{sridhar2022l1c}\nA. Sridhar, and Erik M. Visser (2022). \\textit{Improved Beam Search for Hallucination Mitigation in Abstractive Summarization}. arXiv.org.\n\n\\bibitem{su2024lem}\nWeihang Su, Changyue Wang, Qingyao Ai, et al. (2024). \\textit{Unsupervised Real-Time Hallucination Detection based on the Internal States of Large Language Models}. Annual Meeting of the Association for Computational Linguistics.\n\n\\bibitem{luo2023xyc}\nJunyu Luo, Cao Xiao, and Fenglong Ma (2023). \\textit{Zero-Resource Hallucination Prevention for Large Language Models}. Conference on Empirical Methods in Natural Language Processing.\n\n\\bibitem{wu2024h81}\nJun Wu, Q. Liu, Ding Wang, et al. (2024). \\textit{Logical Closed Loop: Uncovering Object Hallucinations in Large Vision-Language Models}. Annual Meeting of the Association for Computational Linguistics.\n\n\\bibitem{chen2024vy7}\nXuweiyi Chen, Ziqiao Ma, Xuejun Zhang, et al. (2024). \\textit{Multi-Object Hallucination in Vision-Language Models}. Neural Information Processing Systems.\n\n\\bibitem{zheng20246fk}\nKening Zheng, Junkai Chen, Yibo Yan, et al. (2024). \\textit{Reefknot: A Comprehensive Benchmark for Relation Hallucination Evaluation, Analysis and Mitigation in Multimodal Large Language Models}. Annual Meeting of the Association for Computational Linguistics.\n\n\\bibitem{chen2024lc5}\nXiang Chen, Chenxi Wang, Yida Xue, et al. (2024). \\textit{Unified Hallucination Detection for Multimodal Large Language Models}. Annual Meeting of the Association for Computational Linguistics.\n\n\\bibitem{sahoo2024hcb}\nPranab Sahoo, Prabhash Meharia, Akash Ghosh, et al. (2024). \\textit{A Comprehensive Survey of Hallucination in Large Language, Image, Video and Audio Foundation Models}. Conference on Empirical Methods in Natural Language Processing.\n\n\\bibitem{zhang2024mmj}\nRuiyang Zhang, Hu Zhang, and Zhedong Zheng (2024). \\textit{VL-Uncertainty: Detecting Hallucination in Large Vision-Language Model via Uncertainty Estimation}. arXiv.org.\n\n\\bibitem{qiu2024zyc}\nHan Qiu, Jiaxing Huang, Peng Gao, et al. (2024). \\textit{LongHalQA: Long-Context Hallucination Evaluation for MultiModal Large Language Models}. arXiv.org.\n\n\\bibitem{oh2024xa3}\nJio Oh, Soyeon Kim, Junseok Seo, et al. (2024). \\textit{ERBench: An Entity-Relationship based Automatically Verifiable Hallucination Benchmark for Large Language Models}. Neural Information Processing Systems.\n\n\\bibitem{zhang202396g}\nYue Zhang, Leyang Cui, Wei Bi, et al. (2023). \\textit{Alleviating Hallucinations of Large Language Models through Induced Hallucinations}. North American Chapter of the Association for Computational Linguistics.\n\n\\bibitem{yin2025s2b}\nHao Yin, Guangzong Si, and Zilei Wang (2025). \\textit{ClearSight: Visual Signal Enhancement for Object Hallucination Mitigation in Multimodal Large Language Models}. Computer Vision and Pattern Recognition.\n\n\\bibitem{goyal2021onb}\nTanya Goyal, Jiacheng Xu, J. Li, et al. (2021). \\textit{Training Dynamics for Text Summarization Models}. Findings.\n\n\\bibitem{kuan20249pm}\nChun-Yi Kuan, Wei-Ping Huang, and Hung-yi Lee (2024). \\textit{Understanding Sounds, Missing the Questions: The Challenge of Object Hallucination in Large Audio-Language Models}. Interspeech.\n\n\\bibitem{kaul2024ta7}\nPrannay Kaul, Zhizhong Li, Hao Yang, et al. (2024). \\textit{THRONE: An Object-Based Hallucination Benchmark for the Free-Form Generations of Large Vision-Language Models}. Computer Vision and Pattern Recognition.\n\n\\bibitem{yebin2024txh}\nMoon Ye-Bin, Nam Hyeon-Woo, Wonseok Choi, et al. (2024). \\textit{BEAF: Observing BEfore-AFter Changes to Evaluate Hallucination in Vision-language Models}. European Conference on Computer Vision.\n\n\\bibitem{zhao2024ge8}\nLinxi Zhao, Yihe Deng, Weitong Zhang, et al. (2024). \\textit{Mitigating Object Hallucination in Large Vision-Language Models via Image-Grounded Guidance}. Unpublished manuscript.\n\n\\bibitem{ye2023yom}\nHongbin Ye, Tong Liu, Aijia Zhang, et al. (2023). \\textit{Cognitive Mirage: A Review of Hallucinations in Large Language Models}. LKM@IJCAI.\n\n\\bibitem{zhang2023k5a}\nHanning Zhang, Shizhe Diao, Yong Lin, et al. (2023). \\textit{R-Tuning: Instructing Large Language Models to Say â€˜I Donâ€™t Knowâ€™}. North American Chapter of the Association for Computational Linguistics.\n\n\\bibitem{li2022ypy}\nYanyang Li, Jianqiao Zhao, M. Lyu, et al. (2022). \\textit{Eliciting Knowledge from Large Pre-Trained Models for Unsupervised Knowledge-Grounded Conversation}. Conference on Empirical Methods in Natural Language Processing.\n\n\\bibitem{huang2023du3}\nQidong Huang, Xiao-wen Dong, Pan Zhang, et al. (2023). \\textit{OPERA: Alleviating Hallucination in Multi-Modal Large Language Models via Over-Trust Penalty and Retrospection-Allocation}. Computer Vision and Pattern Recognition.\n\n\\bibitem{tang2024a1j}\nJianheng Tang, Qifan Zhang, Yuhan Li, et al. (2024). \\textit{GraphArena: Evaluating and Exploring Large Language Models on Graph Computation}. International Conference on Learning Representations.\n\n\\bibitem{fu2024yqj}\nYuhan Fu, Ruobing Xie, Xingwu Sun, et al. (2024). \\textit{Mitigating Hallucination in Multimodal Large Language Model via Hallucination-targeted Direct Preference Optimization}. Annual Meeting of the Association for Computational Linguistics.\n\n\\bibitem{yao20229uz}\nShunyu Yao, Jeffrey Zhao, Dian Yu, et al. (2022). \\textit{ReAct: Synergizing Reasoning and Acting in Language Models}. International Conference on Learning Representations.\n\n\\bibitem{kanthara2022kuj}\nShankar Kanthara, Rixie Tiffany Ko Leong, Xiang Lin, et al. (2022). \\textit{Chart-to-Text: A Large-Scale Benchmark for Chart Summarization}. Annual Meeting of the Association for Computational Linguistics.\n\n\\bibitem{kim2021obx}\nBoseop Kim, Hyoungseok Kim, Sang-Woo Lee, et al. (2021). \\textit{What Changes Can Large-scale Language Models Bring? Intensive Study on HyperCLOVA: Billions-scale Korean Generative Pretrained Transformers}. Conference on Empirical Methods in Natural Language Processing.\n\n\\bibitem{xia20224cl}\nMengzhou Xia, Mikel Artetxe, Chunting Zhou, et al. (2022). \\textit{Training Trajectories of Language Models Across Scales}. Annual Meeting of the Association for Computational Linguistics.\n\n\\bibitem{aharoni2022ioz}\nRoee Aharoni, Shashi Narayan, Joshua Maynez, et al. (2022). \\textit{mFACE: Multilingual Summarization with Factual Consistency Evaluation}. Annual Meeting of the Association for Computational Linguistics.\n\n\\bibitem{zhang2022p55}\nHaopeng Zhang, Semih Yavuz, Wojciech Kryscinski, et al. (2022). \\textit{Improving the Faithfulness of Abstractive Summarization via Entity Coverage Control}. NAACL-HLT.\n\n\\bibitem{jiang2022reg}\nWenhui Jiang, Minwei Zhu, Yuming Fang, et al. (2022). \\textit{Visual Cluster Grounding for Image Captioning}. IEEE Transactions on Image Processing.\n\n\\bibitem{wang2020vz6}\nHongmin Wang (2020). \\textit{Revisiting Challenges in Data-to-Text Generation with Fact Grounding}. International Conference on Natural Language Generation.\n\n\\bibitem{chen2022gkm}\nSihao Chen, S. Buthpitiya, Alex Fabrikant, et al. (2022). \\textit{PropSegmEnt: A Large-Scale Corpus for Proposition-Level Segmentation and Entailment Recognition}. Annual Meeting of the Association for Computational Linguistics.\n\n\\bibitem{korbak202191w}\nTomasz Korbak, Hady ElSahar, GermÃ¡n Kruszewski, et al. (2021). \\textit{Controlling Conditional Language Models without Catastrophic Forgetting}. International Conference on Machine Learning.\n\n\\bibitem{raman20229ce}\nK. Raman, Iftekhar Naim, Jiecao Chen, et al. (2022). \\textit{Transforming Sequence Tagging Into A Seq2Seq Task}. Conference on Empirical Methods in Natural Language Processing.\n\n\\bibitem{liu2021h6c}\nLing Liu, and Mans Hulden (2021). \\textit{Can a Transformer Pass the Wug Test? Tuning Copying Bias in Neural Morphological Inflection Models}. Annual Meeting of the Association for Computational Linguistics.\n\n\\bibitem{norlund2021462}\nTobias Norlund, Lovisa HagstrÃ¶m, and Richard Johansson (2021). \\textit{Transferring Knowledge from Vision to Language: How to Achieve it and how to Measure it?}. BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP.\n\n\\bibitem{wu20206gt}\nTao Wu, E. Chio, Heng-Tze Cheng, et al. (2020). \\textit{Zero-Shot Heterogeneous Transfer Learning from Recommender Systems to Cold-Start Search Retrieval}. International Conference on Information and Knowledge Management.\n\n\\bibitem{liu2022hw7}\nYongtai Liu, Joshua Maynez, GonÃ§alo SimÃµes, et al. (2022). \\textit{Data Augmentation for Low-Resource Dialogue Summarization}. NAACL-HLT.\n\n\\bibitem{jelinek2016205}\nL. Jelinek, M. Hauschildt, C. Wittekind, et al. (2016). \\textit{Efficacy of Metacognitive Training for Depression: A Randomized Controlled Trial}. Psychotherapy and Psychosomatics.\n\n\\bibitem{raunak2022r58}\nVikas Raunak, and Arul Menezes (2022). \\textit{Finding Memo: Extractive Memorization in Constrained Sequence Generation Tasks}. Conference on Empirical Methods in Natural Language Processing.\n\n\\bibitem{saha20229lo}\nSwarnadeep Saha, Xinyan Velocity Yu, Mohit Bansal, et al. (2022). \\textit{MURMUR: Modular Multi-Step Reasoning for Semi-Structured Data-to-Text Generation}. Annual Meeting of the Association for Computational Linguistics.\n\n\\bibitem{ham20213fx}\nSoomin Ham, Kibaek Park, Yeongjun Jang, et al. (2021). \\textit{KSL-Guide: A Large-scale Korean Sign Language Dataset Including Interrogative Sentences for Guiding the Deaf and Hard-of-Hearing}. IEEE International Conference on Automatic Face & Gesture Recognition.\n\n\\bibitem{jeong20180d2}\nEunji Jeong, Sungwoo Cho, Gyeong-In Yu, et al. (2018). \\textit{JANUS: Fast and Flexible Deep Learning via Symbolic Graph Execution of Imperative Programs}. Symposium on Networked Systems Design and Implementation.\n\n\\bibitem{kedia2022c03}\nAkhil Kedia, Mohd Abbas Zaidi, and Haejun Lee (2022). \\textit{FiE: Building a Global Probability Space by Leveraging Early Fusion in Encoder for Open-Domain Question Answering}. Conference on Empirical Methods in Natural Language Processing.\n\n\\bibitem{gallace201080s}\nA. Gallace, and A. Gallace (2010). \\textit{Touch and the body: The role of the somatosensory cortex in tactile awareness}. Unpublished manuscript.\n\n\\bibitem{li20203k7}\nXintong Li, Aleksandre Maskharashvili, S. Stevens-Guille, et al. (2020). \\textit{Leveraging Large Pretrained Models for WebNLG 2020}. WEBNLG.\n\n\\bibitem{zhou2011j8m}\nTom Chao Zhou, Chin-Yew Lin, Irwin King, et al. (2011). \\textit{Learning to Suggest Questions in Online Forums}. AAAI Conference on Artificial Intelligence.\n\n\\bibitem{injae2016yq6}\nShin In-Jae, Byungkwen Song, and D. Eom (2016). \\textit{Auto-Mapping and Configuration Method of IEC 61850 Information Model Based on OPC UA}. Unpublished manuscript.\n\n\\bibitem{jeong2019z3k}\nEunji Jeong, Sungwoo Cho, Gyeong-In Yu, et al. (2019). \\textit{Speculative Symbolic Graph Execution of Imperative Deep Learning Programs}. ACM SIGOPS Operating Systems Review.\n\n\\bibitem{rajani20171n9}\nNazneen Rajani, Mihaela A. Bornea, and Ken Barker (2017). \\textit{Stacking With Auxiliary Features for Entity Linking in the Medical Domain}. Workshop on Biomedical Natural Language Processing.\n\n\\bibitem{wang202379k}\nShuhe Wang, Xiaofei Sun, Xiaoya Li, et al. (2023). \\textit{GPT-NER: Named Entity Recognition via Large Language Models}. North American Chapter of the Association for Computational Linguistics.\n\n\\bibitem{alshahwan2024v64}\nN. Alshahwan, Jubin Chheda, Anastasia Finogenova, et al. (2024). \\textit{Automated Unit Test Improvement using Large Language Models at Meta}. SIGSOFT FSE Companion.\n\n\\bibitem{zou2024ucl}\nWei Zou, Runpeng Geng, Binghui Wang, et al. (2024). \\textit{PoisonedRAG: Knowledge Poisoning Attacks to Retrieval-Augmented Generation of Large Language Models}. arXiv.org.\n\n\\bibitem{fadeeva2024lt8}\nEkaterina Fadeeva, Aleksandr Rubashevskii, Artem Shelmanov, et al. (2024). \\textit{Fact-Checking the Output of Large Language Models via Token-Level Uncertainty Quantification}. Annual Meeting of the Association for Computational Linguistics.\n\n\\bibitem{nguyen2023obn}\nThuat Nguyen, C. Nguyen, Viet Dac Lai, et al. (2023). \\textit{CulturaX: A Cleaned, Enormous, and Multilingual Dataset for Large Language Models in 167 Languages}. International Conference on Language Resources and Evaluation.\n\n\\bibitem{zou2024c26}\nWei Zou, Runpeng Geng, Binghui Wang, et al. (2024). \\textit{PoisonedRAG: Knowledge Corruption Attacks to Retrieval-Augmented Generation of Large Language Models}. Unpublished manuscript.\n\n\\bibitem{li2023f7d}\nHuao Li, Yu Quan Chong, Simon Stepputtis, et al. (2023). \\textit{Theory of Mind for Multi-Agent Collaboration via Large Language Models}. Conference on Empirical Methods in Natural Language Processing.\n\n\\bibitem{wang2023hgw}\nYiming Wang, Zhuosheng Zhang, and Rui Wang (2023). \\textit{Element-aware Summarization with Large Language Models: Expert-aligned Evaluation and Chain-of-Thought Method}. Annual Meeting of the Association for Computational Linguistics.\n\n\\bibitem{chen2023gii}\nYuyan Chen, Qiang Fu, Yichen Yuan, et al. (2023). \\textit{Hallucination Detection: Robustly Discerning Reliable Answers in Large Language Models}. International Conference on Information and Knowledge Management.\n\n\\bibitem{gilbert2024uu2}\nS. Gilbert, J. Kather, and Aidan Hogan (2024). \\textit{Augmented non-hallucinating large language models as medical information curators}. npj Digit. Medicine.\n\n\\bibitem{kim2024vgn}\nSunkyu Kim, Choong-kun Lee, and Seung-seob Kim (2024). \\textit{Large Language Models: A Guide for Radiologists}. Korean Journal of Radiology.\n\n\\bibitem{wang2024sae}\nJianing Wang, Junda Wu, Yupeng Hou, et al. (2024). \\textit{InstructGraph: Boosting Large Language Models via Graph-centric Instruction Tuning and Preference Alignment}. Annual Meeting of the Association for Computational Linguistics.\n\n\\bibitem{huang20233v0}\nYuheng Huang, Jiayang Song, Zhijie Wang, et al. (2023). \\textit{Look Before You Leap: An Exploratory Study of Uncertainty Measurement for Large Language Models}. arXiv.org.\n\n\\bibitem{zhao2024s3a}\nLinxi Zhao, Yihe Deng, Weitong Zhang, et al. (2024). \\textit{Mitigating Object Hallucination in Large Vision-Language Models via Classifier-Free Guidance}. arXiv.org.\n\n\\bibitem{leiser2024kfo}\nFlorian Leiser, S. Eckhardt, Valentin Leuthe, et al. (2024). \\textit{HILL: A Hallucination Identifier for Large Language Models}. International Conference on Human Factors in Computing Systems.\n\n\\bibitem{malmqvist2024k7x}\nLars Malmqvist (2024). \\textit{Sycophancy in Large Language Models: Causes and Mitigations}. arXiv.org.\n\n\\bibitem{lin2024gru}\nSheng-Chieh Lin, Luyu Gao, Barlas OÄŸuz, et al. (2024). \\textit{FLAME: Factuality-Aware Alignment for Large Language Models}. Neural Information Processing Systems.\n\n\\bibitem{li2023dw0}\nXingxuan Li, Ruochen Zhao, Yew Ken Chia, et al. (2023). \\textit{Chain of Knowledge: A Framework for Grounding Large Language Models with Structured Knowledge Bases}. arXiv.org.\n\n\\bibitem{ma2023mka}\nFan Ma, Xiaojie Jin, Heng Wang, et al. (2023). \\textit{Vista-llama: Reducing Hallucination in Video Language Models via Equal Distance to Visual Tokens}. Computer Vision and Pattern Recognition.\n\n\\bibitem{song2024t8k}\nPeiyang Song, Kaiyu Yang, and Anima Anandkumar (2024). \\textit{Towards Large Language Models as Copilots for Theorem Proving in Lean}. arXiv.org.\n\n\\bibitem{jiang20242kz}\nChe Jiang, Biqing Qi, Xiangyu Hong, et al. (2024). \\textit{On Large Language Modelsâ€™ Hallucination with Regard to Known Facts}. North American Chapter of the Association for Computational Linguistics.\n\n\\bibitem{song2024br2}\nPeiyang Song, Kaiyu Yang, and Anima Anandkumar (2024). \\textit{Lean Copilot: Large Language Models as Copilots for Theorem Proving in Lean}. NeuS.\n\n\\bibitem{liu2024ker}\nHaochen Liu, Song Wang, Yaochen Zhu, et al. (2024). \\textit{Knowledge Graph-Enhanced Large Language Models via Path Selection}. Annual Meeting of the Association for Computational Linguistics.\n\n\\bibitem{sun2024z6b}\nYuhong Sun, Zhangyue Yin, Qipeng Guo, et al. (2024). \\textit{Benchmarking Hallucination in Large Language Models Based on Unanswerable Math Word Problem}. International Conference on Language Resources and Evaluation.\n\n\\bibitem{ling2024hqv}\nChen Ling, Xujiang Zhao, Wei Cheng, et al. (2024). \\textit{Uncertainty Quantification for In-Context Learning of Large Language Models}. North American Chapter of the Association for Computational Linguistics.\n\n\\bibitem{li2024jbb}\nMoxin Li, Wenjie Wang, Fuli Feng, et al. (2024). \\textit{Think Twice Before Trusting: Self-Detection for Large Language Models through Comprehensive Answer Reflection}. Conference on Empirical Methods in Natural Language Processing.\n\n\\bibitem{smith2023i8w}\nAndrew L Smith, Felix Greaves, and T. Panch (2023). \\textit{Hallucination or Confabulation? Neuroanatomy as metaphor in Large Language Models}. PLOS Digital Health.\n\n\\bibitem{shah20242sx}\nSavyasachi V. Shah (2024). \\textit{Accuracy, Consistency, and Hallucination of Large Language Models When Analyzing Unstructured Clinical Notes in Electronic Medical Records.}. JAMA Network Open.\n\n\\bibitem{pan2024hm4}\nZhenyu Pan, Haozheng Luo, Manling Li, et al. (2024). \\textit{Chain-of-Action: Faithful and Multimodal Question Answering through Large Language Models}. International Conference on Learning Representations.\n\n\\bibitem{zhang2024o58}\nHengran Zhang, Ruqing Zhang, J. Guo, et al. (2024). \\textit{Are Large Language Models Good at Utility Judgments?}. Annual International ACM SIGIR Conference on Research and Development in Information Retrieval.\n\n\\bibitem{tang2024cxa}\nJianheng Tang, Qifan Zhang, Yuhan Li, et al. (2024). \\textit{GraphArena: Benchmarking Large Language Models on Graph Computational Problems}. arXiv.org.\n\n\\bibitem{zhou2024d14}\nXiongtao Zhou, Jie He, Yuhua Ke, et al. (2024). \\textit{An Empirical Study on Parameter-Efficient Fine-Tuning for MultiModal Large Language Models}. Annual Meeting of the Association for Computational Linguistics.\n\n\\bibitem{zhao2024g9c}\nRuilin Zhao, Feng Zhao, Long Wang, et al. (2024). \\textit{KG-CoT: Chain-of-Thought Prompting of Large Language Models over Knowledge Graphs for Knowledge-Aware Question Answering}. International Joint Conference on Artificial Intelligence.\n\n\\bibitem{pan2024uot}\nZhenyu Pan, Haozheng Luo, Manling Li, et al. (2024). \\textit{Conv-CoA: Improving Open-domain Question Answering in Large Language Models via Conversational Chain-of-Action}. arXiv.org.\n\n\\bibitem{zhu2024hll}\nDerui Zhu, Dingfan Chen, Qing Li, et al. (2024). \\textit{PoLLMgraph: Unraveling Hallucinations in Large Language Models via State Transition Dynamics}. NAACL-HLT.\n\n\\bibitem{zhang20252at}\nWan Zhang, and Jing Zhang (2025). \\textit{Hallucination Mitigation for Retrieval-Augmented Large Language Models: A Review}. Mathematics.\n\n\\bibitem{chen2024kgu}\nLida Chen, Zujie Liang, Xintao Wang, et al. (2024). \\textit{Teaching Large Language Models to Express Knowledge Boundary from Their Own Signals}. Proceedings of the 3rd Workshop on Towards Knowledgeable Foundation Models (KnowFM).\n\n\\bibitem{dernbach2024w0b}\nStefan Dernbach, Khushbu Agarwal, Alejandro Zuniga, et al. (2024). \\textit{GLaM: Fine-Tuning Large Language Models for Domain Knowledge Graph Alignment via Neighborhood Partitioning and Generative Subgraph Encoding}. AAAI Spring Symposia.\n\n\\bibitem{wu202407f}\nJiageng Wu, Xian Wu, and Jie Yang (2024). \\textit{Guiding Clinical Reasoning with Large Language Models via Knowledge Seeds}. International Joint Conference on Artificial Intelligence.\n\n\\bibitem{ahn2024r1o}\nJaewoo Ahn, Taehyun Lee, Junyoung Lim, et al. (2024). \\textit{TimeChara: Evaluating Point-in-Time Character Hallucination of Role-Playing Large Language Models}. Annual Meeting of the Association for Computational Linguistics.\n\n\\bibitem{mou2024fsy}\nXinyi Mou, Zejun Li, Hanjia Lyu, et al. (2024). \\textit{Unifying Local and Global Knowledge: Empowering Large Language Models as Political Experts with Knowledge Graphs}. The Web Conference.\n\n\\bibitem{xu2024f68}\nDerong Xu, Ziheng Zhang, Zhihong Zhu, et al. (2024). \\textit{Editing Factual Knowledge and Explanatory Ability of Medical Large Language Models}. International Conference on Information and Knowledge Management.\n\n\\bibitem{hu2024fnt}\nXiangkun Hu, Dongyu Ru, Lin Qiu, et al. (2024). \\textit{RefChecker: Reference-based Fine-grained Hallucination Checker and Benchmark for Large Language Models}. arXiv.org.\n\n\\bibitem{mukherjee2024o5w}\nSubhojyoti Mukherjee, Anusha Lalitha, Sailik Sengupta, et al. (2024). \\textit{Multi-Objective Alignment of Large Language Models Through Hypervolume Maximization}. arXiv.org.\n\n\\bibitem{jiao2024l4e}\nQirui Jiao, Daoyuan Chen, Yilun Huang, et al. (2024). \\textit{Enhancing Multimodal Large Language Models with Vision Detection Models: An Empirical Study}. arXiv.org.\n\n\\bibitem{ding20245e3}\nHao Ding, Ziwei Fan, Ingo GÃ¼hring, et al. (2024). \\textit{Reasoning and Planning with Large Language Models in Code Development}. Knowledge Discovery and Data Mining.\n\n\\bibitem{wang2024swy}\nChengpeng Wang, Wuqi Zhang, Zian Su, et al. (2024). \\textit{Sanitizing Large Language Models in Bug Detection with Data-Flow}. Conference on Empirical Methods in Natural Language Processing.\n\n\\bibitem{chen2024md6}\nZhuo Chen, Jiawei Liu, Haotan Liu, et al. (2024). \\textit{Black-Box Opinion Manipulation Attacks to Retrieval-Augmented Generation of Large Language Models}. arXiv.org.\n\n\\bibitem{wang2023ynd}\nXiaohua Wang, Yuliang Yan, Longtao Huang, et al. (2023). \\textit{Hallucination Detection for Generative Large Language Models by Bayesian Sequential Estimation}. Conference on Empirical Methods in Natural Language Processing.\n\n\\bibitem{yeo2024g7d}\nWei Jie Yeo, Teddy Ferdinan, PrzemysÅ‚aw Kazienko, et al. (2024). \\textit{Self-training Large Language Models through Knowledge Detection}. Conference on Empirical Methods in Natural Language Processing.\n\n\\bibitem{hu2024fld}\nSihao Hu, Tiansheng Huang, and Ling Liu (2024). \\textit{PokeLLMon: A Human-Parity Agent for Pokemon Battles with Large Language Models}. arXiv.org.\n\n\\bibitem{zhang2024ia4}\nZhenhong Zhang, Jiajing Chen, Weiyan Shi, et al. (2024). \\textit{Contrastive Learning for Knowledge-Based Question Generation in Large Language Models}. 2024 5th International Conference on Intelligent Computing and Human-Computer Interaction (ICHCI).\n\n\\bibitem{yang2024iia}\nDingkang Yang, Dongling Xiao, Jinjie Wei, et al. (2024). \\textit{Improving Factuality in Large Language Models via Decoding-Time Hallucinatory and Truthful Comparators}. AAAI Conference on Artificial Intelligence.\n\n\\bibitem{yu2023ine}\nXiaodong Yu, Hao Cheng, Xiaodong Liu, et al. (2023). \\textit{ReEval: Automatic Hallucination Evaluation for Retrieval-Augmented Large Language Models via Transferable Adversarial Attacks}. NAACL-HLT.\n\n\\bibitem{gu2024eig}\nZishan Gu, Changchang Yin, Fenglin Liu, et al. (2024). \\textit{MedVH: Towards Systematic Evaluation of Hallucination for Large Vision Language Models in the Medical Context}. arXiv.org.\n\n\\bibitem{liu2024kf2}\nLihui Liu, Zihao Wang, Ruizhong Qiu, et al. (2024). \\textit{Logic Query of Thoughts: Guiding Large Language Models to Answer Complex Logic Queries with Knowledge Graphs}. arXiv.org.\n\n\\bibitem{woo2024dtm}\nB. Woo, Tom Huynh, Arthur Tang, et al. (2024). \\textit{Transforming nursing with large language models: from concept to practice.}. European Journal of Cardiovascular Nursing.\n\n\\bibitem{zhao20246wi}\nXiutian Zhao, Ke Wang, and Wei Peng (2024). \\textit{Measuring the Inconsistency of Large Language Models in Preferential Ranking}. KNOWLLM.\n\n\\bibitem{qian2024mj9}\nXinying Qian, Ying Zhang, Yu Zhao, et al. (2024). \\textit{TimeR^4 : Time-aware Retrieval-Augmented Large Language Models for Temporal Knowledge Graph Question Answering}. Conference on Empirical Methods in Natural Language Processing.\n\n\\bibitem{butler20242xs}\nJ. Butler, James Puleo, Michael Harrington, et al. (2024). \\textit{From technical to understandable: Artificial Intelligence Large Language Models improve the readability of knee radiology reports.}. Knee Surgery, Sports Traumatology, Arthroscopy.\n\n\\bibitem{sahoo202420w}\nN. R. Sahoo, Ashita Saxena, Kishan Maharaj, et al. (2024). \\textit{Addressing Bias and Hallucination in Large Language Models}. International Conference on Language Resources and Evaluation.\n\n\\bibitem{zuo20242i0}\nKaiwen Zuo, and Yirui Jiang (2024). \\textit{MedHallBench: A New Benchmark for Assessing Hallucination in Medical Large Language Models}. arXiv.org.\n\n\\bibitem{parente2024vlq}\nD. J. Parente (2024). \\textit{Generative Artificial Intelligence and Large Language Models in Primary Care Medical Education.}. Family Medicine.\n\n\\bibitem{zhao2024h5n}\nHaiyan Zhao, Fan Yang, Himabindu Lakkaraju, et al. (2024). \\textit{Opening the Black Box of Large Language Models: Two Views on Holistic Interpretability}. arXiv.org.\n\n\\bibitem{zhou2024b0u}\nYue Zhou, Henry Peng Zou, Barbara Di Eugenio, et al. (2024). \\textit{Large Language Models Are Involuntary Truth-Tellers: Exploiting Fallacy Failure for Jailbreak Attacks}. Conference on Empirical Methods in Natural Language Processing.\n\n\\bibitem{adewumi2024lv9}\nTosin P. Adewumi, Nudrat Habib, Lama Alkhaled, et al. (2024). \\textit{On the Limitations of Large Language Models (LLMs): False Attribution}. arXiv.org.\n\n\\bibitem{toroghi2024mxf}\nArmin Toroghi, Willis Guo, Mohammad Mahdi Torabi pour, et al. (2024). \\textit{Right for Right Reasons: Large Language Models for Verifiable Commonsense Knowledge Graph Question Answering}. Conference on Empirical Methods in Natural Language Processing.\n\n\\bibitem{taveekitworachai2024aql}\nPittawat Taveekitworachai, Febri Abdullah, and R. Thawonmas (2024). \\textit{Null-Shot Prompting: Rethinking Prompting Large Language Models With Hallucination}. Conference on Empirical Methods in Natural Language Processing.\n\n\\bibitem{wan2024mh1}\nFanqi Wan, Xinting Huang, Leyang Cui, et al. (2024). \\textit{Mitigating Hallucinations of Large Language Models via Knowledge Consistent Alignment}. arXiv.org.\n\n\\bibitem{alsadat2024i78}\nShayan Meshkat Alsadat, Jean-Raphael Gaglione, D. Neider, et al. (2024). \\textit{Using Large Language Models to Automate and Expedite Reinforcement Learning with Reward Machine}. American Control Conference.\n\n\\bibitem{cao2024o9a}\nQingxing Cao, Junhao Cheng, Xiaodan Liang, et al. (2024). \\textit{VisDiaHalBench: A Visual Dialogue Benchmark For Diagnosing Hallucination in Large Vision-Language Models}. Annual Meeting of the Association for Computational Linguistics.\n\n\\bibitem{benkirane202494i}\nKenza Benkirane, Laura Gongas, Shahar Pelles, et al. (2024). \\textit{Machine Translation Hallucination Detection for Low and High Resource Languages using Large Language Models}. Conference on Empirical Methods in Natural Language Processing.\n\n\\bibitem{jin2024jpw}\nLifeng Jin, Baolin Peng, Linfeng Song, et al. (2024). \\textit{Collaborative decoding of critical tokens for boosting factuality of large language models}. arXiv.org.\n\n\\bibitem{mu2024f3b}\nYida Mu, Peizhen Bai, Kalina Bontcheva, et al. (2024). \\textit{Addressing Topic Granularity and Hallucination in Large Language Models for Topic Modelling}. arXiv.org.\n\n\\bibitem{yu2024pp9}\nJun Yu, Yunxiang Zhang, Zerui Zhang, et al. (2024). \\textit{RAG-Guided Large Language Models for Visual Spatial Description with Adaptive Hallucination Corrector}. ACM Multimedia.\n\n\\bibitem{amirizaniani2024cad}\nMaryam Amirizaniani, Jihan Yao, Adrian Lavergne, et al. (2024). \\textit{LLMAuditor: A Framework for Auditing Large Language Models Using Human-in-the-Loop}. Unpublished manuscript.\n\n\\bibitem{ling2024qto}\nChen Ling, Xujiang Zhao, Wei Cheng, et al. (2024). \\textit{Uncertainty Decomposition and Quantification for In-Context Learning of Large Language Models}. arXiv.org.\n\n\\bibitem{sarmah2023cuq}\nBhaskarjit Sarmah, Dhagash Mehta, Stefano Pasquali, et al. (2023). \\textit{Towards reducing hallucination in extracting information from financial reports using Large Language Models}. International Conference on AI-ML-Systems.\n\n\\bibitem{nathani202338c}\nDeepak Nathani, David Wang, Liangming Pan, et al. (2023). \\textit{MAF: Multi-Aspect Feedback for Improving Reasoning in Large Language Models}. Conference on Empirical Methods in Natural Language Processing.\n\n\\bibitem{chataigner2024cr0}\nCl'ea Chataigner, Afaf TaÃ¯k, and G. Farnadi (2024). \\textit{Multilingual Hallucination Gaps in Large Language Models}. arXiv.org.\n\n\\bibitem{liu2025xwv}\nQ. Liu, Xinlong Chen, Yue Ding, et al. (2025). \\textit{Attention-guided Self-reflection for Zero-shot Hallucination Detection in Large Language Models}. arXiv.org.\n\n\\bibitem{song2024v5n}\nJongyoon Song, Sangwon Yu, and Sungroh Yoon (2024). \\textit{Large Language Models are Skeptics: False Negative Problem of Input-conflicting Hallucination}. arXiv.org.\n\n\\bibitem{barkley202472d}\nLiam Barkley, and Brink van der Merwe (2024). \\textit{Investigating the Role of Prompting and External Tools in Hallucination Rates of Large Language Models}. arXiv.org.\n\n\\bibitem{huang2024c9t}\nChao-Wei Huang, and Yun-Nung Chen (2024). \\textit{FactAlign: Long-form Factuality Alignment of Large Language Models}. Conference on Empirical Methods in Natural Language Processing.\n\n\\bibitem{malin2024fin}\nB. Malin, Tatiana Kalganova, and Nikoloas Boulgouris (2024). \\textit{A review of faithfulness metrics for hallucination assessment in Large Language Models}. IEEE Journal on Selected Topics in Signal Processing.\n\n\\bibitem{yuan2024o7d}\nHongbang Yuan, Pengfei Cao, Zhuoran Jin, et al. (2024). \\textit{Whispers that Shake Foundations: Analyzing and Mitigating False Premise Hallucinations in Large Language Models}. Conference on Empirical Methods in Natural Language Processing.\n\n\\bibitem{pandit20257jx}\nShrey Pandit, Jiawei Xu, Junyuan Hong, et al. (2025). \\textit{MedHallu: A Comprehensive Benchmark for Detecting Medical Hallucinations in Large Language Models}. arXiv.org.\n\n\\bibitem{bellinileite2023y38}\nSamuel C. Bellini-Leite (2023). \\textit{Dual Process Theory for Large Language Models: An overview of using Psychology to address hallucination and reliability issues}. Adaptive Behavior.\n\n\\bibitem{omar2025us3}\nM. Omar, V. Sorin, J. Collins, et al. (2025). \\textit{Large Language Models Are Highly Vulnerable to Adversarial Hallucination Attacks in Clinical Decision Support: A Multi-Model Assurance Analysis}. medRxiv.\n\n\\bibitem{lee2024i72}\nYi-Lun Lee, Yi-Hsuan Tsai, and Wei-Chen Chiu (2024). \\textit{Delve into Visual Contrastive Decoding for Hallucination Mitigation of Large Vision-Language Models}. arXiv.org.\n\n\\bibitem{li2023irg}\nJunyi Li, Xiaoxue Cheng, Wayne Xin Zhao, et al. (2023). \\textit{HELMA: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models}. Unpublished manuscript.\n\n\\bibitem{zhang2023pb6}\nChen Zhang (2023). \\textit{User-Controlled Knowledge Fusion in Large Language Models: Balancing Creativity and Hallucination}. arXiv.org.\n\n\\bibitem{irulandi2023xlg}\nMuneeswaran Irulandi, Shreya Saxena, Siva Prasad, et al. (2023). \\textit{Minimizing Factual Inconsistency and Hallucination in Large Language Models}. arXiv.org.\n\n\\bibitem{li2024ncc}\nDerong Xu Xinhang Li, Ziheng Zhang, Zhenxi Lin, et al. (2024). \\textit{Harnessing Large Language Models for Knowledge Graph Question Answering via Adaptive Multi-Aspect Retrieval-Augmentation}. arXiv.org.\n\n\\bibitem{guo2024tlu}\nHongyi Guo, Zhihan Liu, Yufeng Zhang, et al. (2024). \\textit{Can Large Language Models Play Games? A Case Study of A Self-Play Approach}. arXiv.org.\n\n\\bibitem{xie20247zk}\nZikai Xie (2024). \\textit{Order Matters in Hallucination: Reasoning Order as Benchmark and Reflexive Prompting for Large-Language-Models}. arXiv.org.\n\n\\bibitem{amirizaniani2024493}\nMaryam Amirizaniani, Jihan Yao, Adrian Lavergne, et al. (2024). \\textit{Developing a Framework for Auditing Large Language Models Using Human-in-the-Loop}. arXiv.org.\n\n\\bibitem{yang2025n54}\nTianyun Yang, Ziniu Li, Juan Cao, et al. (2025). \\textit{Understanding and Mitigating Hallucination in Large Vision-Language Models via Modular Attribution and Intervention}. International Conference on Learning Representations.\n\n\\bibitem{agarwal202418c}\nVibhor Agarwal, Yulong Pei, Salwa Alamir, et al. (2024). \\textit{CodeMirage: Hallucinations in Code Generated by Large Language Models}. arXiv.org.\n\n\\bibitem{rrv2024gw0}\nAswin Rrv, Nemika Tyagi, Md Nayem Uddin, et al. (2024). \\textit{Chaos with Keywords: Exposing Large Language Models Sycophantic Hallucination to Misleading Keywords and Evaluating Defense Strategies}. Unpublished manuscript.\n\n\\bibitem{li2024hl9}\nMingchen Li, Zaifu Zhan, Han Yang, et al. (2024). \\textit{Benchmarking Retrieval-Augmented Large Language Models in Biomedical NLP: Application, Robustness, and Self-Awareness}. arXiv.org.\n\n\\bibitem{wang2024t4o}\nShirui Wang, Bohan Xie, Ling Ding, et al. (2024). \\textit{SeCor: Aligning Semantic and Collaborative Representations by Large Language Models for Next-Point-of-Interest Recommendations}. ACM Conference on Recommender Systems.\n\n\\bibitem{hegselmann20249q4}\nS. Hegselmann, Zejiang Shen, Florian Gierse, et al. (2024). \\textit{A Data-Centric Approach To Generate Faithful and High Quality Patient Summaries with Large Language Models}. ACM Conference on Health, Inference, and Learning.\n\n\\bibitem{gao2024ncr}\nJun Gao, Huan Zhao, Wei Wang, et al. (2024). \\textit{EventRL: Enhancing Event Extraction with Outcome Supervision for Large Language Models}. arXiv.org.\n\n\\bibitem{tsai2024klg}\nYao-Hung Tsai, Walter Talbott, and Jian Zhang (2024). \\textit{Efficient Non-Parametric Uncertainty Quantification for Black-Box Large Language Models and Decision Planning}. arXiv.org.\n\n\\bibitem{chen2024qs5}\nXinxi Chen, Li Wang, Wei Wu, et al. (2024). \\textit{Honest AI: Fine-Tuning \"Small\" Language Models to Say \"I Don't Know\", and Reducing Hallucination in RAG}. arXiv.org.\n\n\\bibitem{liu2025juo}\nMingShan Liu, Shi Bo, and Jialing Fang (2025). \\textit{Enhancing Mathematical Reasoning in Large Language Models with Self-Consistency-Based Hallucination Detection}. arXiv.org.\n\n\\bibitem{zhang2024htn}\nTaolin Zhang, Qizhou Chen, Dongyang Li, et al. (2024). \\textit{DAFNet: Dynamic Auxiliary Fusion for Sequential Model Editing in Large Language Models}. Annual Meeting of the Association for Computational Linguistics.\n\n\\bibitem{guo2024hgn}\nYuhang Guo, and Zhiyu Wan (2024). \\textit{Performance Evaluation of Multimodal Large Language Models (LLaVA and GPT-4-based ChatGPT) in Medical Image Classification Tasks}. IEEE International Conference on Healthcare Informatics.\n\n\\bibitem{luo2024uh8}\nWeiqing Luo, Chonggang Song, Lingling Yi, et al. (2024). \\textit{KELLMRec: Knowledge-Enhanced Large Language Models for Recommendation}. arXiv.org.\n\n\\bibitem{hamid2024pwn}\nOussama H. Hamid (2024). \\textit{Beyond Probabilities: Unveiling the Delicate Dance of Large Language Models (LLMs) and AI-Hallucination}. Conference on Cognitive and Computational Aspects of Situation Management.\n\n\\bibitem{zheng20240qd}\nXinxin Zheng, Feihu Che, Jinyang Wu, et al. (2024). \\textit{KS-LLM: Knowledge Selection of Large Language Models with Evidence Document for Question Answering}. arXiv.org.\n\n\\bibitem{rawte2024bu6}\nVipula Rawte, Aman Chadha, Amit P. Sheth, et al. (2024). \\textit{Tutorial Proposal: Hallucination in Large Language Models}. International Conference on Language Resources and Evaluation.\n\n\\bibitem{yin2024iau}\nZhibo Yin (2024). \\textit{A review of methods for alleviating hallucination issues in large language models}. Applied and Computational Engineering.\n\n\\bibitem{tang2025mfi}\nZilu Tang, Rajen Chatterjee, and Sarthak Garg (2025). \\textit{Mitigating Hallucinated Translations in Large Language Models with Hallucination-focused Preference Optimization}. North American Chapter of the Association for Computational Linguistics.\n\n\\bibitem{das2024jdt}\nSouvik Das, Lifeng Jin, Linfeng Song, et al. (2024). \\textit{Entropy Guided Extrapolative Decoding to Improve Factuality in Large Language Models}. International Conference on Computational Linguistics.\n\n\\bibitem{zhang2024h4a}\nYuxiang Zhang, Jing Chen, Junjie Wang, et al. (2024). \\textit{ToolBeHonest: A Multi-level Hallucination Diagnostic Benchmark for Tool-Augmented Large Language Models}. Conference on Empirical Methods in Natural Language Processing.\n\n\\bibitem{xu2024t34}\nDerong Xu, Ziheng Zhang, Zhihong Zhu, et al. (2024). \\textit{Mitigating Hallucinations of Large Language Models in Medical Information Extraction via Contrastive Decoding}. Conference on Empirical Methods in Natural Language Processing.\n\n\\bibitem{zhang2025p1z}\nHongjie Zhang, Hourui Deng, Jie Ou, et al. (2025). \\textit{Mitigating spatial hallucination in large language models for path planning via prompt engineering}. Scientific Reports.\n\n\\bibitem{ahmadi2024j88}\nAli Ahmadi (2024). \\textit{Unravelling the Mysteries of Hallucination in Large Language Models: Strategies for Precision in Artificial Intelligence Language Generation}. Asian Journal of Computer Science and Technology.\n\n\\bibitem{abdelghafour2024efh}\nM. Abdelghafour, Mohammed Mabrouk, and Zaki Taha (2024). \\textit{Hallucination Mitigation Techniques in Large Language Models}. International Journal of Intelligent Computing and Information Sciences.\n\n\\bibitem{zhou20253zv}\nXiaoling Zhou, Mingjie Zhang, Zhemg Lee, et al. (2025). \\textit{HaDeMiF: Hallucination Detection and Mitigation in Large Language Models}. International Conference on Learning Representations.\n\n\\bibitem{karbasi2025j7n}\nAmin Karbasi, Omar Montasser, John Sous, et al. (2025). \\textit{(Im)possibility of Automated Hallucination Detection in Large Language Models}. arXiv.org.\n\n\\bibitem{mubarak2024lx6}\nHamdy Mubarak, Hend Suliman Al-Khalifa, and Khaloud Suliman Alkhalefah (2024). \\textit{Halwasa: Quantify and Analyze Hallucinations in Large Language Models: Arabic as a Case Study}. International Conference on Language Resources and Evaluation.\n\n\\bibitem{zhang2024sbu}\nWenbo Zhang, Zihang Xu, and Hengrui Cai (2024). \\textit{Recognizing Limits: Investigating Infeasibility in Large Language Models}. Unpublished manuscript.\n\n\\bibitem{omar2025cc3}\nMahmud Omar, Vera Sorin, Jeremy D. Collins, et al. (2025). \\textit{Multi-model assurance analysis showing large language models are highly vulnerable to adversarial hallucination attacks during clinical decision support}. Communications Medicine.\n\n\\bibitem{gao20242nu}\nZhengjie Gao, Xuanzi Liu, Yuanshuai Lan, et al. (2024). \\textit{A Brief Survey on Safety of Large Language Models}. Journal of computer & information technology.\n\n\\bibitem{pester20242zt}\nAndreas Pester, Ahmed Tammaa, Christian GÃ¼tl, et al. (2024). \\textit{Conversational Agents, Virtual Worlds, and Beyond: A Review of Large Language Models Enabling Immersive Learning}. IEEE Global Engineering Education Conference.\n\n\\bibitem{wu202415r}\nKangxi Wu, Liang Pang, Huawei Shen, et al. (2024). \\textit{Enhancing Training Data Attribution for Large Language Models with Fitting Error Consideration}. Conference on Empirical Methods in Natural Language Processing.\n\n\\bibitem{tu2024v40}\nYahan Tu, Rui Hu, and Jitao Sang (2024). \\textit{ODE: Open-Set Evaluation of Hallucinations in Multimodal Large Language Models}. Computer Vision and Pattern Recognition.\n\n\\end{thebibliography}\n\n\\end{document}",
  "generation_date": "2025-10-07T23:48:40.466288",
  "processed_papers_data": [
    {
      "success": true,
      "doc_id": "f44efddab0938b2760444f9113234c49",
      "summary": "Here is a focused summary of the technical paper for a literature review:\n\n### Technical Paper Analysis: \"FRESH LLMS: REFRESHING LARGE LANGUAGE MODELS WITH SEARCH ENGINE AUGMENTATION\" \\cite{vu202337s}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Large Language Models (LLMs) are typically trained once and lack the ability to dynamically adapt to rapidly changing world knowledge, leading to factual inaccuracies (hallucinations) and outdated information in their responses \\cite{vu202337s}.\n    *   **Importance and Challenge**: This problem significantly reduces the trustworthiness of LLM outputs, especially in applications requiring accurate and up-to-date information. Traditional methods like additional training are not scalable for real-time knowledge updates (e.g., stock prices), and while in-context learning with web search is explored, its optimal utilization for enhancing LLM factuality remains unclear \\cite{vu202337s}.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: The paper builds upon prior work exploring LLM hallucination (e.g., Maynez et al., 2020; Liu et al., 2023b) and methods to mitigate it, such as additional training with human feedback (Ouyang et al., 2022) or knowledge-enhanced tasks \\cite{vu202337s}. It also relates to emerging approaches that augment LLMs with web search results (e.g., Lazaridou et al., 2022; Press et al., 2022) \\cite{vu202337s}.\n    *   **Limitations of Previous Solutions**: Existing training-based solutions are not easily scalable for real-time knowledge updates \\cite{vu202337s}. Furthermore, it has been unclear how to fully leverage search engine outputs to maximize LLM factuality, a gap this work aims to address by outperforming competing search-augmented prompting methods like SELF-ASK \\cite{vu202337s}.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**:\n        *   **FRESH QA Benchmark**: A novel, dynamic QA benchmark comprising 600 questions designed to test current world knowledge. Questions are categorized into never-changing, slow-changing, fast-changing, and false-premise types, with a commitment to regular updates \\cite{vu202337s}.\n        *   **Two-Mode Evaluation**: A rigorous human evaluation procedure (50K+ judgments) with two modes: RELAXED (measures primary answer correctness) and STRICT (measures if *all* claims are factual and up-to-date, capturing hallucination) \\cite{vu202337s}.\n        *   **FRESH PROMPT**: A simple, few-shot in-context learning method that significantly boosts LLM factuality. It incorporates relevant and up-to-date information retrieved from a search engine directly into the LLM's prompt. This includes extracting diverse information, such as knowledge from \"relevant questions that search users also ask,\" and presenting retrieved evidences in chronological order \\cite{vu202337s}.\n    *   **Novelty/Differentiation**:\n        *   The creation of FRESH QA as a dynamic, regularly updated benchmark specifically targeting fast-changing and false-premise knowledge, which are known LLM weaknesses \\cite{vu202337s}.\n        *   The comprehensive two-mode evaluation (RELAXED vs. STRICT) provides a nuanced understanding of LLM factuality and hallucination \\cite{vu202337s}.\n        *   FRESH PROMPT's innovative approach to effectively integrate a broad range of search engine outputs and leverage few-shot in-context learning to teach models to reason over retrieved evidence, without requiring additional training \\cite{vu202337s}.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms, Methods, or Techniques**:\n        *   **FRESH QA**: A dynamic, diverse, and regularly updated QA benchmark for evaluating LLM factuality on current and evolving knowledge, including challenging false-premise questions \\cite{vu202337s}.\n        *   **FRESH PROMPT**: An effective, training-free few-shot prompting method that significantly enhances LLM factuality by intelligently integrating search engine results into the prompt \\cite{vu202337s}.\n        *   **Two-mode Evaluation (RELAXED/STRICT)**: A robust protocol for comprehensive assessment of LLM correctness and hallucination \\cite{vu202337s}.\n        *   **FRESH EVAL**: A simple automatic metric for evaluating model responses, achieving high agreement with human judgments \\cite{vu202337s}.\n    *   **Theoretical Insights or Analysis**: Analysis of FRESH PROMPT demonstrates that both the number and chronological order of retrieved evidences are crucial for performance, and that encouraging concise answers helps reduce hallucination \\cite{vu202337s}.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**:\n        *   Benchmarking of a diverse array of closed and open-source LLMs (e.g., T5, PaLM, FLAN-PaLM, GPT-3.5, ChatGPT, GPT-4) on FRESH QA using zero-shot, few-shot, and Chain-of-Thought (COT) prompting \\cite{vu202337s}.\n        *   Extensive human evaluations (over 50K judgments) using the RELAXED and STRICT modes \\cite{vu202337s}.\n        *   Comparison of FRESH PROMPT against competing search-augmented prompting methods (e.g., SELF-ASK) and commercial systems (e.g., PERPLEXITY.AI) \\cite{vu202337s}.\n        *   Sensitivity and ablation analyses to understand the impact of various facets of FRESH PROMPT (e.g., number and order of evidences, answer verbosity) \\cite{vu202337s}.\n    *   **Key Performance Metrics and Comparison Results**:\n        *   Baseline LLMs showed low overall accuracy on FRESH QA (0.8% to 32.0% under STRICT, 0.8% to 46.4% under RELAXED), with \"flat scaling curves\" on fast-changing and false-premise questions, indicating that increasing model size alone does not reliably improve performance in these areas \\cite{vu202337s}.\n        *   COT prompting was found to increase hallucination compared to few-shot prompting under STRICT evaluation \\cite{vu202337s}.\n        *   FRESH PROMPT significantly boosted LLM factuality: for instance, the best GPT-4 + FRESH PROMPT variant achieved a 32.6% improvement in RELAXED accuracy and a 49.0% improvement in STRICT accuracy over vanilla GPT-4 on FRESH QA \\cite{vu202337s}.\n        *   FRESH PROMPT consistently outperformed competing search-augmented methods like SELF-ASK and commercial systems such as PERPLEXITY.AI \\cite{vu202337s}.\n        *   Human evaluation inter-rater agreement was high (99% for RELAXED, 96% for STRICT), and the automatic metric FRESH EVAL achieved 96.5% and 96% agreement with human evaluations for RELAXED and STRICT, respectively \\cite{vu202337s}.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The study confirms that LLMs without real-time data access inherently struggle with current information and false premises \\cite{vu202337s}. Chain-of-Thought (COT) prompting, while sometimes beneficial, can introduce more hallucination under strict factuality evaluations \\cite{vu202337s}. Multi-hop reasoning remains challenging for several models \\cite{vu202337s}. FRESH PROMPT's effectiveness relies on the quality and relevance of the retrieved search engine results \\cite{vu202337s}.\n    *   **Scope of Applicability**: The work focuses on question-answering tasks requiring up-to-date world knowledge. FRESH QA questions are designed to be natural and plausible for real-world search queries. The dataset excludes questions whose answers change more frequently than once per week \\cite{vu202337s}.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: This paper significantly advances the technical state-of-the-art by providing a robust, dynamic benchmark (FRESH QA) and a comprehensive evaluation methodology for LLM factuality on evolving knowledge \\cite{vu202337s}. FRESH PROMPT offers a highly effective, training-free, and flexible method to dramatically improve LLM factuality by intelligently integrating search engine augmentation, outperforming existing methods \\cite{vu202337s}.\n    *   **Potential Impact on Future Research**: The release of FRESH QA and the commitment to its regular updates will serve as a crucial resource, driving future research into dynamic knowledge integration, factuality, and hallucination mitigation for LLMs \\cite{vu202337s}. The findings regarding the limitations of scaling laws and the increased hallucination with COT prompting provide critical insights for the design and development of more reliable LLMs \\cite{vu202337s}.",
      "intriguing_abstract": "Large Language Models (LLMs) are notoriously prone to factual inaccuracies and outdated information, an Achilles' heel that undermines their trustworthiness in real-world applications. Addressing this critical challenge, we introduce **FRESH LLMS**, a comprehensive framework for refreshing LLMs with real-time knowledge. Our contributions include **FRESH QA**, a novel, dynamic question-answering benchmark specifically designed to evaluate LLMs on rapidly changing and false-premise world knowledge, supported by a rigorous two-mode human evaluation protocol.\n\nCentral to our approach is **FRESH PROMPT**, an innovative few-shot in-context learning method that significantly boosts LLM factuality. By intelligently integrating diverse, chronologically ordered information retrieved from search engines directly into the prompt, FRESH PROMPT dramatically improves performance without requiring additional model training. Our extensive experiments demonstrate that vanilla LLMs struggle profoundly with current knowledge, and even Chain-of-Thought prompting can increase hallucination. In stark contrast, FRESH PROMPT achieves substantial gains, improving GPT-4's strict factuality by an impressive 49% and consistently outperforming existing search-augmented methods. This work provides crucial tools and insights for building more reliable and up-to-date LLMs, fostering advancements in dynamic knowledge integration and hallucination mitigation.",
      "keywords": [
        "Large Language Models (LLMs)",
        "LLM hallucination",
        "dynamic knowledge adaptation",
        "search engine augmentation",
        "FRESH QA benchmark",
        "FRESH PROMPT",
        "few-shot in-context learning",
        "factuality evaluation",
        "two-mode evaluation",
        "real-time knowledge updates",
        "training-free methods",
        "Chain-of-Thought prompting limitations",
        "false-premise questions"
      ],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/be177300487b6d0f25e6cade9a31900454b13281.pdf",
      "citation_key": "vu202337s",
      "metadata": {
        "title": "FreshLLMs: Refreshing Large Language Models with Search Engine Augmentation",
        "authors": [
          "Tu Vu",
          "Mohit Iyyer",
          "Xuezhi Wang",
          "Noah Constant",
          "Jerry Wei",
          "Jason Wei",
          "C. Tar",
          "Yun-Hsuan Sung",
          "Denny Zhou",
          "Quoc Le",
          "Thang Luong"
        ],
        "published_date": "2023",
        "abstract": "Most large language models (LLMs) are trained once and never updated; thus, they lack the ability to dynamically adapt to our ever-changing world. In this work, we perform a detailed study of the factuality of LLM-generated text in the context of answering questions that test current world knowledge. Specifically, we introduce FreshQA, a novel dynamic QA benchmark encompassing a diverse range of question and answer types, including questions that require fast-changing world knowledge as well as questions with false premises that need to be debunked. We benchmark a diverse array of both closed and open-source LLMs under a two-mode evaluation procedure that allows us to measure both correctness and hallucination. Through human evaluations involving more than 50K judgments, we shed light on limitations of these models and demonstrate significant room for improvement: for instance, all models (regardless of model size) struggle on questions that involve fast-changing knowledge and false premises. Motivated by these results, we present FreshPrompt, a simple few-shot prompting method that substantially boosts the performance of an LLM on FreshQA by incorporating relevant and up-to-date information retrieved from a search engine into the prompt. Our experiments show that FreshPrompt outperforms both competing search engine-augmented prompting methods such as Self-Ask (Press et al., 2022) as well as commercial systems such as Perplexity.AI. Further analysis of FreshPrompt reveals that both the number of retrieved evidences and their order play a key role in influencing the correctness of LLM-generated answers. Additionally, instructing the LLM to generate concise and direct answers helps reduce hallucination compared to encouraging more verbose answers. To facilitate future work, we release FreshQA at github.com/freshllms/freshqa and commit to updating it at regular intervals.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/be177300487b6d0f25e6cade9a31900454b13281.pdf",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "citationCount": 0,
        "score": 0,
        "summary": "Here is a focused summary of the technical paper for a literature review:\n\n### Technical Paper Analysis: \"FRESH LLMS: REFRESHING LARGE LANGUAGE MODELS WITH SEARCH ENGINE AUGMENTATION\" \\cite{vu202337s}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Large Language Models (LLMs) are typically trained once and lack the ability to dynamically adapt to rapidly changing world knowledge, leading to factual inaccuracies (hallucinations) and outdated information in their responses \\cite{vu202337s}.\n    *   **Importance and Challenge**: This problem significantly reduces the trustworthiness of LLM outputs, especially in applications requiring accurate and up-to-date information. Traditional methods like additional training are not scalable for real-time knowledge updates (e.g., stock prices), and while in-context learning with web search is explored, its optimal utilization for enhancing LLM factuality remains unclear \\cite{vu202337s}.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: The paper builds upon prior work exploring LLM hallucination (e.g., Maynez et al., 2020; Liu et al., 2023b) and methods to mitigate it, such as additional training with human feedback (Ouyang et al., 2022) or knowledge-enhanced tasks \\cite{vu202337s}. It also relates to emerging approaches that augment LLMs with web search results (e.g., Lazaridou et al., 2022; Press et al., 2022) \\cite{vu202337s}.\n    *   **Limitations of Previous Solutions**: Existing training-based solutions are not easily scalable for real-time knowledge updates \\cite{vu202337s}. Furthermore, it has been unclear how to fully leverage search engine outputs to maximize LLM factuality, a gap this work aims to address by outperforming competing search-augmented prompting methods like SELF-ASK \\cite{vu202337s}.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**:\n        *   **FRESH QA Benchmark**: A novel, dynamic QA benchmark comprising 600 questions designed to test current world knowledge. Questions are categorized into never-changing, slow-changing, fast-changing, and false-premise types, with a commitment to regular updates \\cite{vu202337s}.\n        *   **Two-Mode Evaluation**: A rigorous human evaluation procedure (50K+ judgments) with two modes: RELAXED (measures primary answer correctness) and STRICT (measures if *all* claims are factual and up-to-date, capturing hallucination) \\cite{vu202337s}.\n        *   **FRESH PROMPT**: A simple, few-shot in-context learning method that significantly boosts LLM factuality. It incorporates relevant and up-to-date information retrieved from a search engine directly into the LLM's prompt. This includes extracting diverse information, such as knowledge from \"relevant questions that search users also ask,\" and presenting retrieved evidences in chronological order \\cite{vu202337s}.\n    *   **Novelty/Differentiation**:\n        *   The creation of FRESH QA as a dynamic, regularly updated benchmark specifically targeting fast-changing and false-premise knowledge, which are known LLM weaknesses \\cite{vu202337s}.\n        *   The comprehensive two-mode evaluation (RELAXED vs. STRICT) provides a nuanced understanding of LLM factuality and hallucination \\cite{vu202337s}.\n        *   FRESH PROMPT's innovative approach to effectively integrate a broad range of search engine outputs and leverage few-shot in-context learning to teach models to reason over retrieved evidence, without requiring additional training \\cite{vu202337s}.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms, Methods, or Techniques**:\n        *   **FRESH QA**: A dynamic, diverse, and regularly updated QA benchmark for evaluating LLM factuality on current and evolving knowledge, including challenging false-premise questions \\cite{vu202337s}.\n        *   **FRESH PROMPT**: An effective, training-free few-shot prompting method that significantly enhances LLM factuality by intelligently integrating search engine results into the prompt \\cite{vu202337s}.\n        *   **Two-mode Evaluation (RELAXED/STRICT)**: A robust protocol for comprehensive assessment of LLM correctness and hallucination \\cite{vu202337s}.\n        *   **FRESH EVAL**: A simple automatic metric for evaluating model responses, achieving high agreement with human judgments \\cite{vu202337s}.\n    *   **Theoretical Insights or Analysis**: Analysis of FRESH PROMPT demonstrates that both the number and chronological order of retrieved evidences are crucial for performance, and that encouraging concise answers helps reduce hallucination \\cite{vu202337s}.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**:\n        *   Benchmarking of a diverse array of closed and open-source LLMs (e.g., T5, PaLM, FLAN-PaLM, GPT-3.5, ChatGPT, GPT-4) on FRESH QA using zero-shot, few-shot, and Chain-of-Thought (COT) prompting \\cite{vu202337s}.\n        *   Extensive human evaluations (over 50K judgments) using the RELAXED and STRICT modes \\cite{vu202337s}.\n        *   Comparison of FRESH PROMPT against competing search-augmented prompting methods (e.g., SELF-ASK) and commercial systems (e.g., PERPLEXITY.AI) \\cite{vu202337s}.\n        *   Sensitivity and ablation analyses to understand the impact of various facets of FRESH PROMPT (e.g., number and order of evidences, answer verbosity) \\cite{vu202337s}.\n    *   **Key Performance Metrics and Comparison Results**:\n        *   Baseline LLMs showed low overall accuracy on FRESH QA (0.8% to 32.0% under STRICT, 0.8% to 46.4% under RELAXED), with \"flat scaling curves\" on fast-changing and false-premise questions, indicating that increasing model size alone does not reliably improve performance in these areas \\cite{vu202337s}.\n        *   COT prompting was found to increase hallucination compared to few-shot prompting under STRICT evaluation \\cite{vu202337s}.\n        *   FRESH PROMPT significantly boosted LLM factuality: for instance, the best GPT-4 + FRESH PROMPT variant achieved a 32.6% improvement in RELAXED accuracy and a 49.0% improvement in STRICT accuracy over vanilla GPT-4 on FRESH QA \\cite{vu202337s}.\n        *   FRESH PROMPT consistently outperformed competing search-augmented methods like SELF-ASK and commercial systems such as PERPLEXITY.AI \\cite{vu202337s}.\n        *   Human evaluation inter-rater agreement was high (99% for RELAXED, 96% for STRICT), and the automatic metric FRESH EVAL achieved 96.5% and 96% agreement with human evaluations for RELAXED and STRICT, respectively \\cite{vu202337s}.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The study confirms that LLMs without real-time data access inherently struggle with current information and false premises \\cite{vu202337s}. Chain-of-Thought (COT) prompting, while sometimes beneficial, can introduce more hallucination under strict factuality evaluations \\cite{vu202337s}. Multi-hop reasoning remains challenging for several models \\cite{vu202337s}. FRESH PROMPT's effectiveness relies on the quality and relevance of the retrieved search engine results \\cite{vu202337s}.\n    *   **Scope of Applicability**: The work focuses on question-answering tasks requiring up-to-date world knowledge. FRESH QA questions are designed to be natural and plausible for real-world search queries. The dataset excludes questions whose answers change more frequently than once per week \\cite{vu202337s}.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: This paper significantly advances the technical state-of-the-art by providing a robust, dynamic benchmark (FRESH QA) and a comprehensive evaluation methodology for LLM factuality on evolving knowledge \\cite{vu202337s}. FRESH PROMPT offers a highly effective, training-free, and flexible method to dramatically improve LLM factuality by intelligently integrating search engine augmentation, outperforming existing methods \\cite{vu202337s}.\n    *   **Potential Impact on Future Research**: The release of FRESH QA and the commitment to its regular updates will serve as a crucial resource, driving future research into dynamic knowledge integration, factuality, and hallucination mitigation for LLMs \\cite{vu202337s}. The findings regarding the limitations of scaling laws and the increased hallucination with COT prompting provide critical insights for the design and development of more reliable LLMs \\cite{vu202337s}.",
        "keywords": [
          "Large Language Models (LLMs)",
          "LLM hallucination",
          "dynamic knowledge adaptation",
          "search engine augmentation",
          "FRESH QA benchmark",
          "FRESH PROMPT",
          "few-shot in-context learning",
          "factuality evaluation",
          "two-mode evaluation",
          "real-time knowledge updates",
          "training-free methods",
          "Chain-of-Thought prompting limitations",
          "false-premise questions"
        ],
        "paper_type": "based on the abstract and introduction, this paper is best classified as **empirical**.\n\nhere's why:\n\n*   **abstract keywords:** \"detailed study,\" \"introduce fresh qa, a novel dynamic qa benchmark,\" \"benchmark a diverse array of both closed and open-source llms,\" \"two-mode evaluation procedure,\" \"human evaluations involving more than 50k judgments,\" \"shed light on limitations,\" \"demonstrate significant room for improvement,\" \"all models... struggle on questions.\" these all point to data collection, experimentation, and analysis of findings.\n*   **introduction keywords:** \"human evaluations involving more than 50k judgments, we shed light on limitations... and demonstrate significant room for improvement,\" \"motivated by these results, we present fresh prompt,\" \"our experiments show that fresh prompt outperforms,\" \"further analysis of fresh prompt reveals.\" the emphasis is on the results of studies and experiments.\n*   **core focus:** the paper performs a \"detailed study\" of llm factuality, introduces a benchmark (fresh qa) for this study, and then *empirically* evaluates existing llms and a new method (fresh prompt) using extensive data (50k human judgments, experiments). while it *presents* a new method (fresh prompt), the primary thrust is the data-driven investigation and evaluation of llm performance and the proposed solution."
      },
      "file_name": "be177300487b6d0f25e6cade9a31900454b13281.pdf"
    },
    {
      "success": true,
      "doc_id": "3421cf1b8a56b29a3b6d42e1e521cd3c",
      "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **1. Research Problem & Motivation**\n    *   **Specific Technical Problem**: Large Vision-Language Models (LVLMs) frequently suffer from \"hallucination,\" where generated content deviates from the actual image content \\cite{chang2024u3t}. This includes both \"perception hallucination\" (incorrectly describing visual attributes) and \"reasoning hallucination\" (producing fallacies in logical reasoning based on the image).\n    *   **Importance & Challenge**: Hallucinations lead to misinformation, degrade user experience, and undermine the reliability of LVLMs. Existing mitigation methods often employ a fixed verification approach, which is ineffective or inappropriate for the diverse types of hallucinations arising from different query types (e.g., object detection is not suitable for reasoning queries). Furthermore, current methods may not fully eradicate hallucinations in a single pass, sometimes leading to inconsistent corrections \\cite{chang2024u3t}.\n\n*   **2. Related Work & Positioning**\n    *   **Relation to Existing Approaches**: Previous work on hallucination mitigation generally falls into two categories: optimizing model training/inference (e.g., RLHF-V, VIGC) or correcting hallucinations during the generation stage without model updates (e.g., Woodpecker, HalluciDoctor) \\cite{chang2024u3t}.\n    *   **Limitations of Previous Solutions**: Existing methods overlook the diversity of hallucinations. They apply a \"one-size-fits-all\" verification strategy, which is suboptimal for different query types (perception vs. reasoning). For instance, methods relying on object detection are effective for perceptual errors but fail for complex reasoning errors. This fixed approach can lead to partial or inconsistent corrections \\cite{chang2024u3t}.\n\n*   **3. Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes \"Dentist,\" a unified hallucination mitigation framework for LVLMs. Its core idea is to first classify the query type and then apply a tailored mitigation strategy within a validation loop \\cite{chang2024u3t}.\n        *   **Potential Hallucination Classification**: Queries are classified into \"perception\" or \"reasoning\" using ChatGPT with a specific prompt. This classification determines the type of potential hallucination in the LVLM's answer.\n        *   **Divide-and-Conquer Treatment**:\n            *   **Visual Verification for Perception Queries**: For perception-based queries, the original LVLM's long descriptive answer is broken down into sub-questions by ChatGPT. These sub-questions are then answered by the *original* LVLM (to demonstrate mitigation ability, not just a better VQA model), and the sub-answers are aggregated by ChatGPT to refine the original hallucinated response.\n            *   **Chain-of-Thought (CoT) for Reasoning Queries**: For reasoning-based queries, a \"Letâ€™s think step by step\" CoT prompt is added to the original query for the LVLM. ChatGPT then uses the LVLM's CoT-enhanced generation to correct the original answer, providing more detailed logical reasoning.\n        *   **Validation Loop**: The entire verification process is embedded in a loop. The revised answer from one iteration becomes the input for the next. The loop continues until the answer no longer changes significantly semantically (determined by ChatGPT) or a maximum iteration limit is reached, preventing \"snowball errors\" \\cite{chang2024u3t}.\n    *   **Novelty/Difference**: The novelty lies in being the first to:\n        *   Distinguish and apply different hallucination mitigation treatments based on the classification of potential hallucinations (perception vs. reasoning).\n        *   Employ a validation loop that iteratively refines answers until semantic convergence, ensuring more complete hallucination removal \\cite{chang2024u3t}.\n        *   The framework is designed for easy integration into various LVLMs and allows for future extensions with new classifications and treatments.\n\n*   **4. Key Technical Contributions**\n    *   **Novel Framework**: Introduction of \"Dentist,\" a unified framework for hallucination classification and mitigation in LVLMs \\cite{chang2024u3t}.\n    *   **Adaptive Mitigation Strategy**: A novel approach that classifies queries (and thus potential hallucinations) into perception or reasoning types and applies tailored mitigation techniques (visual verification with sub-questions for perception, Chain-of-Thought for reasoning).\n    *   **Iterative Refinement**: Implementation of a validation loop that continuously verifies and corrects answers until semantic stability, addressing the issue of incomplete hallucination removal \\cite{chang2024u3t}.\n    *   **System Design**: A modular and easily integrable framework design that can be deployed with various LVLMs and extended with new components.\n\n*   **5. Experimental Validation**\n    *   **Experiments Conducted**: Comprehensive quantitative experiments were performed on several hallucination mitigation benchmarks \\cite{chang2024u3t}.\n    *   **Benchmarks**: MMBench (covering Perception and Reasoning abilities), LLaVA-QA90, CHAIR, and POPE.\n    *   **Models Evaluated**: InstructBLIP, LLaVA, and VisualGLM were used as baseline LVLMs.\n    *   **Comparison**: The method was compared against a current effective LVLM hallucination mitigation method, Woodpecker \\cite{chang2024u3t}.\n    *   **Key Performance Metrics & Results**:\n        *   Significant improvements were observed across various visual language tasks.\n        *   On the Image Quality task (a Coarse Perception VQA task) within MMBench, \"Dentist\" achieved a 13.44% improvement over InstructBLIP, 10.2% over LLaVA, and 15.8% over VisualGLM in accuracy \\cite{chang2024u3t}.\n        *   The experiments demonstrated the effectiveness and superiority of the proposed method in mitigating hallucinations and boosting the performance of baseline LVLMs.\n\n*   **6. Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The framework relies on an external LLM (ChatGPT) for query classification, sub-question generation, answer aggregation, CoT-based correction, and semantic convergence detection. This introduces dependency on the performance and availability of such models. The paper also notes that if the validation loop reaches its maximum iteration limit without semantic convergence, it might indicate a \"snowball error\" phenomenon, in which case only the first verification's answer is returned, implying that not all hallucinations might be fully resolved in such edge cases \\cite{chang2024u3t}.\n    *   **Scope of Applicability**: The framework is designed for mitigating perception and reasoning hallucinations in LVLMs. While it is unified for these two types, its direct applicability to other potential hallucination categories (if any exist beyond these two broad types) is not explicitly discussed.\n\n*   **7. Technical Significance**\n    *   **Advancement of State-of-the-Art**: \"Dentist\" advances the technical state-of-the-art by introducing a novel, adaptive, and iterative approach to hallucination mitigation, moving beyond fixed verification methods. By classifying query types and tailoring mitigation strategies, it addresses the inherent diversity of hallucinations more effectively than prior work \\cite{chang2024u3t}. The validation loop ensures a more thorough and robust correction process.\n    *   **Potential Impact on Future Research**: This work provides a strong foundation for future research in several areas:\n        *   Developing more sophisticated query classification mechanisms.\n        *   Exploring additional specialized mitigation strategies for finer-grained hallucination types.\n        *   Investigating alternative methods for semantic convergence detection in iterative correction loops.\n        *   Improving the robustness of LVLMs by integrating such adaptive post-hoc correction frameworks, leading to more reliable and trustworthy multimodal AI systems \\cite{chang2024u3t}.",
      "intriguing_abstract": "Large Vision-Language Models (LVLMs) are plagued by pervasive \"hallucinations,\" generating content that deviates from visual reality or logical reasoning. This critical issue, encompassing both perception and reasoning hallucinations, undermines reliability and user trust. Existing mitigation strategies often apply a fixed, ineffective approach, failing to address the diverse nature of these errors. We introduce **Dentist**, a novel, unified framework that revolutionizes hallucination mitigation. Dentist first intelligently classifies potential hallucinations into perception or reasoning types. It then deploys a tailored, \"divide-and-conquer\" treatment: visual verification with sub-questions for perceptual errors, and Chain-of-Thought (CoT) prompting for reasoning fallacies. Crucially, Dentist incorporates an iterative validation loop, refining answers until semantic convergence, ensuring comprehensive and consistent correction. Extensive experiments on benchmarks like MMBench and LLaVA-QA90 demonstrate Dentist's superior effectiveness, significantly boosting LVLM performance and advancing the state-of-the-art towards more trustworthy and accurate multimodal AI systems.",
      "keywords": [
        "Large Vision-Language Models (LVLMs)",
        "Hallucination mitigation",
        "Perception and reasoning hallucinations",
        "Dentist framework",
        "Unified hallucination mitigation",
        "Adaptive mitigation strategy",
        "Query type classification",
        "Iterative refinement",
        "Validation loop",
        "Visual verification",
        "Chain-of-Thought (CoT)",
        "Semantic convergence",
        "Hallucination mitigation benchmarks",
        "State-of-the-art advancement",
        "External LLM dependency"
      ],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/088a42203bc9a67e14b1bfd5c1fd25a03c126c08.pdf",
      "citation_key": "chang2024u3t",
      "metadata": {
        "title": "A Unified Hallucination Mitigation Framework for Large Vision-Language Models",
        "authors": [
          "Yue Chang",
          "Liqiang Jing",
          "Xiaopeng Zhang",
          "Yue Zhang"
        ],
        "published_date": "2024",
        "abstract": "Hallucination is a common problem for Large Vision-Language Models (LVLMs) with long generations which is difficult to eradicate. The generation with hallucinations is partially inconsistent with the image content. To mitigate hallucination, current studies either focus on the process of model inference or the results of model generation, but the solutions they design sometimes do not deal appropriately with various types of queries and the hallucinations of the generations about these queries. To accurately deal with various hallucinations, we present a unified framework, Dentist, for hallucination mitigation. The core step is to first classify the queries, then perform different processes of hallucination mitigation based on the classification result, just like a dentist first observes the teeth and then makes a plan. In a simple deployment, Dentist can classify queries as perception or reasoning and easily mitigate potential hallucinations in answers which has been demonstrated in our experiments. On MMbench, we achieve a 13.44%/10.2%/15.8% improvement in accuracy on Image Quality, a Coarse Perception visual question answering (VQA) task, over the baseline InstructBLIP/LLaVA/VisualGLM.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/088a42203bc9a67e14b1bfd5c1fd25a03c126c08.pdf",
        "venue": "Trans. Mach. Learn. Res.",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **1. Research Problem & Motivation**\n    *   **Specific Technical Problem**: Large Vision-Language Models (LVLMs) frequently suffer from \"hallucination,\" where generated content deviates from the actual image content \\cite{chang2024u3t}. This includes both \"perception hallucination\" (incorrectly describing visual attributes) and \"reasoning hallucination\" (producing fallacies in logical reasoning based on the image).\n    *   **Importance & Challenge**: Hallucinations lead to misinformation, degrade user experience, and undermine the reliability of LVLMs. Existing mitigation methods often employ a fixed verification approach, which is ineffective or inappropriate for the diverse types of hallucinations arising from different query types (e.g., object detection is not suitable for reasoning queries). Furthermore, current methods may not fully eradicate hallucinations in a single pass, sometimes leading to inconsistent corrections \\cite{chang2024u3t}.\n\n*   **2. Related Work & Positioning**\n    *   **Relation to Existing Approaches**: Previous work on hallucination mitigation generally falls into two categories: optimizing model training/inference (e.g., RLHF-V, VIGC) or correcting hallucinations during the generation stage without model updates (e.g., Woodpecker, HalluciDoctor) \\cite{chang2024u3t}.\n    *   **Limitations of Previous Solutions**: Existing methods overlook the diversity of hallucinations. They apply a \"one-size-fits-all\" verification strategy, which is suboptimal for different query types (perception vs. reasoning). For instance, methods relying on object detection are effective for perceptual errors but fail for complex reasoning errors. This fixed approach can lead to partial or inconsistent corrections \\cite{chang2024u3t}.\n\n*   **3. Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes \"Dentist,\" a unified hallucination mitigation framework for LVLMs. Its core idea is to first classify the query type and then apply a tailored mitigation strategy within a validation loop \\cite{chang2024u3t}.\n        *   **Potential Hallucination Classification**: Queries are classified into \"perception\" or \"reasoning\" using ChatGPT with a specific prompt. This classification determines the type of potential hallucination in the LVLM's answer.\n        *   **Divide-and-Conquer Treatment**:\n            *   **Visual Verification for Perception Queries**: For perception-based queries, the original LVLM's long descriptive answer is broken down into sub-questions by ChatGPT. These sub-questions are then answered by the *original* LVLM (to demonstrate mitigation ability, not just a better VQA model), and the sub-answers are aggregated by ChatGPT to refine the original hallucinated response.\n            *   **Chain-of-Thought (CoT) for Reasoning Queries**: For reasoning-based queries, a \"Letâ€™s think step by step\" CoT prompt is added to the original query for the LVLM. ChatGPT then uses the LVLM's CoT-enhanced generation to correct the original answer, providing more detailed logical reasoning.\n        *   **Validation Loop**: The entire verification process is embedded in a loop. The revised answer from one iteration becomes the input for the next. The loop continues until the answer no longer changes significantly semantically (determined by ChatGPT) or a maximum iteration limit is reached, preventing \"snowball errors\" \\cite{chang2024u3t}.\n    *   **Novelty/Difference**: The novelty lies in being the first to:\n        *   Distinguish and apply different hallucination mitigation treatments based on the classification of potential hallucinations (perception vs. reasoning).\n        *   Employ a validation loop that iteratively refines answers until semantic convergence, ensuring more complete hallucination removal \\cite{chang2024u3t}.\n        *   The framework is designed for easy integration into various LVLMs and allows for future extensions with new classifications and treatments.\n\n*   **4. Key Technical Contributions**\n    *   **Novel Framework**: Introduction of \"Dentist,\" a unified framework for hallucination classification and mitigation in LVLMs \\cite{chang2024u3t}.\n    *   **Adaptive Mitigation Strategy**: A novel approach that classifies queries (and thus potential hallucinations) into perception or reasoning types and applies tailored mitigation techniques (visual verification with sub-questions for perception, Chain-of-Thought for reasoning).\n    *   **Iterative Refinement**: Implementation of a validation loop that continuously verifies and corrects answers until semantic stability, addressing the issue of incomplete hallucination removal \\cite{chang2024u3t}.\n    *   **System Design**: A modular and easily integrable framework design that can be deployed with various LVLMs and extended with new components.\n\n*   **5. Experimental Validation**\n    *   **Experiments Conducted**: Comprehensive quantitative experiments were performed on several hallucination mitigation benchmarks \\cite{chang2024u3t}.\n    *   **Benchmarks**: MMBench (covering Perception and Reasoning abilities), LLaVA-QA90, CHAIR, and POPE.\n    *   **Models Evaluated**: InstructBLIP, LLaVA, and VisualGLM were used as baseline LVLMs.\n    *   **Comparison**: The method was compared against a current effective LVLM hallucination mitigation method, Woodpecker \\cite{chang2024u3t}.\n    *   **Key Performance Metrics & Results**:\n        *   Significant improvements were observed across various visual language tasks.\n        *   On the Image Quality task (a Coarse Perception VQA task) within MMBench, \"Dentist\" achieved a 13.44% improvement over InstructBLIP, 10.2% over LLaVA, and 15.8% over VisualGLM in accuracy \\cite{chang2024u3t}.\n        *   The experiments demonstrated the effectiveness and superiority of the proposed method in mitigating hallucinations and boosting the performance of baseline LVLMs.\n\n*   **6. Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The framework relies on an external LLM (ChatGPT) for query classification, sub-question generation, answer aggregation, CoT-based correction, and semantic convergence detection. This introduces dependency on the performance and availability of such models. The paper also notes that if the validation loop reaches its maximum iteration limit without semantic convergence, it might indicate a \"snowball error\" phenomenon, in which case only the first verification's answer is returned, implying that not all hallucinations might be fully resolved in such edge cases \\cite{chang2024u3t}.\n    *   **Scope of Applicability**: The framework is designed for mitigating perception and reasoning hallucinations in LVLMs. While it is unified for these two types, its direct applicability to other potential hallucination categories (if any exist beyond these two broad types) is not explicitly discussed.\n\n*   **7. Technical Significance**\n    *   **Advancement of State-of-the-Art**: \"Dentist\" advances the technical state-of-the-art by introducing a novel, adaptive, and iterative approach to hallucination mitigation, moving beyond fixed verification methods. By classifying query types and tailoring mitigation strategies, it addresses the inherent diversity of hallucinations more effectively than prior work \\cite{chang2024u3t}. The validation loop ensures a more thorough and robust correction process.\n    *   **Potential Impact on Future Research**: This work provides a strong foundation for future research in several areas:\n        *   Developing more sophisticated query classification mechanisms.\n        *   Exploring additional specialized mitigation strategies for finer-grained hallucination types.\n        *   Investigating alternative methods for semantic convergence detection in iterative correction loops.\n        *   Improving the robustness of LVLMs by integrating such adaptive post-hoc correction frameworks, leading to more reliable and trustworthy multimodal AI systems \\cite{chang2024u3t}.",
        "keywords": [
          "Large Vision-Language Models (LVLMs)",
          "Hallucination mitigation",
          "Perception and reasoning hallucinations",
          "Dentist framework",
          "Unified hallucination mitigation",
          "Adaptive mitigation strategy",
          "Query type classification",
          "Iterative refinement",
          "Validation loop",
          "Visual verification",
          "Chain-of-Thought (CoT)",
          "Semantic convergence",
          "Hallucination mitigation benchmarks",
          "State-of-the-art advancement",
          "External LLM dependency"
        ],
        "paper_type": "**technical**\n\n**reasoning:**\n\n1.  **abstract:** the abstract explicitly states, \"to accurately deal with various hallucinations, we present a unified framework, dentist, for hallucination mitigation.\" it then describes the core steps of this framework (\"first classify the queries, then perform different processes\"). this directly aligns with the \"technical\" criterion: \"presents new methods, algorithms, or systems\" and \"abstract mentions: 'propose', 'develop', 'present', 'algorithm', 'method'\". while it also mentions \"demonstrated in our experiments\" and provides quantitative results (which are empirical), the primary contribution highlighted is the *development and presentation of a new framework*.\n2.  **introduction:** the introduction sets up a technical problem (hallucination in lvlms) and discusses existing approaches, leading to the implied need for their proposed solution (the \"unified framework\"). this further supports the classification as \"technical\" as it discusses a \"technical problem\" and implicitly prepares for the \"proposed solution\"."
      },
      "file_name": "088a42203bc9a67e14b1bfd5c1fd25a03c126c08.pdf"
    },
    {
      "success": true,
      "doc_id": "5d65512c2ccb0bd27aaf0ba0aa636889",
      "summary": "Here is a focused summary of the paper for a literature review:\n\n### Focused Summary for Literature Review\n\n#### 1. Research Problem & Motivation\n*   **Specific Technical Problem**: Large Vision-Language Models (LVLMs) frequently suffer from \"hallucination,\" where they generate plausible but factually incorrect responses that do not align with the visual input \\cite{wang2024vym}.\n*   **Importance and Challenge**: Hallucinations severely undermine the reliability of LVLMs, posing significant barriers to their deployment in real-world applications \\cite{wang2024vym}. Existing inference-stage mitigation methods primarily attribute hallucinations to the language model itself. However, this paper posits that distortions in the visual encoding process are a critical, overlooked source of these errors, making the problem more complex than previously assumed \\cite{wang2024vym}. The paper identifies \"Visual Encoding Distortion,\" where key visual features captured in earlier layers become distorted as they propagate towards the final output layer, leading to incorrect inferences \\cite{wang2024vym}.\n\n#### 2. Related Work & Positioning\n*   **Relation to Existing Approaches**: Previous hallucination mitigation methods generally fall into three categories:\n    *   **Post-training steps**: Involve additional training with auxiliary supervision or reinforcement learning, which are computationally expensive and require large annotated datasets \\cite{wang2024vym}.\n    *   **Post-hoc correction**: Utilize auxiliary revision models or manual pipelines during inference, often facing challenges with high computational costs, complex implementation, and limited scalability \\cite{wang2024vym}.\n    *   **Decoding strategy modification**: Intervene during the inference phase by adjusting the decoding strategy, reducing computational costs. However, these methods typically assume the language model is the primary source of hallucinations, focusing on language priors or statistical biases \\cite{wang2024vym}.\n*   **Limitations of Previous Solutions**: Existing inference-time decoding strategies largely overlook the impact of the visual encoding process on hallucinations \\cite{wang2024vym}. They fail to address the \"Visual Encoding Distortion\" phenomenon, where crucial visual information is corrupted before reaching the language model, leading to errors even with sophisticated decoding strategies focused solely on the language component \\cite{wang2024vym}.\n\n#### 3. Technical Approach & Innovation\n*   **Core Technical Method**: The paper proposes Visual-Layer Fusion Contrastive Decoding (VaLiD), a novel hallucination mitigation method that addresses distortions from the vision encoder perspective \\cite{wang2024vym}.\n    *   **Uncertainty-guided Visual-Layer Fusion**: VaLiD leverages uncertainty (measured by the entropy of the next token's probability distribution) to identify visual layers where encoding distortion occurs \\cite{wang2024vym}. Instead of relying on a single layer, it dynamically selects and fuses features from the top-k early visual layers exhibiting high uncertainty. This fusion is entropy-weighted to create a robust reference distribution \\cite{wang2024vym}. To ensure computational efficiency, visual layers are partitioned into \"buckets,\" and the optimal bucket is determined via a validation set \\cite{wang2024vym}.\n    *   **Contrastive Decoding**: VaLiD then applies a contrastive decoding approach. It contrasts the next token distribution from the standard visual output layer (Pori) with the fused reference distribution (Pref) derived from the selected early visual layers. This contrastive process generates a corrected probability distribution (Pvalid) that mitigates the adverse effects of inaccurate visual information \\cite{wang2024vym}. An adaptive reliability constraint is also incorporated to prevent penalizing valid outputs and ensure adherence to language standards \\cite{wang2024vym}.\n*   **Novelty**: VaLiD is novel because it is the first visual-centric hallucination mitigation approach that traces the origin of hallucinations to the vision encoder's internal processing \\cite{wang2024vym}. It introduces the concept of \"Visual Encoding Distortion\" and provides a mechanism (uncertainty-guided layer fusion) to identify and correct these distortions by integrating information from multiple visual layers, rather than solely relying on the final visual output or language model adjustments \\cite{wang2024vym}.\n\n#### 4. Key Technical Contributions\n*   **Novel Phenomenon Identification**: Emphasizes the critical role of the vision encoder and identifies \"Visual Encoding Distortion\" as a common phenomenon across different LVLMs, where visual information degrades as it propagates through layers \\cite{wang2024vym}.\n*   **Visual-Centric Mitigation Method**: Introduces VaLiD, the first hallucination mitigation approach that directly addresses issues within the visual encoding process, shifting the focus from solely the language model to the vision encoder \\cite{wang2024vym}.\n*   **Uncertainty-Guided Layer Fusion**: Proposes a novel mechanism for dynamically selecting and fusing visual features from early layers based on their uncertainty, providing a more robust reference for contrastive decoding \\cite{wang2024vym}.\n*   **Enhanced Contrastive Decoding**: Integrates this visual-layer fusion into a contrastive decoding framework, effectively correcting distorted visual information and improving the reliability of generated content \\cite{wang2024vym}.\n\n#### 5. Experimental Validation\n*   **Experiments Conducted**: VaLiD was integrated with three representative LVLMs: InstructBLIP-7B, LLaVA-v1.5-7B, and Qwen-VL-7B \\cite{wang2024vym}.\n*   **Key Performance Metrics and Comparison Results**:\n    *   **Benchmarks**: Evaluated on three hallucination benchmarks: POPE (assessing object existence hallucination across MS-COCO, A-OKVQA, GQA datasets), AMBER (fine-grained evaluation of existence, attribute, and relational hallucinations), and MME (comprehensive benchmark covering 14 tasks, with focus on existence, count, location, and color subsets for hallucination assessment) \\cite{wang2024vym}.\n    *   **Metrics**: Accuracy (Acc.), F1-score, and Yes Accuracy (Yes Acc.) for POPE; Acc. and F1 for AMBER; and specific task scores for MME \\cite{wang2024vym}.\n    *   **Comparison**: Compared against baseline methods including Vanilla decoding and other contrastive decoding approaches like VCD, M3ID, and Ritual \\cite{wang2024vym}.\n    *   **Results**: VaLiD consistently demonstrated state-of-the-art performance in mitigating hallucinations across various benchmarks and LVLMs. For instance, on POPE, VaLiD generally achieved higher Acc. and F1 scores across all datasets and negative sampling strategies compared to baselines \\cite{wang2024vym}. It effectively reduced object-level and attribute-level hallucinations without compromising the model's general reasoning capabilities on other MME tasks \\cite{wang2024vym}.\n\n#### 6. Limitations & Scope\n*   **Technical Limitations/Assumptions**: The method relies on the assumption that earlier visual layers retain key features that become distorted later, and that uncertainty is a reliable indicator of this distortion \\cite{wang2024vym}. While the paper demonstrates this empirically, the theoretical underpinnings of why uncertainty correlates with distortion could be further explored.\n*   **Scope of Applicability**: VaLiD is designed for LVLMs that utilize Vision Transformers (ViT) as their visual encoder, as it operates by accessing and fusing features from different hidden layers of the ViT backbone \\cite{wang2024vym}. Its direct applicability to LVLMs with fundamentally different visual encoder architectures might require adaptation. The \"bucketing\" strategy for layer selection, while improving efficiency, implies a trade-off that needs careful validation for optimal performance \\cite{wang2024vym}.\n\n#### 7. Technical Significance\n*   **Advancement of State-of-the-Art**: VaLiD significantly advances the technical state-of-the-art in hallucination mitigation by introducing a novel visual-centric perspective, moving beyond language-model-centric approaches \\cite{wang2024vym}. It provides a concrete method to identify and correct visual encoding distortions, a previously underexplored root cause of LVLM hallucinations \\cite{wang2024vym}.\n*   **Potential Impact on Future Research**: This work opens new avenues for research into the internal workings of vision encoders within LVLMs. It suggests that future efforts in improving LVLM reliability should not only focus on language model biases or post-hoc corrections but also on ensuring the integrity of visual information throughout the encoding process. It could inspire further investigations into dynamic layer selection, uncertainty quantification in multimodal models, and more sophisticated visual feature fusion techniques for robust multimodal reasoning \\cite{wang2024vym}.",
      "intriguing_abstract": "Large Vision-Language Models (LVLMs) are plagued by pervasive hallucinations, generating plausible but factually incorrect responses that severely undermine their reliability. While prior mitigation efforts largely focus on the language model, we uncover a critical, overlooked root cause: **Visual Encoding Distortion**. This phenomenon reveals that crucial visual features captured in early layers degrade and become corrupted as they propagate through the vision encoder, leading to fundamental misinterpretations.\n\nTo address this, we introduce **Visual-Layer Fusion Contrastive Decoding (VaLiD)**, the first visual-centric inference-time method that directly targets these distortions. VaLiD innovatively leverages **uncertainty-guided visual-layer fusion** to dynamically select and combine robust features from early visual layers, creating a reliable reference distribution. This is then integrated into a **contrastive decoding** framework, effectively correcting the distorted visual information before token generation.\n\nExtensive experiments across InstructBLIP, LLaVA, and Qwen-VL on benchmarks like POPE, AMBER, and MME demonstrate VaLiD's state-of-the-art performance in mitigating object, attribute, and relational hallucinations. Our work fundamentally shifts the paradigm of hallucination research, emphasizing the critical importance of **vision encoder integrity** for building truly reliable and trustworthy multimodal AI systems.",
      "keywords": [
        "Large Vision-Language Models (LVLMs)",
        "Hallucination mitigation",
        "Visual Encoding Distortion",
        "Visual-Layer Fusion Contrastive Decoding (VaLiD)",
        "Vision encoder",
        "Uncertainty-guided visual-layer fusion",
        "Contrastive decoding",
        "Inference-stage mitigation",
        "State-of-the-art performance",
        "Multimodal reasoning",
        "LVLM reliability",
        "Hallucination benchmarks",
        "Adaptive reliability constraint"
      ],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/2feb4d83da1892db3934fcf406c8beb6cd10ded1.pdf",
      "citation_key": "wang2024vym",
      "metadata": {
        "title": "VaLiD: Mitigating the Hallucination of Large Vision Language Models by Visual Layer Fusion Contrastive Decoding",
        "authors": [
          "Jiaqi Wang",
          "Yifei Gao",
          "Jitao Sang"
        ],
        "published_date": "2024",
        "abstract": "Large Vision-Language Models (LVLMs) have demonstrated remarkable capabilities in multimodal task reasoning. However, they often generate responses that appear plausible yet do not accurately reflect the visual content, a phenomenon known as hallucination. Recent approaches have introduced training-free methods to mitigate hallucinations by adjusting the decoding strategy during the inference stage, typically attributing hallucinations to the language model itself. Our analysis, however, reveals that distortions in the visual encoding process significantly affect the model's reasoning capabilities. Specifically, earlier visual layers may retain key features but gradually distort as the information propagates toward the output layer. Building on these insights, we propose a novel hallucination-mitigation method from the visual encoding perspective: \\textbf{V}isu\\textbf{a}l \\textbf{L}ayer Fus\\textbf{i}on Contrastive \\textbf{D}ecoding (\\textbf{VaLiD}). This method utilizes uncertainty to guide the visual layer selection, correcting distortions in the visual encoding process and thereby enhancing the reliability of the generated content. Experimental results demonstrate the effectiveness of VaLiD in mitigating hallucinations across various benchmarks, achieving state-of-the-art performance when compared to baseline methods. Codes are available at \\href{https://github.com/RicardoLuL/VaLiD_LVLMs_hallucinations}{Github}.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/2feb4d83da1892db3934fcf406c8beb6cd10ded1.pdf",
        "venue": "arXiv.org",
        "citationCount": 0,
        "score": 0,
        "summary": "Here is a focused summary of the paper for a literature review:\n\n### Focused Summary for Literature Review\n\n#### 1. Research Problem & Motivation\n*   **Specific Technical Problem**: Large Vision-Language Models (LVLMs) frequently suffer from \"hallucination,\" where they generate plausible but factually incorrect responses that do not align with the visual input \\cite{wang2024vym}.\n*   **Importance and Challenge**: Hallucinations severely undermine the reliability of LVLMs, posing significant barriers to their deployment in real-world applications \\cite{wang2024vym}. Existing inference-stage mitigation methods primarily attribute hallucinations to the language model itself. However, this paper posits that distortions in the visual encoding process are a critical, overlooked source of these errors, making the problem more complex than previously assumed \\cite{wang2024vym}. The paper identifies \"Visual Encoding Distortion,\" where key visual features captured in earlier layers become distorted as they propagate towards the final output layer, leading to incorrect inferences \\cite{wang2024vym}.\n\n#### 2. Related Work & Positioning\n*   **Relation to Existing Approaches**: Previous hallucination mitigation methods generally fall into three categories:\n    *   **Post-training steps**: Involve additional training with auxiliary supervision or reinforcement learning, which are computationally expensive and require large annotated datasets \\cite{wang2024vym}.\n    *   **Post-hoc correction**: Utilize auxiliary revision models or manual pipelines during inference, often facing challenges with high computational costs, complex implementation, and limited scalability \\cite{wang2024vym}.\n    *   **Decoding strategy modification**: Intervene during the inference phase by adjusting the decoding strategy, reducing computational costs. However, these methods typically assume the language model is the primary source of hallucinations, focusing on language priors or statistical biases \\cite{wang2024vym}.\n*   **Limitations of Previous Solutions**: Existing inference-time decoding strategies largely overlook the impact of the visual encoding process on hallucinations \\cite{wang2024vym}. They fail to address the \"Visual Encoding Distortion\" phenomenon, where crucial visual information is corrupted before reaching the language model, leading to errors even with sophisticated decoding strategies focused solely on the language component \\cite{wang2024vym}.\n\n#### 3. Technical Approach & Innovation\n*   **Core Technical Method**: The paper proposes Visual-Layer Fusion Contrastive Decoding (VaLiD), a novel hallucination mitigation method that addresses distortions from the vision encoder perspective \\cite{wang2024vym}.\n    *   **Uncertainty-guided Visual-Layer Fusion**: VaLiD leverages uncertainty (measured by the entropy of the next token's probability distribution) to identify visual layers where encoding distortion occurs \\cite{wang2024vym}. Instead of relying on a single layer, it dynamically selects and fuses features from the top-k early visual layers exhibiting high uncertainty. This fusion is entropy-weighted to create a robust reference distribution \\cite{wang2024vym}. To ensure computational efficiency, visual layers are partitioned into \"buckets,\" and the optimal bucket is determined via a validation set \\cite{wang2024vym}.\n    *   **Contrastive Decoding**: VaLiD then applies a contrastive decoding approach. It contrasts the next token distribution from the standard visual output layer (Pori) with the fused reference distribution (Pref) derived from the selected early visual layers. This contrastive process generates a corrected probability distribution (Pvalid) that mitigates the adverse effects of inaccurate visual information \\cite{wang2024vym}. An adaptive reliability constraint is also incorporated to prevent penalizing valid outputs and ensure adherence to language standards \\cite{wang2024vym}.\n*   **Novelty**: VaLiD is novel because it is the first visual-centric hallucination mitigation approach that traces the origin of hallucinations to the vision encoder's internal processing \\cite{wang2024vym}. It introduces the concept of \"Visual Encoding Distortion\" and provides a mechanism (uncertainty-guided layer fusion) to identify and correct these distortions by integrating information from multiple visual layers, rather than solely relying on the final visual output or language model adjustments \\cite{wang2024vym}.\n\n#### 4. Key Technical Contributions\n*   **Novel Phenomenon Identification**: Emphasizes the critical role of the vision encoder and identifies \"Visual Encoding Distortion\" as a common phenomenon across different LVLMs, where visual information degrades as it propagates through layers \\cite{wang2024vym}.\n*   **Visual-Centric Mitigation Method**: Introduces VaLiD, the first hallucination mitigation approach that directly addresses issues within the visual encoding process, shifting the focus from solely the language model to the vision encoder \\cite{wang2024vym}.\n*   **Uncertainty-Guided Layer Fusion**: Proposes a novel mechanism for dynamically selecting and fusing visual features from early layers based on their uncertainty, providing a more robust reference for contrastive decoding \\cite{wang2024vym}.\n*   **Enhanced Contrastive Decoding**: Integrates this visual-layer fusion into a contrastive decoding framework, effectively correcting distorted visual information and improving the reliability of generated content \\cite{wang2024vym}.\n\n#### 5. Experimental Validation\n*   **Experiments Conducted**: VaLiD was integrated with three representative LVLMs: InstructBLIP-7B, LLaVA-v1.5-7B, and Qwen-VL-7B \\cite{wang2024vym}.\n*   **Key Performance Metrics and Comparison Results**:\n    *   **Benchmarks**: Evaluated on three hallucination benchmarks: POPE (assessing object existence hallucination across MS-COCO, A-OKVQA, GQA datasets), AMBER (fine-grained evaluation of existence, attribute, and relational hallucinations), and MME (comprehensive benchmark covering 14 tasks, with focus on existence, count, location, and color subsets for hallucination assessment) \\cite{wang2024vym}.\n    *   **Metrics**: Accuracy (Acc.), F1-score, and Yes Accuracy (Yes Acc.) for POPE; Acc. and F1 for AMBER; and specific task scores for MME \\cite{wang2024vym}.\n    *   **Comparison**: Compared against baseline methods including Vanilla decoding and other contrastive decoding approaches like VCD, M3ID, and Ritual \\cite{wang2024vym}.\n    *   **Results**: VaLiD consistently demonstrated state-of-the-art performance in mitigating hallucinations across various benchmarks and LVLMs. For instance, on POPE, VaLiD generally achieved higher Acc. and F1 scores across all datasets and negative sampling strategies compared to baselines \\cite{wang2024vym}. It effectively reduced object-level and attribute-level hallucinations without compromising the model's general reasoning capabilities on other MME tasks \\cite{wang2024vym}.\n\n#### 6. Limitations & Scope\n*   **Technical Limitations/Assumptions**: The method relies on the assumption that earlier visual layers retain key features that become distorted later, and that uncertainty is a reliable indicator of this distortion \\cite{wang2024vym}. While the paper demonstrates this empirically, the theoretical underpinnings of why uncertainty correlates with distortion could be further explored.\n*   **Scope of Applicability**: VaLiD is designed for LVLMs that utilize Vision Transformers (ViT) as their visual encoder, as it operates by accessing and fusing features from different hidden layers of the ViT backbone \\cite{wang2024vym}. Its direct applicability to LVLMs with fundamentally different visual encoder architectures might require adaptation. The \"bucketing\" strategy for layer selection, while improving efficiency, implies a trade-off that needs careful validation for optimal performance \\cite{wang2024vym}.\n\n#### 7. Technical Significance\n*   **Advancement of State-of-the-Art**: VaLiD significantly advances the technical state-of-the-art in hallucination mitigation by introducing a novel visual-centric perspective, moving beyond language-model-centric approaches \\cite{wang2024vym}. It provides a concrete method to identify and correct visual encoding distortions, a previously underexplored root cause of LVLM hallucinations \\cite{wang2024vym}.\n*   **Potential Impact on Future Research**: This work opens new avenues for research into the internal workings of vision encoders within LVLMs. It suggests that future efforts in improving LVLM reliability should not only focus on language model biases or post-hoc corrections but also on ensuring the integrity of visual information throughout the encoding process. It could inspire further investigations into dynamic layer selection, uncertainty quantification in multimodal models, and more sophisticated visual feature fusion techniques for robust multimodal reasoning \\cite{wang2024vym}.",
        "keywords": [
          "Large Vision-Language Models (LVLMs)",
          "Hallucination mitigation",
          "Visual Encoding Distortion",
          "Visual-Layer Fusion Contrastive Decoding (VaLiD)",
          "Vision encoder",
          "Uncertainty-guided visual-layer fusion",
          "Contrastive decoding",
          "Inference-stage mitigation",
          "State-of-the-art performance",
          "Multimodal reasoning",
          "LVLM reliability",
          "Hallucination benchmarks",
          "Adaptive reliability constraint"
        ],
        "paper_type": "based on the abstract and introduction:\n\n*   the abstract explicitly states: \"we **propose a novel hallucination-mitigation method** from the visual encoding perspective: visuallayer fusion contrastive decoding (valid).\" it then describes how this method works and mentions \"experimental results demonstrate the effectiveness of valid\".\n*   the introduction sets up a technical problem (hallucination in lvlms), discusses existing approaches, and implicitly positions their proposed method as a solution to the identified shortcomings.\n*   keywords like \"propose\", \"method\", \"algorithm\" (implied by \"decoding strategy\"), and \"system\" (implied by a new approach to model behavior) strongly align with the \"technical\" classification.\n*   while it includes \"experimental results\" and \"benchmarks,\" which are characteristic of \"empirical\" papers, the primary contribution is the *development and presentation of a new method* (valid), with the experiments serving to validate this new technical contribution.\n\ntherefore, the paper type is: **technical**"
      },
      "file_name": "2feb4d83da1892db3934fcf406c8beb6cd10ded1.pdf"
    },
    {
      "success": true,
      "doc_id": "cc0eeecfffd413403103a1723fc9a84b",
      "summary": "Here's a focused summary of the technical paper for a literature review, adhering to your citation requirements and bullet format:\n\n*   **Research Problem & Motivation**\n    *   Large Language Models (LLMs) are highly susceptible to \"hallucinations,\" generating responses that do not align with real-world facts. This is a critical challenge, especially in high-risk domains like healthcare, where non-factual information can lead to severe consequences \\cite{niu2024v97}.\n    *   Existing Knowledge Graph (KG)-augmented approaches to mitigate hallucinations are often resource-intensive, requiring multiple rounds of retrieval and verification for every factual statement. This high computational cost impedes their practical application \\cite{niu2024v97}.\n    *   The problem is challenging because LLMs may lack domain-specific or up-to-date knowledge, struggle to adapt to specialized domains, or have limited capacity, making hallucinations inevitable without external intervention \\cite{niu2024v97}.\n\n*   **Related Work & Positioning**\n    *   **Positioning**: \\cite{niu2024v97} positions its work, Self-Refinement-Enhanced Knowledge Graph Retrieval (Re-KGR), as an efficient post-generation hallucination mitigation method specifically targeting \"fact inconsistency\" hallucinations in medical Question-Answering (QA) tasks.\n    *   **Relation to existing approaches**: It builds upon Retrieval Augmented Generation (RAG) and post-processing techniques but aims to overcome their inefficiencies. It draws inspiration from prior work on hallucination identification that analyzes next-token predictive probability distributions (e.g., entropy, max value) and internal LLM states (e.g., distribution divergence between layers, as in DoLa \\cite{niu2024v97} citing [10]).\n    *   **Limitations of previous solutions**: Prior RAG and post-processing methods (e.g., NPH, RHO, KGR) often incur substantial computational costs, require supervised training for specific models, or entail laborious retrieval efforts by verifying *all* factual statements in a response. They lack an efficient mechanism to identify *which* parts of a response are likely to be erroneous, leading to redundant verification \\cite{niu2024v97}.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method (Re-KGR)**: \\cite{niu2024v97} proposes a \"refine-then-retrieval\" paradigm for injecting external knowledge in the post-generation stage, designed to reduce hallucination with minimal retrieval efforts. The approach consists of four main components:\n        1.  **Entity Detection**: Identifies word entities with a high potential for hallucination by analyzing the predictive uncertainties of token logits. This involves quantifying uncertainty using the max value and entropy of the next-token predictive probability distribution, and measuring distribution divergence between the final and intermediate layers (using Jensen-Shannon divergence, inspired by DoLa \\cite{niu2024v97} citing [10]). Abnormal values are detected via quartile-based assessment.\n        2.  **Triple Extraction**: Prompts the LLM to extract all factual statements as knowledge triples from its generated response. Crucially, this set is *refined* by retaining only those triples that include the previously identified high-risk word entities, thereby reducing the number of retrieval instances.\n        3.  **KG Retrieval**: Retrieves associated knowledge triples from a domain-specific medical Knowledge Graph. This process enhances recall by expanding the refined triple set with synonyms for entities and predicates.\n        4.  **Knowledge Verification & Rectification**: Compares the refined factual triples from the LLM's response with the retrieved KG knowledge to verify truthfulness and subsequently rectifies inaccurate content in the original response.\n    *   **Novelty/Difference**:\n        *   **Selective Retrieval**: Unlike prior methods that retrieve and verify *all* factual statements, Re-KGR proactively identifies and targets only *high-risk* factual statements/entities for KG retrieval, significantly reducing verification rounds and computational overhead \\cite{niu2024v97}.\n        *   **Self-Refinement based on Internal States**: It leverages the LLM's internal next-token predictive probability distributions and layer-wise divergence to \"self-refine\" the scope of knowledge retrieval, indicating that LLMs can implicitly signal their own uncertainties \\cite{niu2024v97}.\n        *   **Post-generation, Pre-verification Refinement**: Integrates a refinement step *after* initial generation but *before* extensive KG verification, optimizing the retrieval process.\n\n*   **Key Technical Contributions**\n    *   **Novel Method**: Introduction of Self-Refinement-Enhanced Knowledge Graph Retrieval (Re-KGR), a novel \"refine-then-retrieval\" paradigm for efficient hallucination mitigation in LLMs \\cite{niu2024v97}.\n    *   **Efficient Hallucination Identification**: A method for identifying hallucination-prone tokens/entities by analyzing next-token predictive probability distributions and layer-wise distribution divergence, enabling targeted knowledge retrieval \\cite{niu2024v97}.\n    *   **Targeted Knowledge Retrieval**: A mechanism to significantly reduce KG retrieval efforts by refining the set of factual statements to be verified, focusing only on those associated with identified high-risk entities \\cite{niu2024v97}.\n    *   **System Design**: Integration of internal LLM uncertainty signals with external KG retrieval in a post-generation, self-refinement framework \\cite{niu2024v97}.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted**: The Re-KGR method was evaluated on medical question-answering tasks \\cite{niu2024v97}.\n    *   **Foundational Models**: Experiments were conducted using LLaMA \\cite{niu2024v97} (a widely-used LLM) and integrated with DoLa \\cite{niu2024v97} citing [10], a state-of-the-art contrastive decoding technique.\n    *   **Dataset & KG**: A medical dataset (MedQuAD) was used, and an expansive domain-specific knowledge graph was constructed upon an existing corpus of medical information \\cite{niu2024v97}.\n    *   **Key Performance Metrics & Results**:\n        *   **Truthfulness/Accuracy**: Experimental results demonstrated that Re-KGR significantly enhances the factual capability of LLMs' responses, achieving the highest scores on truthfulness across various foundational models \\cite{niu2024v97}.\n        *   **Efficiency**: The method achieved a \"notable reduction in time expenditure\" by minimizing KG-based retrieval efforts, confirming its efficiency claims \\cite{niu2024v97}.\n\n*   **Limitations & Scope**\n    *   **Scope of Applicability**: The current study focuses specifically on \"fact inconsistency\" hallucinations within medical Question-Answering (QA) tasks. Its direct applicability to other types of hallucinations (e.g., input-conflicting, context-conflicting) or other domains would require further investigation \\cite{niu2024v97}.\n    *   **Technical Limitations/Assumptions**: The effectiveness relies on the ability of the LLM's internal states (next-token probabilities, layer divergence) to accurately signal uncertainty and potential hallucinations. The quality and comprehensiveness of the domain-specific Knowledge Graph are also crucial for successful retrieval and verification \\cite{niu2024v97}.\n\n*   **Technical Significance**\n    *   **Advances State-of-the-Art**: \\cite{niu2024v97} significantly advances the state-of-the-art in LLM hallucination mitigation by introducing a more efficient and targeted approach to knowledge retrieval. It moves beyond brute-force verification of all facts by leveraging internal LLM signals to identify and prioritize uncertain content.\n    *   **Potential Impact**:\n        *   **Improved LLM Reliability**: Enhances the trustworthiness and factual accuracy of LLMs, particularly in high-stakes applications like healthcare, where non-factual information can have severe consequences \\cite{niu2024v97}.\n        *   **Resource Efficiency**: Offers a more computationally efficient paradigm for integrating external knowledge, making RAG-like systems more practical for real-world deployment by reducing the need for extensive retrieval and verification \\cite{niu2024v97}.\n        *   **Future Research**: Opens avenues for further research into leveraging LLM's internal states for self-correction and uncertainty quantification, potentially leading to more autonomous and reliable AI systems.",
      "intriguing_abstract": "Large Language Models (LLMs) are plagued by factual hallucinations, a critical issue in high-stakes domains like healthcare where misinformation can have severe consequences. Existing Knowledge Graph (KG)-augmented Retrieval Augmented Generation (RAG) methods mitigate this but suffer from high computational costs due to exhaustive verification of all factual statements. We introduce Self-Refinement-Enhanced Knowledge Graph Retrieval (Re-KGR), a novel \"refine-then-retrieval\" paradigm that drastically improves efficiency. Re-KGR uniquely leverages LLM's internal states, analyzing next-token predictive probability distributions and layer-wise divergence (quantified by Jensen-Shannon divergence) to identify hallucination-prone entities. This enables highly targeted KG retrieval, verifying only high-risk factual statements rather than all. Evaluated on medical Question-Answering (QA) tasks, Re-KGR significantly enhances factual truthfulness while achieving a notable reduction in retrieval efforts and time expenditure. This work offers a practical, resource-efficient solution for building more reliable LLMs, crucial for trustworthy AI applications.",
      "keywords": [
        "LLM hallucinations",
        "Knowledge Graphs",
        "Self-Refinement-Enhanced Knowledge Graph Retrieval (Re-KGR)",
        "refine-then-retrieval paradigm",
        "medical Question-Answering",
        "fact inconsistency",
        "next-token predictive probability distributions",
        "layer-wise distribution divergence",
        "targeted knowledge retrieval",
        "efficient hallucination mitigation",
        "internal LLM uncertainty signals",
        "computational efficiency",
        "post-generation verification",
        "healthcare applications"
      ],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/45ed6263e02d219f0542ac743b9c9f837154a58d.pdf",
      "citation_key": "niu2024v97",
      "metadata": {
        "title": "Mitigating Hallucinations in Large Language Models via Self-Refinement-Enhanced Knowledge Retrieval",
        "authors": [
          "Mengjia Niu",
          "Hao Li",
          "Jie Shi",
          "Hamed Haddadi",
          "Fan Mo"
        ],
        "published_date": "2024",
        "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities across various domains, although their susceptibility to hallucination poses significant challenges for their deployment in critical areas such as healthcare. To address this issue, retrieving relevant facts from knowledge graphs (KGs) is considered a promising method. Existing KG-augmented approaches tend to be resource-intensive, requiring multiple rounds of retrieval and verification for each factoid, which impedes their application in real-world scenarios. In this study, we propose Self-Refinement-Enhanced Knowledge Graph Retrieval (Re-KGR) to augment the factuality of LLMs' responses with less retrieval efforts in the medical field. Our approach leverages the attribution of next-token predictive probability distributions across different tokens, and various model layers to primarily identify tokens with a high potential for hallucination, reducing verification rounds by refining knowledge triples associated with these tokens. Moreover, we rectify inaccurate content using retrieved knowledge in the post-processing stage, which improves the truthfulness of generated responses. Experimental results on a medical dataset demonstrate that our approach can enhance the factual capability of LLMs across various foundational models as evidenced by the highest scores on truthfulness.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/45ed6263e02d219f0542ac743b9c9f837154a58d.pdf",
        "venue": "arXiv.org",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the technical paper for a literature review, adhering to your citation requirements and bullet format:\n\n*   **Research Problem & Motivation**\n    *   Large Language Models (LLMs) are highly susceptible to \"hallucinations,\" generating responses that do not align with real-world facts. This is a critical challenge, especially in high-risk domains like healthcare, where non-factual information can lead to severe consequences \\cite{niu2024v97}.\n    *   Existing Knowledge Graph (KG)-augmented approaches to mitigate hallucinations are often resource-intensive, requiring multiple rounds of retrieval and verification for every factual statement. This high computational cost impedes their practical application \\cite{niu2024v97}.\n    *   The problem is challenging because LLMs may lack domain-specific or up-to-date knowledge, struggle to adapt to specialized domains, or have limited capacity, making hallucinations inevitable without external intervention \\cite{niu2024v97}.\n\n*   **Related Work & Positioning**\n    *   **Positioning**: \\cite{niu2024v97} positions its work, Self-Refinement-Enhanced Knowledge Graph Retrieval (Re-KGR), as an efficient post-generation hallucination mitigation method specifically targeting \"fact inconsistency\" hallucinations in medical Question-Answering (QA) tasks.\n    *   **Relation to existing approaches**: It builds upon Retrieval Augmented Generation (RAG) and post-processing techniques but aims to overcome their inefficiencies. It draws inspiration from prior work on hallucination identification that analyzes next-token predictive probability distributions (e.g., entropy, max value) and internal LLM states (e.g., distribution divergence between layers, as in DoLa \\cite{niu2024v97} citing [10]).\n    *   **Limitations of previous solutions**: Prior RAG and post-processing methods (e.g., NPH, RHO, KGR) often incur substantial computational costs, require supervised training for specific models, or entail laborious retrieval efforts by verifying *all* factual statements in a response. They lack an efficient mechanism to identify *which* parts of a response are likely to be erroneous, leading to redundant verification \\cite{niu2024v97}.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method (Re-KGR)**: \\cite{niu2024v97} proposes a \"refine-then-retrieval\" paradigm for injecting external knowledge in the post-generation stage, designed to reduce hallucination with minimal retrieval efforts. The approach consists of four main components:\n        1.  **Entity Detection**: Identifies word entities with a high potential for hallucination by analyzing the predictive uncertainties of token logits. This involves quantifying uncertainty using the max value and entropy of the next-token predictive probability distribution, and measuring distribution divergence between the final and intermediate layers (using Jensen-Shannon divergence, inspired by DoLa \\cite{niu2024v97} citing [10]). Abnormal values are detected via quartile-based assessment.\n        2.  **Triple Extraction**: Prompts the LLM to extract all factual statements as knowledge triples from its generated response. Crucially, this set is *refined* by retaining only those triples that include the previously identified high-risk word entities, thereby reducing the number of retrieval instances.\n        3.  **KG Retrieval**: Retrieves associated knowledge triples from a domain-specific medical Knowledge Graph. This process enhances recall by expanding the refined triple set with synonyms for entities and predicates.\n        4.  **Knowledge Verification & Rectification**: Compares the refined factual triples from the LLM's response with the retrieved KG knowledge to verify truthfulness and subsequently rectifies inaccurate content in the original response.\n    *   **Novelty/Difference**:\n        *   **Selective Retrieval**: Unlike prior methods that retrieve and verify *all* factual statements, Re-KGR proactively identifies and targets only *high-risk* factual statements/entities for KG retrieval, significantly reducing verification rounds and computational overhead \\cite{niu2024v97}.\n        *   **Self-Refinement based on Internal States**: It leverages the LLM's internal next-token predictive probability distributions and layer-wise divergence to \"self-refine\" the scope of knowledge retrieval, indicating that LLMs can implicitly signal their own uncertainties \\cite{niu2024v97}.\n        *   **Post-generation, Pre-verification Refinement**: Integrates a refinement step *after* initial generation but *before* extensive KG verification, optimizing the retrieval process.\n\n*   **Key Technical Contributions**\n    *   **Novel Method**: Introduction of Self-Refinement-Enhanced Knowledge Graph Retrieval (Re-KGR), a novel \"refine-then-retrieval\" paradigm for efficient hallucination mitigation in LLMs \\cite{niu2024v97}.\n    *   **Efficient Hallucination Identification**: A method for identifying hallucination-prone tokens/entities by analyzing next-token predictive probability distributions and layer-wise distribution divergence, enabling targeted knowledge retrieval \\cite{niu2024v97}.\n    *   **Targeted Knowledge Retrieval**: A mechanism to significantly reduce KG retrieval efforts by refining the set of factual statements to be verified, focusing only on those associated with identified high-risk entities \\cite{niu2024v97}.\n    *   **System Design**: Integration of internal LLM uncertainty signals with external KG retrieval in a post-generation, self-refinement framework \\cite{niu2024v97}.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted**: The Re-KGR method was evaluated on medical question-answering tasks \\cite{niu2024v97}.\n    *   **Foundational Models**: Experiments were conducted using LLaMA \\cite{niu2024v97} (a widely-used LLM) and integrated with DoLa \\cite{niu2024v97} citing [10], a state-of-the-art contrastive decoding technique.\n    *   **Dataset & KG**: A medical dataset (MedQuAD) was used, and an expansive domain-specific knowledge graph was constructed upon an existing corpus of medical information \\cite{niu2024v97}.\n    *   **Key Performance Metrics & Results**:\n        *   **Truthfulness/Accuracy**: Experimental results demonstrated that Re-KGR significantly enhances the factual capability of LLMs' responses, achieving the highest scores on truthfulness across various foundational models \\cite{niu2024v97}.\n        *   **Efficiency**: The method achieved a \"notable reduction in time expenditure\" by minimizing KG-based retrieval efforts, confirming its efficiency claims \\cite{niu2024v97}.\n\n*   **Limitations & Scope**\n    *   **Scope of Applicability**: The current study focuses specifically on \"fact inconsistency\" hallucinations within medical Question-Answering (QA) tasks. Its direct applicability to other types of hallucinations (e.g., input-conflicting, context-conflicting) or other domains would require further investigation \\cite{niu2024v97}.\n    *   **Technical Limitations/Assumptions**: The effectiveness relies on the ability of the LLM's internal states (next-token probabilities, layer divergence) to accurately signal uncertainty and potential hallucinations. The quality and comprehensiveness of the domain-specific Knowledge Graph are also crucial for successful retrieval and verification \\cite{niu2024v97}.\n\n*   **Technical Significance**\n    *   **Advances State-of-the-Art**: \\cite{niu2024v97} significantly advances the state-of-the-art in LLM hallucination mitigation by introducing a more efficient and targeted approach to knowledge retrieval. It moves beyond brute-force verification of all facts by leveraging internal LLM signals to identify and prioritize uncertain content.\n    *   **Potential Impact**:\n        *   **Improved LLM Reliability**: Enhances the trustworthiness and factual accuracy of LLMs, particularly in high-stakes applications like healthcare, where non-factual information can have severe consequences \\cite{niu2024v97}.\n        *   **Resource Efficiency**: Offers a more computationally efficient paradigm for integrating external knowledge, making RAG-like systems more practical for real-world deployment by reducing the need for extensive retrieval and verification \\cite{niu2024v97}.\n        *   **Future Research**: Opens avenues for further research into leveraging LLM's internal states for self-correction and uncertainty quantification, potentially leading to more autonomous and reliable AI systems.",
        "keywords": [
          "LLM hallucinations",
          "Knowledge Graphs",
          "Self-Refinement-Enhanced Knowledge Graph Retrieval (Re-KGR)",
          "refine-then-retrieval paradigm",
          "medical Question-Answering",
          "fact inconsistency",
          "next-token predictive probability distributions",
          "layer-wise distribution divergence",
          "targeted knowledge retrieval",
          "efficient hallucination mitigation",
          "internal LLM uncertainty signals",
          "computational efficiency",
          "post-generation verification",
          "healthcare applications"
        ],
        "paper_type": "the paper should be classified as **technical**.\n\nhere's why:\n\n*   **abstract:** explicitly states \"we **propose** self-refinement-enhanced knowledge graph retrieval (re-kgr) to augment the factuality of llmsâ€™ responses...\" and then describes the mechanism of this proposed approach. it also mentions \"experimental results... demonstrate that our approach can enhance...\" which indicates validation of the proposed method.\n*   **introduction:** identifies a technical problem (\"hallucinations\" in llms) and discusses existing approaches (rag) before setting the stage for their specific solution.\n*   **keywords:** \"hallucination identification, hallucination mitigation, kg retrieval\" are all related to technical problems and solutions in the field.\n\nwhile it includes \"experimental results\" (which points to empirical), the core contribution is the **development and presentation of a new method/system (re-kgr)** to solve a specific technical problem. the empirical results serve to validate the effectiveness of this proposed technical solution."
      },
      "file_name": "45ed6263e02d219f0542ac743b9c9f837154a58d.pdf"
    },
    {
      "success": true,
      "doc_id": "11fa5211f686060d35af30a9e6cfc2ad",
      "summary": "Here's a focused summary of the technical paper \\cite{liu2024gxh} for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Addressing the critical challenge of large language models (LLMs) generating misinformation. This includes factual errors (hallucinations) and intentionally deceptive content.\n    *   **Importance and Challenge**: Misinformation from LLMs can have severe consequences, particularly in high-stakes domains, making its prevention and detection crucial.\n\n*   **Related Work & Positioning**\n    *   This paper \\cite{liu2024gxh} is presented as a comprehensive tutorial that surveys and categorizes existing strategies for preventing and detecting LLM-generated misinformation. It introduces the types of misinformation and their root causes, then systematically explores various mitigation techniques.\n    *   **Limitations of Previous Solutions**: While the tutorial discusses various methods, it implicitly highlights the need for a structured, comprehensive overview given the diverse and evolving landscape of LLM misinformation.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper \\cite{liu2024gxh} adopts a structured, two-pronged approach:\n        1.  **Preventing misinformation generation**: Strategies applied during model training and inference.\n        2.  **Detecting misinformation after generation**: Methods to identify problematic content post-generation.\n    *   **Novelty**: The innovation lies in providing a comprehensive, categorized framework that synthesizes a wide array of techniques, from AI alignment to watermarking, offering a holistic view of the problem space and potential solutions.\n\n*   **Key Technical Contributions**\n    *   **Novel Algorithms, Methods, or Techniques (Surveyed)**:\n        *   **Prevention**:\n            *   **AI Alignment Training**: Techniques to reduce LLMs' propensity for misinformation and refuse malicious instructions.\n            *   **Training-Free Mitigation**: Prompt guardrails, Retrieval-Augmented Generation (RAG), and decoding strategies to curb misinformation at inference time.\n        *   **Detection**:\n            *   **LLM-based Detection**: Using LLMs themselves, leveraging embedded knowledge or retrieval-enhanced judgments.\n            *   **Distinguishing LLM-generated Text**:\n                *   **Black-box approaches**: Classifiers and probability analysis.\n                *   **White-box approaches**: Watermarking.\n    *   **System Design/Architectural Innovations**: The paper \\cite{liu2024gxh} provides a conceptual architecture for understanding where different mitigation strategies fit within the LLM lifecycle (training vs. inference, prevention vs. detection).\n\n*   **Experimental Validation**\n    *   The provided content for this paper \\cite{liu2024gxh} describes various technical methods and strategies but does not report on specific experiments conducted by the authors of this tutorial. As a tutorial, its focus is on surveying and categorizing existing and proposed techniques rather than presenting new empirical results.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The paper \\cite{liu2024gxh} explicitly states it discusses \"challenges and limitations of detecting LLM-generated misinformation,\" indicating an awareness of the inherent difficulties and trade-offs in current methods.\n    *   **Scope of Applicability**: The strategies covered are broadly applicable to various LLM architectures and deployment scenarios, addressing misinformation across different forms (factual errors, deceptive content).\n\n*   **Technical Significance**\n    *   **Advancement of State-of-the-Art**: This tutorial \\cite{liu2024gxh} significantly advances the understanding of LLM misinformation by providing a structured, comprehensive overview of the problem, its causes, and a wide range of technical solutions. It serves as a valuable reference for researchers and practitioners.\n    *   **Potential Impact on Future Research**: By categorizing and detailing existing methods, the paper \\cite{liu2024gxh} helps identify gaps and directs future research efforts towards more effective prevention and detection mechanisms, fostering the development of more reliable and trustworthy LLMs.",
      "intriguing_abstract": "The proliferation of Large Language Models (LLMs) has ushered in unprecedented capabilities, yet it simultaneously amplifies the critical challenge of misinformation, from factual hallucinations to intentionally deceptive content. The societal and practical implications of such errors, especially in high-stakes domains, demand robust mitigation strategies. This comprehensive tutorial addresses this urgent need by presenting a novel, structured framework for understanding and combating LLM-generated misinformation.\n\nWe systematically categorize and survey a vast landscape of technical approaches, dissecting them into two crucial pillars: proactive prevention during model training and inference, and reactive detection post-generation. From advanced AI alignment techniques and training-free inference-time mitigations like Retrieval-Augmented Generation (RAG) and prompt guardrails, to sophisticated detection methods including LLM-based verification and white-box watermarking for distinguishing synthetic text, this paper offers a holistic overview. By synthesizing diverse methodologies and highlighting their underlying principles, this work serves as an indispensable guide, illuminating current limitations and charting a clear path for future research towards building truly reliable and trustworthy LLM systems.",
      "keywords": [
        "Large Language Models (LLMs)",
        "LLM-generated misinformation",
        "Hallucinations",
        "Misinformation prevention",
        "Misinformation detection",
        "Comprehensive tutorial",
        "Categorized framework",
        "AI Alignment Training",
        "Retrieval-Augmented Generation (RAG)",
        "Watermarking",
        "Prompt guardrails",
        "Trustworthy LLMs",
        "Challenges and limitations"
      ],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/5667f64b23cf48c94ff7413122bc56e5aad7e6a2.pdf",
      "citation_key": "liu2024gxh",
      "metadata": {
        "title": "Preventing and Detecting Misinformation Generated by Large Language Models",
        "authors": [
          "Aiwei Liu",
          "Qiang Sheng",
          "Xuming Hu"
        ],
        "published_date": "2024",
        "abstract": "As large language models (LLMs) become increasingly capable and widely deployed, the risk of them generating misinformation poses a critical challenge. Misinformation from LLMs can take various forms, from factual errors due to hallucination to intentionally deceptive content, and can have severe consequences in high-stakes domains.This tutorial covers comprehensive strategies to prevent and detect misinformation generated by LLMs. We first introduce the types of misinformation LLMs can produce and their root causes. We then explore two broad categories: Preventing misinformation generation: a) AI alignment training techniques to reduce LLMs' propensity for misinformation and refuse malicious instructions during model training. b) Training-free mitigation methods like prompt guardrails, retrieval-augmented generation (RAG), and decoding strategies to curb misinformation at inference time. Detecting misinformation after generation, including a) using LLMs themselves to detect misinformation through embedded knowledge or retrieval-enhanced judgments, and b) distinguishing LLM-generated text from human-written text through black-box approaches (e.g., classifiers, probability analysis) and white-box approaches (e.g., watermarking). We also discuss the challenges and limitations of detecting LLM-generated misinformation.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/5667f64b23cf48c94ff7413122bc56e5aad7e6a2.pdf",
        "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the technical paper \\cite{liu2024gxh} for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Addressing the critical challenge of large language models (LLMs) generating misinformation. This includes factual errors (hallucinations) and intentionally deceptive content.\n    *   **Importance and Challenge**: Misinformation from LLMs can have severe consequences, particularly in high-stakes domains, making its prevention and detection crucial.\n\n*   **Related Work & Positioning**\n    *   This paper \\cite{liu2024gxh} is presented as a comprehensive tutorial that surveys and categorizes existing strategies for preventing and detecting LLM-generated misinformation. It introduces the types of misinformation and their root causes, then systematically explores various mitigation techniques.\n    *   **Limitations of Previous Solutions**: While the tutorial discusses various methods, it implicitly highlights the need for a structured, comprehensive overview given the diverse and evolving landscape of LLM misinformation.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper \\cite{liu2024gxh} adopts a structured, two-pronged approach:\n        1.  **Preventing misinformation generation**: Strategies applied during model training and inference.\n        2.  **Detecting misinformation after generation**: Methods to identify problematic content post-generation.\n    *   **Novelty**: The innovation lies in providing a comprehensive, categorized framework that synthesizes a wide array of techniques, from AI alignment to watermarking, offering a holistic view of the problem space and potential solutions.\n\n*   **Key Technical Contributions**\n    *   **Novel Algorithms, Methods, or Techniques (Surveyed)**:\n        *   **Prevention**:\n            *   **AI Alignment Training**: Techniques to reduce LLMs' propensity for misinformation and refuse malicious instructions.\n            *   **Training-Free Mitigation**: Prompt guardrails, Retrieval-Augmented Generation (RAG), and decoding strategies to curb misinformation at inference time.\n        *   **Detection**:\n            *   **LLM-based Detection**: Using LLMs themselves, leveraging embedded knowledge or retrieval-enhanced judgments.\n            *   **Distinguishing LLM-generated Text**:\n                *   **Black-box approaches**: Classifiers and probability analysis.\n                *   **White-box approaches**: Watermarking.\n    *   **System Design/Architectural Innovations**: The paper \\cite{liu2024gxh} provides a conceptual architecture for understanding where different mitigation strategies fit within the LLM lifecycle (training vs. inference, prevention vs. detection).\n\n*   **Experimental Validation**\n    *   The provided content for this paper \\cite{liu2024gxh} describes various technical methods and strategies but does not report on specific experiments conducted by the authors of this tutorial. As a tutorial, its focus is on surveying and categorizing existing and proposed techniques rather than presenting new empirical results.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The paper \\cite{liu2024gxh} explicitly states it discusses \"challenges and limitations of detecting LLM-generated misinformation,\" indicating an awareness of the inherent difficulties and trade-offs in current methods.\n    *   **Scope of Applicability**: The strategies covered are broadly applicable to various LLM architectures and deployment scenarios, addressing misinformation across different forms (factual errors, deceptive content).\n\n*   **Technical Significance**\n    *   **Advancement of State-of-the-Art**: This tutorial \\cite{liu2024gxh} significantly advances the understanding of LLM misinformation by providing a structured, comprehensive overview of the problem, its causes, and a wide range of technical solutions. It serves as a valuable reference for researchers and practitioners.\n    *   **Potential Impact on Future Research**: By categorizing and detailing existing methods, the paper \\cite{liu2024gxh} helps identify gaps and directs future research efforts towards more effective prevention and detection mechanisms, fostering the development of more reliable and trustworthy LLMs.",
        "keywords": [
          "Large Language Models (LLMs)",
          "LLM-generated misinformation",
          "Hallucinations",
          "Misinformation prevention",
          "Misinformation detection",
          "Comprehensive tutorial",
          "Categorized framework",
          "AI Alignment Training",
          "Retrieval-Augmented Generation (RAG)",
          "Watermarking",
          "Prompt guardrails",
          "Trustworthy LLMs",
          "Challenges and limitations"
        ],
        "paper_type": "based on the abstract and introduction, this paper is a **survey**.\n\nhere's why:\n\n*   **abstract:** explicitly states \"this tutorial covers comprehensive strategies to prevent and detect misinformation generated by llms.\" it then details how it will \"introduce the types of misinformation\" and \"explore two broad categories\" of techniques. this indicates a comprehensive review and organization of existing knowledge.\n*   **introduction (first part):** continues this pattern by listing and categorizing different approaches (\"black-box approaches,\" \"white-box approaches\") and discussing \"challenges and limitations,\" which are common elements of a survey or review.\n*   **keywords from criteria:** the abstract's \"covers comprehensive strategies\" aligns with \"reviews existing literature comprehensively\" and \"comprehensive analysis\" from the `survey` criteria. the discussion of \"types\" and \"categories\" aligns with \"literature organization, classification schemes.\""
      },
      "file_name": "5667f64b23cf48c94ff7413122bc56e5aad7e6a2.pdf"
    },
    {
      "success": true,
      "doc_id": "9eb6e7822d299195530039d4d6c1488a",
      "summary": "Here's a focused summary of the paper \"CHAIN-OF-KNOWLEDGE: GROUNDING LARGE LANGUAGE MODELS VIA DYNAMIC KNOWLEDGE ADAPTING OVER HETEROGENEOUS SOURCES\" \\cite{li2023v3v} for a literature review:\n\n---\n\n*   **Research Problem & Motivation**\n    *   Large Language Models (LLMs) frequently suffer from **hallucination**, generating factually incorrect yet plausible text, especially in knowledge-intensive tasks \\cite{li2023v3v}.\n    *   Effectively updating or controlling the factual knowledge within LLMs remains a significant challenge \\cite{li2023v3v}.\n    *   The problem is important for ensuring the reliability and trustworthiness of LLM outputs in critical applications.\n\n*   **Related Work & Positioning**\n    *   Existing retrieval-augmented LLM methods aim to guide generation with external knowledge (e.g., web documents, knowledge bases) \\cite{li2023v3v}.\n    *   Approaches like Chain-of-Thought (CoT) and Verify-and-Edit (VE) improve reasoning by incorporating retrieval systems \\cite{li2023v3v}.\n    *   **Limitations of previous solutions**:\n        *   **Fixed knowledge sources**: They often use a single, fixed knowledge source, which may not be effective for specialized or domain-specific knowledge \\cite{li2023v3v}.\n        *   **Limited query generation**: Primarily rely on LLMs to generate natural language queries, which are ineffective for structured query languages like SPARQL or SQL \\cite{li2023v3v}.\n        *   **Lack of progressive correction**: Existing methods (e.g., ReAct, VE) lack the ability to progressively correct rationales, leading to error propagation where mistakes in early reasoning steps can misguide subsequent generations \\cite{li2023v3v}.\n\n*   **Technical Approach & Innovation**\n    *   **Chain-of-Knowledge (CoK)** is a novel framework that augments LLMs by dynamically incorporating grounding information from heterogeneous sources \\cite{li2023v3v}.\n    *   **Three-stage framework**:\n        1.  **Reasoning Preparation**: Generates preliminary rationales and answers, and crucially, identifies relevant knowledge domains (e.g., factual, medical, physics, biology). Questions without majority consensus proceed to the next stage \\cite{li2023v3v}.\n        2.  **Dynamic Knowledge Adapting**: This is the core stage where rationales are rectified.\n            *   **Adaptive Query Generator (AQG)**: A versatile component that generates queries tailored to each knowledge source's specific query language (e.g., SPARQL for Wikidata, SQL for tables, natural sentences for Wikipedia/domain-specific texts) \\cite{li2023v3v}. AQG can be a fine-tuned model (e.g., Llama-2-LoRA) or an off-the-shelf LLM (e.g., ChatGPT) \\cite{li2023v3v}.\n            *   **Heterogeneous Knowledge Sources**: Leverages both unstructured (e.g., Wikipedia, domain-specific texts) and structured (e.g., Wikidata, tables) knowledge sources for more reliable factual information \\cite{li2023v3v}.\n            *   **Progressive Rationale Correction**: Rationales are corrected sequentially. The *preceding corrected rationales* are used to generate and correct subsequent rationales, minimizing error propagation \\cite{li2023v3v}.\n        3.  **Answer Consolidation**: The LLM generates a final answer based on the fully corrected chain of rationales \\cite{li2023v3v}.\n    *   **Novelty**: The dynamic selection of knowledge domains, the adaptive generation of diverse query types (structured and unstructured), and the progressive, sequential correction of rationales are key innovations \\cite{li2023v3v}.\n\n*   **Key Technical Contributions**\n    *   Introduction of **Chain-of-Knowledge (CoK)**, a framework for enhancing LLM factual correctness using heterogeneous knowledge sources \\cite{li2023v3v}.\n    *   Proposal of an **Adaptive Query Generator (AQG)** capable of generating queries for various types of query languages (SPARQL, SQL, natural sentences) across different knowledge sources \\cite{li2023v3v}.\n    *   Implementation of a **progressive rationale correction mechanism** to prevent error propagation between reasoning steps \\cite{li2023v3v}.\n    *   Integration of **heterogeneous knowledge sources** (unstructured and structured) for improved factual accuracy and reliability \\cite{li2023v3v}.\n\n*   **Experimental Validation**\n    *   **Experiments conducted**: Extensive experiments on knowledge-intensive tasks across diverse domains \\cite{li2023v3v}.\n    *   **Datasets**: Factual (FEVER, HotpotQA, FeTaQA), Medical (MedMCQA), Physics (MMLU Physics), and Biology (MMLU Biology) \\cite{li2023v3v}.\n    *   **Key performance metrics**: Accuracy (Acc.), Exact Match (E.M.), and BLEU score \\cite{li2023v3v}.\n    *   **Comparison results**: CoK consistently outperforms baseline methods (Standard, CoT, CoT-SC, VE) \\cite{li2023v3v}.\n        *   CoK improves performance over the CoT baseline by **4.3% on average** \\cite{li2023v3v}.\n        *   For example, on FEVER, CoK achieved 63.4% Acc. (3-shot) vs. CoT's 57.8%; on MedMCQA, CoK achieved 70.5% Acc. (3-shot) vs. VE's 67.8% \\cite{li2023v3v}.\n\n*   **Limitations & Scope**\n    *   **Technical limitations**: For domain-specific unstructured knowledge sources, relying solely on black-box LLMs like ChatGPT for AQG may lead to hallucination, necessitating instruction-tuning of smaller models (e.g., LLaMA-2-7B with LoRA) \\cite{li2023v3v}.\n    *   **Scope of applicability**: Primarily focused on knowledge-intensive questions requiring factual grounding and multi-step reasoning across various domains (factual, medical, scientific) \\cite{li2023v3v}.\n\n*   **Technical Significance**\n    *   Advances the technical state-of-the-art in grounding LLMs by providing a robust framework to mitigate hallucination and enhance factual correctness \\cite{li2023v3v}.\n    *   Introduces a flexible and adaptive mechanism for integrating diverse knowledge sources and query languages, addressing a key limitation of prior retrieval-augmented LLMs \\cite{li2023v3v}.\n    *   The progressive correction strategy offers a significant improvement in reasoning reliability by preventing error propagation, which is crucial for complex multi-step reasoning tasks \\cite{li2023v3v}.\n    *   Potential impact on future research includes developing more reliable and trustworthy LLM applications, especially in domains where factual accuracy is paramount (e.g., education, healthcare, scientific research) \\cite{li2023v3v}.",
      "intriguing_abstract": "The pervasive challenge of **hallucination** in Large Language Models (LLMs) severely limits their trustworthiness in knowledge-intensive applications. We introduce **Chain-of-Knowledge (CoK)**, a novel framework that dynamically grounds LLM outputs by adapting to **heterogeneous knowledge sources**. CoK revolutionizes **retrieval-augmentation** through its **Adaptive Query Generator (AQG)**, which intelligently crafts queries in native languages (e.g., **SPARQL, SQL**, or natural sentences) for both structured and unstructured data. Crucially, it employs a **progressive rationale correction** mechanism, iteratively refining reasoning steps to prevent error propagation and ensure robust **factual accuracy**. Evaluated across diverse knowledge domains, CoK consistently outperforms state-of-the-art baselines, achieving an average 4.3% improvement in accuracy. This framework significantly advances the state-of-the-art in **knowledge grounding**, offering a powerful solution to mitigate LLM hallucination and unlock their full potential for reliable **multi-step reasoning** in critical applications.",
      "keywords": [
        "Large Language Models (LLMs)",
        "Hallucination mitigation",
        "Chain-of-Knowledge (CoK)",
        "Dynamic knowledge adapting",
        "Heterogeneous knowledge sources",
        "Adaptive Query Generator (AQG)",
        "Progressive rationale correction",
        "Retrieval-augmented LLMs",
        "Factual correctness enhancement",
        "Knowledge-intensive tasks",
        "Error propagation prevention",
        "Structured query languages",
        "Multi-step reasoning"
      ],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/e468ed6b824e60f45ba9a20b034e4090c6630751.pdf",
      "citation_key": "li2023v3v",
      "metadata": {
        "title": "Chain-of-Knowledge: Grounding Large Language Models via Dynamic Knowledge Adapting over Heterogeneous Sources",
        "authors": [
          "Xingxuan Li",
          "Ruochen Zhao",
          "Yew Ken Chia",
          "Bosheng Ding",
          "Shafiq R. Joty",
          "Soujanya Poria",
          "Lidong Bing"
        ],
        "published_date": "2023",
        "abstract": "We present chain-of-knowledge (CoK), a novel framework that augments large language models (LLMs) by dynamically incorporating grounding information from heterogeneous sources. It results in more factual rationales and reduced hallucination in generation. Specifically, CoK consists of three stages: reasoning preparation, dynamic knowledge adapting, and answer consolidation. Given a knowledge-intensive question, CoK first prepares several preliminary rationales and answers while identifying the relevant knowledge domains. If there is no majority consensus among the answers from samples, CoK corrects the rationales step by step by adapting knowledge from the identified domains. These corrected rationales can plausibly serve as a better foundation for the final answer consolidation. Unlike prior studies that primarily use unstructured data, CoK also leverages structured knowledge sources such as Wikidata and tables that provide more reliable factual information. To access both unstructured and structured knowledge sources in the dynamic knowledge adapting stage, we propose an adaptive query generator that allows the generation of queries for various types of query languages, including SPARQL, SQL, and natural sentences. Moreover, to minimize error propagation between rationales, CoK corrects the rationales progressively using preceding corrected rationales to generate and correct subsequent rationales. Extensive experiments show that CoK consistently improves the performance of LLMs on knowledge-intensive tasks across different domains.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/e468ed6b824e60f45ba9a20b034e4090c6630751.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the paper \"CHAIN-OF-KNOWLEDGE: GROUNDING LARGE LANGUAGE MODELS VIA DYNAMIC KNOWLEDGE ADAPTING OVER HETEROGENEOUS SOURCES\" \\cite{li2023v3v} for a literature review:\n\n---\n\n*   **Research Problem & Motivation**\n    *   Large Language Models (LLMs) frequently suffer from **hallucination**, generating factually incorrect yet plausible text, especially in knowledge-intensive tasks \\cite{li2023v3v}.\n    *   Effectively updating or controlling the factual knowledge within LLMs remains a significant challenge \\cite{li2023v3v}.\n    *   The problem is important for ensuring the reliability and trustworthiness of LLM outputs in critical applications.\n\n*   **Related Work & Positioning**\n    *   Existing retrieval-augmented LLM methods aim to guide generation with external knowledge (e.g., web documents, knowledge bases) \\cite{li2023v3v}.\n    *   Approaches like Chain-of-Thought (CoT) and Verify-and-Edit (VE) improve reasoning by incorporating retrieval systems \\cite{li2023v3v}.\n    *   **Limitations of previous solutions**:\n        *   **Fixed knowledge sources**: They often use a single, fixed knowledge source, which may not be effective for specialized or domain-specific knowledge \\cite{li2023v3v}.\n        *   **Limited query generation**: Primarily rely on LLMs to generate natural language queries, which are ineffective for structured query languages like SPARQL or SQL \\cite{li2023v3v}.\n        *   **Lack of progressive correction**: Existing methods (e.g., ReAct, VE) lack the ability to progressively correct rationales, leading to error propagation where mistakes in early reasoning steps can misguide subsequent generations \\cite{li2023v3v}.\n\n*   **Technical Approach & Innovation**\n    *   **Chain-of-Knowledge (CoK)** is a novel framework that augments LLMs by dynamically incorporating grounding information from heterogeneous sources \\cite{li2023v3v}.\n    *   **Three-stage framework**:\n        1.  **Reasoning Preparation**: Generates preliminary rationales and answers, and crucially, identifies relevant knowledge domains (e.g., factual, medical, physics, biology). Questions without majority consensus proceed to the next stage \\cite{li2023v3v}.\n        2.  **Dynamic Knowledge Adapting**: This is the core stage where rationales are rectified.\n            *   **Adaptive Query Generator (AQG)**: A versatile component that generates queries tailored to each knowledge source's specific query language (e.g., SPARQL for Wikidata, SQL for tables, natural sentences for Wikipedia/domain-specific texts) \\cite{li2023v3v}. AQG can be a fine-tuned model (e.g., Llama-2-LoRA) or an off-the-shelf LLM (e.g., ChatGPT) \\cite{li2023v3v}.\n            *   **Heterogeneous Knowledge Sources**: Leverages both unstructured (e.g., Wikipedia, domain-specific texts) and structured (e.g., Wikidata, tables) knowledge sources for more reliable factual information \\cite{li2023v3v}.\n            *   **Progressive Rationale Correction**: Rationales are corrected sequentially. The *preceding corrected rationales* are used to generate and correct subsequent rationales, minimizing error propagation \\cite{li2023v3v}.\n        3.  **Answer Consolidation**: The LLM generates a final answer based on the fully corrected chain of rationales \\cite{li2023v3v}.\n    *   **Novelty**: The dynamic selection of knowledge domains, the adaptive generation of diverse query types (structured and unstructured), and the progressive, sequential correction of rationales are key innovations \\cite{li2023v3v}.\n\n*   **Key Technical Contributions**\n    *   Introduction of **Chain-of-Knowledge (CoK)**, a framework for enhancing LLM factual correctness using heterogeneous knowledge sources \\cite{li2023v3v}.\n    *   Proposal of an **Adaptive Query Generator (AQG)** capable of generating queries for various types of query languages (SPARQL, SQL, natural sentences) across different knowledge sources \\cite{li2023v3v}.\n    *   Implementation of a **progressive rationale correction mechanism** to prevent error propagation between reasoning steps \\cite{li2023v3v}.\n    *   Integration of **heterogeneous knowledge sources** (unstructured and structured) for improved factual accuracy and reliability \\cite{li2023v3v}.\n\n*   **Experimental Validation**\n    *   **Experiments conducted**: Extensive experiments on knowledge-intensive tasks across diverse domains \\cite{li2023v3v}.\n    *   **Datasets**: Factual (FEVER, HotpotQA, FeTaQA), Medical (MedMCQA), Physics (MMLU Physics), and Biology (MMLU Biology) \\cite{li2023v3v}.\n    *   **Key performance metrics**: Accuracy (Acc.), Exact Match (E.M.), and BLEU score \\cite{li2023v3v}.\n    *   **Comparison results**: CoK consistently outperforms baseline methods (Standard, CoT, CoT-SC, VE) \\cite{li2023v3v}.\n        *   CoK improves performance over the CoT baseline by **4.3% on average** \\cite{li2023v3v}.\n        *   For example, on FEVER, CoK achieved 63.4% Acc. (3-shot) vs. CoT's 57.8%; on MedMCQA, CoK achieved 70.5% Acc. (3-shot) vs. VE's 67.8% \\cite{li2023v3v}.\n\n*   **Limitations & Scope**\n    *   **Technical limitations**: For domain-specific unstructured knowledge sources, relying solely on black-box LLMs like ChatGPT for AQG may lead to hallucination, necessitating instruction-tuning of smaller models (e.g., LLaMA-2-7B with LoRA) \\cite{li2023v3v}.\n    *   **Scope of applicability**: Primarily focused on knowledge-intensive questions requiring factual grounding and multi-step reasoning across various domains (factual, medical, scientific) \\cite{li2023v3v}.\n\n*   **Technical Significance**\n    *   Advances the technical state-of-the-art in grounding LLMs by providing a robust framework to mitigate hallucination and enhance factual correctness \\cite{li2023v3v}.\n    *   Introduces a flexible and adaptive mechanism for integrating diverse knowledge sources and query languages, addressing a key limitation of prior retrieval-augmented LLMs \\cite{li2023v3v}.\n    *   The progressive correction strategy offers a significant improvement in reasoning reliability by preventing error propagation, which is crucial for complex multi-step reasoning tasks \\cite{li2023v3v}.\n    *   Potential impact on future research includes developing more reliable and trustworthy LLM applications, especially in domains where factual accuracy is paramount (e.g., education, healthcare, scientific research) \\cite{li2023v3v}.",
        "keywords": [
          "Large Language Models (LLMs)",
          "Hallucination mitigation",
          "Chain-of-Knowledge (CoK)",
          "Dynamic knowledge adapting",
          "Heterogeneous knowledge sources",
          "Adaptive Query Generator (AQG)",
          "Progressive rationale correction",
          "Retrieval-augmented LLMs",
          "Factual correctness enhancement",
          "Knowledge-intensive tasks",
          "Error propagation prevention",
          "Structured query languages",
          "Multi-step reasoning"
        ],
        "paper_type": "based on the abstract and introduction:\n\n*   the abstract states: \"we present chain-of-knowledge (cok) , a novel framework that augments large language models (llms)...\" and describes its components and how it works (\"cok consists of three stages...\", \"cok first prepares...\", \"cok corrects...\").\n*   the introduction further elaborates on the proposed solution: \"we propose an adaptive query generator...\", \"cok corrects the rationales progressively...\".\n*   it also mentions \"extensive experiments show that cok consistently improves the performance...\", indicating an empirical evaluation of the proposed method.\n\nthis content strongly aligns with the criteria for a **technical** paper, which presents new methods, algorithms, or systems. the empirical evaluation serves to validate the proposed technical contribution.\n\n**classification: technical**"
      },
      "file_name": "e468ed6b824e60f45ba9a20b034e4090c6630751.pdf"
    },
    {
      "success": true,
      "doc_id": "456eb738194abd92831438ceceadf196",
      "summary": "Here's a focused summary of the paper \"THaMES: An End-to-End Tool for Hallucination Mitigation and Evaluation in Large Language Models\" \\cite{liang2024hoo} for a literature review:\n\n---\n\n### Analysis of \"THaMES: An End-to-End Tool for Hallucination Mitigation and Evaluation in Large Language Models\" \\cite{liang2024hoo}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: The paper addresses the pervasive issue of hallucination (generation of factually incorrect or unverifiable content) in Large Language Models (LLMs).\n    *   **Importance and Challenge**: Hallucinations undermine LLM reliability. Existing solutions are often isolated, inadequate for domain-specific use cases, and lack a standardized, end-to-end pipeline combining domain-pertinent dataset generation, multifaceted benchmarking, and adaptive mitigation strategies. Current benchmarks often rely on time-consuming human annotation, lack question complexity, and focus on single evaluation criteria (e.g., identification *or* generation, but not both).\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: While hallucination benchmarks (e.g., HaluEval \\cite{Li et al., 2023}, DelucionQA \\cite{Sadat et al., 2023}) and mitigation strategies (e.g., RAG \\cite{Lewis et al., 2021}, CoVe \\cite{Dhuliawala et al., 2023}) exist, they are largely fragmented.\n    *   **Limitations of Previous Solutions**:\n        *   Existing testsets often rely heavily on human annotation, making them costly and slow to scale.\n        *   They tend to feature simple questions with limited complexity and variety.\n        *   Many frameworks focus on a single hallucination evaluation criterion (e.g., identification *or* generation), failing to provide a holistic assessment of an LLM's robustness.\n        *   No single tool integrates automated testset generation, comprehensive benchmarking, and flexible mitigation strategies into an end-to-end pipeline.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: \\cite{liang2024hoo} proposes THaMES, an end-to-end framework and library for evaluating and mitigating LLM hallucinations. It comprises three main components:\n        1.  **Automated Testset Generation**: Creates synthetic Question-Answer (QA) testsets from any user-provided corpus, including correct and hallucinated answers.\n        2.  **Multifaceted Hallucination Evaluation**: Benchmarks LLMs on their ability to both identify hallucinations and generate less hallucinated outputs across various tasks.\n        3.  **Flexible Mitigation Strategies**: Applies and evaluates optimal hallucination mitigation techniques tailored to specific models and knowledge bases.\n    *   **Novelty/Differentiation**:\n        *   **Automated, Diverse, and Cost-Effective Testset Generation**: Employs weighted random sampling for diverse text block selection, batch generation for cost-effectiveness, and complex question types (simple, reasoning, multi-context, situational, distracting, double) beyond basic QA.\n        *   **Sophisticated Hallucinated Answer Generation**: Uses fine-tuned NLI (deberta-v3-base-tasksource-nli) and hallucination evaluation models (HHEM-2.1-Open) to calculate an \"Ensemble Score\" (Entailment + Factual Consistency) to select the *most distracting* hallucinated answers, improving over random or less interpretable methods.\n        *   **Dual-Criteria Evaluation**: Assesses models on both hallucination *identification* (binary classification) and hallucination *generation* (text generation), providing a more robust measure of LLM performance.\n        *   **Adaptive Mitigation Strategy Selection**: Integrates and evaluates multiple advanced mitigation strategiesâ€”In-Context Learning (ICL) including Chain-of-Verification (CoVe), Retrieval-Augmented Generation (RAG) with few-shot context from failed cases, and Parameter-Efficient Fine-tuning (PEFT) like LoRAâ€”allowing users to select the optimal strategy based on model and task.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**:\n        *   **Weighted Random Sampling**: For knowledge base text node selection, ensuring balanced and diverse context for question generation ($p_i = w_i / \\sum w_j$, where $w_i = 1/(c_i+1)$).\n        *   **Batch Generation Technique**: For questions and answers, reducing cost and improving diversity.\n        *   **Complex Question Type Taxonomy**: Six predefined question types designed for comprehensive hallucination evaluation.\n        *   **Ensemble Score for Hallucinated Answer Selection**: Combines NLI entailment and factual consistency scores to identify the most potent hallucinated answers.\n        *   **RAG with Failed Case Injection**: A novel RAG approach that injects previously failed QA pairs as few-shot context to guide the model.\n    *   **System Design/Architectural Innovations**: An end-to-end, modular framework (THaMES) that seamlessly integrates testset generation, benchmarking, and mitigation, making it domain-flexible and adaptable to various LLMs and knowledge bases.\n    *   **Theoretical Insights/Analysis**: Demonstrates that no single mitigation strategy is universally optimal, highlighting the need for adaptive, model- and task-specific approaches.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**:\n        *   Generated a QA testset of 2,100 data sets (300 for each of six question types) from a knowledge base of academic papers, political news, and Wikipedia articles.\n        *   Evaluated various state-of-the-art LLMs: GPT-4o, GPT-4o-mini, Llama-3.1-8B-Instruct, and Mistral-Nemo.\n        *   Compared baseline performance against performance with ICL (CoVe), RAG, and PEFT (LoRA, applied to Llama-3.1-8B-Instruct only).\n        *   Evaluated models on two tasks: Hallucination Generation (text generation) and Hallucination Identification (binary classification).\n    *   **Key Performance Metrics**:\n        *   **Hallucination Generation**: Answer Faithfulness (AF), Answer Relevancy (AR), Answer Correctness (AC), Answer Similarity (AS) (derived from RAGAS \\cite{Es et al., 2023}).\n        *   **Hallucination Identification**: Accuracy, Precision, Recall, F1-Score (derived from HaluEval \\cite{Li et al., 2023}).\n    *   **Comparison Results**:\n        *   **GPT-4o**: Benefited significantly more from RAG strategies (external knowledge) than ICL (prompt-based reasoning), suggesting high inherent reasoning capabilities. RAG improved AF, AR, AC, and Acc.\n        *   **Llama-3.1-8B-Instruct**: Showed improvements with RAG for hallucination generation, but ICL notably improved its accuracy in detecting hallucinations.\n        *   **PEFT (LoRA on Llama-3.1-8B-Instruct)**: Demonstrated significant improvements over the base model in text generation (AR, AC, AS) and hallucination identification (Recall, F1-score), highlighting its potential despite limited experimentation.\n        *   Overall, mitigation strategies generally improved performance, but the optimal strategy varied significantly across different LLMs.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**:\n        *   **Computational Resources**: Experiments were limited to quantized and smaller-parameter versions of models, potentially constraining the full effectiveness of mitigation methods.\n        *   **Dataset Quality**: Reliance on GPT-4o-mini for dataset generation, while cost-optimized, means dataset quality is bounded by the model's capabilities.\n        *   **Limited PEFT Exploration**: Due to resource constraints, full optimization and exploration of LoRA fine-tuning techniques were not possible.\n    *   **Scope of Applicability**: Currently focused on Question-Answering (QA) tasks. Future work aims to extend to other downstream tasks like text summarization.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: \\cite{liang2024hoo} advances the technical state-of-the-art by providing the first comprehensive, end-to-end, and domain-flexible framework for LLM hallucination analysis, from automated testset generation to adaptive mitigation. It moves beyond isolated solutions by integrating diverse components into a unified pipeline.\n    *   **Potential Impact on Future Research**:\n        *   Establishes a new standard for reliable LLM development and deployment by offering a robust tool for systematic evaluation and mitigation.\n        *   Encourages research into adaptive mitigation strategies tailored to specific LLM architectures and use cases.\n        *   Provides a foundation for developing more cost-effective and high-quality synthetic dataset generation methods.\n        *   Opens avenues for extending hallucination evaluation and mitigation to a broader range of NLP tasks beyond QA.",
      "intriguing_abstract": "The pervasive challenge of hallucination in Large Language Models (LLMs) critically undermines their trustworthiness and utility. We introduce THaMES, an innovative end-to-end framework designed to systematically mitigate and evaluate LLM hallucinations. THaMES revolutionizes the process with automated, cost-effective testset generation from any user-provided corpus, crafting diverse, complex Question-Answering (QA) scenarios and sophisticated hallucinated answers using an Ensemble Score (NLI + Factual Consistency) for maximal distraction. It offers a dual-criteria evaluation, assessing models on both hallucination *identification* and *generation*. Crucially, THaMES integrates and adaptively evaluates advanced mitigation strategies, including In-Context Learning (ICL), Retrieval-Augmented Generation (RAG) with novel failed-case injection, and Parameter-Efficient Fine-tuning (PEFT), demonstrating that optimal approaches are model- and task-specific. By providing a unified, domain-flexible pipeline, THaMES moves beyond fragmented solutions, establishing a new standard for robust LLM development and deployment. This framework empowers researchers and developers to build more reliable and factually grounded LLM applications, accelerating progress in trustworthy AI.",
      "keywords": [
        "LLM Hallucination Mitigation",
        "THaMES",
        "End-to-End Framework",
        "Automated Testset Generation",
        "Multifaceted Hallucination Evaluation",
        "Adaptive Mitigation Strategies",
        "Retrieval-Augmented Generation (RAG)",
        "In-Context Learning (ICL)",
        "Parameter-Efficient Fine-tuning (PEFT)",
        "Sophisticated Hallucinated Answer Generation",
        "Dual-Criteria Evaluation",
        "RAG with Failed Case Injection",
        "Question-Answering (QA) tasks",
        "No universally optimal mitigation strategy"
      ],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/94c81ec4364d63fe67f98098547d0d09f063931d.pdf",
      "citation_key": "liang2024hoo",
      "metadata": {
        "title": "THaMES: An End-to-End Tool for Hallucination Mitigation and Evaluation in Large Language Models",
        "authors": [
          "Mengfei Liang",
          "Archish Arun",
          "Zekun Wu",
          "Cristian Munoz",
          "Jonathan Lutch",
          "Emre Kazim",
          "A. Koshiyama",
          "Philip C. Treleaven"
        ],
        "published_date": "2024",
        "abstract": "Hallucination, the generation of factually incorrect content, is a growing challenge in Large Language Models (LLMs). Existing detection and mitigation methods are often isolated and insufficient for domain-specific needs, lacking a standardized pipeline. This paper introduces THaMES (Tool for Hallucination Mitigations and EvaluationS), an integrated framework and library addressing this gap. THaMES offers an end-to-end solution for evaluating and mitigating hallucinations in LLMs, featuring automated test set generation, multifaceted benchmarking, and adaptable mitigation strategies. It automates test set creation from any corpus, ensuring high data quality, diversity, and cost-efficiency through techniques like batch processing, weighted sampling, and counterfactual validation. THaMES assesses a model's ability to detect and reduce hallucinations across various tasks, including text generation and binary classification, applying optimal mitigation strategies like In-Context Learning (ICL), Retrieval Augmented Generation (RAG), and Parameter-Efficient Fine-tuning (PEFT). Evaluations of state-of-the-art LLMs using a knowledge base of academic papers, political news, and Wikipedia reveal that commercial models like GPT-4o benefit more from RAG than ICL, while open-weight models like Llama-3.1-8B-Instruct and Mistral-Nemo gain more from ICL. Additionally, PEFT significantly enhances the performance of Llama-3.1-8B-Instruct in both evaluation tasks.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/94c81ec4364d63fe67f98098547d0d09f063931d.pdf",
        "venue": "arXiv.org",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the paper \"THaMES: An End-to-End Tool for Hallucination Mitigation and Evaluation in Large Language Models\" \\cite{liang2024hoo} for a literature review:\n\n---\n\n### Analysis of \"THaMES: An End-to-End Tool for Hallucination Mitigation and Evaluation in Large Language Models\" \\cite{liang2024hoo}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: The paper addresses the pervasive issue of hallucination (generation of factually incorrect or unverifiable content) in Large Language Models (LLMs).\n    *   **Importance and Challenge**: Hallucinations undermine LLM reliability. Existing solutions are often isolated, inadequate for domain-specific use cases, and lack a standardized, end-to-end pipeline combining domain-pertinent dataset generation, multifaceted benchmarking, and adaptive mitigation strategies. Current benchmarks often rely on time-consuming human annotation, lack question complexity, and focus on single evaluation criteria (e.g., identification *or* generation, but not both).\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: While hallucination benchmarks (e.g., HaluEval \\cite{Li et al., 2023}, DelucionQA \\cite{Sadat et al., 2023}) and mitigation strategies (e.g., RAG \\cite{Lewis et al., 2021}, CoVe \\cite{Dhuliawala et al., 2023}) exist, they are largely fragmented.\n    *   **Limitations of Previous Solutions**:\n        *   Existing testsets often rely heavily on human annotation, making them costly and slow to scale.\n        *   They tend to feature simple questions with limited complexity and variety.\n        *   Many frameworks focus on a single hallucination evaluation criterion (e.g., identification *or* generation), failing to provide a holistic assessment of an LLM's robustness.\n        *   No single tool integrates automated testset generation, comprehensive benchmarking, and flexible mitigation strategies into an end-to-end pipeline.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: \\cite{liang2024hoo} proposes THaMES, an end-to-end framework and library for evaluating and mitigating LLM hallucinations. It comprises three main components:\n        1.  **Automated Testset Generation**: Creates synthetic Question-Answer (QA) testsets from any user-provided corpus, including correct and hallucinated answers.\n        2.  **Multifaceted Hallucination Evaluation**: Benchmarks LLMs on their ability to both identify hallucinations and generate less hallucinated outputs across various tasks.\n        3.  **Flexible Mitigation Strategies**: Applies and evaluates optimal hallucination mitigation techniques tailored to specific models and knowledge bases.\n    *   **Novelty/Differentiation**:\n        *   **Automated, Diverse, and Cost-Effective Testset Generation**: Employs weighted random sampling for diverse text block selection, batch generation for cost-effectiveness, and complex question types (simple, reasoning, multi-context, situational, distracting, double) beyond basic QA.\n        *   **Sophisticated Hallucinated Answer Generation**: Uses fine-tuned NLI (deberta-v3-base-tasksource-nli) and hallucination evaluation models (HHEM-2.1-Open) to calculate an \"Ensemble Score\" (Entailment + Factual Consistency) to select the *most distracting* hallucinated answers, improving over random or less interpretable methods.\n        *   **Dual-Criteria Evaluation**: Assesses models on both hallucination *identification* (binary classification) and hallucination *generation* (text generation), providing a more robust measure of LLM performance.\n        *   **Adaptive Mitigation Strategy Selection**: Integrates and evaluates multiple advanced mitigation strategiesâ€”In-Context Learning (ICL) including Chain-of-Verification (CoVe), Retrieval-Augmented Generation (RAG) with few-shot context from failed cases, and Parameter-Efficient Fine-tuning (PEFT) like LoRAâ€”allowing users to select the optimal strategy based on model and task.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**:\n        *   **Weighted Random Sampling**: For knowledge base text node selection, ensuring balanced and diverse context for question generation ($p_i = w_i / \\sum w_j$, where $w_i = 1/(c_i+1)$).\n        *   **Batch Generation Technique**: For questions and answers, reducing cost and improving diversity.\n        *   **Complex Question Type Taxonomy**: Six predefined question types designed for comprehensive hallucination evaluation.\n        *   **Ensemble Score for Hallucinated Answer Selection**: Combines NLI entailment and factual consistency scores to identify the most potent hallucinated answers.\n        *   **RAG with Failed Case Injection**: A novel RAG approach that injects previously failed QA pairs as few-shot context to guide the model.\n    *   **System Design/Architectural Innovations**: An end-to-end, modular framework (THaMES) that seamlessly integrates testset generation, benchmarking, and mitigation, making it domain-flexible and adaptable to various LLMs and knowledge bases.\n    *   **Theoretical Insights/Analysis**: Demonstrates that no single mitigation strategy is universally optimal, highlighting the need for adaptive, model- and task-specific approaches.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**:\n        *   Generated a QA testset of 2,100 data sets (300 for each of six question types) from a knowledge base of academic papers, political news, and Wikipedia articles.\n        *   Evaluated various state-of-the-art LLMs: GPT-4o, GPT-4o-mini, Llama-3.1-8B-Instruct, and Mistral-Nemo.\n        *   Compared baseline performance against performance with ICL (CoVe), RAG, and PEFT (LoRA, applied to Llama-3.1-8B-Instruct only).\n        *   Evaluated models on two tasks: Hallucination Generation (text generation) and Hallucination Identification (binary classification).\n    *   **Key Performance Metrics**:\n        *   **Hallucination Generation**: Answer Faithfulness (AF), Answer Relevancy (AR), Answer Correctness (AC), Answer Similarity (AS) (derived from RAGAS \\cite{Es et al., 2023}).\n        *   **Hallucination Identification**: Accuracy, Precision, Recall, F1-Score (derived from HaluEval \\cite{Li et al., 2023}).\n    *   **Comparison Results**:\n        *   **GPT-4o**: Benefited significantly more from RAG strategies (external knowledge) than ICL (prompt-based reasoning), suggesting high inherent reasoning capabilities. RAG improved AF, AR, AC, and Acc.\n        *   **Llama-3.1-8B-Instruct**: Showed improvements with RAG for hallucination generation, but ICL notably improved its accuracy in detecting hallucinations.\n        *   **PEFT (LoRA on Llama-3.1-8B-Instruct)**: Demonstrated significant improvements over the base model in text generation (AR, AC, AS) and hallucination identification (Recall, F1-score), highlighting its potential despite limited experimentation.\n        *   Overall, mitigation strategies generally improved performance, but the optimal strategy varied significantly across different LLMs.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**:\n        *   **Computational Resources**: Experiments were limited to quantized and smaller-parameter versions of models, potentially constraining the full effectiveness of mitigation methods.\n        *   **Dataset Quality**: Reliance on GPT-4o-mini for dataset generation, while cost-optimized, means dataset quality is bounded by the model's capabilities.\n        *   **Limited PEFT Exploration**: Due to resource constraints, full optimization and exploration of LoRA fine-tuning techniques were not possible.\n    *   **Scope of Applicability**: Currently focused on Question-Answering (QA) tasks. Future work aims to extend to other downstream tasks like text summarization.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: \\cite{liang2024hoo} advances the technical state-of-the-art by providing the first comprehensive, end-to-end, and domain-flexible framework for LLM hallucination analysis, from automated testset generation to adaptive mitigation. It moves beyond isolated solutions by integrating diverse components into a unified pipeline.\n    *   **Potential Impact on Future Research**:\n        *   Establishes a new standard for reliable LLM development and deployment by offering a robust tool for systematic evaluation and mitigation.\n        *   Encourages research into adaptive mitigation strategies tailored to specific LLM architectures and use cases.\n        *   Provides a foundation for developing more cost-effective and high-quality synthetic dataset generation methods.\n        *   Opens avenues for extending hallucination evaluation and mitigation to a broader range of NLP tasks beyond QA.",
        "keywords": [
          "LLM Hallucination Mitigation",
          "THaMES",
          "End-to-End Framework",
          "Automated Testset Generation",
          "Multifaceted Hallucination Evaluation",
          "Adaptive Mitigation Strategies",
          "Retrieval-Augmented Generation (RAG)",
          "In-Context Learning (ICL)",
          "Parameter-Efficient Fine-tuning (PEFT)",
          "Sophisticated Hallucinated Answer Generation",
          "Dual-Criteria Evaluation",
          "RAG with Failed Case Injection",
          "Question-Answering (QA) tasks",
          "No universally optimal mitigation strategy"
        ],
        "paper_type": "the paper should be classified as **technical**.\n\nhere's why:\n\n1.  **proposes a new system/tool:** the abstract explicitly states, \"this paper **proposes** the thames framework and library... thames is an end-to-end solution that evaluates and mitigates hallucinations in llms through automated testset generation, multifaceted benchmarking techniques, and flexible mitigation strategies.\" this directly aligns with the \"technical\" criterion of presenting new methods, algorithms, or systems.\n2.  **focus on solution development:** the introduction highlights the lack of a \"standardized pipeline combining the necessary components\" and positions thames as the solution to this technical problem.\n3.  **empirical evaluation as part of validation:** while the paper *does* include empirical findings (\"evaluating a variety of state-of-the-art llms... we find that...\", \"in an experiment... the peft mitigation significantly improved...\"), these experiments serve to validate and demonstrate the effectiveness of the *proposed* thames framework and its integrated strategies. the core contribution is the framework itself."
      },
      "file_name": "94c81ec4364d63fe67f98098547d0d09f063931d.pdf"
    },
    {
      "success": true,
      "doc_id": "e6bbeef1b15e1a221c179244afcbd352",
      "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **CITATION REQUIREMENTS**: Always use \"\\cite{zhang2023k1j}\" when referencing this paper.\n\n---\n\n**TECHNICAL PAPER ANALYSIS**:\n\n1.  **Research Problem & Motivation**\n    *   **What specific technical problem does this paper address?**\n        This paper addresses the pervasive problem of \"hallucination\" in Large Language Models (LLMs), where models generate content that deviates from user input, contradicts previously generated context, or misaligns with established world knowledge \\cite{zhang2023k1j}.\n    *   **Why is this problem important and challenging?**\n        Hallucination significantly undermines the reliability and trustworthiness of LLMs in real-world applications, potentially leading to severe consequences (e.g., fabricating erroneous medical diagnoses). The problem is particularly challenging for LLMs due to:\n        *   **Massive training data:** Pre-training on web-scale data often includes fabricated, outdated, or biased information, making it difficult to eliminate sources of hallucination \\cite{zhang2023k1j}.\n        *   **Versatility of LLMs:** Their general-purpose nature across tasks, languages, and domains complicates comprehensive evaluation and robust mitigation strategies \\cite{zhang2023k1j}.\n        *   **Imperceptibility of errors:** LLMs can generate highly plausible, yet false, information that is difficult for both models and humans to detect \\cite{zhang2023k1j}.\n        *   Additional factors like Reinforcement Learning from Human Feedback (RLHF), vague knowledge boundaries, and the black-box nature of LLMs further complicate detection, explanation, and mitigation \\cite{zhang2023k1j}.\n\n2.  **Related Work & Positioning**\n    *   **How does this work relate to existing approaches?**\n        While hallucination has been extensively studied in conventional Natural Language Generation (NLG) tasks, this survey specifically focuses on the *unique and amplified challenges* presented by LLMs \\cite{zhang2023k1j}. It positions itself as a comprehensive review of recent efforts tailored to the LLM era.\n    *   **What are the limitations of previous solutions?**\n        Previous studies on hallucination in traditional NLG are often task-specific and do not adequately address the complexities arising from LLMs' massive scale, versatility, and the subtle nature of their errors \\cite{zhang2023k1j}. The paper highlights that while input- and context-conflicting hallucinations were previously explored, *fact-conflicting hallucination* in LLMs poses more complex challenges due to the absence of an authoritative knowledge source and its greater practical impact \\cite{zhang2023k1j}.\n\n3.  **Technical Approach & Innovation**\n    *   **What is the core technical method or algorithm?**\n        As a survey, the core technical approach is a systematic and comprehensive review and structured analysis of the current landscape of LLM hallucination research \\cite{zhang2023k1j}. This involves:\n        *   Defining and categorizing LLM hallucination into three distinct types: input-conflicting, context-conflicting, and fact-conflicting \\cite{zhang2023k1j}.\n        *   Presenting taxonomies of hallucination phenomena and evaluation benchmarks.\n        *   Analyzing potential sources of hallucination and reviewing existing mitigation approaches across the LLM lifecycle (pre-training, SFT, RLHF, inference) \\cite{zhang2023k1j}.\n        *   Discussing future research directions.\n    *   **What makes this approach novel or different?**\n        The novelty lies in providing a timely, structured, and LLM-centric overview of hallucination, emphasizing the unique challenges and the predominant research focus on fact-conflicting hallucinations \\cite{zhang2023k1j}. It offers a systematic framework for understanding and addressing this critical problem, clearly distinguishing it from other LLM issues like ambiguity or incompleteness \\cite{zhang2023k1j}.\n\n4.  **Key Technical Contributions**\n    *   **Novel algorithms, methods, or techniques:**\n        *   A clear and expanded **taxonomy of LLM hallucination**, categorizing it into Input-conflicting, Context-conflicting, and Fact-conflicting types, complete with illustrative examples \\cite{zhang2023k1j}.\n        *   An analytical framework that explores the **sources of LLM hallucinations** and systematically reviews **mitigation strategies** throughout the LLM lifecycle (pre-training, supervised finetuning (SFT), reinforcement learning from human feedback (RLHF), and inference) \\cite{zhang2023k1j}.\n        *   Detailed identification and discussion of the **unique challenges** of hallucination in the LLM era, including issues related to massive training data, model versatility, and the imperceptibility of errors \\cite{zhang2023k1j}.\n    *   **System design or architectural innovations:** Not applicable as this is a survey paper.\n    *   **Theoretical insights or analysis:** The paper provides a conceptual framework for understanding the multifaceted nature of LLM hallucination and its distinction from other LLM problems \\cite{zhang2023k1j}. It also highlights the shift in research focus towards fact-conflicting hallucinations due to their inherent complexity and significant practical implications \\cite{zhang2023k1j}.\n\n5.  **Experimental Validation**\n    *   **What experiments were conducted?**\n        As a survey, the paper does not conduct its own experiments.\n    *   **Key performance metrics and comparison results:**\n        The paper *reviews* and *introduces* relevant benchmarks and metrics used in the field for evaluating LLM hallucination \\cite{zhang2023k1j}. It categorizes these benchmarks according to the defined hallucination types, such as Input-conflicting Benchmarks (e.g., BEGIN, QMSum), Context-conflicting Benchmarks (e.g., HADES), and Fact-conflicting Benchmarks (e.g., TruthfulQA, FActScore, HaluEval, FACTOR) \\cite{zhang2023k1j}. This provides a crucial overview of the empirical landscape for future research.\n\n6.  **Limitations & Scope**\n    *   **Technical limitations or assumptions:**\n        The survey primarily concentrates on **fact-conflicting hallucinations**, acknowledging that input- and context-conflicting types have been extensively studied in conventional NLG and are particularly prone to occur in long-context scenarios involving LLMs \\cite{zhang2023k1j}. It also notes the ongoing debate regarding the classification of unverifiable content as hallucination \\cite{zhang2023k1j}.\n    *   **Scope of applicability:**\n        The survey focuses on hallucination specifically within the context of Large Language Models, addressing challenges arising from their scale, versatility, and training paradigms \\cite{zhang2023k1j}. It aims to cover recent publications from the past few years to ensure the timeliness of its insights \\cite{zhang2023k1j}.\n\n7.  **Technical Significance**\n    *   **How does this advance the technical state-of-the-art?**\n        This survey significantly advances the technical state-of-the-art by providing a much-needed, comprehensive, and up-to-date overview of LLM hallucination \\cite{zhang2023k1j}. It synthesizes diverse research efforts, clarifies definitions, and structures the problem space, which is crucial for a rapidly evolving field. By distinguishing hallucination from other LLM issues, it refines the understanding of LLM failures \\cite{zhang2023k1j}.\n    *   **Potential impact on future research:**\n        The paper's taxonomies, analysis of sources and mitigation strategies, and discussion of unique challenges provide a foundational roadmap for future research \\cite{zhang2023k1j}. It explicitly points out future research directions and encourages further discourse on complex issues like unverifiable content, thereby guiding researchers toward critical unsolved problems in improving LLM reliability and trustworthiness \\cite{zhang2023k1j}. The associated open-source materials also facilitate community engagement and reproducibility \\cite{zhang2023k1j}.",
      "intriguing_abstract": "The pervasive problem of **hallucination** remains the Achilles' heel of **Large Language Models (LLMs)**, where models generate plausible but false information, severely compromising their **reliability** and **trustworthiness**. This challenge, amplified by LLMs' massive scale, versatility, and the imperceptibility of errors, demands a systematic understanding.\n\nThis paper presents the first comprehensive and **LLM-centric survey** on hallucination, moving beyond traditional Natural Language Generation (NLG) to address the unique complexities of the modern era. We introduce an expanded **taxonomy**, meticulously categorizing hallucinations into input-conflicting, context-conflicting, and critically, **fact-conflicting** types, which pose the most significant practical implications. Our analytical framework dissects the multifaceted sources of hallucination and systematically reviews cutting-edge **mitigation strategies** across the entire LLM lifecycle, from **pre-training** and **supervised finetuning (SFT)** to **reinforcement learning from human feedback (RLHF)** and **inference**. By synthesizing diverse research efforts and outlining future directions, this work provides a foundational roadmap for enhancing LLM reliability and trustworthiness, serving as an indispensable resource for researchers striving to build more dependable and accurate AI systems.",
      "keywords": [
        "Large Language Models (LLMs)",
        "LLM hallucination",
        "fact-conflicting hallucination",
        "hallucination mitigation strategies",
        "LLM lifecycle",
        "sources of LLM hallucination",
        "LLM reliability and trustworthiness",
        "systematic review",
        "taxonomy of LLM hallucination",
        "hallucination evaluation benchmarks",
        "unique challenges of LLM hallucination",
        "input-conflicting hallucination",
        "context-conflicting hallucination"
      ],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/d00735241af700d21762d2f3ca00d920241a15a4.pdf",
      "citation_key": "zhang2023k1j",
      "metadata": {
        "title": "Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models",
        "authors": [
          "Yue Zhang",
          "Yafu Li",
          "Leyang Cui",
          "Deng Cai",
          "Lemao Liu",
          "Tingchen Fu",
          "Xinting Huang",
          "Enbo Zhao",
          "Yu Zhang",
          "Yulong Chen",
          "Longyue Wang",
          "A. Luu",
          "Wei Bi",
          "Freda Shi",
          "Shuming Shi"
        ],
        "published_date": "2023",
        "abstract": "\n While large language models (LLMs) have demonstrated remarkable capabilities across a range of downstream tasks, a significant concern revolves around their propensity to exhibit hallucinations: LLMs occasionally generate content that diverges from the user input, contradicts previously generated context, or misaligns with established world knowledge. This phenomenon poses a substantial challenge to the reliability of LLMs in real-world scenarios. In this paper, we survey recent efforts on the detection, explanation, and mitigation of hallucination, with an emphasis on the unique challenges posed by LLMs. We present taxonomies of the LLM hallucination phenomena and evaluation benchmarks, analyze existing approaches aiming at mitigating LLM hallucination, and discuss potential directions for future research.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/d00735241af700d21762d2f3ca00d920241a15a4.pdf",
        "venue": "Computational Linguistics",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **CITATION REQUIREMENTS**: Always use \"\\cite{zhang2023k1j}\" when referencing this paper.\n\n---\n\n**TECHNICAL PAPER ANALYSIS**:\n\n1.  **Research Problem & Motivation**\n    *   **What specific technical problem does this paper address?**\n        This paper addresses the pervasive problem of \"hallucination\" in Large Language Models (LLMs), where models generate content that deviates from user input, contradicts previously generated context, or misaligns with established world knowledge \\cite{zhang2023k1j}.\n    *   **Why is this problem important and challenging?**\n        Hallucination significantly undermines the reliability and trustworthiness of LLMs in real-world applications, potentially leading to severe consequences (e.g., fabricating erroneous medical diagnoses). The problem is particularly challenging for LLMs due to:\n        *   **Massive training data:** Pre-training on web-scale data often includes fabricated, outdated, or biased information, making it difficult to eliminate sources of hallucination \\cite{zhang2023k1j}.\n        *   **Versatility of LLMs:** Their general-purpose nature across tasks, languages, and domains complicates comprehensive evaluation and robust mitigation strategies \\cite{zhang2023k1j}.\n        *   **Imperceptibility of errors:** LLMs can generate highly plausible, yet false, information that is difficult for both models and humans to detect \\cite{zhang2023k1j}.\n        *   Additional factors like Reinforcement Learning from Human Feedback (RLHF), vague knowledge boundaries, and the black-box nature of LLMs further complicate detection, explanation, and mitigation \\cite{zhang2023k1j}.\n\n2.  **Related Work & Positioning**\n    *   **How does this work relate to existing approaches?**\n        While hallucination has been extensively studied in conventional Natural Language Generation (NLG) tasks, this survey specifically focuses on the *unique and amplified challenges* presented by LLMs \\cite{zhang2023k1j}. It positions itself as a comprehensive review of recent efforts tailored to the LLM era.\n    *   **What are the limitations of previous solutions?**\n        Previous studies on hallucination in traditional NLG are often task-specific and do not adequately address the complexities arising from LLMs' massive scale, versatility, and the subtle nature of their errors \\cite{zhang2023k1j}. The paper highlights that while input- and context-conflicting hallucinations were previously explored, *fact-conflicting hallucination* in LLMs poses more complex challenges due to the absence of an authoritative knowledge source and its greater practical impact \\cite{zhang2023k1j}.\n\n3.  **Technical Approach & Innovation**\n    *   **What is the core technical method or algorithm?**\n        As a survey, the core technical approach is a systematic and comprehensive review and structured analysis of the current landscape of LLM hallucination research \\cite{zhang2023k1j}. This involves:\n        *   Defining and categorizing LLM hallucination into three distinct types: input-conflicting, context-conflicting, and fact-conflicting \\cite{zhang2023k1j}.\n        *   Presenting taxonomies of hallucination phenomena and evaluation benchmarks.\n        *   Analyzing potential sources of hallucination and reviewing existing mitigation approaches across the LLM lifecycle (pre-training, SFT, RLHF, inference) \\cite{zhang2023k1j}.\n        *   Discussing future research directions.\n    *   **What makes this approach novel or different?**\n        The novelty lies in providing a timely, structured, and LLM-centric overview of hallucination, emphasizing the unique challenges and the predominant research focus on fact-conflicting hallucinations \\cite{zhang2023k1j}. It offers a systematic framework for understanding and addressing this critical problem, clearly distinguishing it from other LLM issues like ambiguity or incompleteness \\cite{zhang2023k1j}.\n\n4.  **Key Technical Contributions**\n    *   **Novel algorithms, methods, or techniques:**\n        *   A clear and expanded **taxonomy of LLM hallucination**, categorizing it into Input-conflicting, Context-conflicting, and Fact-conflicting types, complete with illustrative examples \\cite{zhang2023k1j}.\n        *   An analytical framework that explores the **sources of LLM hallucinations** and systematically reviews **mitigation strategies** throughout the LLM lifecycle (pre-training, supervised finetuning (SFT), reinforcement learning from human feedback (RLHF), and inference) \\cite{zhang2023k1j}.\n        *   Detailed identification and discussion of the **unique challenges** of hallucination in the LLM era, including issues related to massive training data, model versatility, and the imperceptibility of errors \\cite{zhang2023k1j}.\n    *   **System design or architectural innovations:** Not applicable as this is a survey paper.\n    *   **Theoretical insights or analysis:** The paper provides a conceptual framework for understanding the multifaceted nature of LLM hallucination and its distinction from other LLM problems \\cite{zhang2023k1j}. It also highlights the shift in research focus towards fact-conflicting hallucinations due to their inherent complexity and significant practical implications \\cite{zhang2023k1j}.\n\n5.  **Experimental Validation**\n    *   **What experiments were conducted?**\n        As a survey, the paper does not conduct its own experiments.\n    *   **Key performance metrics and comparison results:**\n        The paper *reviews* and *introduces* relevant benchmarks and metrics used in the field for evaluating LLM hallucination \\cite{zhang2023k1j}. It categorizes these benchmarks according to the defined hallucination types, such as Input-conflicting Benchmarks (e.g., BEGIN, QMSum), Context-conflicting Benchmarks (e.g., HADES), and Fact-conflicting Benchmarks (e.g., TruthfulQA, FActScore, HaluEval, FACTOR) \\cite{zhang2023k1j}. This provides a crucial overview of the empirical landscape for future research.\n\n6.  **Limitations & Scope**\n    *   **Technical limitations or assumptions:**\n        The survey primarily concentrates on **fact-conflicting hallucinations**, acknowledging that input- and context-conflicting types have been extensively studied in conventional NLG and are particularly prone to occur in long-context scenarios involving LLMs \\cite{zhang2023k1j}. It also notes the ongoing debate regarding the classification of unverifiable content as hallucination \\cite{zhang2023k1j}.\n    *   **Scope of applicability:**\n        The survey focuses on hallucination specifically within the context of Large Language Models, addressing challenges arising from their scale, versatility, and training paradigms \\cite{zhang2023k1j}. It aims to cover recent publications from the past few years to ensure the timeliness of its insights \\cite{zhang2023k1j}.\n\n7.  **Technical Significance**\n    *   **How does this advance the technical state-of-the-art?**\n        This survey significantly advances the technical state-of-the-art by providing a much-needed, comprehensive, and up-to-date overview of LLM hallucination \\cite{zhang2023k1j}. It synthesizes diverse research efforts, clarifies definitions, and structures the problem space, which is crucial for a rapidly evolving field. By distinguishing hallucination from other LLM issues, it refines the understanding of LLM failures \\cite{zhang2023k1j}.\n    *   **Potential impact on future research:**\n        The paper's taxonomies, analysis of sources and mitigation strategies, and discussion of unique challenges provide a foundational roadmap for future research \\cite{zhang2023k1j}. It explicitly points out future research directions and encourages further discourse on complex issues like unverifiable content, thereby guiding researchers toward critical unsolved problems in improving LLM reliability and trustworthiness \\cite{zhang2023k1j}. The associated open-source materials also facilitate community engagement and reproducibility \\cite{zhang2023k1j}.",
        "keywords": [
          "Large Language Models (LLMs)",
          "LLM hallucination",
          "fact-conflicting hallucination",
          "hallucination mitigation strategies",
          "LLM lifecycle",
          "sources of LLM hallucination",
          "LLM reliability and trustworthiness",
          "systematic review",
          "taxonomy of LLM hallucination",
          "hallucination evaluation benchmarks",
          "unique challenges of LLM hallucination",
          "input-conflicting hallucination",
          "context-conflicting hallucination"
        ],
        "paper_type": "based on the abstract and introduction, this paper is a **survey**.\n\nhere's why:\n*   **title:** explicitly states \"a **survey** on hallucination in large language models\".\n*   **abstract:** uses the word \"**survey**\" (\"we **survey** recent efforts\"), mentions \"present **taxonomies**\", \"**analyze existing approaches**\", and \"discuss potential directions for future research\" â€“ all characteristic of a survey paper that reviews and organizes existing literature.\n*   **introduction:** explicitly refers to itself as \"âˆ—this **survey paper** was completed during...\""
      },
      "file_name": "d00735241af700d21762d2f3ca00d920241a15a4.pdf"
    },
    {
      "success": true,
      "doc_id": "8f7eeaf13fc43fae5bf05b5c15e57a11",
      "summary": "Here's a focused summary of the technical paper \\cite{zhou2024lvp} for a literature review:\n\n### Focused Summary for Literature Review: Mitigating Modality Prior-Induced Hallucinations in MLLMs\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem:** Multimodal Large Language Models (MLLMs) frequently suffer from multimodal hallucinations, which are biases introduced by visual and language priors. These priors negatively impact output quality via the attention mechanism.\n    *   **Importance & Challenge:** Hallucinations degrade MLLM performance and reliability. Existing decoding-based mitigation methods are limited because they focus on statistical correlations and predetermined conclusions, overlooking the crucial causal relationships between attention mechanisms, modality priors, and model output. This prevents them from fully addressing the underlying biases.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches:** The work builds upon advancements in MLLMs and existing training-free hallucination mitigation techniques like Visual Contrastive Decoding (VCD) \\cite{leng2024vcd} and OPERA \\cite{huang2024opera}. It also draws from the field of causal inference applied to LLMs and vision systems.\n    *   **Limitations of Previous Solutions:**\n        *   Current decoding strategies (e.g., VCD) primarily rely on statistical correlations and posterior analysis, failing to systematically study the *causal* relationship between visual attention, language attention, modality priors, and model output.\n        *   This statistical focus limits the model's ability to comprehend underlying dependencies, exacerbating bias and leading to hallucinations.\n        *   Previous causal inference applications in LLMs/vision have not specifically focused on balancing visual and language priors to mitigate hallucinations.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method:** The paper proposes a causal inference framework called **CAUSAL MM** that applies structural causal modeling (SCM) to MLLMs.\n    *   **Novelty/Difference:**\n        *   It treats modality priors (visual and language) as confounding factors in the causal path between attention mechanisms and model output.\n        *   Employs **back-door adjustment** and **counterfactual reasoning** at both the visual and language attention levels.\n        *   This allows for deciphering the causal impact of effective attention on MLLM output by isolating the effects of modality priors.\n        *   Introduces specific counterfactual attention states (Random, Uniform, Reversed, Shuffled) to simulate attention failure and measure causal effects.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods:**\n        *   Construction of a flexible **structural causal framework (CAUSAL MM)** for MLLMs to explicitly model and address visual and language priors.\n        *   Application of **counterfactual reasoning** at both visual encoder attention and LLM backbone attention levels to ensure model output is more aligned with multimodal inputs.\n        *   Formalization of causal effect measurement using **back-door adjustment** to account for confounding factors (image, tokens, visual/language priors).\n        *   Definition of various **counterfactual attention states** (Random, Uniform, Reversed, Shuffled) for intervention.\n    *   **System Design/Architectural Innovations:** The framework is designed as a **plug-and-play solution**, allowing integration with any MLLM and other training-free methods without modifying model weights.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted:** Comprehensive experiments were performed on various benchmarks to evaluate hallucination mitigation and modality prior balancing. Ablation studies were also conducted on different counterfactual attention categories and intervention layers.\n    *   **Key Performance Metrics & Comparison Results:**\n        *   **VLind-Bench:** Designed to measure language priors. CAUSAL MM achieved a maximum score improvement of **65.3%** on 6 VLind-Bench indicators. For LLaVA-1.5, it significantly improved performance in the multimodal collaborative setting, indicating balanced priors. For Qwen2-VL, it improved visual priors, leading to optimal performance in language and multimodal settings.\n        *   **MME Benchmark:** Assesses perception and cognition. CAUSAL MM showed an improvement of **164 points**.\n        *   **POPE (Polling-based Object Probing Evaluation):** Evaluates object-level hallucination (accuracy, precision, recall, F1). CAUSAL MM achieved an average improvement of **5.37%** across three POPE benchmarks.\n        *   **Baselines:** Compared against LLaVa-1.5 and Qwen2-VL (MLLMs), and VCD and OPERA (hallucination mitigation methods). CAUSAL MM consistently outperformed these baselines.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions:** The paper assumes that modality priors act as confounders and that their effects can be isolated and mitigated through causal interventions on attention mechanisms. The effectiveness relies on the chosen counterfactual attention states and the back-door adjustment principle.\n    *   **Scope of Applicability:** The CAUSAL MM framework is a **plug-and-play, training-free solution** applicable to any MLLM, making it broadly usable for enhancing existing models.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art:** This work significantly advances the technical state-of-the-art by introducing a principled **causal inference framework** for hallucination mitigation in MLLMs, moving beyond purely statistical approaches. It provides a deeper understanding of how modality priors causally influence MLLM outputs via attention.\n    *   **Potential Impact:** CAUSAL MM offers a robust and effective method to improve the fidelity and reliability of MLLMs by directly addressing the root cause of modality prior-induced hallucinations. Its plug-and-play nature makes it highly practical for immediate application. It could inspire further research into causal mechanisms within complex AI models and lead to more trustworthy multimodal AI systems.",
      "intriguing_abstract": "Multimodal Large Language Models (MLLMs) are frequently plagued by pervasive hallucinations, often rooted in imbalanced visual and language modality priors that subtly bias their attention mechanisms. Current mitigation strategies, largely statistical, overlook the crucial causal relationships driving these biases. We introduce **CAUSAL MM**, a novel causal inference framework that revolutionizes hallucination mitigation by applying **Structural Causal Modeling (SCM)** to MLLMs.\n\nCAUSAL MM treats modality priors as confounding factors, employing sophisticated **back-door adjustment** and **counterfactual reasoning** directly on both visual encoder and LLM backbone attention. This allows us to precisely decipher the true causal impact of effective attention, isolating and mitigating the detrimental effects of prior-induced biases. As a **plug-and-play, training-free solution**, CAUSAL MM significantly boosts MLLM reliability. Experiments demonstrate remarkable improvements: up to 65.3% on VLind-Bench for language prior balancing, 164 points on MME for perception/cognition, and an average 5.37% accuracy gain on POPE for object-level hallucinations, consistently outperforming state-of-the-art baselines. Our work offers a principled path towards truly trustworthy and robust multimodal AI.",
      "keywords": [
        "Multimodal Large Language Models (MLLMs)",
        "multimodal hallucinations",
        "modality priors",
        "causal inference framework",
        "CAUSAL MM",
        "Structural Causal Modeling (SCM)",
        "back-door adjustment",
        "counterfactual reasoning",
        "attention mechanism intervention",
        "plug-and-play solution",
        "training-free solution",
        "hallucination mitigation",
        "balanced modality priors",
        "MLLM reliability"
      ],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/2635c1aeee582dacb865f00d1289b443c3d96d02.pdf",
      "citation_key": "zhou2024lvp",
      "metadata": {
        "title": "Mitigating Modality Prior-Induced Hallucinations in Multimodal Large Language Models via Deciphering Attention Causality",
        "authors": [
          "Guanyu Zhou",
          "Yibo Yan",
          "Xin Zou",
          "Kun Wang",
          "Aiwei Liu",
          "Xuming Hu"
        ],
        "published_date": "2024",
        "abstract": "Multimodal Large Language Models (MLLMs) have emerged as a central focus in both industry and academia, but often suffer from biases introduced by visual and language priors, which can lead to multimodal hallucination. These biases arise from the visual encoder and the Large Language Model (LLM) backbone, affecting the attention mechanism responsible for aligning multimodal inputs. Existing decoding-based mitigation methods focus on statistical correlations and overlook the causal relationships between attention mechanisms and model output, limiting their effectiveness in addressing these biases. To tackle this issue, we propose a causal inference framework termed CausalMM that applies structural causal modeling to MLLMs, treating modality priors as a confounder between attention mechanisms and output. Specifically, by employing backdoor adjustment and counterfactual reasoning at both the visual and language attention levels, our method mitigates the negative effects of modality priors and enhances the alignment of MLLM's inputs and outputs, with a maximum score improvement of 65.3% on 6 VLind-Bench indicators and 164 points on MME Benchmark compared to conventional methods. Extensive experiments validate the effectiveness of our approach while being a plug-and-play solution. Our code is available at: https://github.com/The-Martyr/CausalMM",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/2635c1aeee582dacb865f00d1289b443c3d96d02.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the technical paper \\cite{zhou2024lvp} for a literature review:\n\n### Focused Summary for Literature Review: Mitigating Modality Prior-Induced Hallucinations in MLLMs\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem:** Multimodal Large Language Models (MLLMs) frequently suffer from multimodal hallucinations, which are biases introduced by visual and language priors. These priors negatively impact output quality via the attention mechanism.\n    *   **Importance & Challenge:** Hallucinations degrade MLLM performance and reliability. Existing decoding-based mitigation methods are limited because they focus on statistical correlations and predetermined conclusions, overlooking the crucial causal relationships between attention mechanisms, modality priors, and model output. This prevents them from fully addressing the underlying biases.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches:** The work builds upon advancements in MLLMs and existing training-free hallucination mitigation techniques like Visual Contrastive Decoding (VCD) \\cite{leng2024vcd} and OPERA \\cite{huang2024opera}. It also draws from the field of causal inference applied to LLMs and vision systems.\n    *   **Limitations of Previous Solutions:**\n        *   Current decoding strategies (e.g., VCD) primarily rely on statistical correlations and posterior analysis, failing to systematically study the *causal* relationship between visual attention, language attention, modality priors, and model output.\n        *   This statistical focus limits the model's ability to comprehend underlying dependencies, exacerbating bias and leading to hallucinations.\n        *   Previous causal inference applications in LLMs/vision have not specifically focused on balancing visual and language priors to mitigate hallucinations.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method:** The paper proposes a causal inference framework called **CAUSAL MM** that applies structural causal modeling (SCM) to MLLMs.\n    *   **Novelty/Difference:**\n        *   It treats modality priors (visual and language) as confounding factors in the causal path between attention mechanisms and model output.\n        *   Employs **back-door adjustment** and **counterfactual reasoning** at both the visual and language attention levels.\n        *   This allows for deciphering the causal impact of effective attention on MLLM output by isolating the effects of modality priors.\n        *   Introduces specific counterfactual attention states (Random, Uniform, Reversed, Shuffled) to simulate attention failure and measure causal effects.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods:**\n        *   Construction of a flexible **structural causal framework (CAUSAL MM)** for MLLMs to explicitly model and address visual and language priors.\n        *   Application of **counterfactual reasoning** at both visual encoder attention and LLM backbone attention levels to ensure model output is more aligned with multimodal inputs.\n        *   Formalization of causal effect measurement using **back-door adjustment** to account for confounding factors (image, tokens, visual/language priors).\n        *   Definition of various **counterfactual attention states** (Random, Uniform, Reversed, Shuffled) for intervention.\n    *   **System Design/Architectural Innovations:** The framework is designed as a **plug-and-play solution**, allowing integration with any MLLM and other training-free methods without modifying model weights.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted:** Comprehensive experiments were performed on various benchmarks to evaluate hallucination mitigation and modality prior balancing. Ablation studies were also conducted on different counterfactual attention categories and intervention layers.\n    *   **Key Performance Metrics & Comparison Results:**\n        *   **VLind-Bench:** Designed to measure language priors. CAUSAL MM achieved a maximum score improvement of **65.3%** on 6 VLind-Bench indicators. For LLaVA-1.5, it significantly improved performance in the multimodal collaborative setting, indicating balanced priors. For Qwen2-VL, it improved visual priors, leading to optimal performance in language and multimodal settings.\n        *   **MME Benchmark:** Assesses perception and cognition. CAUSAL MM showed an improvement of **164 points**.\n        *   **POPE (Polling-based Object Probing Evaluation):** Evaluates object-level hallucination (accuracy, precision, recall, F1). CAUSAL MM achieved an average improvement of **5.37%** across three POPE benchmarks.\n        *   **Baselines:** Compared against LLaVa-1.5 and Qwen2-VL (MLLMs), and VCD and OPERA (hallucination mitigation methods). CAUSAL MM consistently outperformed these baselines.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions:** The paper assumes that modality priors act as confounders and that their effects can be isolated and mitigated through causal interventions on attention mechanisms. The effectiveness relies on the chosen counterfactual attention states and the back-door adjustment principle.\n    *   **Scope of Applicability:** The CAUSAL MM framework is a **plug-and-play, training-free solution** applicable to any MLLM, making it broadly usable for enhancing existing models.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art:** This work significantly advances the technical state-of-the-art by introducing a principled **causal inference framework** for hallucination mitigation in MLLMs, moving beyond purely statistical approaches. It provides a deeper understanding of how modality priors causally influence MLLM outputs via attention.\n    *   **Potential Impact:** CAUSAL MM offers a robust and effective method to improve the fidelity and reliability of MLLMs by directly addressing the root cause of modality prior-induced hallucinations. Its plug-and-play nature makes it highly practical for immediate application. It could inspire further research into causal mechanisms within complex AI models and lead to more trustworthy multimodal AI systems.",
        "keywords": [
          "Multimodal Large Language Models (MLLMs)",
          "multimodal hallucinations",
          "modality priors",
          "causal inference framework",
          "CAUSAL MM",
          "Structural Causal Modeling (SCM)",
          "back-door adjustment",
          "counterfactual reasoning",
          "attention mechanism intervention",
          "plug-and-play solution",
          "training-free solution",
          "hallucination mitigation",
          "balanced modality priors",
          "MLLM reliability"
        ],
        "paper_type": "based on the abstract and introduction:\n\n*   the abstract explicitly states: \"to tackle this issue, **we propose a causal inference framework termed causal mm** that applies structural causal modeling to mllms...\" and describes the technical details of this method (\"employing back-door adjustment and counterfactual reasoning\").\n*   the introduction continues this by detailing the proposed method and mentioning its effectiveness validated by \"extensive experiments\" and \"score improvement\" on benchmarks.\n*   the title itself, \"mitigating modality prior-induced hallucinations... via deciphering attention causality,\" points to a new approach or method.\n\nthis strongly aligns with the criteria for a **technical** paper, which \"presents new methods, algorithms, or systems\" and discusses a \"technical problem, proposed solution.\" while it includes empirical validation, the core contribution is the *proposal of a new method*.\n\n**classification: technical**"
      },
      "file_name": "2635c1aeee582dacb865f00d1289b443c3d96d02.pdf"
    },
    {
      "success": true,
      "doc_id": "42eaa01e7fc259c1cdd06910d7e6bc95",
      "summary": "Here's a focused summary of the paper for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem**: The paper addresses the phenomenon of \"hallucination\" in Multimodal Large Language Models (MLLMs), where models generate text outputs that are inconsistent or factually spurious with the provided visual content \\cite{bai2024tkm}. This is distinct from LLM hallucinations, which focus on factual or faithfulness inconsistencies within text \\cite{bai2024tkm}.\n    *   **Importance and Challenge**: Hallucinations pose significant obstacles to the practical deployment and reliability of MLLMs in real-world applications \\cite{bai2024tkm}. The unique cross-modal nature of MLLM hallucinations means that solutions from pure LLMs cannot be directly transferred, necessitating dedicated research \\cite{bai2024tkm}. The survey primarily focuses on \"object hallucination,\" categorized into category, attribute, and relation inconsistencies \\cite{bai2024tkm}.\n\n*   **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: This work is a comprehensive survey that reviews recent advancements in identifying, evaluating, and mitigating MLLM hallucinations \\cite{bai2024tkm}. It builds upon the general understanding of hallucination from LLMs but highlights the unique challenges in the multimodal context \\cite{bai2024tkm}.\n    *   **Limitations of Previous Solutions**: While surveys exist for LLM hallucinations, they do not adequately cover the cross-modal inconsistencies in MLLMs \\cite{bai2024tkm}. The paper distinguishes itself from a concurrent short survey on LVLM hallucinations by offering a more granular classification, broader scope, and a more intricate linkage between mitigation strategies and underlying causes \\cite{bai2024tkm}.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method**: As a survey, the core \"method\" is a systematic and comprehensive analysis, review, and categorization of the MLLM hallucination landscape \\cite{bai2024tkm}. This involves dissecting underlying causes, evaluation benchmarks, metrics, and mitigation strategies.\n    *   **Novelty/Difference**: The survey's innovation lies in its \"layered and granular classification\" of hallucinations, tracing their origins to specific contributing factors across data, model, training, and inference stages \\cite{bai2024tkm}. It provides a detailed overview of object hallucination types (category, attribute, relation) and links mitigation methods directly to these identified causes, offering a cohesive and targeted approach \\cite{bai2024tkm}.\n\n*   **Key Technical Contributions**\n    *   **Novel Algorithms, Methods, or Techniques**: The primary contribution is a novel, comprehensive taxonomy and landscape of MLLM hallucination, including:\n        *   A detailed classification of hallucination causes (from data, model, training, and inference stages) \\cite{bai2024tkm}.\n        *   A structured overview of evaluation benchmarks (discriminative and generative tasks) and metrics (e.g., CHAIR, POPE, LLM-based) \\cite{bai2024tkm}.\n        *   A categorized review of mitigation methods, intricately linked to their corresponding causes (e.g., data-related, model-related, training-related, inference-related mitigations) \\cite{bai2024tkm}.\n    *   **Theoretical Insights or Analysis**: The paper provides deep insights into the unique origins of hallucinations in MLLMs, distinguishing them from LLMs and offering a structured understanding of their manifestation and potential remedies \\cite{bai2024tkm}.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted**: As a survey paper, it does not conduct new experiments. Instead, it synthesizes and reviews the experimental validation efforts from numerous existing research papers.\n    *   **Key Performance Metrics and Comparison Results**: The survey details various metrics used to quantify hallucination, such as CHAIR \\cite{bai2024tkm}, POPE \\cite{bai2024tkm}, and LLM-based evaluation methods (e.g., GAVIE, HaELM, HallusionBench) \\cite{bai2024tkm}. It also outlines benchmarks like POPE, MME, MMBench for discriminative tasks, and MMHal-Bench, AMBER for generative tasks, which are used by other works to validate their proposed solutions \\cite{bai2024tkm}.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The survey primarily focuses on \"visual-to-text generation\" (multimodal understanding tasks) and specifically on \"object hallucination\" within MLLMs \\cite{bai2024tkm}. It acknowledges that other forms of hallucination or multimodal generation tasks (e.g., text-to-visual) are outside its primary scope.\n    *   **Scope of Applicability**: The insights and categorizations provided are highly applicable to researchers and practitioners working on improving the robustness, reliability, and trustworthiness of MLLMs, particularly in visual question answering and image captioning domains \\cite{bai2024tkm}.\n\n*   **Technical Significance**\n    *   **Advance State-of-the-Art**: This survey significantly advances the technical state-of-the-art by providing the first comprehensive and granular analysis of MLLM hallucinations \\cite{bai2024tkm}. It offers a structured framework for understanding a critical challenge that has emerged with the rapid development of MLLMs.\n    *   **Potential Impact on Future Research**: By delineating the underlying causes, evaluation methodologies, and mitigation strategies, the survey aims to deepen understanding, inspire new ideas, and foster the development of more robust and trustworthy MLLMs \\cite{bai2024tkm}. It explicitly formulates open questions to guide future research directions in this rapidly evolving field \\cite{bai2024tkm}.",
      "intriguing_abstract": "The pervasive issue of \"hallucination\" in Multimodal Large Language Models (MLLMs), where generated text outputs contradict visual inputs, severely impedes their real-world deployment. Unlike unimodal LLM hallucinations, these cross-modal inconsistencies demand dedicated solutions. This paper presents the first comprehensive survey dissecting MLLM hallucinations, with a particular focus on *object hallucination* (category, attribute, and relation inconsistencies). We introduce a novel, layered taxonomy that meticulously classifies hallucination causes across data, model, training, and inference stages. Our work systematically reviews cutting-edge evaluation benchmarks (e.g., CHAIR, POPE, LLM-based metrics) and categorizes diverse mitigation strategies, crucially linking them to their underlying origins. By providing a structured understanding of this critical challenge and outlining open research questions, this survey aims to significantly advance the development of more robust, reliable, and trustworthy MLLMs, paving the way for their responsible integration into practical applications.",
      "keywords": [
        "Multimodal Large Language Models (MLLMs)",
        "MLLM hallucination",
        "object hallucination",
        "cross-modal inconsistencies",
        "hallucination mitigation strategies",
        "hallucination evaluation benchmarks",
        "comprehensive taxonomy",
        "granular classification",
        "underlying causes of hallucination",
        "visual-to-text generation",
        "model robustness and reliability",
        "visual question answering",
        "image captioning",
        "future research directions"
      ],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/c2f3d3e847faf3a8448eabb5bd5fdb6bebbc3a05.pdf",
      "citation_key": "bai2024tkm",
      "metadata": {
        "title": "Hallucination of Multimodal Large Language Models: A Survey",
        "authors": [
          "Zechen Bai",
          "Pichao Wang",
          "Tianjun Xiao",
          "Tong He",
          "Zongbo Han",
          "Zheng Zhang",
          "Mike Zheng Shou"
        ],
        "published_date": "2024",
        "abstract": "This survey presents a comprehensive analysis of the phenomenon of hallucination in multimodal large language models (MLLMs), also known as Large Vision-Language Models (LVLMs), which have demonstrated significant advancements and remarkable abilities in multimodal tasks. Despite these promising developments, MLLMs often generate outputs that are inconsistent with the visual content, a challenge known as hallucination, which poses substantial obstacles to their practical deployment and raises concerns regarding their reliability in real-world applications. This problem has attracted increasing attention, prompting efforts to detect and mitigate such inaccuracies. We review recent advances in identifying, evaluating, and mitigating these hallucinations, offering a detailed overview of the underlying causes, evaluation benchmarks, metrics, and strategies developed to address this issue. Additionally, we analyze the current challenges and limitations, formulating open questions that delineate potential pathways for future research. By drawing the granular classification and landscapes of hallucination causes, evaluation benchmarks, and mitigation methods, this survey aims to deepen the understanding of hallucinations in MLLMs and inspire further advancements in the field. Through our thorough and in-depth review, we contribute to the ongoing dialogue on enhancing the robustness and reliability of MLLMs, providing valuable insights and resources for researchers and practitioners alike. Resources are available at: https://github.com/showlab/Awesome-MLLM-Hallucination.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/c2f3d3e847faf3a8448eabb5bd5fdb6bebbc3a05.pdf",
        "venue": "arXiv.org",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the paper for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem**: The paper addresses the phenomenon of \"hallucination\" in Multimodal Large Language Models (MLLMs), where models generate text outputs that are inconsistent or factually spurious with the provided visual content \\cite{bai2024tkm}. This is distinct from LLM hallucinations, which focus on factual or faithfulness inconsistencies within text \\cite{bai2024tkm}.\n    *   **Importance and Challenge**: Hallucinations pose significant obstacles to the practical deployment and reliability of MLLMs in real-world applications \\cite{bai2024tkm}. The unique cross-modal nature of MLLM hallucinations means that solutions from pure LLMs cannot be directly transferred, necessitating dedicated research \\cite{bai2024tkm}. The survey primarily focuses on \"object hallucination,\" categorized into category, attribute, and relation inconsistencies \\cite{bai2024tkm}.\n\n*   **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: This work is a comprehensive survey that reviews recent advancements in identifying, evaluating, and mitigating MLLM hallucinations \\cite{bai2024tkm}. It builds upon the general understanding of hallucination from LLMs but highlights the unique challenges in the multimodal context \\cite{bai2024tkm}.\n    *   **Limitations of Previous Solutions**: While surveys exist for LLM hallucinations, they do not adequately cover the cross-modal inconsistencies in MLLMs \\cite{bai2024tkm}. The paper distinguishes itself from a concurrent short survey on LVLM hallucinations by offering a more granular classification, broader scope, and a more intricate linkage between mitigation strategies and underlying causes \\cite{bai2024tkm}.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method**: As a survey, the core \"method\" is a systematic and comprehensive analysis, review, and categorization of the MLLM hallucination landscape \\cite{bai2024tkm}. This involves dissecting underlying causes, evaluation benchmarks, metrics, and mitigation strategies.\n    *   **Novelty/Difference**: The survey's innovation lies in its \"layered and granular classification\" of hallucinations, tracing their origins to specific contributing factors across data, model, training, and inference stages \\cite{bai2024tkm}. It provides a detailed overview of object hallucination types (category, attribute, relation) and links mitigation methods directly to these identified causes, offering a cohesive and targeted approach \\cite{bai2024tkm}.\n\n*   **Key Technical Contributions**\n    *   **Novel Algorithms, Methods, or Techniques**: The primary contribution is a novel, comprehensive taxonomy and landscape of MLLM hallucination, including:\n        *   A detailed classification of hallucination causes (from data, model, training, and inference stages) \\cite{bai2024tkm}.\n        *   A structured overview of evaluation benchmarks (discriminative and generative tasks) and metrics (e.g., CHAIR, POPE, LLM-based) \\cite{bai2024tkm}.\n        *   A categorized review of mitigation methods, intricately linked to their corresponding causes (e.g., data-related, model-related, training-related, inference-related mitigations) \\cite{bai2024tkm}.\n    *   **Theoretical Insights or Analysis**: The paper provides deep insights into the unique origins of hallucinations in MLLMs, distinguishing them from LLMs and offering a structured understanding of their manifestation and potential remedies \\cite{bai2024tkm}.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted**: As a survey paper, it does not conduct new experiments. Instead, it synthesizes and reviews the experimental validation efforts from numerous existing research papers.\n    *   **Key Performance Metrics and Comparison Results**: The survey details various metrics used to quantify hallucination, such as CHAIR \\cite{bai2024tkm}, POPE \\cite{bai2024tkm}, and LLM-based evaluation methods (e.g., GAVIE, HaELM, HallusionBench) \\cite{bai2024tkm}. It also outlines benchmarks like POPE, MME, MMBench for discriminative tasks, and MMHal-Bench, AMBER for generative tasks, which are used by other works to validate their proposed solutions \\cite{bai2024tkm}.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The survey primarily focuses on \"visual-to-text generation\" (multimodal understanding tasks) and specifically on \"object hallucination\" within MLLMs \\cite{bai2024tkm}. It acknowledges that other forms of hallucination or multimodal generation tasks (e.g., text-to-visual) are outside its primary scope.\n    *   **Scope of Applicability**: The insights and categorizations provided are highly applicable to researchers and practitioners working on improving the robustness, reliability, and trustworthiness of MLLMs, particularly in visual question answering and image captioning domains \\cite{bai2024tkm}.\n\n*   **Technical Significance**\n    *   **Advance State-of-the-Art**: This survey significantly advances the technical state-of-the-art by providing the first comprehensive and granular analysis of MLLM hallucinations \\cite{bai2024tkm}. It offers a structured framework for understanding a critical challenge that has emerged with the rapid development of MLLMs.\n    *   **Potential Impact on Future Research**: By delineating the underlying causes, evaluation methodologies, and mitigation strategies, the survey aims to deepen understanding, inspire new ideas, and foster the development of more robust and trustworthy MLLMs \\cite{bai2024tkm}. It explicitly formulates open questions to guide future research directions in this rapidly evolving field \\cite{bai2024tkm}.",
        "keywords": [
          "Multimodal Large Language Models (MLLMs)",
          "MLLM hallucination",
          "object hallucination",
          "cross-modal inconsistencies",
          "hallucination mitigation strategies",
          "hallucination evaluation benchmarks",
          "comprehensive taxonomy",
          "granular classification",
          "underlying causes of hallucination",
          "visual-to-text generation",
          "model robustness and reliability",
          "visual question answering",
          "image captioning",
          "future research directions"
        ],
        "paper_type": "**survey**\n\n**reasoning:**\n1.  **title:** the title \"hallucination of multimodal large language models: a survey\" explicitly uses the keyword \"survey,\" which is a direct match for the \"survey\" classification criterion.\n2.  **introduction:** the introduction discusses the emergence of llms and mllms, identifies a \"concerning trend\" (hallucinations), and proceeds to categorize and define the problem of hallucination (factuality vs. faithfulness). this approach of defining, categorizing, and setting the scope of a problem is characteristic of a survey paper that aims to review and organize existing knowledge in a field."
      },
      "file_name": "c2f3d3e847faf3a8448eabb5bd5fdb6bebbc3a05.pdf"
    },
    {
      "success": true,
      "doc_id": "65f02f0914b3f13fcd6ad595c7fa5b92",
      "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Problem**: Multimodal Large Language Models (MLLMs) frequently suffer from \"hallucinations,\" where generated text descriptions are inconsistent with the image content. These hallucinations can be object-level (claiming non-existent objects) or attribute-level (incorrectly describing object attributes).\n    *   **Importance & Challenge**: Hallucinations are significant obstacles to the practical application and reliability of MLLMs. Existing mitigation strategies primarily rely on instruction-tuning, which is data-intensive, computationally expensive, and can sometimes sacrifice detail or generative efficiency \\cite{yin2023hx3}.\n\n*   **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: Previous work on hallucination mitigation (e.g., LRV-Instruction, VIGC) focuses on *preventing* hallucinations during the MLLM's generation process, typically through instruction-tuning and data optimization. `\\cite{yin2023hx3}` takes a different, *corrective* approach, refining MLLM responses *after* generation.\n    *   **Limitations of Previous Solutions**: Instruction-tuning methods are data- and computation-intensive, making them less flexible and harder to adapt to various MLLMs. They also often involve compromises, such as limiting text length (sacrificing detail) or using multi-step generation (sacrificing efficiency) \\cite{yin2023hx3}.\n    *   **Positioning**: `\\cite{yin2023hx3}` is positioned as a training-free, post-remedy framework that can be easily integrated as a plug-and-play module with different MLLMs. It extends the idea of knowledge-augmented LLMs to the vision-language domain by constructing a structured visual knowledge base and leverages LLM-aided visual reasoning for various subtasks \\cite{yin2023hx3}.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method**: `\\cite{yin2023hx3}` introduces Woodpecker, a training-free, five-stage pipeline for hallucination correction:\n        1.  **Key Concept Extraction**: An LLM (GPT-3.5-turbo) identifies main objects from the MLLM's initial response.\n        2.  **Question Formulation**: An LLM (GPT-3.5-turbo) generates object-level (existence, count) and attribute-level questions around the extracted concepts.\n        3.  **Visual Knowledge Validation**: Expert models answer the formulated questions: an open-set object detector (Grounding DINO) for object existence and counts, and a pre-trained VQA model (BLIP-2-FlanT5 XXL) for attribute-level questions.\n        4.  **Visual Claim Generation**: The validated QA pairs are combined into a structured visual knowledge base, comprising object-level and attribute-level claims about the image.\n        5.  **Hallucination Correction**: An LLM (GPT-3.5-turbo) uses this visual knowledge base to correct the original MLLM response, explicitly adding bounding box evidence for interpretability and fact-checking \\cite{yin2023hx3}.\n    *   **Novelty**: The approach is novel due to its training-free, post-remedy paradigm for hallucination correction, which contrasts with existing generative prevention methods. It offers high interpretability through its clear, transparent pipeline and the explicit inclusion of visual grounding (bounding boxes) in the corrected output \\cite{yin2023hx3}.\n\n*   **Key Technical Contributions**\n    *   **Novel Framework**: Proposes the first training-free, corrective framework (Woodpecker) to mitigate visual hallucinations in MLLMs, operating in a post-remedy manner \\cite{yin2023hx3}.\n    *   **System Design & Interpretability**: Designs a modular and transparent pipeline with five distinct stages, allowing for clear diagnosis and correction. The integration of bounding boxes directly into the corrected text enhances interpretability and facilitates visual fact-checking \\cite{yin2023hx3}.\n    *   **Leveraging Expert Models**: Effectively combines the reasoning capabilities of LLMs with the strong perception capabilities of specialized vision foundation models (e.g., open-set object detectors, VQA models) to validate visual facts \\cite{yin2023hx3}.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted**: `\\cite{yin2023hx3}` evaluates Woodpecker quantitatively and qualitatively on benchmarks designed for MLLM hallucination.\n    *   **Datasets**: POPE `\\cite{yin2023hx3}` (object-level hallucination, \"Yes-or-No\" questions across random, popular, adversarial settings), MME `\\cite{yin2023hx3}` (object-level and attribute-level hallucination, \"Yes-or-No\" questions), and LLaVA-QA90 `\\cite{yin2023hx3}` (description-type queries, evaluated by GPT-4V for accuracy and detailedness).\n    *   **Key Performance Metrics**: Accuracy, Precision, Recall, F1-Score, and Yes Rate for POPE; a combined score (accuracy + accuracy+) for MME. For LLaVA-QA90, GPT-4V ratings on accuracy and detailedness.\n    *   **Comparison Results**: Woodpecker significantly boosts the accuracy of baseline MLLMs. For instance, on the POPE benchmark (random setting), it improved MiniGPT-4's accuracy from 54.67% to 85.33% (a 30.66% gain) and mPLUG-Owl's accuracy from 62.00% to 86.33% (a 24.33% gain) \\cite{yin2023hx3}. Similar improvements were observed across other baselines (LLaVA, Otter) and settings.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations**: The performance of Woodpecker is inherently dependent on the accuracy and capabilities of the underlying expert models (GPT-3.5-turbo, Grounding DINO, BLIP-2-FlanT5 XXL) used in its pipeline.\n    *   **Scope of Applicability**: Woodpecker is designed as a post-remedy solution, correcting hallucinations *after* an MLLM has generated its response, rather than preventing them during generation. It primarily addresses visual hallucinations (object-level and attribute-level) in descriptive text \\cite{yin2023hx3}.\n\n*   **Technical Significance**\n    *   **Advancement of State-of-the-Art**: `\\cite{yin2023hx3}` introduces a novel, training-free paradigm for hallucination correction, offering a flexible and generalizable solution that can be applied to various MLLMs without retraining. This significantly advances the state-of-the-art beyond data-intensive instruction-tuning methods \\cite{yin2023hx3}.\n    *   **Potential Impact**: By substantially improving the factual consistency and interpretability of MLLM outputs, Woodpecker enhances the reliability of MLLMs for practical applications. Its modular and transparent design, coupled with explicit visual grounding, paves the way for future research into more robust and verifiable multimodal AI systems \\cite{yin2023hx3}.",
      "intriguing_abstract": "Multimodal Large Language Models (MLLMs) are transforming AI, yet their practical deployment is severely hindered by persistent \"hallucinations\"â€”generating text descriptions inconsistent with visual content. Current mitigation strategies, primarily instruction-tuning, are computationally intensive and often compromise detail or generative efficiency. We introduce Woodpecker, a groundbreaking **training-free, post-remedy framework** that fundamentally shifts the paradigm from hallucination prevention to correction.\n\nWoodpecker employs a novel five-stage pipeline, synergistically combining the reasoning power of **Large Language Models (LLMs)** with the precise perception of **expert vision models** like **open-set object detectors** and **Visual Question Answering (VQA)** systems. It constructs a structured **visual knowledge base** from validated facts, then leverages this to meticulously refine initial MLLM responses. Crucially, Woodpecker enhances **interpretability** by integrating explicit **visual grounding** (bounding boxes) into the corrected output, enabling direct fact-checking. Our experiments demonstrate Woodpecker's remarkable ability to significantly boost the **factual consistency** and accuracy of various MLLMs across challenging benchmarks, offering substantial gains (e.g., 30.66% accuracy improvement on POPE). This plug-and-play solution paves the way for more reliable, verifiable, and trustworthy multimodal AI systems.",
      "keywords": [
        "Multimodal Large Language Models (MLLMs)",
        "Visual Hallucinations",
        "Hallucination Correction",
        "Woodpecker Framework",
        "Training-Free Post-Remedy",
        "Visual Knowledge Base",
        "LLM-Aided Visual Reasoning",
        "Expert Models",
        "Interpretability",
        "Visual Grounding (Bounding Boxes)",
        "Factual Consistency",
        "Modular Pipeline"
      ],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/807f336176070bd3f95b82a16f125ee99b7d2c80.pdf",
      "citation_key": "yin2023hx3",
      "metadata": {
        "title": "Woodpecker: Hallucination Correction for Multimodal Large Language Models",
        "authors": [
          "Shukang Yin",
          "Chaoyou Fu",
          "Sirui Zhao",
          "Tong Xu",
          "Hao Wang",
          "Dianbo Sui",
          "Yunhang Shen",
          "Ke Li",
          "Xingguo Sun",
          "Enhong Chen"
        ],
        "published_date": "2023",
        "abstract": "Hallucination is a big shadow hanging over the rapidly evolving Multimodal Large Language Models (MLLMs), referring to the phenomenon that the generated text is inconsistent with the image content. In order to mitigate hallucinations, existing studies mainly resort to an instruction-tuning manner that requires retraining the models with specific data. In this paper, we pave a different way, introducing a training-free method named Woodpecker. Like a woodpecker heals trees, it picks out and corrects hallucinations from the generated text. Concretely, Woodpecker consists of five stages: key concept extraction, question formulation, visual knowledge validation, visual claim generation, and hallucination correction. Implemented in a post-remedy manner, Woodpecker can easily serve different MLLMs, while being interpretable by accessing intermediate outputs of the five stages. We evaluate Woodpecker both quantitatively and qualitatively and show the huge potential of this new paradigm. On the POPE benchmark, our method obtains a 30.66%/24.33% improvement in accuracy over the baseline MiniGPT-4/mPLUG-Owl. The source code is released at https://github.com/BradyFU/Woodpecker.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/807f336176070bd3f95b82a16f125ee99b7d2c80.pdf",
        "venue": "Science China Information Sciences",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Problem**: Multimodal Large Language Models (MLLMs) frequently suffer from \"hallucinations,\" where generated text descriptions are inconsistent with the image content. These hallucinations can be object-level (claiming non-existent objects) or attribute-level (incorrectly describing object attributes).\n    *   **Importance & Challenge**: Hallucinations are significant obstacles to the practical application and reliability of MLLMs. Existing mitigation strategies primarily rely on instruction-tuning, which is data-intensive, computationally expensive, and can sometimes sacrifice detail or generative efficiency \\cite{yin2023hx3}.\n\n*   **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: Previous work on hallucination mitigation (e.g., LRV-Instruction, VIGC) focuses on *preventing* hallucinations during the MLLM's generation process, typically through instruction-tuning and data optimization. `\\cite{yin2023hx3}` takes a different, *corrective* approach, refining MLLM responses *after* generation.\n    *   **Limitations of Previous Solutions**: Instruction-tuning methods are data- and computation-intensive, making them less flexible and harder to adapt to various MLLMs. They also often involve compromises, such as limiting text length (sacrificing detail) or using multi-step generation (sacrificing efficiency) \\cite{yin2023hx3}.\n    *   **Positioning**: `\\cite{yin2023hx3}` is positioned as a training-free, post-remedy framework that can be easily integrated as a plug-and-play module with different MLLMs. It extends the idea of knowledge-augmented LLMs to the vision-language domain by constructing a structured visual knowledge base and leverages LLM-aided visual reasoning for various subtasks \\cite{yin2023hx3}.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method**: `\\cite{yin2023hx3}` introduces Woodpecker, a training-free, five-stage pipeline for hallucination correction:\n        1.  **Key Concept Extraction**: An LLM (GPT-3.5-turbo) identifies main objects from the MLLM's initial response.\n        2.  **Question Formulation**: An LLM (GPT-3.5-turbo) generates object-level (existence, count) and attribute-level questions around the extracted concepts.\n        3.  **Visual Knowledge Validation**: Expert models answer the formulated questions: an open-set object detector (Grounding DINO) for object existence and counts, and a pre-trained VQA model (BLIP-2-FlanT5 XXL) for attribute-level questions.\n        4.  **Visual Claim Generation**: The validated QA pairs are combined into a structured visual knowledge base, comprising object-level and attribute-level claims about the image.\n        5.  **Hallucination Correction**: An LLM (GPT-3.5-turbo) uses this visual knowledge base to correct the original MLLM response, explicitly adding bounding box evidence for interpretability and fact-checking \\cite{yin2023hx3}.\n    *   **Novelty**: The approach is novel due to its training-free, post-remedy paradigm for hallucination correction, which contrasts with existing generative prevention methods. It offers high interpretability through its clear, transparent pipeline and the explicit inclusion of visual grounding (bounding boxes) in the corrected output \\cite{yin2023hx3}.\n\n*   **Key Technical Contributions**\n    *   **Novel Framework**: Proposes the first training-free, corrective framework (Woodpecker) to mitigate visual hallucinations in MLLMs, operating in a post-remedy manner \\cite{yin2023hx3}.\n    *   **System Design & Interpretability**: Designs a modular and transparent pipeline with five distinct stages, allowing for clear diagnosis and correction. The integration of bounding boxes directly into the corrected text enhances interpretability and facilitates visual fact-checking \\cite{yin2023hx3}.\n    *   **Leveraging Expert Models**: Effectively combines the reasoning capabilities of LLMs with the strong perception capabilities of specialized vision foundation models (e.g., open-set object detectors, VQA models) to validate visual facts \\cite{yin2023hx3}.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted**: `\\cite{yin2023hx3}` evaluates Woodpecker quantitatively and qualitatively on benchmarks designed for MLLM hallucination.\n    *   **Datasets**: POPE `\\cite{yin2023hx3}` (object-level hallucination, \"Yes-or-No\" questions across random, popular, adversarial settings), MME `\\cite{yin2023hx3}` (object-level and attribute-level hallucination, \"Yes-or-No\" questions), and LLaVA-QA90 `\\cite{yin2023hx3}` (description-type queries, evaluated by GPT-4V for accuracy and detailedness).\n    *   **Key Performance Metrics**: Accuracy, Precision, Recall, F1-Score, and Yes Rate for POPE; a combined score (accuracy + accuracy+) for MME. For LLaVA-QA90, GPT-4V ratings on accuracy and detailedness.\n    *   **Comparison Results**: Woodpecker significantly boosts the accuracy of baseline MLLMs. For instance, on the POPE benchmark (random setting), it improved MiniGPT-4's accuracy from 54.67% to 85.33% (a 30.66% gain) and mPLUG-Owl's accuracy from 62.00% to 86.33% (a 24.33% gain) \\cite{yin2023hx3}. Similar improvements were observed across other baselines (LLaVA, Otter) and settings.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations**: The performance of Woodpecker is inherently dependent on the accuracy and capabilities of the underlying expert models (GPT-3.5-turbo, Grounding DINO, BLIP-2-FlanT5 XXL) used in its pipeline.\n    *   **Scope of Applicability**: Woodpecker is designed as a post-remedy solution, correcting hallucinations *after* an MLLM has generated its response, rather than preventing them during generation. It primarily addresses visual hallucinations (object-level and attribute-level) in descriptive text \\cite{yin2023hx3}.\n\n*   **Technical Significance**\n    *   **Advancement of State-of-the-Art**: `\\cite{yin2023hx3}` introduces a novel, training-free paradigm for hallucination correction, offering a flexible and generalizable solution that can be applied to various MLLMs without retraining. This significantly advances the state-of-the-art beyond data-intensive instruction-tuning methods \\cite{yin2023hx3}.\n    *   **Potential Impact**: By substantially improving the factual consistency and interpretability of MLLM outputs, Woodpecker enhances the reliability of MLLMs for practical applications. Its modular and transparent design, coupled with explicit visual grounding, paves the way for future research into more robust and verifiable multimodal AI systems \\cite{yin2023hx3}.",
        "keywords": [
          "Multimodal Large Language Models (MLLMs)",
          "Visual Hallucinations",
          "Hallucination Correction",
          "Woodpecker Framework",
          "Training-Free Post-Remedy",
          "Visual Knowledge Base",
          "LLM-Aided Visual Reasoning",
          "Expert Models",
          "Interpretability",
          "Visual Grounding (Bounding Boxes)",
          "Factual Consistency",
          "Modular Pipeline"
        ],
        "paper_type": "based on the abstract and introduction:\n\n1.  **new method/system:** the abstract explicitly states, \"we pave a different way, introducing a training-free method named woodpecker.\" it then describes the components of this method: \"woodpecker consists of five stages: key concept extraction, question formulation, visual knowledge validation, visual claim generation, and hallucination correction.\" this clearly indicates the presentation of a new system or method.\n2.  **problem and solution:** the introduction sets up a technical problem (\"hallucination is a big shadow hanging over the rapidly evolving multimodal large language models\") and the abstract presents \"woodpecker\" as the proposed solution.\n3.  **empirical evaluation:** while the paper also includes strong empirical evidence (\"we evaluate woodpecker both quantitatively and qualitatively... on the pope benchmark, our method obtains a 30.66%/24.33% improvement\"), this evaluation serves to validate the effectiveness of the *new method* being proposed. the primary contribution is the method itself.\n\ntherefore, the paper's core contribution is the development and presentation of a new method/system.\n\n**classification:** **technical**"
      },
      "file_name": "807f336176070bd3f95b82a16f125ee99b7d2c80.pdf"
    },
    {
      "success": true,
      "doc_id": "82b8ffaa9f96e341b420d0d7878b044a",
      "summary": "Here's a focused summary of the paper for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Specific technical problem**: The paper addresses the challenge of detecting non-factual or hallucinatory content generated by Large Language Models (LLMs) \\cite{cao2023ecl}.\n    *   **Why important and challenging**: LLMs frequently generate inaccurate or fabricated information (\"hallucinations\"), which significantly impacts their reliability, enterprise security, and user trust \\cite{cao2023ecl}. A major challenge in hallucination detection is the laborious, time-consuming, and expensive nature of manual annotation for creating model-specific hallucination datasets. These datasets are also time-sensitive, becoming obsolete with LLM upgrades \\cite{cao2023ecl}.\n\n*   **Related Work & Positioning**\n    *   **Relation to existing approaches**: Prior work on hallucination detection either leverages external knowledge sources (e.g., CRITIC \\cite{gou2023critic}, search engines \\cite{chern2023invoking}) or adopts zero-resource approaches focusing on inherent model resources (e.g., Chain of Thoughts \\cite{xue2023hallucination}, sampling-based methods like SelfCheckGPT \\cite{manakul2023selfcheckgpt}). Various hallucination benchmarks also exist, often relying on human annotation \\cite{li2023hallucination, umapathi2023hallucination, dale2023human}.\n    *   **Limitations of previous solutions**: Existing methods are often constrained by the need for manually annotated, model-specific, and time-sensitive datasets, which are expensive and do not scale well with frequent model updates. Furthermore, there is still significant room for improvement in the performance of current hallucination detection techniques \\cite{cao2023ecl}.\n\n*   **Technical Approach & Innovation**\n    *   **Core technical method or algorithm**:\n        *   **AutoHall (Automated Hallucination Dataset Generation)**: A three-step pipeline to automatically construct model-specific hallucination datasets from existing fact-checking datasets (which contain claims with ground truth labels and evidence).\n            1.  **References Generation**: An LLM is prompted to generate references for claims.\n            2.  **Claim Classification**: The LLM is then prompted to classify whether each generated reference supports or refutes the original claim.\n            3.  **Hallucination Collection**: If the LLM's classification contradicts the ground truth label of the claim, the generated reference is labeled as hallucinatory \\cite{cao2023ecl}.\n        *   **Zero-Resource, Black-Box Hallucination Detection**: A method based on self-contradiction. For a given claim and an LLM's initial response, the LLM is independently queried multiple times (K times) to generate additional references. The LLM then detects contradictions between the original response and each of the K sampled references. If any contradiction is found, the original response is flagged as hallucinatory \\cite{cao2023ecl}.\n    *   **What makes this approach novel or different**:\n        *   AutoHall innovates by automating the creation of hallucination datasets, eliminating the need for costly and time-consuming manual annotation and addressing the issue of dataset obsolescence with model upgrades \\cite{cao2023ecl}.\n        *   The detection method is novel in its end-to-end use of the LLM for self-contradiction detection, directly comparing the original response with independently sampled ones. This differs from methods like SelfCheckGPT \\cite{manakul2023selfcheckgpt} which rely on token probabilities and external metrics (e.g., BERTScore, n-gram) and may incorrectly attribute conflicts \\cite{cao2023ecl}.\n\n*   **Key Technical Contributions**\n    *   **Novel algorithms, methods, or techniques**:\n        *   The AutoHall pipeline for automatically generating model-specific hallucination datasets \\cite{cao2023ecl}.\n        *   A novel zero-resource, black-box hallucination detection method leveraging LLM-driven self-contradiction analysis \\cite{cao2023ecl}.\n    *   **System design or architectural innovations**: The structured three-step processes for both dataset generation and hallucination detection, enabling efficient and automated workflows \\cite{cao2023ecl}.\n    *   **Theoretical insights or analysis**: The core rationale that an LLM's consistent understanding of a claim should lead to non-contradictory independently generated references; any contradiction signals hallucination \\cite{cao2023ecl}.\n\n*   **Experimental Validation**\n    *   **Experiments conducted**: Experiments were conducted to evaluate the effectiveness of the AutoHall-generated datasets and the proposed self-contradiction-based hallucination detection method. The study also analyzed variations in hallucination proportions and types across different LLMs and temperatures \\cite{cao2023ecl}.\n    *   **Key performance metrics and comparison results**:\n        *   **Models**: ChatGPT (gpt-3.5-turbo) and Llama 2-Chat (7B & 13B parameters) were evaluated \\cite{cao2023ecl}.\n        *   **Datasets**: Three fact-checking datasets (Climate-fever \\cite{diggelmann2020climate}, PUBHEALTH \\cite{kotonya2020pubhealth}, WICE \\cite{kamoi2023wice}) were used for dataset generation and evaluation \\cite{cao2023ecl}.\n        *   **Metrics**: Accuracy (Acc) and F1-score were used to measure detection performance \\cite{cao2023ecl}.\n        *   **Comparison**: The proposed method consistently achieved superior hallucination detection performance (higher Acc and F1-score) compared to existing baselines (Zero-SelfCk, Few-SelfCk, SelfCk-1gm) across all tested LLMs and temperature settings (0.1, 0.5, 0.9). For instance, on Climate-fever with ChatGPT (temp 0.1), the proposed method achieved 64.59 Acc and 69.32 F1, significantly outperforming SelfCk-1gm (53.59 Acc, 34.88 F1) \\cite{cao2023ecl}.\n        *   **Insights**: Experiments estimated LLM hallucination prevalence at 20% to 30% and provided insights into specific types or topics of content prone to hallucination \\cite{cao2023ecl}.\n\n*   **Limitations & Scope**\n    *   **Technical limitations or assumptions**: The method primarily focuses on non-factual extrinsic hallucinations \\cite{cao2023ecl}. The accuracy of the dataset generation step relies on the LLM's capability for claim classification, though human evaluation supports its reliability \\cite{cao2023ecl}. The detection method assumes that factual consistency should be maintained across independently sampled responses from a well-understood claim \\cite{cao2023ecl}.\n    *   **Scope of applicability**: The approach is applicable for generating model-specific hallucination datasets and performing zero-resource, black-box hallucination detection for LLMs, particularly for factual claims across various domains \\cite{cao2023ecl}.\n\n*   **Technical Significance**\n    *   **How this advances the technical state-of-the-art**: The paper significantly advances the state-of-the-art by providing an automated, efficient, and model-specific solution for hallucination dataset generation, overcoming the limitations of manual annotation. It also introduces a novel and demonstrably more effective zero-resource, black-box hallucination detection method that outperforms existing baselines \\cite{cao2023ecl}.\n    *   **Potential impact on future research**: This work can facilitate more rapid and cost-effective evaluation and mitigation of LLM hallucinations, enabling continuous monitoring as models evolve. It contributes to building more reliable and trustworthy LLM applications and provides valuable insights into hallucination prevalence and types, guiding future research into understanding and reducing LLM hallucinations \\cite{cao2023ecl}.",
      "intriguing_abstract": "The pervasive challenge of **hallucinations** in Large Language Models (LLMs) severely undermines their reliability and user trust, yet creating robust, model-specific detection datasets remains a costly, time-consuming, and rapidly obsolete bottleneck. We introduce a groundbreaking, end-to-end solution to this critical problem. Our novel **AutoHall** pipeline revolutionizes dataset generation by automatically constructing model-specific hallucination datasets from existing fact-checking resources, eliminating the need for laborious manual annotation.\n\nComplementing this, we propose an innovative **zero-resource, black-box hallucination detection** method. Leveraging the LLM's inherent capabilities, our approach identifies non-factual content through **self-contradiction analysis**: by independently querying the LLM multiple times and detecting inconsistencies with its original response. This method significantly outperforms existing baselines (e.g., SelfCheckGPT) in accuracy and F1-score across various LLMs (ChatGPT, Llama 2) and temperature settings. Our work offers a scalable, cost-effective pathway to continuous LLM evaluation, providing crucial insights into hallucination prevalence and paving the way for more trustworthy and reliable AI systems.",
      "keywords": [
        "Large Language Models (LLMs)",
        "Hallucination detection",
        "Automated hallucination dataset generation (AutoHall)",
        "Zero-resource black-box detection",
        "LLM-driven self-contradiction analysis",
        "Non-factual content",
        "Model reliability",
        "User trust",
        "Fact-checking datasets",
        "Superior detection performance",
        "Hallucination prevalence",
        "Scalable evaluation",
        "Enterprise security"
      ],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/bb3cc013c462ff2bf3dc5be90f731ebf34996f86.pdf",
      "citation_key": "cao2023ecl",
      "metadata": {
        "title": "AutoHall: Automated Hallucination Dataset Generation for Large Language Models",
        "authors": [
          "Zouying Cao",
          "Yifei Yang",
          "Hai Zhao"
        ],
        "published_date": "2023",
        "abstract": "While Large language models (LLMs) have garnered widespread applications across various domains due to their powerful language understanding and generation capabilities, the detection of non-factual or hallucinatory content generated by LLMs remains scarce. Currently, one significant challenge in hallucination detection is the laborious task of time-consuming and expensive manual annotation of the hallucinatory generation. To address this issue, this paper first introduces a method for automatically constructing model-specific hallucination datasets based on existing fact-checking datasets called AutoHall. Furthermore, we propose a zero-resource and black-box hallucination detection method based on self-contradiction. We conduct experiments towards prevalent open-/closed-source LLMs, achieving superior hallucination detection performance compared to extant baselines. Moreover, our experiments reveal variations in hallucination proportions and types among different models.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/bb3cc013c462ff2bf3dc5be90f731ebf34996f86.pdf",
        "venue": "arXiv.org",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the paper for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Specific technical problem**: The paper addresses the challenge of detecting non-factual or hallucinatory content generated by Large Language Models (LLMs) \\cite{cao2023ecl}.\n    *   **Why important and challenging**: LLMs frequently generate inaccurate or fabricated information (\"hallucinations\"), which significantly impacts their reliability, enterprise security, and user trust \\cite{cao2023ecl}. A major challenge in hallucination detection is the laborious, time-consuming, and expensive nature of manual annotation for creating model-specific hallucination datasets. These datasets are also time-sensitive, becoming obsolete with LLM upgrades \\cite{cao2023ecl}.\n\n*   **Related Work & Positioning**\n    *   **Relation to existing approaches**: Prior work on hallucination detection either leverages external knowledge sources (e.g., CRITIC \\cite{gou2023critic}, search engines \\cite{chern2023invoking}) or adopts zero-resource approaches focusing on inherent model resources (e.g., Chain of Thoughts \\cite{xue2023hallucination}, sampling-based methods like SelfCheckGPT \\cite{manakul2023selfcheckgpt}). Various hallucination benchmarks also exist, often relying on human annotation \\cite{li2023hallucination, umapathi2023hallucination, dale2023human}.\n    *   **Limitations of previous solutions**: Existing methods are often constrained by the need for manually annotated, model-specific, and time-sensitive datasets, which are expensive and do not scale well with frequent model updates. Furthermore, there is still significant room for improvement in the performance of current hallucination detection techniques \\cite{cao2023ecl}.\n\n*   **Technical Approach & Innovation**\n    *   **Core technical method or algorithm**:\n        *   **AutoHall (Automated Hallucination Dataset Generation)**: A three-step pipeline to automatically construct model-specific hallucination datasets from existing fact-checking datasets (which contain claims with ground truth labels and evidence).\n            1.  **References Generation**: An LLM is prompted to generate references for claims.\n            2.  **Claim Classification**: The LLM is then prompted to classify whether each generated reference supports or refutes the original claim.\n            3.  **Hallucination Collection**: If the LLM's classification contradicts the ground truth label of the claim, the generated reference is labeled as hallucinatory \\cite{cao2023ecl}.\n        *   **Zero-Resource, Black-Box Hallucination Detection**: A method based on self-contradiction. For a given claim and an LLM's initial response, the LLM is independently queried multiple times (K times) to generate additional references. The LLM then detects contradictions between the original response and each of the K sampled references. If any contradiction is found, the original response is flagged as hallucinatory \\cite{cao2023ecl}.\n    *   **What makes this approach novel or different**:\n        *   AutoHall innovates by automating the creation of hallucination datasets, eliminating the need for costly and time-consuming manual annotation and addressing the issue of dataset obsolescence with model upgrades \\cite{cao2023ecl}.\n        *   The detection method is novel in its end-to-end use of the LLM for self-contradiction detection, directly comparing the original response with independently sampled ones. This differs from methods like SelfCheckGPT \\cite{manakul2023selfcheckgpt} which rely on token probabilities and external metrics (e.g., BERTScore, n-gram) and may incorrectly attribute conflicts \\cite{cao2023ecl}.\n\n*   **Key Technical Contributions**\n    *   **Novel algorithms, methods, or techniques**:\n        *   The AutoHall pipeline for automatically generating model-specific hallucination datasets \\cite{cao2023ecl}.\n        *   A novel zero-resource, black-box hallucination detection method leveraging LLM-driven self-contradiction analysis \\cite{cao2023ecl}.\n    *   **System design or architectural innovations**: The structured three-step processes for both dataset generation and hallucination detection, enabling efficient and automated workflows \\cite{cao2023ecl}.\n    *   **Theoretical insights or analysis**: The core rationale that an LLM's consistent understanding of a claim should lead to non-contradictory independently generated references; any contradiction signals hallucination \\cite{cao2023ecl}.\n\n*   **Experimental Validation**\n    *   **Experiments conducted**: Experiments were conducted to evaluate the effectiveness of the AutoHall-generated datasets and the proposed self-contradiction-based hallucination detection method. The study also analyzed variations in hallucination proportions and types across different LLMs and temperatures \\cite{cao2023ecl}.\n    *   **Key performance metrics and comparison results**:\n        *   **Models**: ChatGPT (gpt-3.5-turbo) and Llama 2-Chat (7B & 13B parameters) were evaluated \\cite{cao2023ecl}.\n        *   **Datasets**: Three fact-checking datasets (Climate-fever \\cite{diggelmann2020climate}, PUBHEALTH \\cite{kotonya2020pubhealth}, WICE \\cite{kamoi2023wice}) were used for dataset generation and evaluation \\cite{cao2023ecl}.\n        *   **Metrics**: Accuracy (Acc) and F1-score were used to measure detection performance \\cite{cao2023ecl}.\n        *   **Comparison**: The proposed method consistently achieved superior hallucination detection performance (higher Acc and F1-score) compared to existing baselines (Zero-SelfCk, Few-SelfCk, SelfCk-1gm) across all tested LLMs and temperature settings (0.1, 0.5, 0.9). For instance, on Climate-fever with ChatGPT (temp 0.1), the proposed method achieved 64.59 Acc and 69.32 F1, significantly outperforming SelfCk-1gm (53.59 Acc, 34.88 F1) \\cite{cao2023ecl}.\n        *   **Insights**: Experiments estimated LLM hallucination prevalence at 20% to 30% and provided insights into specific types or topics of content prone to hallucination \\cite{cao2023ecl}.\n\n*   **Limitations & Scope**\n    *   **Technical limitations or assumptions**: The method primarily focuses on non-factual extrinsic hallucinations \\cite{cao2023ecl}. The accuracy of the dataset generation step relies on the LLM's capability for claim classification, though human evaluation supports its reliability \\cite{cao2023ecl}. The detection method assumes that factual consistency should be maintained across independently sampled responses from a well-understood claim \\cite{cao2023ecl}.\n    *   **Scope of applicability**: The approach is applicable for generating model-specific hallucination datasets and performing zero-resource, black-box hallucination detection for LLMs, particularly for factual claims across various domains \\cite{cao2023ecl}.\n\n*   **Technical Significance**\n    *   **How this advances the technical state-of-the-art**: The paper significantly advances the state-of-the-art by providing an automated, efficient, and model-specific solution for hallucination dataset generation, overcoming the limitations of manual annotation. It also introduces a novel and demonstrably more effective zero-resource, black-box hallucination detection method that outperforms existing baselines \\cite{cao2023ecl}.\n    *   **Potential impact on future research**: This work can facilitate more rapid and cost-effective evaluation and mitigation of LLM hallucinations, enabling continuous monitoring as models evolve. It contributes to building more reliable and trustworthy LLM applications and provides valuable insights into hallucination prevalence and types, guiding future research into understanding and reducing LLM hallucinations \\cite{cao2023ecl}.",
        "keywords": [
          "Large Language Models (LLMs)",
          "Hallucination detection",
          "Automated hallucination dataset generation (AutoHall)",
          "Zero-resource black-box detection",
          "LLM-driven self-contradiction analysis",
          "Non-factual content",
          "Model reliability",
          "User trust",
          "Fact-checking datasets",
          "Superior detection performance",
          "Hallucination prevalence",
          "Scalable evaluation",
          "Enterprise security"
        ],
        "paper_type": "based on the abstract and introduction, this paper is best classified as **technical**.\n\nhere's why:\n\n*   **abstract mentions:** \"this paper first introduces a method called autohall for automatically constructing...\", \"we propose a zero-resource and black-box hallucination detection method...\" these phrases directly align with the \"technical\" criteria of presenting new methods or algorithms (\"propose\", \"develop\", \"present\", \"algorithm\", \"method\").\n*   **introduction discusses:** the problem of \"hallucination\" in llms, which is a technical challenge, setting the stage for the proposed technical solutions.\n*   while the paper also \"conduct experiments\" and discusses \"findings\" (elements of an empirical paper), these experiments are conducted *to evaluate the performance of the proposed methods*. the primary contribution is the development and presentation of these new methods."
      },
      "file_name": "bb3cc013c462ff2bf3dc5be90f731ebf34996f86.pdf"
    },
    {
      "success": true,
      "doc_id": "120ebacc888e2992d78740a0e5e8d5d6",
      "summary": "This paper introduces R-Bench \\cite{wu2024bxt}, a novel benchmark designed to evaluate and analyze relationship hallucinations in Large Vision-Language Models (LVLMs).\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem:** Existing LVLMs suffer from \"relationship hallucinations,\" where they generate responses that are inconsistent with the inter-object relationships present in an image. Previous research primarily focused on \"object hallucinations,\" which are easier to mitigate with object detectors.\n    *   **Importance & Challenge:** Understanding inter-object relationships is crucial for comprehensive visual comprehension. Relationship hallucinations are more challenging to address than object hallucinations and reflect a deeper flaw in LVLMs' understanding. Current benchmarks for relationship hallucinations are either insufficient or suffer from data leakage, as they are often built on datasets used for LVLM pre-training.\n\n*   **Related Work & Positioning**\n    *   **Relation to Existing Approaches:** Acknowledges the success of LLMs and the development of LVLMs, as well as prior work on evaluating object hallucinations (e.g., POPE \\cite{li2023e}).\n    *   **Limitations of Previous Solutions:**\n        *   Prior hallucination research largely neglected inter-object relationships, focusing instead on individual object presence.\n        *   Existing benchmarks that touch upon relationship hallucinations (e.g., Chen et al., 2023c; Wang et al., 2023a) are prone to \"data leakage\" because they utilize datasets (like COCO and Visual Genome) that are extensively used in LVLM pre-training or visual instruction tuning, leading to biased evaluations.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method:** The paper introduces **R-Bench \\cite{wu2024bxt}**, a new benchmark for Vision Relationship Hallucination. It comprises two main types of questions:\n        *   **Image-level questions:** Assess the general existence of relationships within an image.\n        *   **Instance-level questions:** Evaluate local visual comprehension by focusing on relationships between specific objects, which are highlighted using colored bounding boxes or masks.\n    *   **Novelty & Differentiation:**\n        *   **Data Leakage Prevention:** R-Bench is meticulously constructed using the `nocaps` validation set, ensuring that the evaluation data is distinct from common LVLM training data, thus providing a more rigorous and unbiased assessment.\n        *   **Hybrid Question Generation:** Questions are generated through a combination of automatic methods using Large Language Models (LLMs) and a subsequent rigorous manual curation process to filter out noise and ensure logical consistency and correct labels.\n        *   **Integration of Vision Models:** The benchmark leverages advanced vision models like GroundingDINO (for object detection and bounding box extraction) and SAM (for mask generation) to create precise instance-level questions.\n\n*   **Key Technical Contributions**\n    *   **Novel Benchmark (R-Bench):** The primary contribution is the development of R-Bench \\cite{wu2024bxt}, a comprehensive and data-leakage-free benchmark specifically for relationship hallucinations, addressing a critical gap in LVLM evaluation.\n    *   **Multi-faceted Evaluation Design:** The inclusion of both image-level and instance-level questions allows for a granular assessment of LVLMs' relationship understanding, from global scene context to fine-grained object interactions.\n    *   **Identification of Hallucination Patterns:** The work identifies three specific co-occurrence patterns (relationship-relationship, subject-relationship, and relationship-object) that contribute to relationship hallucinations, offering insights into their underlying causes.\n    *   **Empirical Insights into LVLM Behavior:** The analysis reveals that current LVLMs tend to disregard visual content, over-rely on common sense knowledge from their LLM components, and struggle with reasoning about spatial relationships based on contextual information.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted:**\n        *   Evaluated several popular LVLMs (LLAVA-1.5, InstructBLIP, Mplug-OWL2, Qwen-VL) on R-Bench, covering both image-level and instance-level questions (with bounding boxes and masks).\n        *   Conducted a preliminary assessment of LVLMs' ability to discriminate reference objects using bounding boxes and masks (Table 1), showing relatively high accuracy in this foundational task.\n        *   Analyzed the impact of the long-tail distribution in visual instruction tuning datasets on LVLMs' understanding of visual relationships.\n    *   **Key Performance Metrics & Comparison Results:**\n        *   Metrics: Accuracy, Precision, Recall, F1 Score, and the proportion of 'Yes' answers ('YES' rate).\n        *   **Overall Low Performance:** LVLMs demonstrated significantly lower performance on relationship understanding compared to their object recognition capabilities. F1 scores were generally low across models and question types (e.g., Qwen-VL, the best performer, achieved ~80.5% F1 on image-level subset, but only ~68% on instance-level box questions and ~65% on instance-level mask questions).\n        *   **Bias Towards 'Yes':** Models exhibited a strong bias towards answering 'Yes' (high 'YES' rates and high recall but low precision), indicating a high rate of false positive relationship hallucinations.\n        *   **Instance-level Difficulty:** Performance on instance-level questions was consistently lower than on image-level questions, underscoring LVLMs' difficulty with fine-grained local visual comprehension.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations:** The benchmark generation process, while LLM-assisted, still required extensive manual filtering to ensure question quality and label correctness. The evaluation of reference object discrimination might be affected by semantic variations in object naming. The inherent imbalance of positive and negative questions in the raw dataset necessitated the use of balanced subsets for evaluation.\n    *   **Scope of Applicability:** R-Bench specifically targets relationship hallucinations. While providing deep insights into visual comprehension, it does not cover other types of LVLM hallucinations or broader capabilities. The analysis is primarily based on the `nocaps` dataset and relationships extracted from COCO captions.\n\n*   **Technical Significance**\n    *   **Advances State-of-the-Art:** R-Bench \\cite{wu2024bxt} establishes a new, robust, and unbiased standard for evaluating relationship hallucinations in LVLMs, addressing a critical gap in the field.\n    *   **Deepens Understanding of LVLM Failures:** The detailed analysis of hallucination patterns and their root causes (e.g., reliance on common sense, poor spatial reasoning, long-tail data distribution) provides invaluable insights into the fundamental limitations of current LVLMs.\n    *   **Guides Future Research:** The findings highlight urgent research directions, including the need for improved fine-grained image-text alignment, strategies to reduce over-reliance on common sense, and enhanced spatial reasoning capabilities in LVLMs to achieve more accurate and reliable visual comprehension.",
      "intriguing_abstract": "Large Vision-Language Models (LVLMs) demonstrate remarkable capabilities, yet a critical flaw persists: \"relationship hallucinations,\" where models misinterpret inter-object interactions. Unlike simpler object hallucinations, these reflect a deeper failure in visual comprehension, exacerbated by existing benchmarks' susceptibility to data leakage. We introduce **R-Bench**, a novel, data-leakage-free benchmark meticulously designed to rigorously evaluate relationship understanding. R-Bench features a hybrid question generation process and multi-faceted evaluation, including both image-level and precise instance-level questions leveraging bounding boxes and masks. Our comprehensive evaluation of leading LVLMs on R-Bench reveals alarmingly low performance, particularly at the instance level, coupled with a significant \"Yes\" bias indicating pervasive false positives. We uncover that LVLMs often disregard visual evidence, over-rely on common sense, and struggle with spatial reasoning, identifying specific co-occurrence patterns driving these hallucinations. R-Bench provides an unbiased standard, deepening our understanding of LVLM limitations and charting a vital path for developing truly robust visual intelligence.",
      "keywords": [
        "R-Bench",
        "relationship hallucinations",
        "Large Vision-Language Models (LVLMs)",
        "data leakage prevention",
        "image-level and instance-level evaluation",
        "bounding boxes and masks",
        "visual comprehension",
        "spatial reasoning",
        "over-reliance on common sense",
        "hallucination patterns",
        "fine-grained visual understanding"
      ],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/7b181a867f243d83ed0731201b69a82e038feea3.pdf",
      "citation_key": "wu2024bxt",
      "metadata": {
        "title": "Evaluating and Analyzing Relationship Hallucinations in Large Vision-Language Models",
        "authors": [
          "Ming-Kuan Wu",
          "Jiayi Ji",
          "Oucheng Huang",
          "Jiale Li",
          "Yuhang Wu",
          "Xiaoshuai Sun",
          "Rongrong Ji"
        ],
        "published_date": "2024",
        "abstract": "The issue of hallucinations is a prevalent concern in existing Large Vision-Language Models (LVLMs). Previous efforts have primarily focused on investigating object hallucinations, which can be easily alleviated by introducing object detectors. However, these efforts neglect hallucinations in inter-object relationships, which is essential for visual comprehension. In this work, we introduce R-Bench, a novel benchmark for evaluating Vision Relationship Hallucination. R-Bench features image-level questions that focus on the existence of relationships and instance-level questions that assess local visual comprehension. We identify three types of relationship co-occurrences that lead to hallucinations: relationship-relationship, subject-relationship, and relationship-object. The visual instruction tuning dataset's long-tail distribution significantly impacts LVLMs' understanding of visual relationships. Furthermore, our analysis reveals that current LVLMs tend to disregard visual content and overly rely on the common sense knowledge of Large Language Models. They also struggle with reasoning about spatial relationships based on contextual information.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/7b181a867f243d83ed0731201b69a82e038feea3.pdf",
        "venue": "International Conference on Machine Learning",
        "citationCount": 0,
        "score": 0,
        "summary": "This paper introduces R-Bench \\cite{wu2024bxt}, a novel benchmark designed to evaluate and analyze relationship hallucinations in Large Vision-Language Models (LVLMs).\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem:** Existing LVLMs suffer from \"relationship hallucinations,\" where they generate responses that are inconsistent with the inter-object relationships present in an image. Previous research primarily focused on \"object hallucinations,\" which are easier to mitigate with object detectors.\n    *   **Importance & Challenge:** Understanding inter-object relationships is crucial for comprehensive visual comprehension. Relationship hallucinations are more challenging to address than object hallucinations and reflect a deeper flaw in LVLMs' understanding. Current benchmarks for relationship hallucinations are either insufficient or suffer from data leakage, as they are often built on datasets used for LVLM pre-training.\n\n*   **Related Work & Positioning**\n    *   **Relation to Existing Approaches:** Acknowledges the success of LLMs and the development of LVLMs, as well as prior work on evaluating object hallucinations (e.g., POPE \\cite{li2023e}).\n    *   **Limitations of Previous Solutions:**\n        *   Prior hallucination research largely neglected inter-object relationships, focusing instead on individual object presence.\n        *   Existing benchmarks that touch upon relationship hallucinations (e.g., Chen et al., 2023c; Wang et al., 2023a) are prone to \"data leakage\" because they utilize datasets (like COCO and Visual Genome) that are extensively used in LVLM pre-training or visual instruction tuning, leading to biased evaluations.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method:** The paper introduces **R-Bench \\cite{wu2024bxt}**, a new benchmark for Vision Relationship Hallucination. It comprises two main types of questions:\n        *   **Image-level questions:** Assess the general existence of relationships within an image.\n        *   **Instance-level questions:** Evaluate local visual comprehension by focusing on relationships between specific objects, which are highlighted using colored bounding boxes or masks.\n    *   **Novelty & Differentiation:**\n        *   **Data Leakage Prevention:** R-Bench is meticulously constructed using the `nocaps` validation set, ensuring that the evaluation data is distinct from common LVLM training data, thus providing a more rigorous and unbiased assessment.\n        *   **Hybrid Question Generation:** Questions are generated through a combination of automatic methods using Large Language Models (LLMs) and a subsequent rigorous manual curation process to filter out noise and ensure logical consistency and correct labels.\n        *   **Integration of Vision Models:** The benchmark leverages advanced vision models like GroundingDINO (for object detection and bounding box extraction) and SAM (for mask generation) to create precise instance-level questions.\n\n*   **Key Technical Contributions**\n    *   **Novel Benchmark (R-Bench):** The primary contribution is the development of R-Bench \\cite{wu2024bxt}, a comprehensive and data-leakage-free benchmark specifically for relationship hallucinations, addressing a critical gap in LVLM evaluation.\n    *   **Multi-faceted Evaluation Design:** The inclusion of both image-level and instance-level questions allows for a granular assessment of LVLMs' relationship understanding, from global scene context to fine-grained object interactions.\n    *   **Identification of Hallucination Patterns:** The work identifies three specific co-occurrence patterns (relationship-relationship, subject-relationship, and relationship-object) that contribute to relationship hallucinations, offering insights into their underlying causes.\n    *   **Empirical Insights into LVLM Behavior:** The analysis reveals that current LVLMs tend to disregard visual content, over-rely on common sense knowledge from their LLM components, and struggle with reasoning about spatial relationships based on contextual information.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted:**\n        *   Evaluated several popular LVLMs (LLAVA-1.5, InstructBLIP, Mplug-OWL2, Qwen-VL) on R-Bench, covering both image-level and instance-level questions (with bounding boxes and masks).\n        *   Conducted a preliminary assessment of LVLMs' ability to discriminate reference objects using bounding boxes and masks (Table 1), showing relatively high accuracy in this foundational task.\n        *   Analyzed the impact of the long-tail distribution in visual instruction tuning datasets on LVLMs' understanding of visual relationships.\n    *   **Key Performance Metrics & Comparison Results:**\n        *   Metrics: Accuracy, Precision, Recall, F1 Score, and the proportion of 'Yes' answers ('YES' rate).\n        *   **Overall Low Performance:** LVLMs demonstrated significantly lower performance on relationship understanding compared to their object recognition capabilities. F1 scores were generally low across models and question types (e.g., Qwen-VL, the best performer, achieved ~80.5% F1 on image-level subset, but only ~68% on instance-level box questions and ~65% on instance-level mask questions).\n        *   **Bias Towards 'Yes':** Models exhibited a strong bias towards answering 'Yes' (high 'YES' rates and high recall but low precision), indicating a high rate of false positive relationship hallucinations.\n        *   **Instance-level Difficulty:** Performance on instance-level questions was consistently lower than on image-level questions, underscoring LVLMs' difficulty with fine-grained local visual comprehension.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations:** The benchmark generation process, while LLM-assisted, still required extensive manual filtering to ensure question quality and label correctness. The evaluation of reference object discrimination might be affected by semantic variations in object naming. The inherent imbalance of positive and negative questions in the raw dataset necessitated the use of balanced subsets for evaluation.\n    *   **Scope of Applicability:** R-Bench specifically targets relationship hallucinations. While providing deep insights into visual comprehension, it does not cover other types of LVLM hallucinations or broader capabilities. The analysis is primarily based on the `nocaps` dataset and relationships extracted from COCO captions.\n\n*   **Technical Significance**\n    *   **Advances State-of-the-Art:** R-Bench \\cite{wu2024bxt} establishes a new, robust, and unbiased standard for evaluating relationship hallucinations in LVLMs, addressing a critical gap in the field.\n    *   **Deepens Understanding of LVLM Failures:** The detailed analysis of hallucination patterns and their root causes (e.g., reliance on common sense, poor spatial reasoning, long-tail data distribution) provides invaluable insights into the fundamental limitations of current LVLMs.\n    *   **Guides Future Research:** The findings highlight urgent research directions, including the need for improved fine-grained image-text alignment, strategies to reduce over-reliance on common sense, and enhanced spatial reasoning capabilities in LVLMs to achieve more accurate and reliable visual comprehension.",
        "keywords": [
          "R-Bench",
          "relationship hallucinations",
          "Large Vision-Language Models (LVLMs)",
          "data leakage prevention",
          "image-level and instance-level evaluation",
          "bounding boxes and masks",
          "visual comprehension",
          "spatial reasoning",
          "over-reliance on common sense",
          "hallucination patterns",
          "fine-grained visual understanding"
        ],
        "paper_type": "based on the abstract and introduction:\n\n1.  **\"in this work, we introduce r-bench, a novel benchmark for evaluating vision relationship hallucination.\"** this is a strong indicator of presenting a new system or method (a benchmark is a tool/system for evaluation).\n2.  the abstract then describes the features of r-bench (\"r-bench features image-level questions... and instance-level questions...\").\n3.  the subsequent analysis (\"we identify three types...\", \"our analysis reveals...\") are findings derived from using this newly introduced benchmark.\n\nwhile the paper clearly involves empirical analysis, its primary contribution, as highlighted in the abstract, is the **development and presentation of a novel benchmark (r-bench)**. the creation of a benchmark is a technical endeavor, involving design, data collection, and validation of a new evaluation system. the empirical findings are a result of applying this technical contribution.\n\ntherefore, the most fitting classification is **technical**."
      },
      "file_name": "7b181a867f243d83ed0731201b69a82e038feea3.pdf"
    },
    {
      "success": true,
      "doc_id": "2e8efa29b24989bae9faa86070c5de92",
      "summary": "Here's a focused summary of the technical paper for a literature review, emphasizing technical innovations and empirical validation:\n\n### Analysis of \\cite{ghosh2024tj5}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem:** Large Language Models (LLMs) exhibit significant logical inconsistency, particularly when dealing with complex queries involving primitive logical operators (negation, conjunction, disjunction). Existing consistency assessments primarily focus on simple paraphrasing, neglecting the deeper logical reasoning required for complex inputs.\n    *   **Importance and Challenge:** LLM inconsistency leads to vulnerabilities like hallucination and undermines their trustworthiness, especially in high-stakes applications (e.g., healthcare, finance, law). Ensuring logical consistency is crucial for building reliable and verifiable LLM-based systems, as it allows for easier verification of correctness without sophisticated benchmarks.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches:** Previous research on LLM consistency largely focuses on semantic similarity or paraphrasing of input sentences (e.g., Kuhn et al., 2023; Elazar et al., 2021).\n    *   **Limitations of Previous Solutions:** These existing methods do not assess LLMs' consistency on complex queries that necessitate a robust understanding of logical reasoning, leaving a significant gap in evaluating LLM trustworthiness.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method:** The paper proposes a framework to assess and improve the logical consistency of Retrieval-Augmented LLMs (RAG) in fact-checking tasks, specifically for propositional logic queries derived from Knowledge Graphs (KGs).\n        *   **Logical Consistency Measures:** Defines quantitative measures for LLM consistency across primitive logical operators (negation, conjunction, disjunction), extending to complex DNF (Disjunctive Normal Form) and CNF (Conjunctive Normal Form) facts, and logical rules (e.g., commutative, associative, distributive laws).\n        *   **KG-based Context:** Utilizes real-world KGs (Freebase, NELL, WikiKG90Mv2) to provide authoritative context for fact-checking, transforming KG triplets into `(Fact, Context)` pairs (LLMQuery).\n        *   **Improvement Strategy:** Employs supervised fine-tuning (SFT) and parameter-efficient fine-tuning (QLoRA) to enhance LLMs' logical consistency.\n    *   **Novelty/Differentiation:**\n        *   **Novel Datasets:** Introduces three new logical fact-checking datasets (FreebaseLFC, NELLLFC, WikiLFC) specifically designed to test logical consistency over KGs, addressing a critical lack in the literature.\n        *   **Comprehensive Logical Assessment:** Moves beyond simple paraphrasing to systematically assess consistency on complex propositional logic queries and rules, which is a novel and more rigorous evaluation approach.\n        *   **Targeted Fine-tuning:** Demonstrates the effectiveness of SFT (with QLoRA) for improving logical consistency in complex fact-checking with KG contexts, a previously unexplored application of fine-tuning.\n        *   **Theoretical Insight:** Proposes that complex facts and rules are decomposable into simple facts (Proposition 1), allowing models fine-tuned on simple facts to generalize to complex ones.\n\n4.  **Key Technical Contributions**\n    *   **Novel Datasets:** Creation of FreebaseLFC, NELLLFC, and WikiLFC, the first logical fact-checking datasets over KGs for community development.\n    *   **Novel Consistency Measures:** Formalization of quantitative logical consistency measures for LLMs on propositional logic queries, covering primitive operators, DNF/CNF facts, and logical rules.\n    *   **Method for Improvement:** Introduction and empirical validation of supervised fine-tuning (leveraging QLoRA for efficiency) as an effective method to improve LLM logical consistency in RAG-based fact-checking.\n    *   **Theoretical Insight:** Proposition 1, demonstrating that consistency on complex DNF facts can be derived from consistency on their constituent atomic facts, enabling generalization.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted:**\n        *   Evaluated the logical consistency and accuracy of pre-trained LLMs (Llama 2-7B, Llama 2-13B, Gemma-2B) on the newly introduced LFC datasets.\n        *   Compared LLM performance with and without KG contexts.\n        *   Applied supervised fine-tuning (SFT) using QLoRA on these LLMs and assessed the improvement in logical consistency.\n        *   Investigated the generalization capability of fine-tuned models from simple to complex, unseen facts and rules.\n        *   Explored the impact of various factors like context length, KG retrieval methods, prompting strategies, learning rate, and epochs (detailed in appendices).\n        *   Tested larger models (Llama 2-70B, GPT-4o) with instruction prompting.\n    *   **Key Performance Metrics:** Accuracy (standard fact-checking correctness) and a newly defined Consistency score (measuring adherence to logical equivalences).\n    *   **Comparison Results:**\n        *   Existing LLMs (Llama 2-7B, 13B, Gemma-2B) showed a significant lack of logical consistency, especially on complex queries, even when achieving reasonable accuracy with KG contexts.\n        *   Supervised fine-tuning improved logical consistency by an average of 14%.\n        *   The QLoRA optimization technique ensured efficient fine-tuning and inference, scaling to large KGs.\n        *   Models fine-tuned on simple facts demonstrated good generalization to more complex, unseen facts and rules, supporting Proposition 1.\n        *   For very large or closed-source models (Llama 2-70B, GPT-4o), instruction prompting was found to be a sufficient alternative to fine-tuning for improving logical consistency.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations:** The primary focus is on propositional logic queries and primitive logical operators. While extensible to first-order logic, the core assessment is within propositional logic.\n    *   **Assumptions:** Relies on the availability and structure of Knowledge Graphs for providing factual context.\n    *   **Scope of Applicability:** The proposed fine-tuning approach is most feasible for open-source or smaller LLMs where fine-tuning is practical. For larger, closed-source models, instruction prompting is suggested as an alternative.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art:** This work significantly advances the technical state-of-the-art by providing a rigorous framework and benchmarks for evaluating and improving LLMs' logical consistency, moving beyond superficial semantic evaluations.\n    *   **Potential Impact:**\n        *   Enables the development of more trustworthy and reliable LLMs for critical applications by addressing a fundamental weakness in their reasoning capabilities.\n        *   Provides a foundation for future research into deeper logical reasoning in LLMs, potentially leading to models that are not only accurate but also logically sound and verifiable.\n        *   The introduced datasets serve as valuable resources for the research community to foster development in logically consistent LLMs.",
      "intriguing_abstract": "Large Language Models (LLMs) harbor a critical vulnerability: profound logical inconsistency, particularly when confronted with complex propositional logic queries involving negation, conjunction, and disjunction. This deficiency fundamentally undermines their trustworthiness, fostering hallucination in high-stakes applications where verifiable reasoning is paramount. Existing consistency assessments largely overlook this deep logical reasoning, focusing instead on superficial semantic similarity.\n\nWe introduce a novel framework to rigorously evaluate and significantly enhance the logical consistency of Retrieval-Augmented LLMs (RAG) in fact-checking tasks. Our approach defines quantitative measures for consistency across primitive logical operators, complex DNF/CNF facts, and fundamental logical rules, leveraging real-world Knowledge Graphs (KGs) for authoritative context. We present three pioneering logical fact-checking datasets (FreebaseLFC, NELLLFC, WikiLFC), the first of their kind. Empirically, we demonstrate that supervised fine-tuning (SFT) with QLoRA dramatically improves LLM logical consistency by an average of 14%, enabling robust generalization from simple to complex facts. This work establishes crucial benchmarks and methods for building truly reliable and verifiable LLM systems, paving the way for a new generation of logically sound AI.",
      "keywords": [
        "LLM logical inconsistency",
        "Propositional logic queries",
        "Knowledge Graphs (KGs)",
        "Retrieval-Augmented LLMs (RAG)",
        "Quantitative consistency measures",
        "Novel logical fact-checking datasets",
        "Supervised fine-tuning (SFT)",
        "Parameter-efficient fine-tuning (QLoRA)",
        "Improved logical consistency",
        "Generalization capability",
        "Instruction prompting",
        "LLM trustworthiness",
        "Complex logical reasoning"
      ],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/6947893915861e8c30bc6b010eb1faf0d82f0a19.pdf",
      "citation_key": "ghosh2024tj5",
      "metadata": {
        "title": "Logical Consistency of Large Language Models in Fact-checking",
        "authors": [
          "Bishwamittra Ghosh",
          "Sarah Hasan",
          "Naheed Anjum Arafat",
          "Arijit Khan"
        ],
        "published_date": "2024",
        "abstract": "In recent years, large language models (LLMs) have demonstrated significant success in performing varied natural language tasks such as language translation, question-answering, summarizing, fact-checking, etc. Despite LLMs' impressive ability to generate human-like texts, LLMs are infamous for their inconsistent responses - a meaning-preserving change in the input query results in an inconsistent response and attributes to vulnerabilities of LLMs such as hallucination. Consequently, existing research focuses on simple paraphrasing-based consistency assessment of LLMs, and ignores complex queries that necessitate an even better understanding of logical reasoning by an LLM. Our work therefore addresses the logical inconsistency of LLMs under complex logical queries with primitive logical operators, e.g., negation, conjunction, and disjunction. As a test bed, we consider retrieval-augmented LLMs on a fact-checking task involving propositional logic queries from knowledge graphs (KGs). Our contributions are threefold. Benchmark: We introduce three logical fact-checking datasets over KGs for community development towards logically consistent LLMs. Assessment: We propose consistency measures of LLMs on propositional logic queries and demonstrate that existing LLMs lack logical consistency, especially on complex queries. Improvement: We employ supervised fine-tuning to improve the logical consistency of LLMs on the complex fact-checking task with KG contexts. We have made our source code and benchmarks available.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/6947893915861e8c30bc6b010eb1faf0d82f0a19.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the technical paper for a literature review, emphasizing technical innovations and empirical validation:\n\n### Analysis of \\cite{ghosh2024tj5}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem:** Large Language Models (LLMs) exhibit significant logical inconsistency, particularly when dealing with complex queries involving primitive logical operators (negation, conjunction, disjunction). Existing consistency assessments primarily focus on simple paraphrasing, neglecting the deeper logical reasoning required for complex inputs.\n    *   **Importance and Challenge:** LLM inconsistency leads to vulnerabilities like hallucination and undermines their trustworthiness, especially in high-stakes applications (e.g., healthcare, finance, law). Ensuring logical consistency is crucial for building reliable and verifiable LLM-based systems, as it allows for easier verification of correctness without sophisticated benchmarks.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches:** Previous research on LLM consistency largely focuses on semantic similarity or paraphrasing of input sentences (e.g., Kuhn et al., 2023; Elazar et al., 2021).\n    *   **Limitations of Previous Solutions:** These existing methods do not assess LLMs' consistency on complex queries that necessitate a robust understanding of logical reasoning, leaving a significant gap in evaluating LLM trustworthiness.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method:** The paper proposes a framework to assess and improve the logical consistency of Retrieval-Augmented LLMs (RAG) in fact-checking tasks, specifically for propositional logic queries derived from Knowledge Graphs (KGs).\n        *   **Logical Consistency Measures:** Defines quantitative measures for LLM consistency across primitive logical operators (negation, conjunction, disjunction), extending to complex DNF (Disjunctive Normal Form) and CNF (Conjunctive Normal Form) facts, and logical rules (e.g., commutative, associative, distributive laws).\n        *   **KG-based Context:** Utilizes real-world KGs (Freebase, NELL, WikiKG90Mv2) to provide authoritative context for fact-checking, transforming KG triplets into `(Fact, Context)` pairs (LLMQuery).\n        *   **Improvement Strategy:** Employs supervised fine-tuning (SFT) and parameter-efficient fine-tuning (QLoRA) to enhance LLMs' logical consistency.\n    *   **Novelty/Differentiation:**\n        *   **Novel Datasets:** Introduces three new logical fact-checking datasets (FreebaseLFC, NELLLFC, WikiLFC) specifically designed to test logical consistency over KGs, addressing a critical lack in the literature.\n        *   **Comprehensive Logical Assessment:** Moves beyond simple paraphrasing to systematically assess consistency on complex propositional logic queries and rules, which is a novel and more rigorous evaluation approach.\n        *   **Targeted Fine-tuning:** Demonstrates the effectiveness of SFT (with QLoRA) for improving logical consistency in complex fact-checking with KG contexts, a previously unexplored application of fine-tuning.\n        *   **Theoretical Insight:** Proposes that complex facts and rules are decomposable into simple facts (Proposition 1), allowing models fine-tuned on simple facts to generalize to complex ones.\n\n4.  **Key Technical Contributions**\n    *   **Novel Datasets:** Creation of FreebaseLFC, NELLLFC, and WikiLFC, the first logical fact-checking datasets over KGs for community development.\n    *   **Novel Consistency Measures:** Formalization of quantitative logical consistency measures for LLMs on propositional logic queries, covering primitive operators, DNF/CNF facts, and logical rules.\n    *   **Method for Improvement:** Introduction and empirical validation of supervised fine-tuning (leveraging QLoRA for efficiency) as an effective method to improve LLM logical consistency in RAG-based fact-checking.\n    *   **Theoretical Insight:** Proposition 1, demonstrating that consistency on complex DNF facts can be derived from consistency on their constituent atomic facts, enabling generalization.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted:**\n        *   Evaluated the logical consistency and accuracy of pre-trained LLMs (Llama 2-7B, Llama 2-13B, Gemma-2B) on the newly introduced LFC datasets.\n        *   Compared LLM performance with and without KG contexts.\n        *   Applied supervised fine-tuning (SFT) using QLoRA on these LLMs and assessed the improvement in logical consistency.\n        *   Investigated the generalization capability of fine-tuned models from simple to complex, unseen facts and rules.\n        *   Explored the impact of various factors like context length, KG retrieval methods, prompting strategies, learning rate, and epochs (detailed in appendices).\n        *   Tested larger models (Llama 2-70B, GPT-4o) with instruction prompting.\n    *   **Key Performance Metrics:** Accuracy (standard fact-checking correctness) and a newly defined Consistency score (measuring adherence to logical equivalences).\n    *   **Comparison Results:**\n        *   Existing LLMs (Llama 2-7B, 13B, Gemma-2B) showed a significant lack of logical consistency, especially on complex queries, even when achieving reasonable accuracy with KG contexts.\n        *   Supervised fine-tuning improved logical consistency by an average of 14%.\n        *   The QLoRA optimization technique ensured efficient fine-tuning and inference, scaling to large KGs.\n        *   Models fine-tuned on simple facts demonstrated good generalization to more complex, unseen facts and rules, supporting Proposition 1.\n        *   For very large or closed-source models (Llama 2-70B, GPT-4o), instruction prompting was found to be a sufficient alternative to fine-tuning for improving logical consistency.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations:** The primary focus is on propositional logic queries and primitive logical operators. While extensible to first-order logic, the core assessment is within propositional logic.\n    *   **Assumptions:** Relies on the availability and structure of Knowledge Graphs for providing factual context.\n    *   **Scope of Applicability:** The proposed fine-tuning approach is most feasible for open-source or smaller LLMs where fine-tuning is practical. For larger, closed-source models, instruction prompting is suggested as an alternative.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art:** This work significantly advances the technical state-of-the-art by providing a rigorous framework and benchmarks for evaluating and improving LLMs' logical consistency, moving beyond superficial semantic evaluations.\n    *   **Potential Impact:**\n        *   Enables the development of more trustworthy and reliable LLMs for critical applications by addressing a fundamental weakness in their reasoning capabilities.\n        *   Provides a foundation for future research into deeper logical reasoning in LLMs, potentially leading to models that are not only accurate but also logically sound and verifiable.\n        *   The introduced datasets serve as valuable resources for the research community to foster development in logically consistent LLMs.",
        "keywords": [
          "LLM logical inconsistency",
          "Propositional logic queries",
          "Knowledge Graphs (KGs)",
          "Retrieval-Augmented LLMs (RAG)",
          "Quantitative consistency measures",
          "Novel logical fact-checking datasets",
          "Supervised fine-tuning (SFT)",
          "Parameter-efficient fine-tuning (QLoRA)",
          "Improved logical consistency",
          "Generalization capability",
          "Instruction prompting",
          "LLM trustworthiness",
          "Complex logical reasoning"
        ],
        "paper_type": "the paper should be classified as **empirical**.\n\nhere's why:\n\n1.  **data-driven studies with statistical analysis:** the abstract explicitly mentions: \"we introduce three logical fact-checking datasets,\" \"we propose consistency measures... and demonstrate that existing llms lack logical consistency,\" \"we employ supervised fine-tuning to improve... we have made our source code and benchmarks available.\" the introduction further details the creation of these datasets and the experimental setup.\n2.  **research questions & methodology:** section 5 is titled \"experimental results\" and lists specific \"research questions empirically\" investigated. it details \"datasets and experimental setup\" and presents findings in tables (e.g., table 1, 2, 3) showing \"accuracy\" and \"logical consistency\" percentages, which are statistical measures.\n3.  **demonstration and validation:** the core of the paper is to *demonstrate* a problem (llm inconsistency) and *validate* a proposed solution (supervised fine-tuning) through experiments and quantitative results.\n4.  **keywords from criteria:** the text uses keywords like \"study,\" \"experiment,\" \"data,\" \"statistical\" (implicitly through accuracy/consistency metrics), \"findings,\" \"research questions,\" and \"methodology.\"\n\nwhile the paper also has strong **technical** elements (proposing consistency measures, developing a fine-tuning methodology, algorithm 1) and **theoretical** elements (formal definitions of logical consistency, propositions with proofs in the appendix, use of propositional logic), the overarching goal and the significant portion of the paper dedicated to setting up experiments, collecting data, running models, and analyzing results firmly place it in the **empirical** category. the technical and theoretical contributions serve as the foundation for the empirical investigation."
      },
      "file_name": "6947893915861e8c30bc6b010eb1faf0d82f0a19.pdf"
    },
    {
      "success": true,
      "doc_id": "cdf1a7e1a9a97960a405ed49967ee69c",
      "summary": "Here's a focused summary of the paper for a literature review, emphasizing technical innovations and empirical validation:\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Large Language Models (LLMs) are prone to hallucination and factual incorrectness, making their outputs untrustworthy and difficult to verify. The paper addresses the challenge of enabling LLMs to generate text with verifiable citations.\n    *   **Importance and Challenge**: This problem is crucial for building trustworthy LLM applications for information seeking. Existing approaches often rely on commercial search engines and human evaluation, which are expensive, difficult to reproduce, and hinder systematic comparison and advancement of modeling approaches.\n\n*   **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: Commercial systems (e.g., Bing Chat, perplexity.ai) and prior research (e.g., Nakano et al., 2021; Menick et al., 2022) share the motivation of providing references. Retrieval-augmented LMs (e.g., Borgeaud et al., 2022; Izacard et al., 2022) incorporate retrieved passages but do not guarantee faithfulness or explicitly provide citations.\n    *   **Limitations of Previous Solutions**: Previous work primarily uses commercial/closed-source models and relies heavily on human evaluation, making results irreproducible and progress difficult to measure. There was a lack of a standardized, automatically evaluable benchmark for LLM citation generation.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method**: \\cite{gao2023ht7} proposes **ALCE (Automatic LLMsâ€™ Citation Evaluation)**, the first reproducible benchmark for evaluating end-to-end systems that retrieve supporting evidence, generate answers, and provide citations. The task involves a natural-language question and a retrieval corpus, requiring systems to output statements with explicit citations to passages from the corpus.\n    *   **Novelty/Differentiation**:\n        *   **Benchmark Design**: ALCE compiles three diverse datasets (ASQA, QAMPARI, ELI5) covering various question types and corpora (Wikipedia, Web-scale Sphere), requiring long-text generation and multi-source synthesis. It allows citing multiple passages per statement.\n        *   **Automatic Evaluation Metrics**: Introduces a robust suite of automatic metrics across three dimensions:\n            *   **Fluency**: Measured by MAUVE \\cite{gao2023ht7}.\n            *   **Correctness**: Tailored metrics for each dataset: Exact Match Recall (ASQA), Precision/Recall-5 (QAMPARI), and NLI-based Claim Recall using InstructGPT and TRUE (ELI5) \\cite{gao2023ht7}.\n            *   **Citation Quality**: Novel NLI-based metrics (using TRUE) for **citation recall** (whether a statement is fully supported by its citations) and **citation precision** (identifying irrelevant citations) \\cite{gao2023ht7}.\n        *   **Prompting Strategies**: Explores novel prompting strategies for LLMs to synthesize retrieved text and generate citations, including VANILLA (direct prompting), SUMM/SNIPPET (using summaries/snippets of passages to fit more context), INTERACTIVE (allowing LLM to decide when/what to retrieve), RERANK (reranking multiple generations), and LONG-CONTEXT (incorporating more passages) \\cite{gao2023ht7}.\n\n*   **Key Technical Contributions**\n    *   **Novel Benchmark**: ALCE, a publicly available and reproducible benchmark for evaluating LLMs' ability to generate text with citations, addressing a critical gap in the field \\cite{gao2023ht7}.\n    *   **Automated Evaluation Framework**: A comprehensive set of automatic metrics for fluency, correctness, and citation quality, demonstrated to correlate strongly with human judgments, enabling scalable and objective evaluation \\cite{gao2023ht7}.\n    *   **Formalization of Citation Task**: Clearly defines the task setup, including statement segmentation and the mechanism for multi-passage citation, providing a standardized framework for future research \\cite{gao2023ht7}.\n    *   **Empirical Analysis & Insights**: Extensive experiments and analyses that reveal current LLM limitations and highlight promising future research directions in retrieval, long-context models, and multi-document synthesis \\cite{gao2023ht7}.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted**: Evaluated state-of-the-art LLMs (ChatGPT, GPT-4) and off-the-shelf retrievers (GTR, DPR, BM25) on the ALCE benchmark using various prompting strategies \\cite{gao2023ht7}.\n    *   **Key Performance Metrics**: Fluency (MAUVE), Correctness (EM recall, precision/recall-5, claim recall), and Citation Quality (citation recall, citation precision) \\cite{gao2023ht7}.\n    *   **Comparison Results**:\n        *   Automatic metrics showed strong correlation with human judgments, validating their robustness \\cite{gao2023ht7}.\n        *   Current systems, even the best models (e.g., GPT-4), demonstrated significant room for improvement; for instance, on ELI5, around 50% of generations lacked complete citation support \\cite{gao2023ht7}.\n        *   **Specific Findings**:\n            *   Closed-book models with post-hoc citing achieved good correctness but poor citation quality \\cite{gao2023ht7}.\n            *   Interactive retrieval approaches did not significantly improve performance on this benchmark \\cite{gao2023ht7}.\n            *   Summarizing retrieved passages improved correctness but not citation quality \\cite{gao2023ht7}.\n            *   Reranking multiple generations boosted human-evaluated citation quality \\cite{gao2023ht7}.\n            *   Incorporating more retrieved passages in context improved GPT-4's performance but not ChatGPT's \\cite{gao2023ht7}.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations**: Identified major challenges: (1) retrieval quality is a crucial bottleneck, (2) LLMs' limited context windows restrict the number of passages they can incorporate, and (3) current LLMs struggle to synthesize information from multiple documents without being distracted by irrelevant content \\cite{gao2023ht7}.\n    *   **Scope of Applicability**: The benchmark primarily assesses the citation capabilities of *existing* LLMs and does not provide training data for citation supervision. The chosen datasets focus on factual questions requiring long-text answers and multi-source synthesis.\n\n*   **Technical Significance**\n    *   **Advancement of State-of-the-Art**: \\cite{gao2023ht7} significantly advances the technical state-of-the-art by providing the first reproducible benchmark and a robust automatic evaluation framework for LLM citation generation. This enables systematic research and development in a previously challenging area.\n    *   **Potential Impact on Future Research**: The work addresses a critical issue of LLM hallucination and verifiability, paving the way for more trustworthy and reliable LLM applications. It highlights key research directions, including improving retrieval systems, developing more capable long-context LLMs, and enhancing LLMs' ability to synthesize information from multiple sources faithfully \\cite{gao2023ht7}.",
      "intriguing_abstract": "The pervasive issue of hallucination and factual incorrectness severely undermines the trustworthiness of Large Language Models (LLMs), hindering their deployment in critical information-seeking applications. Current efforts to enable verifiable LLM outputs with citations are hampered by irreproducible methods and a lack of standardized evaluation. We introduce **ALCE (Automatic LLMsâ€™ Citation Evaluation)**, the first reproducible, end-to-end benchmark designed to systematically assess systems that retrieve evidence, generate answers, and provide explicit citations. ALCE features three diverse datasets and a novel suite of automatic metrics, including NLI-based measures for **citation recall** and **citation precision**, which demonstrate strong correlation with human judgments. Our extensive experiments with state-of-the-art LLMs (e.g., GPT-4) reveal significant shortcomings; for instance, nearly 50% of generated claims on some datasets lack complete citation support. This work not only formalizes the complex task of LLM citation generation but also provides a critical framework for advancing research in retrieval-augmented generation, long-context models, and multi-document synthesis, ultimately paving the way for truly trustworthy and verifiable LLM applications.",
      "keywords": [
        "LLM hallucination",
        "verifiable citations",
        "ALCE benchmark",
        "automatic evaluation framework",
        "NLI-based citation metrics",
        "novel prompting strategies",
        "retrieval-augmented LMs",
        "multi-document synthesis",
        "reproducible benchmark",
        "trustworthy LLM applications",
        "LLM context windows",
        "retrieval quality bottleneck",
        "human judgments correlation"
      ],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/e7c97e953849f1a8e5d85ceb4cfcc0a5d54d2365.pdf",
      "citation_key": "gao2023ht7",
      "metadata": {
        "title": "Enabling Large Language Models to Generate Text with Citations",
        "authors": [
          "Tianyu Gao",
          "Howard Yen",
          "Jiatong Yu",
          "Danqi Chen"
        ],
        "published_date": "2023",
        "abstract": "Large language models (LLMs) have emerged as a widely-used tool for information seeking, but their generated outputs are prone to hallucination. In this work, our aim is to allow LLMs to generate text with citations, improving their factual correctness and verifiability. Existing work mainly relies on commercial search engines and human evaluation, making it challenging to reproduce and compare different modeling approaches. We propose ALCE, the first benchmark for Automatic LLMs' Citation Evaluation. ALCE collects a diverse set of questions and retrieval corpora and requires building end-to-end systems to retrieve supporting evidence and generate answers with citations. We develop automatic metrics along three dimensions -- fluency, correctness, and citation quality -- and demonstrate their strong correlation with human judgements. Our experiments with state-of-the-art LLMs and novel prompting strategies show that current systems have considerable room for improvement -- For example, on the ELI5 dataset, even the best models lack complete citation support 50% of the time. Our analyses further highlight promising future directions, including developing better retrievers, advancing long-context LLMs, and improving the ability to synthesize information from multiple sources.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/e7c97e953849f1a8e5d85ceb4cfcc0a5d54d2365.pdf",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the paper for a literature review, emphasizing technical innovations and empirical validation:\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Large Language Models (LLMs) are prone to hallucination and factual incorrectness, making their outputs untrustworthy and difficult to verify. The paper addresses the challenge of enabling LLMs to generate text with verifiable citations.\n    *   **Importance and Challenge**: This problem is crucial for building trustworthy LLM applications for information seeking. Existing approaches often rely on commercial search engines and human evaluation, which are expensive, difficult to reproduce, and hinder systematic comparison and advancement of modeling approaches.\n\n*   **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: Commercial systems (e.g., Bing Chat, perplexity.ai) and prior research (e.g., Nakano et al., 2021; Menick et al., 2022) share the motivation of providing references. Retrieval-augmented LMs (e.g., Borgeaud et al., 2022; Izacard et al., 2022) incorporate retrieved passages but do not guarantee faithfulness or explicitly provide citations.\n    *   **Limitations of Previous Solutions**: Previous work primarily uses commercial/closed-source models and relies heavily on human evaluation, making results irreproducible and progress difficult to measure. There was a lack of a standardized, automatically evaluable benchmark for LLM citation generation.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method**: \\cite{gao2023ht7} proposes **ALCE (Automatic LLMsâ€™ Citation Evaluation)**, the first reproducible benchmark for evaluating end-to-end systems that retrieve supporting evidence, generate answers, and provide citations. The task involves a natural-language question and a retrieval corpus, requiring systems to output statements with explicit citations to passages from the corpus.\n    *   **Novelty/Differentiation**:\n        *   **Benchmark Design**: ALCE compiles three diverse datasets (ASQA, QAMPARI, ELI5) covering various question types and corpora (Wikipedia, Web-scale Sphere), requiring long-text generation and multi-source synthesis. It allows citing multiple passages per statement.\n        *   **Automatic Evaluation Metrics**: Introduces a robust suite of automatic metrics across three dimensions:\n            *   **Fluency**: Measured by MAUVE \\cite{gao2023ht7}.\n            *   **Correctness**: Tailored metrics for each dataset: Exact Match Recall (ASQA), Precision/Recall-5 (QAMPARI), and NLI-based Claim Recall using InstructGPT and TRUE (ELI5) \\cite{gao2023ht7}.\n            *   **Citation Quality**: Novel NLI-based metrics (using TRUE) for **citation recall** (whether a statement is fully supported by its citations) and **citation precision** (identifying irrelevant citations) \\cite{gao2023ht7}.\n        *   **Prompting Strategies**: Explores novel prompting strategies for LLMs to synthesize retrieved text and generate citations, including VANILLA (direct prompting), SUMM/SNIPPET (using summaries/snippets of passages to fit more context), INTERACTIVE (allowing LLM to decide when/what to retrieve), RERANK (reranking multiple generations), and LONG-CONTEXT (incorporating more passages) \\cite{gao2023ht7}.\n\n*   **Key Technical Contributions**\n    *   **Novel Benchmark**: ALCE, a publicly available and reproducible benchmark for evaluating LLMs' ability to generate text with citations, addressing a critical gap in the field \\cite{gao2023ht7}.\n    *   **Automated Evaluation Framework**: A comprehensive set of automatic metrics for fluency, correctness, and citation quality, demonstrated to correlate strongly with human judgments, enabling scalable and objective evaluation \\cite{gao2023ht7}.\n    *   **Formalization of Citation Task**: Clearly defines the task setup, including statement segmentation and the mechanism for multi-passage citation, providing a standardized framework for future research \\cite{gao2023ht7}.\n    *   **Empirical Analysis & Insights**: Extensive experiments and analyses that reveal current LLM limitations and highlight promising future research directions in retrieval, long-context models, and multi-document synthesis \\cite{gao2023ht7}.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted**: Evaluated state-of-the-art LLMs (ChatGPT, GPT-4) and off-the-shelf retrievers (GTR, DPR, BM25) on the ALCE benchmark using various prompting strategies \\cite{gao2023ht7}.\n    *   **Key Performance Metrics**: Fluency (MAUVE), Correctness (EM recall, precision/recall-5, claim recall), and Citation Quality (citation recall, citation precision) \\cite{gao2023ht7}.\n    *   **Comparison Results**:\n        *   Automatic metrics showed strong correlation with human judgments, validating their robustness \\cite{gao2023ht7}.\n        *   Current systems, even the best models (e.g., GPT-4), demonstrated significant room for improvement; for instance, on ELI5, around 50% of generations lacked complete citation support \\cite{gao2023ht7}.\n        *   **Specific Findings**:\n            *   Closed-book models with post-hoc citing achieved good correctness but poor citation quality \\cite{gao2023ht7}.\n            *   Interactive retrieval approaches did not significantly improve performance on this benchmark \\cite{gao2023ht7}.\n            *   Summarizing retrieved passages improved correctness but not citation quality \\cite{gao2023ht7}.\n            *   Reranking multiple generations boosted human-evaluated citation quality \\cite{gao2023ht7}.\n            *   Incorporating more retrieved passages in context improved GPT-4's performance but not ChatGPT's \\cite{gao2023ht7}.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations**: Identified major challenges: (1) retrieval quality is a crucial bottleneck, (2) LLMs' limited context windows restrict the number of passages they can incorporate, and (3) current LLMs struggle to synthesize information from multiple documents without being distracted by irrelevant content \\cite{gao2023ht7}.\n    *   **Scope of Applicability**: The benchmark primarily assesses the citation capabilities of *existing* LLMs and does not provide training data for citation supervision. The chosen datasets focus on factual questions requiring long-text answers and multi-source synthesis.\n\n*   **Technical Significance**\n    *   **Advancement of State-of-the-Art**: \\cite{gao2023ht7} significantly advances the technical state-of-the-art by providing the first reproducible benchmark and a robust automatic evaluation framework for LLM citation generation. This enables systematic research and development in a previously challenging area.\n    *   **Potential Impact on Future Research**: The work addresses a critical issue of LLM hallucination and verifiability, paving the way for more trustworthy and reliable LLM applications. It highlights key research directions, including improving retrieval systems, developing more capable long-context LLMs, and enhancing LLMs' ability to synthesize information from multiple sources faithfully \\cite{gao2023ht7}.",
        "keywords": [
          "LLM hallucination",
          "verifiable citations",
          "ALCE benchmark",
          "automatic evaluation framework",
          "NLI-based citation metrics",
          "novel prompting strategies",
          "retrieval-augmented LMs",
          "multi-document synthesis",
          "reproducible benchmark",
          "trustworthy LLM applications",
          "LLM context windows",
          "retrieval quality bottleneck",
          "human judgments correlation"
        ],
        "paper_type": "the paper type is **technical**.\n\n**reasoning:**\n\n1.  **\"we propose alce, the first benchmark for automatic llmsâ€™ citation evaluation.\"** - this clearly indicates the development and presentation of a new system/resource (a benchmark).\n2.  **\"we develop automatic metrics along three dimensionsâ€”fluency, correctness, and citation qualityâ€”and demonstrate their strong correlation with human judgements.\"** - this describes the creation of new methods (metrics) for evaluation.\n3.  the introduction discusses a \"new generation paradigm for llms\" and illustrates \"the task setup of alce,\" which is a proposed system/framework.\n4.  while the paper also includes \"experiments\" and \"analyses\" (elements of an empirical paper), these are conducted *using* the newly proposed benchmark and metrics. the primary contribution described in the abstract and introduction is the *creation* of alce and its associated evaluation methods, which are technical innovations."
      },
      "file_name": "e7c97e953849f1a8e5d85ceb4cfcc0a5d54d2365.pdf"
    },
    {
      "success": true,
      "doc_id": "5cea58b2e279f8fd2232c34f790f3806",
      "summary": "Here's a focused summary of the technical paper for a literature review, emphasizing technical innovations and empirical validation:\n\n### Focused Summary for Literature Review\n\n**1. Research Problem & Motivation**\n*   **Specific Technical Problem:** Large Language Models (LLMs) are prone to generating hallucinations, specifically factually incorrect or irrelevant information, which undermines their reliability in applications demanding high factual accuracy \\cite{yang20251dw}. The paper focuses on detecting \"fact-conflicting hallucinations\" \\cite{yang20251dw}.\n*   **Importance & Challenge:** Hallucinations, especially fact-conflicting ones, can mislead users, erode trust, and have serious consequences (e.g., in legal contexts) \\cite{yang20251dw}. Existing detection methods face significant challenges:\n    *   Reliance on external resources (databases, search engines) suffers from low availability, incomplete coverage, privacy concerns, high latency, low reliability, and poor scalability \\cite{yang20251dw}.\n    *   Methods depending on output probabilities (token confidence, entropy) are often inaccessible for closed-source LLMs like GPT models \\cite{yang20251dw}.\n\n**2. Related Work & Positioning**\n*   **Relation to Existing Approaches:** This work positions itself against methods relying on external knowledge bases and those requiring access to internal LLM probabilities \\cite{yang20251dw}. It is a \"zero-resource\" and \"self-contained\" approach, similar in spirit to SelfCheckGPT \\cite{yang20251dw}.\n*   **Limitations of Previous Solutions:**\n    *   External resource-dependent methods are limited to specific domains, may lack comprehensive databases, and don't cover all types of hallucinations \\cite{yang20251dw}.\n    *   Token-level information is often unavailable for closed-source LLMs \\cite{yang20251dw}.\n    *   SelfCheckGPT, a state-of-the-art zero-resource method, often fails because the LLM tends to generate samples identical or highly similar to the original (potentially hallucinated) response, reinforcing incorrect information and leading to low hallucination scores \\cite{yang20251dw}.\n\n**3. Technical Approach & Innovation**\n*   **Core Technical Method:** The paper introduces **MetaQA**, a self-contained hallucination detection approach that leverages **metamorphic relations (MRs)** and **prompt mutation** \\cite{yang20251dw}.\n    *   It operates by generating a set of *mutations* (synonymous and antonymous) from an LLM's base response using MRs \\cite{yang20251dw}.\n    *   Each mutation is then independently verified for factual consistency by the LLM itself, acting as a \"test oracle\" \\cite{yang20251dw}.\n    *   A hallucination score is calculated based on the factual alignment of these mutation sets \\cite{yang20251dw}.\n    *   The methodology involves four steps: (1) Concise Question-Answering, (2) Mutation Generation, (3) Mutation Verification, and (4) Hallucination Evaluation \\cite{yang20251dw}.\n*   **Novelty/Difference:**\n    *   **Zero-resource and Self-contained:** Unlike many existing methods, MetaQA requires no external databases, search engines, or access to internal LLM probabilities \\cite{yang20251dw}.\n    *   **Metamorphic Relations for Hallucination:** It is the first to apply synonym and antonym-based metamorphic relations to detect hallucinations in LLM responses \\cite{yang20251dw}. This allows for controlled transformations of the response to expose inconsistencies more effectively than simply re-prompting the LLM \\cite{yang20251dw}.\n    *   **Compatibility:** It is compatible with both open-source and closed-source LLMs \\cite{yang20251dw}.\n\n**4. Key Technical Contributions**\n*   **Novel Algorithms/Methods:** Introduction of MetaQA, a novel framework for hallucination detection based on metamorphic relations and prompt mutation \\cite{yang20251dw}.\n*   **Technique:** The specific application of synonym and antonym-based metamorphic relations to generate diverse response mutations, which are then individually verified by the LLM itself to detect factual inconsistencies \\cite{yang20251dw}.\n*   **System Design:** A self-contained, zero-resource architecture that relies solely on the target LLM for both mutation generation and verification, eliminating dependencies on external tools or inaccessible internal model states \\cite{yang20251dw}.\n*   **Dataset Improvement:** The paper contributes an improved version of the TruthfulQA benchmark, named TruthfulQA-Enhanced, by updating 238 questions with new correct answers, supporting more accurate hallucination detection research \\cite{yang20251dw}.\n\n**5. Experimental Validation**\n*   **Experiments Conducted:** A large-scale evaluation comparing MetaQA against SelfCheckGPT, the state-of-the-art zero-resource baseline \\cite{yang20251dw}. Ablation studies were also conducted to assess stability and temperature effects \\cite{yang20251dw}.\n*   **Datasets:** TruthfulQA-Enhanced (improved version), HotpotQA, and FreshQA \\cite{yang20251dw}.\n*   **LLMs Tested:** Four LLMs: GPT-4, GPT-3.5 (closed-source), Llama3, and Mistral (open-source) \\cite{yang20251dw}.\n*   **Key Performance Metrics:** Precision, Recall, and F1-score \\cite{yang20251dw}.\n*   **Comparison Results:**\n    *   MetaQA consistently outperforms SelfCheckGPT across all tested LLMs and datasets \\cite{yang20251dw}.\n    *   **F1-score superiority margin:** Ranged from 0.154 to 0.368 over SelfCheckGPT \\cite{yang20251dw}.\n    *   **Example (Mistral-7B):** MetaQA achieved an average F1-score of 0.435, compared to SelfCheckGPTâ€™s 0.205, representing an improvement rate of 112.2% \\cite{yang20251dw}.\n    *   **Precision superiority margin:** Ranged from 0.041 to 0.113 \\cite{yang20251dw}.\n    *   **Recall superiority margin:** Ranged from 0.143 to 0.430 \\cite{yang20251dw}.\n    *   MetaQA also demonstrated superiority across all different categories of questions \\cite{yang20251dw}.\n    *   Ablation studies showed MetaQA has considerable stability across multiple runs and performs better with lower temperatures \\cite{yang20251dw}.\n\n**6. Limitations & Scope**\n*   **Technical Limitations/Assumptions:** The effectiveness of MetaQA relies on the LLM's ability to correctly generate and verify mutations based on metamorphic relations \\cite{yang20251dw}. The quality of the prompt mutations and the LLM's judgment in verification are critical.\n*   **Scope of Applicability:** MetaQA is specifically designed for detecting \"fact-conflicting hallucinations\" in LLM responses \\cite{yang20251dw}. While applicable to both open and closed-source LLMs, its performance might vary depending on the LLM's inherent capabilities and robustness to prompt variations \\cite{yang20251dw}.\n\n**7. Technical Significance**\n*   **Advancement of State-of-the-Art:** MetaQA significantly advances the technical state-of-the-art in zero-resource hallucination detection by introducing a novel, robust, and self-contained method based on metamorphic relations \\cite{yang20251dw}. It addresses key limitations of previous approaches, particularly for closed-source models \\cite{yang20251dw}.\n*   **Potential Impact:**\n    *   Enables more reliable deployment of LLMs in applications requiring high factual accuracy, especially where external knowledge bases are unavailable or internal model access is restricted \\cite{yang20251dw}.\n    *   Provides a new paradigm for LLM evaluation and testing, leveraging the LLM's own capabilities as an oracle \\cite{yang20251dw}.\n    *   The improved TruthfulQA-Enhanced benchmark will support more accurate and rigorous future research in hallucination detection \\cite{yang20251dw}.",
      "intriguing_abstract": "Large Language Models (LLMs) are plagued by fact-conflicting hallucinations, severely undermining their reliability in critical applications. Current detection methods struggle with external resource dependencies, privacy concerns, or inaccessibility to internal model probabilities, especially for closed-source LLMs. We introduce **MetaQA**, a novel, zero-resource, and self-contained framework that revolutionizes hallucination detection. MetaQA leverages **metamorphic relations (MRs)** and **prompt mutation** to generate diverse response variations (synonymous and antonymous) from an LLM's initial output. The LLM then acts as its own \"test oracle,\" verifying the factual consistency of these mutations. A hallucination score is derived from this internal verification, eliminating the need for external knowledge or model internals.\n\nEvaluated across GPT-4, GPT-3.5, Llama3, and Mistral on datasets like TruthfulQA-Enhanced, MetaQA consistently and significantly outperforms state-of-the-art zero-resource baselines, achieving F1-score improvements up to 112.2%. By applying MRs for the first time in this context, MetaQA offers a robust, scalable solution compatible with all LLM types. This work establishes a new paradigm for enhancing LLM trustworthiness and enabling their safe deployment in high-stakes environments.",
      "keywords": [
        "LLM Hallucination Detection",
        "Fact-Conflicting Hallucinations",
        "MetaQA",
        "Metamorphic Relations (MRs)",
        "Prompt Mutation",
        "Zero-Resource Hallucination Detection",
        "Self-Contained Approach",
        "LLM Reliability and Factual Accuracy",
        "Synonym and Antonym-Based MRs",
        "TruthfulQA-Enhanced Benchmark",
        "Outperforms SelfCheckGPT",
        "Closed-Source LLM Compatibility"
      ],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/425d16205b28ce175c8429965a964d19b6f390c1.pdf",
      "citation_key": "yang20251dw",
      "metadata": {
        "title": "Hallucination Detection in Large Language Models with Metamorphic Relations",
        "authors": [
          "Borui Yang",
          "Md Afif Al Mamun",
          "Jie M. Zhang",
          "Gias Uddin"
        ],
        "published_date": "2025",
        "abstract": "Large Language Models (LLMs) are prone to hallucinations, e.g., factually incorrect information, in their responses. These hallucinations present challenges for LLM-based applications that demand high factual accuracy. Existing hallucination detection methods primarily depend on external resources, which can suffer from issues such as low availability, incomplete coverage, privacy concerns, high latency, low reliability, and poor scalability. There are also methods depending on output probabilities, which are often inaccessible for closed-source LLMs like GPT models. This paper presents MetaQA, a self-contained hallucination detection approach that leverages metamorphic relation and prompt mutation. Unlike existing methods, MetaQA operates without any external resources and is compatible with both open-source and closed-source LLMs.\n \n \n \nMetaQA is based on the hypothesis that if an LLMâ€™s response is a hallucination, the designed metamorphic relations will be violated. We compare MetaQA with the state-of-the-art zero-resource hallucination detection method, SelfCheckGPT, across multiple datasets, and on two open-source and two closed-source LLMs. Our results reveal that MetaQA outperforms SelfCheckGPT in terms of precision, recall, and f1 score. For the four LLMs we study, MetaQA outperforms SelfCheckGPT with a superiority margin ranging from 0.041 - 0.113 (for precision), 0.143 - 0.430 (for recall), and 0.154 - 0.368 (for F1-score). For instance, with Mistral-7B, MetaQA achieves an average F1-score of 0.435, compared to SelfCheckGPTâ€™s F1-score of 0.205, representing an improvement rate of 112.2%. MetaQA also demonstrates superiority across all different categories of questions.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/425d16205b28ce175c8429965a964d19b6f390c1.pdf",
        "venue": "Proc. ACM Softw. Eng.",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the technical paper for a literature review, emphasizing technical innovations and empirical validation:\n\n### Focused Summary for Literature Review\n\n**1. Research Problem & Motivation**\n*   **Specific Technical Problem:** Large Language Models (LLMs) are prone to generating hallucinations, specifically factually incorrect or irrelevant information, which undermines their reliability in applications demanding high factual accuracy \\cite{yang20251dw}. The paper focuses on detecting \"fact-conflicting hallucinations\" \\cite{yang20251dw}.\n*   **Importance & Challenge:** Hallucinations, especially fact-conflicting ones, can mislead users, erode trust, and have serious consequences (e.g., in legal contexts) \\cite{yang20251dw}. Existing detection methods face significant challenges:\n    *   Reliance on external resources (databases, search engines) suffers from low availability, incomplete coverage, privacy concerns, high latency, low reliability, and poor scalability \\cite{yang20251dw}.\n    *   Methods depending on output probabilities (token confidence, entropy) are often inaccessible for closed-source LLMs like GPT models \\cite{yang20251dw}.\n\n**2. Related Work & Positioning**\n*   **Relation to Existing Approaches:** This work positions itself against methods relying on external knowledge bases and those requiring access to internal LLM probabilities \\cite{yang20251dw}. It is a \"zero-resource\" and \"self-contained\" approach, similar in spirit to SelfCheckGPT \\cite{yang20251dw}.\n*   **Limitations of Previous Solutions:**\n    *   External resource-dependent methods are limited to specific domains, may lack comprehensive databases, and don't cover all types of hallucinations \\cite{yang20251dw}.\n    *   Token-level information is often unavailable for closed-source LLMs \\cite{yang20251dw}.\n    *   SelfCheckGPT, a state-of-the-art zero-resource method, often fails because the LLM tends to generate samples identical or highly similar to the original (potentially hallucinated) response, reinforcing incorrect information and leading to low hallucination scores \\cite{yang20251dw}.\n\n**3. Technical Approach & Innovation**\n*   **Core Technical Method:** The paper introduces **MetaQA**, a self-contained hallucination detection approach that leverages **metamorphic relations (MRs)** and **prompt mutation** \\cite{yang20251dw}.\n    *   It operates by generating a set of *mutations* (synonymous and antonymous) from an LLM's base response using MRs \\cite{yang20251dw}.\n    *   Each mutation is then independently verified for factual consistency by the LLM itself, acting as a \"test oracle\" \\cite{yang20251dw}.\n    *   A hallucination score is calculated based on the factual alignment of these mutation sets \\cite{yang20251dw}.\n    *   The methodology involves four steps: (1) Concise Question-Answering, (2) Mutation Generation, (3) Mutation Verification, and (4) Hallucination Evaluation \\cite{yang20251dw}.\n*   **Novelty/Difference:**\n    *   **Zero-resource and Self-contained:** Unlike many existing methods, MetaQA requires no external databases, search engines, or access to internal LLM probabilities \\cite{yang20251dw}.\n    *   **Metamorphic Relations for Hallucination:** It is the first to apply synonym and antonym-based metamorphic relations to detect hallucinations in LLM responses \\cite{yang20251dw}. This allows for controlled transformations of the response to expose inconsistencies more effectively than simply re-prompting the LLM \\cite{yang20251dw}.\n    *   **Compatibility:** It is compatible with both open-source and closed-source LLMs \\cite{yang20251dw}.\n\n**4. Key Technical Contributions**\n*   **Novel Algorithms/Methods:** Introduction of MetaQA, a novel framework for hallucination detection based on metamorphic relations and prompt mutation \\cite{yang20251dw}.\n*   **Technique:** The specific application of synonym and antonym-based metamorphic relations to generate diverse response mutations, which are then individually verified by the LLM itself to detect factual inconsistencies \\cite{yang20251dw}.\n*   **System Design:** A self-contained, zero-resource architecture that relies solely on the target LLM for both mutation generation and verification, eliminating dependencies on external tools or inaccessible internal model states \\cite{yang20251dw}.\n*   **Dataset Improvement:** The paper contributes an improved version of the TruthfulQA benchmark, named TruthfulQA-Enhanced, by updating 238 questions with new correct answers, supporting more accurate hallucination detection research \\cite{yang20251dw}.\n\n**5. Experimental Validation**\n*   **Experiments Conducted:** A large-scale evaluation comparing MetaQA against SelfCheckGPT, the state-of-the-art zero-resource baseline \\cite{yang20251dw}. Ablation studies were also conducted to assess stability and temperature effects \\cite{yang20251dw}.\n*   **Datasets:** TruthfulQA-Enhanced (improved version), HotpotQA, and FreshQA \\cite{yang20251dw}.\n*   **LLMs Tested:** Four LLMs: GPT-4, GPT-3.5 (closed-source), Llama3, and Mistral (open-source) \\cite{yang20251dw}.\n*   **Key Performance Metrics:** Precision, Recall, and F1-score \\cite{yang20251dw}.\n*   **Comparison Results:**\n    *   MetaQA consistently outperforms SelfCheckGPT across all tested LLMs and datasets \\cite{yang20251dw}.\n    *   **F1-score superiority margin:** Ranged from 0.154 to 0.368 over SelfCheckGPT \\cite{yang20251dw}.\n    *   **Example (Mistral-7B):** MetaQA achieved an average F1-score of 0.435, compared to SelfCheckGPTâ€™s 0.205, representing an improvement rate of 112.2% \\cite{yang20251dw}.\n    *   **Precision superiority margin:** Ranged from 0.041 to 0.113 \\cite{yang20251dw}.\n    *   **Recall superiority margin:** Ranged from 0.143 to 0.430 \\cite{yang20251dw}.\n    *   MetaQA also demonstrated superiority across all different categories of questions \\cite{yang20251dw}.\n    *   Ablation studies showed MetaQA has considerable stability across multiple runs and performs better with lower temperatures \\cite{yang20251dw}.\n\n**6. Limitations & Scope**\n*   **Technical Limitations/Assumptions:** The effectiveness of MetaQA relies on the LLM's ability to correctly generate and verify mutations based on metamorphic relations \\cite{yang20251dw}. The quality of the prompt mutations and the LLM's judgment in verification are critical.\n*   **Scope of Applicability:** MetaQA is specifically designed for detecting \"fact-conflicting hallucinations\" in LLM responses \\cite{yang20251dw}. While applicable to both open and closed-source LLMs, its performance might vary depending on the LLM's inherent capabilities and robustness to prompt variations \\cite{yang20251dw}.\n\n**7. Technical Significance**\n*   **Advancement of State-of-the-Art:** MetaQA significantly advances the technical state-of-the-art in zero-resource hallucination detection by introducing a novel, robust, and self-contained method based on metamorphic relations \\cite{yang20251dw}. It addresses key limitations of previous approaches, particularly for closed-source models \\cite{yang20251dw}.\n*   **Potential Impact:**\n    *   Enables more reliable deployment of LLMs in applications requiring high factual accuracy, especially where external knowledge bases are unavailable or internal model access is restricted \\cite{yang20251dw}.\n    *   Provides a new paradigm for LLM evaluation and testing, leveraging the LLM's own capabilities as an oracle \\cite{yang20251dw}.\n    *   The improved TruthfulQA-Enhanced benchmark will support more accurate and rigorous future research in hallucination detection \\cite{yang20251dw}.",
        "keywords": [
          "LLM Hallucination Detection",
          "Fact-Conflicting Hallucinations",
          "MetaQA",
          "Metamorphic Relations (MRs)",
          "Prompt Mutation",
          "Zero-Resource Hallucination Detection",
          "Self-Contained Approach",
          "LLM Reliability and Factual Accuracy",
          "Synonym and Antonym-Based MRs",
          "TruthfulQA-Enhanced Benchmark",
          "Outperforms SelfCheckGPT",
          "Closed-Source LLM Compatibility"
        ],
        "paper_type": "based on the provided abstract and introduction snippets, and especially the title:\n\n*   **title:** \"hallucination detection in large language models with metamorphic relations\"\n    *   this title strongly suggests the paper introduces a *method* or *system* for \"hallucination detection\" using a specific technique, \"metamorphic relations.\" this aligns directly with the \"technical\" classification.\n*   **abstract & introduction:** both snippets focus on defining and emphasizing the problem of \"hallucinations\" in large language models (llms), highlighting the challenges they pose. this problem statement is a typical precursor to presenting a technical solution.\n\nwhile the snippets don't explicitly contain phrases like \"we propose\" or \"we develop\" (likely due to being incomplete), the overall context, particularly the title, points to the development and presentation of a new method or system.\n\ntherefore, the paper is best classified as **technical**."
      },
      "file_name": "425d16205b28ce175c8429965a964d19b6f390c1.pdf"
    },
    {
      "success": true,
      "doc_id": "44885d601c11c6551d0eba9cfca220ad",
      "summary": "This paper introduces CCHall, a novel benchmark designed to evaluate Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) for hallucinations in joint cross-lingual and cross-modal scenarios \\cite{zhang2025pex}.\n\n*   **1. Research Problem & Motivation**\n    *   **Specific Technical Problem**: The paper addresses the critical issue of hallucinations in LLMs, particularly when operating in complex, real-world scenarios that simultaneously involve both cross-lingual and cross-modal inputs \\cite{zhang2025pex}.\n    *   **Importance and Challenge**: Hallucinations severely hinder the reliable deployment of LLMs in applications like medical diagnosis and image captioning. Existing research primarily focuses on either cross-lingual *or* cross-modal hallucinations in isolation. The joint scenario is more prevalent in real-world applications and presents significantly greater challenges, as MLLMs must align visual content with text across multiple languages, leading to increased hallucination rates \\cite{zhang2025pex}.\n\n*   **2. Related Work & Positioning**\n    *   **Relation to Existing Approaches**: Previous work has explored cross-lingual hallucinations (e.g., mFACT, X-fact, HalOmi, MM-Eval, XTRUST) and cross-modal hallucinations (e.g., CHAIR, POPE, M-HalDetect, HallusionBench, MHaluBench) \\cite{zhang2025pex}.\n    *   **Limitations of Previous Solutions**: The primary limitation is that these existing benchmarks and studies address cross-lingual or cross-modal scenarios *separately*. There is a significant gap in evaluating and understanding hallucinations that arise from the *joint* interaction of language differences and multimodal inputs \\cite{zhang2025pex}. CCHall is positioned to fill this specific gap.\n\n*   **3. Technical Approach & Innovation**\n    *   **Core Technical Method**: CCHall is a benchmark constructed by integrating two multimodal tasks: Visual Question Answering (VQA) and Image Captioning (IC), drawing from datasets like GQA, AMBER, XM3600, and xFlickr&Co \\cite{zhang2025pex}. The benchmark systematically generates four types of data: non-hallucination, cross-lingual non-cross-modal, cross-modal non-cross-lingual, and crucially, *cross-modal and cross-lingual hallucination* \\cite{zhang2025pex}.\n    *   **Novelty**: The key innovation lies in the *joint* construction of cross-lingual and cross-modal hallucination data. This involves:\n        *   Generating cross-modal hallucinations by using Gemini-1.5-Pro to insert semantically similar but incorrect entities into image captions, ensuring natural-sounding hallucinations \\cite{zhang2025pex}.\n        *   Constructing cross-lingual hallucinations through machine translation into a diverse set of low, medium, and high-resource languages (Croatian, Welsh, Swahili; Czech, Dutch, Swedish; French, Spanish, Portuguese), followed by manual verification \\cite{zhang2025pex}.\n        *   Combining these to create scenarios where responses are inconsistent with visual content *and* meanings differ significantly across languages, representing the most challenging joint hallucination type \\cite{zhang2025pex}.\n    *   The benchmark also ensures multi-modal diversity (categorized by CLIP) and multi-lingual diversity (demonstrated by t-SNE of CLIP text embeddings) \\cite{zhang2025pex}.\n\n*   **4. Key Technical Contributions**\n    *   **Novel Benchmark**: Introduction of CCHall, the first benchmark specifically designed for detecting joint cross-lingual and cross-modal hallucinations in MLLMs \\cite{zhang2025pex}.\n    *   **Data Construction Methodology**: A systematic process for generating complex hallucination types, including automated hallucination generation and multi-stage translation and human rechecking for cross-lingual aspects \\cite{zhang2025pex}.\n    *   **Problem Formulation**: Formal definition of joint cross-lingual and cross-modal hallucinations, highlighting the dual inconsistency with visual content and semantic meaning across languages \\cite{zhang2025pex}.\n\n*   **5. Experimental Validation**\n    *   **Experiments Conducted**: A comprehensive evaluation was performed on CCHall using several mainstream open-source and closed-source MLLMs, including GPT-4o, Gemini-1.5-Flash, Llama-3.2-11B-Vision-Instruct, Qwen2-VL-7B-Instruct, Pixtral-12B-2409, and InternVL2-8B \\cite{zhang2025pex}. The study also explored various hallucination mitigation strategies like CoT (Chain-of-Thought), SRO (Self-Reflection Optimization), VDGD (Visual Description Guided Decoding), and HalluciMAD (Multi-Agent Debate) \\cite{zhang2025pex}.\n    *   **Key Performance Metrics and Results**:\n        *   MLLMs generally struggle on the CCHall benchmark, with performance significantly lower than desired \\cite{zhang2025pex}.\n        *   Crucially, MLLMs exhibit substantially worse performance (e.g., F1-score 3.4 lower than cross-lingual only, and 10.9 lower than cross-modal only) when addressing joint cross-lingual and cross-modal hallucinations compared to individual scenarios \\cite{zhang2025pex}.\n        *   Mitigation strategies showed varying effectiveness: basic strategies were more suitable for smaller MLLMs (<12B parameters), while more advanced strategies benefited powerful MLLMs. The use of multilingual contexts and tool-assisted invocation was found to be effective in mitigating hallucinations \\cite{zhang2025pex}.\n\n*   **6. Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The paper implicitly highlights the current technical limitations of state-of-the-art MLLMs, which \"still struggle\" with the complexities introduced by joint cross-lingual and cross-modal scenarios \\cite{zhang2025pex}. The benchmark's scope is limited to specific VQA and IC tasks and a selected set of nine non-English languages.\n    *   **Scope of Applicability**: CCHall is designed to assess MLLMs' capabilities in detecting and mitigating hallucinations in scenarios where both language diversity and multimodal inputs are present, making it relevant for global, multimodal AI applications \\cite{zhang2025pex}.\n\n*   **7. Technical Significance**\n    *   **Advancement of State-of-the-Art**: CCHall advances the technical state-of-the-art by providing the first dedicated benchmark for the previously unaddressed problem of joint cross-lingual and cross-modal hallucinations. It moves beyond isolated evaluations to a more realistic and challenging setting \\cite{zhang2025pex}.\n    *   **Potential Impact**: This benchmark serves as a crucial resource for future research, enabling more rigorous evaluation and development of robust MLLMs. It provides insights into the specific weaknesses of current models in complex multimodal and multilingual contexts, guiding the development of more effective hallucination mitigation techniques and fostering the deployment of safer and more reliable LLMs in diverse global applications \\cite{zhang2025pex}.",
      "intriguing_abstract": "The pervasive challenge of hallucinations severely undermines the reliability of Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs), particularly in complex, real-world scenarios demanding simultaneous cross-lingual and cross-modal understanding. Existing benchmarks, however, critically overlook the intricate interplay of these factors, evaluating cross-lingual or cross-modal hallucinations in isolation.\n\nWe introduce CCHall, the first comprehensive benchmark specifically engineered to expose and quantify *joint cross-lingual and cross-modal hallucinations* in MLLMs. CCHall systematically constructs challenging Visual Question Answering (VQA) and Image Captioning (IC) tasks, generating data where models must reconcile visual content with text across diverse languages, leading to dual inconsistencies. Our novel methodology leverages advanced LLMs for natural cross-modal hallucination generation and rigorous multi-stage translation for cross-lingual fidelity.\n\nEvaluating state-of-the-art MLLMs, including GPT-4o and Gemini-1.5-Flash, on CCHall reveals a stark reality: models exhibit significantly degraded performance (e.g., F1-score 10.9 lower than cross-modal only) in joint scenarios compared to individual challenges. This benchmark provides an indispensable resource for advancing robust MLLM development, guiding the creation of effective mitigation strategies, and fostering the deployment of safer, more reliable AI systems in a globally interconnected world.",
      "keywords": [
        "CCHall",
        "joint cross-lingual and cross-modal hallucinations",
        "Multimodal Large Language Models (MLLMs)",
        "hallucination evaluation benchmark",
        "automated hallucination generation",
        "Visual Question Answering (VQA)",
        "Image Captioning (IC)",
        "multilingual contexts",
        "hallucination mitigation strategies",
        "significant performance degradation",
        "robust MLLM development"
      ],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/4661b7e8f6bb4f0cc1d4a767a92534f1def344b8.pdf",
      "citation_key": "zhang2025pex",
      "metadata": {
        "title": "CCHall: A Novel Benchmark for Joint Cross-Lingual and Cross-Modal Hallucinations Detection in Large Language Models",
        "authors": [
          "Yongheng Zhang",
          "Xu Liu",
          "Ruoxi Zhou",
          "Qiguang Chen",
          "Hao Fei",
          "Wenpeng Lu",
          "Libo Qin"
        ],
        "published_date": "2025",
        "abstract": "Investigating hallucination issues in large language models (LLMs) within cross-lingual and cross-modal scenarios can greatly advance the large-scale deployment in real-world applications. Nevertheless, the current studies are limited to a single scenario, either cross-lingual or cross-modal, leaving a gap in the exploration of hallucinations in the joint cross-lingual and cross-modal scenarios. Motivated by this, we introduce a novel joint Cross-lingual and Cross-modal Hallucinations benchmark (CCHall) to fill this gap. Specifically, CCHall simultaneously incorporates both cross-lingual and cross-modal hallucination scenarios, which can be used to assess the cross-lingual and cross-modal capabilities of LLMs. Furthermore, we conduct a comprehensive evaluation on CCHall, exploring both mainstream open-source and closed-source LLMs. The experimental results highlight that current LLMs still struggle with CCHall. We hope CCHall can serve as a valuable resource to assess LLMs in joint cross-lingual and cross-modal scenarios.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/4661b7e8f6bb4f0cc1d4a767a92534f1def344b8.pdf",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "citationCount": 0,
        "score": 0,
        "summary": "This paper introduces CCHall, a novel benchmark designed to evaluate Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) for hallucinations in joint cross-lingual and cross-modal scenarios \\cite{zhang2025pex}.\n\n*   **1. Research Problem & Motivation**\n    *   **Specific Technical Problem**: The paper addresses the critical issue of hallucinations in LLMs, particularly when operating in complex, real-world scenarios that simultaneously involve both cross-lingual and cross-modal inputs \\cite{zhang2025pex}.\n    *   **Importance and Challenge**: Hallucinations severely hinder the reliable deployment of LLMs in applications like medical diagnosis and image captioning. Existing research primarily focuses on either cross-lingual *or* cross-modal hallucinations in isolation. The joint scenario is more prevalent in real-world applications and presents significantly greater challenges, as MLLMs must align visual content with text across multiple languages, leading to increased hallucination rates \\cite{zhang2025pex}.\n\n*   **2. Related Work & Positioning**\n    *   **Relation to Existing Approaches**: Previous work has explored cross-lingual hallucinations (e.g., mFACT, X-fact, HalOmi, MM-Eval, XTRUST) and cross-modal hallucinations (e.g., CHAIR, POPE, M-HalDetect, HallusionBench, MHaluBench) \\cite{zhang2025pex}.\n    *   **Limitations of Previous Solutions**: The primary limitation is that these existing benchmarks and studies address cross-lingual or cross-modal scenarios *separately*. There is a significant gap in evaluating and understanding hallucinations that arise from the *joint* interaction of language differences and multimodal inputs \\cite{zhang2025pex}. CCHall is positioned to fill this specific gap.\n\n*   **3. Technical Approach & Innovation**\n    *   **Core Technical Method**: CCHall is a benchmark constructed by integrating two multimodal tasks: Visual Question Answering (VQA) and Image Captioning (IC), drawing from datasets like GQA, AMBER, XM3600, and xFlickr&Co \\cite{zhang2025pex}. The benchmark systematically generates four types of data: non-hallucination, cross-lingual non-cross-modal, cross-modal non-cross-lingual, and crucially, *cross-modal and cross-lingual hallucination* \\cite{zhang2025pex}.\n    *   **Novelty**: The key innovation lies in the *joint* construction of cross-lingual and cross-modal hallucination data. This involves:\n        *   Generating cross-modal hallucinations by using Gemini-1.5-Pro to insert semantically similar but incorrect entities into image captions, ensuring natural-sounding hallucinations \\cite{zhang2025pex}.\n        *   Constructing cross-lingual hallucinations through machine translation into a diverse set of low, medium, and high-resource languages (Croatian, Welsh, Swahili; Czech, Dutch, Swedish; French, Spanish, Portuguese), followed by manual verification \\cite{zhang2025pex}.\n        *   Combining these to create scenarios where responses are inconsistent with visual content *and* meanings differ significantly across languages, representing the most challenging joint hallucination type \\cite{zhang2025pex}.\n    *   The benchmark also ensures multi-modal diversity (categorized by CLIP) and multi-lingual diversity (demonstrated by t-SNE of CLIP text embeddings) \\cite{zhang2025pex}.\n\n*   **4. Key Technical Contributions**\n    *   **Novel Benchmark**: Introduction of CCHall, the first benchmark specifically designed for detecting joint cross-lingual and cross-modal hallucinations in MLLMs \\cite{zhang2025pex}.\n    *   **Data Construction Methodology**: A systematic process for generating complex hallucination types, including automated hallucination generation and multi-stage translation and human rechecking for cross-lingual aspects \\cite{zhang2025pex}.\n    *   **Problem Formulation**: Formal definition of joint cross-lingual and cross-modal hallucinations, highlighting the dual inconsistency with visual content and semantic meaning across languages \\cite{zhang2025pex}.\n\n*   **5. Experimental Validation**\n    *   **Experiments Conducted**: A comprehensive evaluation was performed on CCHall using several mainstream open-source and closed-source MLLMs, including GPT-4o, Gemini-1.5-Flash, Llama-3.2-11B-Vision-Instruct, Qwen2-VL-7B-Instruct, Pixtral-12B-2409, and InternVL2-8B \\cite{zhang2025pex}. The study also explored various hallucination mitigation strategies like CoT (Chain-of-Thought), SRO (Self-Reflection Optimization), VDGD (Visual Description Guided Decoding), and HalluciMAD (Multi-Agent Debate) \\cite{zhang2025pex}.\n    *   **Key Performance Metrics and Results**:\n        *   MLLMs generally struggle on the CCHall benchmark, with performance significantly lower than desired \\cite{zhang2025pex}.\n        *   Crucially, MLLMs exhibit substantially worse performance (e.g., F1-score 3.4 lower than cross-lingual only, and 10.9 lower than cross-modal only) when addressing joint cross-lingual and cross-modal hallucinations compared to individual scenarios \\cite{zhang2025pex}.\n        *   Mitigation strategies showed varying effectiveness: basic strategies were more suitable for smaller MLLMs (<12B parameters), while more advanced strategies benefited powerful MLLMs. The use of multilingual contexts and tool-assisted invocation was found to be effective in mitigating hallucinations \\cite{zhang2025pex}.\n\n*   **6. Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The paper implicitly highlights the current technical limitations of state-of-the-art MLLMs, which \"still struggle\" with the complexities introduced by joint cross-lingual and cross-modal scenarios \\cite{zhang2025pex}. The benchmark's scope is limited to specific VQA and IC tasks and a selected set of nine non-English languages.\n    *   **Scope of Applicability**: CCHall is designed to assess MLLMs' capabilities in detecting and mitigating hallucinations in scenarios where both language diversity and multimodal inputs are present, making it relevant for global, multimodal AI applications \\cite{zhang2025pex}.\n\n*   **7. Technical Significance**\n    *   **Advancement of State-of-the-Art**: CCHall advances the technical state-of-the-art by providing the first dedicated benchmark for the previously unaddressed problem of joint cross-lingual and cross-modal hallucinations. It moves beyond isolated evaluations to a more realistic and challenging setting \\cite{zhang2025pex}.\n    *   **Potential Impact**: This benchmark serves as a crucial resource for future research, enabling more rigorous evaluation and development of robust MLLMs. It provides insights into the specific weaknesses of current models in complex multimodal and multilingual contexts, guiding the development of more effective hallucination mitigation techniques and fostering the deployment of safer and more reliable LLMs in diverse global applications \\cite{zhang2025pex}.",
        "keywords": [
          "CCHall",
          "joint cross-lingual and cross-modal hallucinations",
          "Multimodal Large Language Models (MLLMs)",
          "hallucination evaluation benchmark",
          "automated hallucination generation",
          "Visual Question Answering (VQA)",
          "Image Captioning (IC)",
          "multilingual contexts",
          "hallucination mitigation strategies",
          "significant performance degradation",
          "robust MLLM development"
        ],
        "paper_type": "the paper type is **technical**.\n\n**reasoning:**\n\n1.  **abstract keywords:** the abstract explicitly states, \"we introduce a novel joint cross-lingual and cross-modal hallucinations benchmark (cchall) to fill this gap.\" the creation and presentation of this \"novel benchmark\" (a new system/resource) is a core technical contribution. keywords like \"propose,\" \"develop,\" and \"present\" are strongly implied by \"introduce a novel benchmark.\"\n2.  **introduction focus:** the introduction sets up a problem (hallucinations in llms, especially cross-lingual/cross-modal) and the abstract immediately follows by stating the paper's solution: the cchall benchmark.\n3.  **empirical aspect:** while the paper also \"conducts a comprehensive evaluation on cchall\" and presents \"experimental results,\" this empirical work is performed *using* the newly developed benchmark. the primary contribution, as highlighted by the \"novel\" aspect and the \"introduction\" of cchall, is the benchmark itself. therefore, the empirical evaluation serves to validate and demonstrate the utility of the technical contribution.\n\nthe paper's main goal is to present a new tool/system (the cchall benchmark) to address a specific technical problem, making it primarily a technical paper."
      },
      "file_name": "4661b7e8f6bb4f0cc1d4a767a92534f1def344b8.pdf"
    },
    {
      "success": true,
      "doc_id": "fa74e92d569c28676871bc064578f958",
      "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **1. Research Problem & Motivation**\n    *   **Problem**: Multimodal Large Language Models (MLLMs) suffer from \"object hallucination,\" where they generate descriptions of objects not present in the corresponding images \\cite{xing2024itg}.\n    *   **Importance & Challenge**: This phenomenon leads to misinformation, undermining user trust in MLLM applications. Existing finetuning-based methods for mitigation are computationally intensive and require expensive human annotation to construct paired hallucinated and non-hallucinated data for alignment algorithms \\cite{xing2024itg}.\n\n*   **2. Related Work & Positioning**\n    *   **Existing Approaches**:\n        *   **Inference-based methods** (e.g., LURE, Woodpecker, VIGC, VOLCANO, VCD, ICD, HIO) correct or restrict generated content during inference.\n        *   **Finetuning-based methods** (e.g., LLaVA-RLHF, RLHF-V, DPO, instruction tuning, contrastive learning) adjust the model directly through specialized datasets and preference alignment algorithms.\n    *   **Limitations of Previous Solutions**:\n        *   Inference-based methods incur additional inference steps, increased costs, delays, and often require task-specific procedures or prompts.\n        *   Finetuning-based methods demand substantial data (paired positive and negative samples), rely on expensive human annotation, and require considerable computational resources due to the simultaneous operation of multiple models for preference alignment \\cite{xing2024itg}.\n\n*   **3. Technical Approach & Innovation**\n    *   **Core Method**: The paper proposes the Efficient Fine-Grained Unlearning Framework (EFUF) \\cite{xing2024itg}.\n    *   **Novelty**: EFUF mitigates hallucinations by performing gradient ascent using three tailored losses, crucially *without requiring manually-annotated paired data* \\cite{xing2024itg}. It also significantly reduces computational resources compared to traditional alignment algorithms.\n    *   **Data Curation**: EFUF leverages the CLIP model to automatically evaluate text-image congruence. CLIP scores are used to reliably and cost-effectively distinguish between real and hallucinated objects, enabling the automatic curation of positive and negative samples \\cite{xing2024itg}.\n    *   **Fine-Grained Unlearning**: Instead of unlearning entire responses, EFUF focuses on unlearning hallucinated objects at the subsentence level. It constructs three datasets: positive subsentences (D+), negative subsentences (D-), and complete sentences (Ds), based on CLIP-derived image-relevance scores and predefined thresholds \\cite{xing2024itg}.\n    *   **Loss Functions**: EFUF employs a dual-faceted unlearning approach with three distinct losses:\n        *   **Negative Loss**: Applied to subsentences containing hallucinated objects via gradient ascent to curtail their production.\n        *   **Positive Loss**: Applied to subsentences containing non-hallucinated objects via gradient descent to encourage precise object representation.\n        *   **Sentence Loss**: Applied to complete sentences via gradient descent to preserve the model's overall linguistic comprehension and ability to generate cohesive, long-form text \\cite{xing2024itg}.\n\n*   **4. Key Technical Contributions**\n    *   **Novel Application of Unlearning**: EFUF introduces a new perspective by being the first to utilize unlearning specifically for mitigating multimodal hallucination in MLLMs \\cite{xing2024itg}.\n    *   **Efficient Framework**: Proposes EFUF, an efficient and fine-grained unlearning framework that addresses the data and computational bottlenecks of prior methods \\cite{xing2024itg}.\n    *   **Automated Data Curation**: Develops a cost-effective and reliable method for automatically obtaining positive and negative examples using CLIP scores, eliminating the need for expensive manual annotation \\cite{xing2024itg}.\n    *   **Tailored Loss Functions**: Designs a set of three specific loss functions (negative, positive, and sentence losses) for fine-grained unlearning, balancing hallucination reduction with generation quality preservation \\cite{xing2024itg}.\n    *   **Compatibility**: EFUF demonstrates good compatibility and can be easily extended to existing MLLMs \\cite{xing2024itg}.\n\n*   **5. Experimental Validation**\n    *   **Preliminary Experiment**: Conducted a study using MiniGPT4 and LLaVA, manually annotating 200 image captions. It validated that CLIP's fine-grained image-relevance scores significantly differ between hallucinated and non-hallucinated objects (p-values as low as 6.0x10^-30), confirming CLIP's utility for hallucination detection and sample segregation \\cite{xing2024itg}.\n    *   **Main Experiments**: \"Extensive experiments\" were conducted across a range of MLLMs (though specific models beyond MiniGPT4 and LLaVA are not detailed in the provided text) \\cite{xing2024itg}.\n    *   **Key Results**: EFUF consistently reduces hallucinations while preserving the overall generation quality of MLLMs, all with modest computational overhead \\cite{xing2024itg}.\n\n*   **6. Limitations & Scope**\n    *   **Assumptions**: The method's effectiveness relies on the CLIP model's ability to accurately assess text-image congruence and the validity of established thresholds for distinguishing hallucinated content \\cite{xing2024itg}.\n    *   **Scope**: EFUF specifically targets object hallucination in MLLMs and is designed as a finetuning-based solution. While it addresses the potential for unlearning to undermine linguistic comprehension through its sentence loss, the inherent trade-offs in unlearning remain a consideration.\n\n*   **7. Technical Significance**\n    *   **Advances State-of-the-Art**: EFUF significantly advances the technical state-of-the-art by providing an efficient, data-agnostic, and computationally lighter method for mitigating MLLM hallucinations, overcoming key limitations of prior finetuning approaches \\cite{xing2024itg}.\n    *   **Potential Impact**: It enables the development of more reliable and trustworthy MLLMs by effectively reducing misinformation. The framework's efficiency and independence from manual annotation could democratize hallucination mitigation, making it more accessible and scalable for future research and practical applications \\cite{xing2024itg}.",
      "intriguing_abstract": "Multimodal Large Language Models (MLLMs) are increasingly powerful, yet their utility is severely hampered by \"object hallucination\"â€”the generation of descriptions for objects not present in corresponding images, leading to misinformation and eroding user trust. Existing finetuning-based mitigation strategies are computationally intensive and demand extensive, manually-annotated paired data, creating significant bottlenecks. We introduce the **Efficient Fine-Grained Unlearning Framework (EFUF)**, a novel approach that pioneers the use of **unlearning** to combat multimodal hallucination. EFUF uniquely eliminates the need for costly human annotation by leveraging **CLIP scores** for automated, reliable text-image congruence evaluation, enabling efficient **data curation** of positive and negative subsentence samples. Our framework employs a sophisticated dual-faceted unlearning mechanism with three tailored loss functions: a **negative loss** using **gradient ascent** to suppress hallucinated objects, a positive loss to reinforce accurate descriptions, and a sentence loss to preserve overall linguistic coherence. Extensive experiments demonstrate EFUF's ability to consistently reduce hallucinations across various MLLMs while maintaining generation quality and significantly lowering computational overhead. EFUF advances the state-of-the-art, offering an efficient, scalable, and data-agnostic solution to build more trustworthy MLLMs and combat misinformation.",
      "keywords": [
        "Multimodal Large Language Models (MLLMs)",
        "object hallucination",
        "Efficient Fine-Grained Unlearning Framework (EFUF)",
        "unlearning for multimodal hallucination",
        "automated data curation with CLIP",
        "fine-grained unlearning",
        "tailored loss functions",
        "manual annotation-free",
        "reduced computational resources",
        "finetuning-based methods",
        "hallucination mitigation",
        "preserving generation quality",
        "gradient ascent"
      ],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/bf54792cf01761a2c51ac3410287797fff665cd4.pdf",
      "citation_key": "xing2024itg",
      "metadata": {
        "title": "EFUF: Efficient Fine-Grained Unlearning Framework for Mitigating Hallucinations in Multimodal Large Language Models",
        "authors": [
          "Shangyu Xing",
          "Fei Zhao",
          "Zhen Wu",
          "Tuo An",
          "Weihao Chen",
          "Chunhui Li",
          "Jianbing Zhang",
          "Xinyu Dai"
        ],
        "published_date": "2024",
        "abstract": "Multimodal large language models (MLLMs) have attracted increasing attention in the past few years, but they may still generate descriptions that include objects not present in the corresponding images, a phenomenon known as object hallucination. To eliminate hallucinations, existing methods manually annotate paired responses with and without hallucinations, and then employ various alignment algorithms to improve the alignment capability between images and text. However, they not only demand considerable computation resources during the finetuning stage but also require expensive human annotation to construct paired data needed by the alignment algorithms. To address these issues, we propose an efficient fine-grained unlearning framework (EFUF), which performs gradient ascent utilizing three tailored losses to eliminate hallucinations without paired data. Extensive experiments show that our method consistently reduces hallucinations while preserving the generation quality with modest computational overhead. Our code and datasets will be publicly available.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/bf54792cf01761a2c51ac3410287797fff665cd4.pdf",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **1. Research Problem & Motivation**\n    *   **Problem**: Multimodal Large Language Models (MLLMs) suffer from \"object hallucination,\" where they generate descriptions of objects not present in the corresponding images \\cite{xing2024itg}.\n    *   **Importance & Challenge**: This phenomenon leads to misinformation, undermining user trust in MLLM applications. Existing finetuning-based methods for mitigation are computationally intensive and require expensive human annotation to construct paired hallucinated and non-hallucinated data for alignment algorithms \\cite{xing2024itg}.\n\n*   **2. Related Work & Positioning**\n    *   **Existing Approaches**:\n        *   **Inference-based methods** (e.g., LURE, Woodpecker, VIGC, VOLCANO, VCD, ICD, HIO) correct or restrict generated content during inference.\n        *   **Finetuning-based methods** (e.g., LLaVA-RLHF, RLHF-V, DPO, instruction tuning, contrastive learning) adjust the model directly through specialized datasets and preference alignment algorithms.\n    *   **Limitations of Previous Solutions**:\n        *   Inference-based methods incur additional inference steps, increased costs, delays, and often require task-specific procedures or prompts.\n        *   Finetuning-based methods demand substantial data (paired positive and negative samples), rely on expensive human annotation, and require considerable computational resources due to the simultaneous operation of multiple models for preference alignment \\cite{xing2024itg}.\n\n*   **3. Technical Approach & Innovation**\n    *   **Core Method**: The paper proposes the Efficient Fine-Grained Unlearning Framework (EFUF) \\cite{xing2024itg}.\n    *   **Novelty**: EFUF mitigates hallucinations by performing gradient ascent using three tailored losses, crucially *without requiring manually-annotated paired data* \\cite{xing2024itg}. It also significantly reduces computational resources compared to traditional alignment algorithms.\n    *   **Data Curation**: EFUF leverages the CLIP model to automatically evaluate text-image congruence. CLIP scores are used to reliably and cost-effectively distinguish between real and hallucinated objects, enabling the automatic curation of positive and negative samples \\cite{xing2024itg}.\n    *   **Fine-Grained Unlearning**: Instead of unlearning entire responses, EFUF focuses on unlearning hallucinated objects at the subsentence level. It constructs three datasets: positive subsentences (D+), negative subsentences (D-), and complete sentences (Ds), based on CLIP-derived image-relevance scores and predefined thresholds \\cite{xing2024itg}.\n    *   **Loss Functions**: EFUF employs a dual-faceted unlearning approach with three distinct losses:\n        *   **Negative Loss**: Applied to subsentences containing hallucinated objects via gradient ascent to curtail their production.\n        *   **Positive Loss**: Applied to subsentences containing non-hallucinated objects via gradient descent to encourage precise object representation.\n        *   **Sentence Loss**: Applied to complete sentences via gradient descent to preserve the model's overall linguistic comprehension and ability to generate cohesive, long-form text \\cite{xing2024itg}.\n\n*   **4. Key Technical Contributions**\n    *   **Novel Application of Unlearning**: EFUF introduces a new perspective by being the first to utilize unlearning specifically for mitigating multimodal hallucination in MLLMs \\cite{xing2024itg}.\n    *   **Efficient Framework**: Proposes EFUF, an efficient and fine-grained unlearning framework that addresses the data and computational bottlenecks of prior methods \\cite{xing2024itg}.\n    *   **Automated Data Curation**: Develops a cost-effective and reliable method for automatically obtaining positive and negative examples using CLIP scores, eliminating the need for expensive manual annotation \\cite{xing2024itg}.\n    *   **Tailored Loss Functions**: Designs a set of three specific loss functions (negative, positive, and sentence losses) for fine-grained unlearning, balancing hallucination reduction with generation quality preservation \\cite{xing2024itg}.\n    *   **Compatibility**: EFUF demonstrates good compatibility and can be easily extended to existing MLLMs \\cite{xing2024itg}.\n\n*   **5. Experimental Validation**\n    *   **Preliminary Experiment**: Conducted a study using MiniGPT4 and LLaVA, manually annotating 200 image captions. It validated that CLIP's fine-grained image-relevance scores significantly differ between hallucinated and non-hallucinated objects (p-values as low as 6.0x10^-30), confirming CLIP's utility for hallucination detection and sample segregation \\cite{xing2024itg}.\n    *   **Main Experiments**: \"Extensive experiments\" were conducted across a range of MLLMs (though specific models beyond MiniGPT4 and LLaVA are not detailed in the provided text) \\cite{xing2024itg}.\n    *   **Key Results**: EFUF consistently reduces hallucinations while preserving the overall generation quality of MLLMs, all with modest computational overhead \\cite{xing2024itg}.\n\n*   **6. Limitations & Scope**\n    *   **Assumptions**: The method's effectiveness relies on the CLIP model's ability to accurately assess text-image congruence and the validity of established thresholds for distinguishing hallucinated content \\cite{xing2024itg}.\n    *   **Scope**: EFUF specifically targets object hallucination in MLLMs and is designed as a finetuning-based solution. While it addresses the potential for unlearning to undermine linguistic comprehension through its sentence loss, the inherent trade-offs in unlearning remain a consideration.\n\n*   **7. Technical Significance**\n    *   **Advances State-of-the-Art**: EFUF significantly advances the technical state-of-the-art by providing an efficient, data-agnostic, and computationally lighter method for mitigating MLLM hallucinations, overcoming key limitations of prior finetuning approaches \\cite{xing2024itg}.\n    *   **Potential Impact**: It enables the development of more reliable and trustworthy MLLMs by effectively reducing misinformation. The framework's efficiency and independence from manual annotation could democratize hallucination mitigation, making it more accessible and scalable for future research and practical applications \\cite{xing2024itg}.",
        "keywords": [
          "Multimodal Large Language Models (MLLMs)",
          "object hallucination",
          "Efficient Fine-Grained Unlearning Framework (EFUF)",
          "unlearning for multimodal hallucination",
          "automated data curation with CLIP",
          "fine-grained unlearning",
          "tailored loss functions",
          "manual annotation-free",
          "reduced computational resources",
          "finetuning-based methods",
          "hallucination mitigation",
          "preserving generation quality",
          "gradient ascent"
        ],
        "paper_type": "based on the abstract and introduction, this paper should be classified as:\n\n**technical**\n\n**reasoning:**\n\n*   the abstract explicitly states: \"to address these issues, we **propose an efficient fine-grained unlearning framework (efuf)**, which performs gradient ascent utilizing three tailored losses to eliminate hallucinations without paired data.\" this directly aligns with the \"technical\" criterion of presenting new methods, algorithms, or systems.\n*   it discusses a \"technical problem\" (object hallucination in mllms) and a \"proposed solution\" (efuf).\n*   while it mentions \"extensive experiments show that our method consistently reduces hallucinations...\", these experiments serve to validate the *proposed method*, which is characteristic of a technical paper evaluating its new contribution, rather than being a purely data-driven study focused on statistical findings about an existing phenomenon (which would be \"empirical\")."
      },
      "file_name": "bf54792cf01761a2c51ac3410287797fff665cd4.pdf"
    },
    {
      "success": true,
      "doc_id": "2d177f48a9bfddc5591b7f4dc26b4e16",
      "summary": "Here's a focused summary of the paper \\cite{leng2023ohr} for a literature review:\n\n### Analysis of \"Mitigating Object Hallucinations in Large Vision-Language Models through Visual Contrastive Decoding\" \\cite{leng2023ohr}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Large Vision-Language Models (LVLMs) suffer from \"object hallucinations,\" where they generate plausible but incorrect outputs containing objects not present in the image.\n    *   **Importance and Challenge**: This problem impacts the reliability and applicability of LVLMs, leading to misinformation and erroneous decision-making in critical domains (e.g., healthcare, autonomous systems). It stems from fundamental issues like LVLMs' over-reliance on statistical biases from training data and unimodal (language) priors embedded in their LLM components.\n\n2.  **Related Work & Positioning**\n    *   **Existing Approaches**:\n        *   **Early VLMs**: Fine-grained modality alignment \\cite{leng2023ohr}, reducing statistical bias with data augmentation \\cite{leng2023ohr}. These are often impractical for modern LVLMs due to differing architectures and scale.\n        *   **Recent LVLMs**: Hallucination-targeted datasets for fine-tuning \\cite{leng2023ohr}, training post-hoc revisors \\cite{leng2023ohr}, or adapting factually augmented Reinforcement Learning from Human Feedback (RLHF) \\cite{leng2023ohr}.\n    *   **Limitations of Previous Solutions**: Existing interventions for LVLMs are often time-consuming, labor-intensive, and computationally costly, requiring additional training, new datasets, or external pretrained models.\n    *   **Positioning**: \\cite{leng2023ohr} proposes Visual Contrastive Decoding (VCD) as a simpler, training-free, and efficient approach that circumvents the need for additional training or external tools.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method (Visual Contrastive Decoding - VCD)**: VCD mitigates object hallucinations by contrasting output distributions derived from original and *distorted* visual inputs. This acts as a corrective mechanism to calibrate the model's over-reliance on language priors and statistical biases.\n    *   **Introduction of Visual Uncertainty**: Visual uncertainty is introduced by applying a Gaussian noise mask to the original image, following a forward diffusion process. This creates a distorted image `v'` from the original `v`.\n    *   **Mechanism of Amplification**: The paper empirically demonstrates that increased visual uncertainty (via distortion) amplifies LVLMs' reliance on language priors (e.g., favoring \"yellow\" for a banana even if it's black in a distorted image) and statistical biases (e.g., hallucinating frequent or co-occurring objects more often).\n    *   **Contrastive Probability Distribution**: The core of VCD is defined by:\n        `pvcd(y|v, vâ€², x) = softmax [(1 + Î±) logitÎ¸(y|v, x) âˆ’ Î± logitÎ¸(y|vâ€², x)]`\n        where `Î±` controls the strength of the contrast, and `v'` is the distorted visual input. This formulation penalizes tokens that are highly probable under the distorted input (which is prone to hallucinations) relative to the original input.\n    *   **Adaptive Plausibility Constraints**: To prevent VCD from inadvertently promoting implausible tokens by indiscriminately penalizing all outputs from distorted inputs, an adaptive plausibility constraint is applied. This constraint prunes the candidate token pool `Vhead(y<t)` based on the confidence level of the output distribution with original visual inputs, ensuring only high-probability tokens are considered.\n\n4.  **Key Technical Contributions**\n    *   **In-depth Analysis**: Conducted an analysis demonstrating how visual uncertainty amplifies object hallucinations in LVLMs, specifically due to statistical bias and unimodal priors.\n    *   **Novel Training-Free Technique (VCD)**: Designed Visual Contrastive Decoding, a novel method that effectively mitigates object hallucinations by contrasting output distributions from original and distorted visual inputs, without requiring additional training or external models.\n    *   **Adaptive Plausibility Constraint**: Introduced an adaptive plausibility constraint to refine VCD, preventing the generation of implausible tokens and maintaining output integrity.\n    *   **Empirical Validation**: Demonstrated VCD's efficacy in alleviating object hallucination and enhancing general perception capabilities across various LVLM families.\n\n5.  **Experimental Validation**\n    *   **Datasets**:\n        *   **POPE (Polling-based Object Probing Evaluation)** \\cite{leng2023ohr}: Assesses object hallucination by querying LVLMs on object existence. Includes three sampling settings: random, popular, and adversarial, aggregating data from MSCOCO, A-OKVQA, and GQA.\n        *   **MME**: An extensive benchmark for LVLMs covering perception and cognition subtasks.\n    *   **Key Performance Metrics**:\n        *   **POPE**: Accuracy, Precision, Recall, and F1 score.\n        *   **MME**: General perception capacities.\n    *   **Comparison Results**:\n        *   **Object Hallucination Mitigation**: VCD significantly mitigates object hallucination, achieving up to a +7.4 F1 score boost on POPE and an +18% improvement on MME.\n        *   **General Perception**: Beyond hallucination mitigation, VCD also excels in general LVLM benchmarks (MME and LLaVA-Bench), indicating broader applicability.\n        *   **Model Agnostic**: Demonstrated consistent improvements across different LVLM families, including LLaVA-1.5 \\cite{leng2023ohr}, InstructBLIP \\cite{leng2023ohr}, and Qwen-VL \\cite{leng2023ohr}.\n        *   **Efficiency**: Achieves these improvements without additional training or external tools.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations**: The method of introducing visual uncertainty (Gaussian noise mask) is described as \"elementary,\" suggesting potential for more sophisticated distortion methods. The adaptive plausibility constraint is crucial to prevent VCD from promoting implausible tokens, indicating that naive contrastive decoding could have adverse effects.\n    *   **Scope of Applicability**: While shown to be effective across different LVLM families and beneficial for general perception, the primary focus and validation are on mitigating object hallucinations.\n\n7.  **Technical Significance**\n    *   **Advances State-of-the-Art**: VCD offers a novel, training-free, and computationally efficient approach to a critical problem in LVLMs. By directly addressing the root causes (statistical bias and language priors) through a decoding-time intervention, it provides a practical alternative to costly fine-tuning or external model integration.\n    *   **Potential Impact**: This work opens avenues for future research into more sophisticated visual distortion techniques and adaptive contrastive decoding strategies. Its training-free nature makes it highly adaptable and deployable for improving the reliability of existing LVLMs in real-world applications without extensive retraining.",
      "intriguing_abstract": "The pervasive issue of **object hallucinations** in **Large Vision-Language Models (LVLMs)** critically undermines their reliability, leading to the generation of plausible but non-existent objects. This fundamental flaw stems from LVLMs' over-reliance on **unimodal priors** and **statistical biases**, hindering their deployment in sensitive applications. We introduce **Visual Contrastive Decoding (VCD)**, a novel and remarkably efficient **training-free** approach that directly tackles this challenge.\n\nVCD operates as a sophisticated **decoding-time intervention**, contrasting output distributions derived from original and strategically **distorted visual inputs**. Our key insight reveals that introducing **visual uncertainty** amplifies LVLMs' susceptibility to hallucinations, allowing VCD to precisely penalize hallucinated tokens. Further enhanced by an **adaptive plausibility constraint**, VCD prevents the generation of implausible outputs. Empirical validation demonstrates VCD's profound impact, achieving up to a +7.4 F1 score boost on POPE and an +18% improvement on MME, while also enhancing **general perception** across diverse LVLM families like LLaVA-1.5, InstructBLIP, and Qwen-VL. VCD offers a practical, model-agnostic solution to significantly improve LVLM trustworthiness and applicability.",
      "keywords": [
        "Large Vision-Language Models (LVLMs)",
        "object hallucinations",
        "Visual Contrastive Decoding (VCD)",
        "training-free technique",
        "statistical biases",
        "language priors",
        "visual uncertainty",
        "adaptive plausibility constraint",
        "decoding-time intervention",
        "POPE dataset",
        "general perception capabilities",
        "model-agnostic",
        "reliability"
      ],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/328eb183007bf4aefbf42437b42a15db375803e3.pdf",
      "citation_key": "leng2023ohr",
      "metadata": {
        "title": "Mitigating Object Hallucinations in Large Vision-Language Models through Visual Contrastive Decoding",
        "authors": [
          "Sicong Leng",
          "Hang Zhang",
          "Guanzheng Chen",
          "Xin Li",
          "Shijian Lu",
          "Chunyan Miao",
          "Li Bing"
        ],
        "published_date": "2023",
        "abstract": "Large Vision-Language Models (LVLMs) have advanced considerably, intertwining visual recognition and language understanding to generate content that is not only coherent but also contextually attuned. Despite their success, LVLMs still suffer from the issue of object hallucinations, where models generate plausible yet incorrect outputs that include objects that do not exist in the images. To mitigate this issue, we introduce Visual Contrastive Decoding (VCD), a simple and training-free method that contrasts output distributions derived from original and distorted visual inputs. The proposed VCD effectively reduces the over-reliance on statistical bias and unimodal priors, two essential causes of object hallucinations. This adjustment ensures the generated content is closely grounded to visual inputs, resulting in contextually accurate outputs. Our experiments show that VCD, without either additional training or the usage of external tools, significantly mitigates the object hallucination issue across different LVLM families. Beyond mitigating object hallucinations, VCD also excels in general LVLM benchmarks, highlighting its wide-ranging applicability.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/328eb183007bf4aefbf42437b42a15db375803e3.pdf",
        "venue": "Computer Vision and Pattern Recognition",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the paper \\cite{leng2023ohr} for a literature review:\n\n### Analysis of \"Mitigating Object Hallucinations in Large Vision-Language Models through Visual Contrastive Decoding\" \\cite{leng2023ohr}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Large Vision-Language Models (LVLMs) suffer from \"object hallucinations,\" where they generate plausible but incorrect outputs containing objects not present in the image.\n    *   **Importance and Challenge**: This problem impacts the reliability and applicability of LVLMs, leading to misinformation and erroneous decision-making in critical domains (e.g., healthcare, autonomous systems). It stems from fundamental issues like LVLMs' over-reliance on statistical biases from training data and unimodal (language) priors embedded in their LLM components.\n\n2.  **Related Work & Positioning**\n    *   **Existing Approaches**:\n        *   **Early VLMs**: Fine-grained modality alignment \\cite{leng2023ohr}, reducing statistical bias with data augmentation \\cite{leng2023ohr}. These are often impractical for modern LVLMs due to differing architectures and scale.\n        *   **Recent LVLMs**: Hallucination-targeted datasets for fine-tuning \\cite{leng2023ohr}, training post-hoc revisors \\cite{leng2023ohr}, or adapting factually augmented Reinforcement Learning from Human Feedback (RLHF) \\cite{leng2023ohr}.\n    *   **Limitations of Previous Solutions**: Existing interventions for LVLMs are often time-consuming, labor-intensive, and computationally costly, requiring additional training, new datasets, or external pretrained models.\n    *   **Positioning**: \\cite{leng2023ohr} proposes Visual Contrastive Decoding (VCD) as a simpler, training-free, and efficient approach that circumvents the need for additional training or external tools.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method (Visual Contrastive Decoding - VCD)**: VCD mitigates object hallucinations by contrasting output distributions derived from original and *distorted* visual inputs. This acts as a corrective mechanism to calibrate the model's over-reliance on language priors and statistical biases.\n    *   **Introduction of Visual Uncertainty**: Visual uncertainty is introduced by applying a Gaussian noise mask to the original image, following a forward diffusion process. This creates a distorted image `v'` from the original `v`.\n    *   **Mechanism of Amplification**: The paper empirically demonstrates that increased visual uncertainty (via distortion) amplifies LVLMs' reliance on language priors (e.g., favoring \"yellow\" for a banana even if it's black in a distorted image) and statistical biases (e.g., hallucinating frequent or co-occurring objects more often).\n    *   **Contrastive Probability Distribution**: The core of VCD is defined by:\n        `pvcd(y|v, vâ€², x) = softmax [(1 + Î±) logitÎ¸(y|v, x) âˆ’ Î± logitÎ¸(y|vâ€², x)]`\n        where `Î±` controls the strength of the contrast, and `v'` is the distorted visual input. This formulation penalizes tokens that are highly probable under the distorted input (which is prone to hallucinations) relative to the original input.\n    *   **Adaptive Plausibility Constraints**: To prevent VCD from inadvertently promoting implausible tokens by indiscriminately penalizing all outputs from distorted inputs, an adaptive plausibility constraint is applied. This constraint prunes the candidate token pool `Vhead(y<t)` based on the confidence level of the output distribution with original visual inputs, ensuring only high-probability tokens are considered.\n\n4.  **Key Technical Contributions**\n    *   **In-depth Analysis**: Conducted an analysis demonstrating how visual uncertainty amplifies object hallucinations in LVLMs, specifically due to statistical bias and unimodal priors.\n    *   **Novel Training-Free Technique (VCD)**: Designed Visual Contrastive Decoding, a novel method that effectively mitigates object hallucinations by contrasting output distributions from original and distorted visual inputs, without requiring additional training or external models.\n    *   **Adaptive Plausibility Constraint**: Introduced an adaptive plausibility constraint to refine VCD, preventing the generation of implausible tokens and maintaining output integrity.\n    *   **Empirical Validation**: Demonstrated VCD's efficacy in alleviating object hallucination and enhancing general perception capabilities across various LVLM families.\n\n5.  **Experimental Validation**\n    *   **Datasets**:\n        *   **POPE (Polling-based Object Probing Evaluation)** \\cite{leng2023ohr}: Assesses object hallucination by querying LVLMs on object existence. Includes three sampling settings: random, popular, and adversarial, aggregating data from MSCOCO, A-OKVQA, and GQA.\n        *   **MME**: An extensive benchmark for LVLMs covering perception and cognition subtasks.\n    *   **Key Performance Metrics**:\n        *   **POPE**: Accuracy, Precision, Recall, and F1 score.\n        *   **MME**: General perception capacities.\n    *   **Comparison Results**:\n        *   **Object Hallucination Mitigation**: VCD significantly mitigates object hallucination, achieving up to a +7.4 F1 score boost on POPE and an +18% improvement on MME.\n        *   **General Perception**: Beyond hallucination mitigation, VCD also excels in general LVLM benchmarks (MME and LLaVA-Bench), indicating broader applicability.\n        *   **Model Agnostic**: Demonstrated consistent improvements across different LVLM families, including LLaVA-1.5 \\cite{leng2023ohr}, InstructBLIP \\cite{leng2023ohr}, and Qwen-VL \\cite{leng2023ohr}.\n        *   **Efficiency**: Achieves these improvements without additional training or external tools.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations**: The method of introducing visual uncertainty (Gaussian noise mask) is described as \"elementary,\" suggesting potential for more sophisticated distortion methods. The adaptive plausibility constraint is crucial to prevent VCD from promoting implausible tokens, indicating that naive contrastive decoding could have adverse effects.\n    *   **Scope of Applicability**: While shown to be effective across different LVLM families and beneficial for general perception, the primary focus and validation are on mitigating object hallucinations.\n\n7.  **Technical Significance**\n    *   **Advances State-of-the-Art**: VCD offers a novel, training-free, and computationally efficient approach to a critical problem in LVLMs. By directly addressing the root causes (statistical bias and language priors) through a decoding-time intervention, it provides a practical alternative to costly fine-tuning or external model integration.\n    *   **Potential Impact**: This work opens avenues for future research into more sophisticated visual distortion techniques and adaptive contrastive decoding strategies. Its training-free nature makes it highly adaptable and deployable for improving the reliability of existing LVLMs in real-world applications without extensive retraining.",
        "keywords": [
          "Large Vision-Language Models (LVLMs)",
          "object hallucinations",
          "Visual Contrastive Decoding (VCD)",
          "training-free technique",
          "statistical biases",
          "language priors",
          "visual uncertainty",
          "adaptive plausibility constraint",
          "decoding-time intervention",
          "POPE dataset",
          "general perception capabilities",
          "model-agnostic",
          "reliability"
        ],
        "paper_type": "the paper type is: **technical**\n\n**reasoning:**\n\n*   the **abstract** explicitly states: \"we introduce visual contrastive decoding (vcd), a simple and training-free method that contrasts output distributions...\" and \"the proposed vcd effectively reduces...\" this clearly indicates the presentation of a new method or algorithm.\n*   the **introduction** further elaborates on the problem (object hallucinations) and immediately presents figure 1, which illustrates the \"visual contrastive decoding\" method, including a mathematical formulation of its core idea.\n*   the language used (\"propose\", \"introduce\", \"method\", \"algorithm\") aligns directly with the criteria for a **technical** paper. while experiments are mentioned to validate the method, the primary contribution is the method itself."
      },
      "file_name": "328eb183007bf4aefbf42437b42a15db375803e3.pdf"
    },
    {
      "success": true,
      "doc_id": "933063370200084a8130b972333fd430",
      "summary": "Here's a focused summary of the paper \"What if...?: Thinking Counterfactual Keywords Helps to Mitigate Hallucination in Large Multi-modal Models\" \\cite{kim2024ozf} for a literature review:\n\n---\n\n### Analysis of \\cite{kim2024ozf}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Large Multi-modal Models (LMMs) frequently suffer from \"hallucination,\" where they generate responses that are inconsistent with the visual content, including false premises, incorrect object attributes, nonexistent objects, or erroneous object relations.\n    *   **Importance and Challenge**: Hallucination undermines the reliability and trustworthiness of LMMs, hindering their adoption as general-purpose AI systems. Existing solutions often require extensive additional training, fine-tuning, or labor-intensive human feedback, which can be resource-intensive and limit their applicability to pre-trained models.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: Previous efforts to mitigate hallucination in LMMs include fine-tuning with robust instructions, multi-step LMM-aided reasoning, Reinforcement Learning from Human Feedback (RLHF), and contrastive decoding during inference.\n    *   **Limitations of Previous Solutions**: These methods typically necessitate additional training, tailored instruction datasets, or significant computational resources for fine-tuning. \\cite{kim2024ozf} explicitly positions its work as a *training-free* approach that does not require additional fine-tuning or human-resource instructions, addressing a key limitation of prior art.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes **Counterfactual Inception**, a novel, training-free method that implants counterfactual thinking into LMMs. It leverages the LMMs' own generative capabilities to create \"counterfactual keywords\" that intentionally deviate from the actual visual content. These keywords are then incorporated into a prompt that instructs the LMM to *carefully avoid* them in its response.\n    *   **Novelty**:\n        *   **Self-Generated Counterfactuals**: LMMs themselves generate counterfactual keywords across object, attribute, and relation levels, inspired by human cognitive processes of \"what if\" scenarios. This promotes broader contextual exploration.\n        *   **Plausibility Verification Process (PVP)**: A robust keyword constraint is introduced to filter out suboptimal counterfactual keywords. PVP uses CLIP \\cite{radford2021learning} scores to measure the semantic alignment between visual content and generated keywords, ensuring that selected keywords are \"meaningful, yet not direct, alternatives\" (i.e., not too factual or too nonsensical). This ensures consistent triggering of counterfactual thinking.\n        *   **Training-Free Mitigation**: The entire process is applied during inference via prompt engineering, making it highly adaptable to existing LMMs without requiring model architecture changes or retraining.\n\n4.  **Key Technical Contributions**\n    *   **Novel Method**: Introduction of **Counterfactual Inception** for mitigating hallucination by prompting LMMs to engage in counterfactual thinking using self-generated, deliberately deviated language keywords.\n    *   **Algorithmic Innovation**: Development of the **Plausibility Verification Process (PVP)**, which employs CLIP-based semantic alignment to dynamically select optimal counterfactual keywords, ensuring they are sufficiently counterfactual yet plausible enough to guide the model's reasoning.\n    *   **System Design**: A modular approach where LMMs first generate keywords, then these keywords are filtered by PVP, and finally, the refined keywords are integrated into a conditional prompt for the LMM's response generation.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**: Comprehensive analyses were performed across a diverse set of LMMs, including both open-source (LLaVA-1.5 (13B), InternLM-XComposer2 (7B), LLaVA-NeXT (34B), InternVL 1.5 (26B)) and proprietary models (Gemini 1.5 Pro, GPT-4V).\n    *   **Key Performance Metrics and Comparison Results**:\n        *   **Hallucination Discrimination**:\n            *   **POPE \\cite{li2023pope} (Adversarial Setting)**: Evaluated accuracy, precision, recall, and F1-score for object hallucinations. Counterfactual Inception consistently improved F1-scores across all tested models (e.g., InternVL 1.5 F1 improved from 86.45 to 89.16).\n            *   **MMVP \\cite{tong2024mmvp} (CLIP-blind pairs)**: Measured accuracy across 9 visual patterns. The proposed method showed consistent improvements in average accuracy (e.g., InternVL 1.5 Avg improved from 48.00 to 51.33).\n        *   **Non-Hallucinatory Generation**:\n            *   **CHAIR \\cite{rohrbach2018object}**: Assessed the proportion of hallucinatory objects.\n            *   **MMHal-Bench \\cite{sun2024mmhalbench}**: Evaluated descriptive score and hallucination severity using GPT-4.\n        *   The results consistently demonstrated that Counterfactual Inception significantly reduces hallucination and broadens contextual understanding based on true visual clues.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations**: The effectiveness of PVP relies on the quality of CLIP-based semantic alignment and the empirical setting of truncation hyperparameters (0.11 to 0.18 for CLIP score). The paper notes that LMMs might still produce nonsensical keywords, necessitating the PVP.\n    *   **Scope of Applicability**: The method is primarily demonstrated for mitigating object, attribute, and relational hallucinations in LMMs. While shown to be effective across various LMM architectures, its generalizability to all types of LMM hallucinations or other generative tasks is implied but not exhaustively proven.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: \\cite{kim2024ozf} introduces a novel, training-free paradigm for hallucination mitigation in LMMs, moving beyond resource-intensive fine-tuning approaches. By leveraging LMMs' inherent generative capabilities for self-correction, it offers a highly efficient and adaptable solution.\n    *   **Potential Impact on Future Research**: This work opens new avenues for research into cognitive-inspired AI, particularly in how models can be prompted to engage in higher-order reasoning like counterfactual thinking. It suggests that explicit \"thought processes\" can be engineered into prompts to enhance model reliability and contextual understanding, potentially leading to more robust and trustworthy LMMs for various real-world applications.",
      "intriguing_abstract": "Large Multi-modal Models (LMMs) are revolutionizing AI, yet their pervasive tendency to \"hallucinate\"â€”generating visually inconsistent or fabricated contentâ€”severely undermines their reliability and trustworthiness. We introduce **Counterfactual Inception**, a novel, *training-free* method that fundamentally rethinks hallucination mitigation by instilling cognitive-inspired \"what if\" thinking directly into LMMs.\n\nOur approach leverages the LMM's own generative capabilities to create **self-generated counterfactual keywords** that intentionally deviate from the actual visual content across object, attribute, and relation levels. These carefully crafted keywords are then integrated into prompts, instructing the LMM to *explicitly avoid* them in its response. A crucial innovation, the **Plausibility Verification Process (PVP)**, employs CLIP-based semantic alignment to dynamically filter and select optimal counterfactuals, ensuring they are sufficiently divergent yet plausible enough to guide robust reasoning.\n\nApplied entirely during inference via prompt engineering, Counterfactual Inception offers an efficient and highly adaptable solution without requiring extensive retraining or fine-tuning. Comprehensive experiments across diverse LMMs (e.g., LLaVA, InternVL, Gemini 1.5 Pro, GPT-4V) demonstrate consistent and significant reductions in hallucination on benchmarks like POPE and MMVP, improving F1-scores and accuracy. This paradigm shift enhances LMM reliability and trustworthiness, paving the way for more robust and cognitively-inspired AI systems.",
      "keywords": [
        "Large Multi-modal Models (LMMs)",
        "Hallucination mitigation",
        "Counterfactual Inception",
        "Training-free method",
        "Self-generated counterfactual keywords",
        "Plausibility Verification Process (PVP)",
        "CLIP-based semantic alignment",
        "Prompt engineering",
        "Inference-time mitigation",
        "Cognitive-inspired AI",
        "Enhanced reliability and trustworthiness",
        "Improved contextual understanding",
        "Object",
        "attribute",
        "and relation hallucinations"
      ],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/15aaf20d02a1e26be9106e66d065fd1ca5600e29.pdf",
      "citation_key": "kim2024ozf",
      "metadata": {
        "title": "What if...?: Thinking Counterfactual Keywords Helps to Mitigate Hallucination in Large Multi-modal Models",
        "authors": [
          "Junho Kim",
          "Yeonju Kim",
          "Yonghyun Ro"
        ],
        "published_date": "2024",
        "abstract": "This paper presents a way of enhancing the reliability of Large Multi-modal Models (LMMs) in addressing hallucination, where the models generate cross-modal inconsistent responses. Without additional training, we propose Counterfactual Inception, a novel method that implants counterfactual thinking into LMMs using self-generated counterfactual keywords. Our method is grounded in the concept of counterfactual thinking, a cognitive process where human considers alternative realities, enabling more extensive context exploration. Bridging the human cognition mechanism into LMMs, we aim for the models to engage with and generate responses that span a wider contextual scene understanding, mitigating hallucinatory outputs. We further introduce Plausibility Verification Process (PVP), a simple yet robust keyword constraint that effectively filters out sub-optimal keywords to enable the consistent triggering of counterfactual thinking in the model responses. Comprehensive analyses across various LMMs, including both open-source and proprietary models, corroborate that counterfactual thinking significantly reduces hallucination and helps to broaden contextual understanding based on true visual clues.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/15aaf20d02a1e26be9106e66d065fd1ca5600e29.pdf",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the paper \"What if...?: Thinking Counterfactual Keywords Helps to Mitigate Hallucination in Large Multi-modal Models\" \\cite{kim2024ozf} for a literature review:\n\n---\n\n### Analysis of \\cite{kim2024ozf}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Large Multi-modal Models (LMMs) frequently suffer from \"hallucination,\" where they generate responses that are inconsistent with the visual content, including false premises, incorrect object attributes, nonexistent objects, or erroneous object relations.\n    *   **Importance and Challenge**: Hallucination undermines the reliability and trustworthiness of LMMs, hindering their adoption as general-purpose AI systems. Existing solutions often require extensive additional training, fine-tuning, or labor-intensive human feedback, which can be resource-intensive and limit their applicability to pre-trained models.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: Previous efforts to mitigate hallucination in LMMs include fine-tuning with robust instructions, multi-step LMM-aided reasoning, Reinforcement Learning from Human Feedback (RLHF), and contrastive decoding during inference.\n    *   **Limitations of Previous Solutions**: These methods typically necessitate additional training, tailored instruction datasets, or significant computational resources for fine-tuning. \\cite{kim2024ozf} explicitly positions its work as a *training-free* approach that does not require additional fine-tuning or human-resource instructions, addressing a key limitation of prior art.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes **Counterfactual Inception**, a novel, training-free method that implants counterfactual thinking into LMMs. It leverages the LMMs' own generative capabilities to create \"counterfactual keywords\" that intentionally deviate from the actual visual content. These keywords are then incorporated into a prompt that instructs the LMM to *carefully avoid* them in its response.\n    *   **Novelty**:\n        *   **Self-Generated Counterfactuals**: LMMs themselves generate counterfactual keywords across object, attribute, and relation levels, inspired by human cognitive processes of \"what if\" scenarios. This promotes broader contextual exploration.\n        *   **Plausibility Verification Process (PVP)**: A robust keyword constraint is introduced to filter out suboptimal counterfactual keywords. PVP uses CLIP \\cite{radford2021learning} scores to measure the semantic alignment between visual content and generated keywords, ensuring that selected keywords are \"meaningful, yet not direct, alternatives\" (i.e., not too factual or too nonsensical). This ensures consistent triggering of counterfactual thinking.\n        *   **Training-Free Mitigation**: The entire process is applied during inference via prompt engineering, making it highly adaptable to existing LMMs without requiring model architecture changes or retraining.\n\n4.  **Key Technical Contributions**\n    *   **Novel Method**: Introduction of **Counterfactual Inception** for mitigating hallucination by prompting LMMs to engage in counterfactual thinking using self-generated, deliberately deviated language keywords.\n    *   **Algorithmic Innovation**: Development of the **Plausibility Verification Process (PVP)**, which employs CLIP-based semantic alignment to dynamically select optimal counterfactual keywords, ensuring they are sufficiently counterfactual yet plausible enough to guide the model's reasoning.\n    *   **System Design**: A modular approach where LMMs first generate keywords, then these keywords are filtered by PVP, and finally, the refined keywords are integrated into a conditional prompt for the LMM's response generation.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**: Comprehensive analyses were performed across a diverse set of LMMs, including both open-source (LLaVA-1.5 (13B), InternLM-XComposer2 (7B), LLaVA-NeXT (34B), InternVL 1.5 (26B)) and proprietary models (Gemini 1.5 Pro, GPT-4V).\n    *   **Key Performance Metrics and Comparison Results**:\n        *   **Hallucination Discrimination**:\n            *   **POPE \\cite{li2023pope} (Adversarial Setting)**: Evaluated accuracy, precision, recall, and F1-score for object hallucinations. Counterfactual Inception consistently improved F1-scores across all tested models (e.g., InternVL 1.5 F1 improved from 86.45 to 89.16).\n            *   **MMVP \\cite{tong2024mmvp} (CLIP-blind pairs)**: Measured accuracy across 9 visual patterns. The proposed method showed consistent improvements in average accuracy (e.g., InternVL 1.5 Avg improved from 48.00 to 51.33).\n        *   **Non-Hallucinatory Generation**:\n            *   **CHAIR \\cite{rohrbach2018object}**: Assessed the proportion of hallucinatory objects.\n            *   **MMHal-Bench \\cite{sun2024mmhalbench}**: Evaluated descriptive score and hallucination severity using GPT-4.\n        *   The results consistently demonstrated that Counterfactual Inception significantly reduces hallucination and broadens contextual understanding based on true visual clues.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations**: The effectiveness of PVP relies on the quality of CLIP-based semantic alignment and the empirical setting of truncation hyperparameters (0.11 to 0.18 for CLIP score). The paper notes that LMMs might still produce nonsensical keywords, necessitating the PVP.\n    *   **Scope of Applicability**: The method is primarily demonstrated for mitigating object, attribute, and relational hallucinations in LMMs. While shown to be effective across various LMM architectures, its generalizability to all types of LMM hallucinations or other generative tasks is implied but not exhaustively proven.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: \\cite{kim2024ozf} introduces a novel, training-free paradigm for hallucination mitigation in LMMs, moving beyond resource-intensive fine-tuning approaches. By leveraging LMMs' inherent generative capabilities for self-correction, it offers a highly efficient and adaptable solution.\n    *   **Potential Impact on Future Research**: This work opens new avenues for research into cognitive-inspired AI, particularly in how models can be prompted to engage in higher-order reasoning like counterfactual thinking. It suggests that explicit \"thought processes\" can be engineered into prompts to enhance model reliability and contextual understanding, potentially leading to more robust and trustworthy LMMs for various real-world applications.",
        "keywords": [
          "Large Multi-modal Models (LMMs)",
          "Hallucination mitigation",
          "Counterfactual Inception",
          "Training-free method",
          "Self-generated counterfactual keywords",
          "Plausibility Verification Process (PVP)",
          "CLIP-based semantic alignment",
          "Prompt engineering",
          "Inference-time mitigation",
          "Cognitive-inspired AI",
          "Enhanced reliability and trustworthiness",
          "Improved contextual understanding",
          "Object",
          "attribute",
          "and relation hallucinations"
        ],
        "paper_type": "based on the abstract and introduction:\n\n*   the abstract explicitly states: \"we propose counterfactual inception, a novel method...\" and \"we further introduce plausibility verification process (pvp)...\". it discusses a technical problem (hallucination in lmms) and presents a proposed solution (the new method).\n*   the introduction sets the context for lmms and the problem, and the accompanying diagram illustrates the proposed \"counterfactual inception\" and \"plausibility verification process.\"\n*   while \"comprehensive analyses\" are mentioned, indicating an empirical evaluation, this evaluation is performed *on the proposed method* to demonstrate its effectiveness. the core contribution is the method itself.\n\nthis aligns perfectly with the criteria for a **technical** paper.\n\n**classification: technical**"
      },
      "file_name": "15aaf20d02a1e26be9106e66d065fd1ca5600e29.pdf"
    },
    {
      "success": true,
      "doc_id": "efede4fcad62a9af562826fe92c4e195",
      "summary": "Here's a focused summary of the technical paper \\cite{li2024wyb} for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Multimodal Large Language Models (MLLMs) for video understanding are prone to \"hallucinations\" â€“ generating inaccurate or misleading content â€“ particularly when distinguishing visually different yet semantically similar video pairs. This issue is underexplored in the video domain.\n    *   **Importance and Challenge**: Hallucinations undermine the reliability of MLLMs in practical applications. Existing benchmarks for MLLM hallucinations are limited in scale (typically <1,000 videos), primarily focus on static elements (objects, attributes, spatial relationships), and offer restricted question types (e.g., binary QA), thus failing to comprehensively evaluate dynamic and temporal content crucial for video understanding. Furthermore, current mitigation strategies often require computationally expensive fine-tuning.\n\n*   **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: \\cite{li2024wyb} builds upon advancements in MLLMs for video understanding (e.g., Video-ChatGPT, Video-LLaVA) and existing hallucination benchmarks (e.g., HallusionBench, VideoHallucer).\n    *   **Limitations of Previous Solutions**:\n        *   **Benchmarks**: Existing hallucination benchmarks are small-scale, focus on static video elements, and use limited question types, making them insufficient for evaluating dynamic and temporal aspects like action, temporal sequence, and scene transitions.\n        *   **Mitigation Strategies**: Prior methods for reducing MLLM hallucinations (e.g., instruction-tuning, RLHF) typically require fine-tuning, incurring significant computational costs and necessitating additional dataset creation. \\cite{li2024wyb} observes that MLLMs' vulnerability stems from visual encoders (like CLIP series) over-relying on contextual scenes due to image-text contrastive learning, leading to a discrepancy with the language model's need for both semantic and vision-only representations.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method (VIDHALLUC Benchmark)**:\n        *   **Semi-Automated Data Collection**: A pipeline that identifies potential hallucination video pairs from existing datasets (ActivityNet, YouCook2, VALOR32K).\n        *   **Semantic and Visual Similarity Filtering**: A novel filtering mechanism using CLIP/SigLIP for high semantic similarity (threshold $\\lambda_{sem}=0.9$) and DINOv2 for low visual similarity (threshold $\\lambda_{vis}=0.6$). This identifies adversarial pairs where models might over-rely on semantic cues despite visual differences.\n        *   **Targeted Hallucination Types**: Specifically designed to evaluate three dynamic hallucination types: Action Hallucination (ACH), Temporal Sequence Hallucination (TSH), and Scene Transition Hallucination (STH).\n        *   **Diverse Question Formats**: Employs binary QA and Multiple-Choice Questions (MCQs) for ACH, sorting questions for TSH, and open-ended questions for STH, providing a comprehensive assessment.\n    *   **Core Technical Method (DINO-HEAL Algorithm)**:\n        *   **Training-Free Mitigation**: A novel, inference-time algorithm designed to mitigate MLLM hallucinations without requiring any additional training or fine-tuning.\n        *   **Saliency-Based Feature Reweighting**: Incorporates spatial saliency maps from DINOv2 to reweight visual features extracted by CLIP-series vision encoders. This enhances the visual encoder's focus on critical, salient spatial regions within video frames, thereby reducing reliance on potentially misleading contextual cues.\n\n*   **Key Technical Contributions**\n    *   **Novel Benchmark (VIDHALLUC)**: The largest benchmark for evaluating temporal hallucinations in MLLMs for video understanding, comprising 5,002 videos and 9,295 QA pairs, specifically targeting action, temporal sequence, and scene transition hallucinations with diverse question formats.\n    *   **Novel Algorithm (DINO-HEAL)**: A training-free method that leverages DINOv2 saliency to reweight visual features during inference, enhancing the visual encoder's focus on critical regions and improving MLLM robustness against hallucinations.\n    *   **Comprehensive Evaluation Framework**: Provides a rigorous framework for assessing MLLM reliability by curating adversarial video pairs and diverse question types that expose model vulnerabilities to dynamic and temporal inconsistencies.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted**:\n        *   Comprehensive testing of ten state-of-the-art MLLMs (e.g., Video-LLaVA, Video-ChatGPT, Video-LLaMA2) on the VIDHALLUC benchmark across all three hallucination categories (ACH, TSH, STH).\n        *   Evaluation of the DINO-HEAL algorithm's effectiveness in mitigating hallucinations across various MLLMs.\n    *   **Key Performance Metrics and Comparison Results**:\n        *   Initial results showed that most MLLMs are highly vulnerable to hallucinations across all three dimensions on VIDHALLUC.\n        *   DINO-HEAL consistently improved performance, achieving an average gain of **3.02%** in mitigating hallucinations across all tasks and five evaluated models.\n        *   Specific improvements include:\n            *   **Action Hallucination**: +5.01% for Video-LLaVA and +4.77% for Video-ChatGPT.\n            *   **Temporal Sequence Hallucination**: +11.67% for Video-ChatGPT and +18.83% for VideoLLaMA2.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The paper primarily focuses on addressing the limitations of *existing* benchmarks and mitigation strategies. It does not explicitly detail specific technical limitations or assumptions of the VIDHALLUC benchmark or the DINO-HEAL method itself, beyond the scope of its targeted hallucination types and the reliance on DINOv2 for saliency.\n    *   **Scope of Applicability**: VIDHALLUC is designed for evaluating temporal and dynamic hallucinations in MLLMs for video understanding. DINO-HEAL is applicable to any MLLM utilizing CLIP-series vision encoders.\n\n*   **Technical Significance**\n    *   **Advancement of State-of-the-Art**: \\cite{li2024wyb} significantly advances the technical state-of-the-art by introducing the largest and most comprehensive benchmark specifically designed to evaluate dynamic and temporal hallucinations in MLLMs for video understanding. It moves beyond static content evaluation to address critical temporal reasoning failures.\n    *   **Potential Impact on Future Research**:\n        *   Provides a robust and scalable tool (VIDHALLUC) for researchers to rigorously assess and compare the reliability of future MLLMs in handling complex video content.\n        *   Offers an efficient, training-free mitigation strategy (DINO-HEAL) that can be readily integrated into existing MLLMs, paving the way for more reliable and trustworthy video understanding systems.\n        *   Highlights the importance of spatial saliency in visual encoders for reducing hallucinations, suggesting new directions for architectural improvements or inference-time optimizations in MLLMs.",
      "intriguing_abstract": "The promise of Multimodal Large Language Models (MLLMs) in video understanding is critically undermined by persistent hallucinations, particularly when distinguishing visually subtle yet semantically distinct video events. Current benchmarks fall short, neglecting dynamic and temporal content crucial for real-world reliability. To address this, we introduce **VIDHALLUC**, the largest and most comprehensive benchmark (5,002 videos, 9,295 QA pairs) specifically designed to expose **Action, Temporal Sequence, and Scene Transition Hallucinations**. Utilizing a novel semi-automated pipeline with CLIP/SigLIP for semantic similarity and DINOv2 for visual dissimilarity, VIDHALLUC curates adversarial video pairs and diverse question formats to rigorously test MLLM robustness.\n\nFurthermore, we present **DINO-HEAL**, a novel **training-free, inference-time algorithm** to mitigate these errors. DINO-HEAL leverages DINOv2 spatial saliency to reweight visual features from CLIP-series encoders, compelling MLLMs to focus on critical visual evidence rather than misleading contextual cues. Our extensive evaluation reveals that state-of-the-art MLLMs are highly susceptible to temporal hallucinations. Crucially, DINO-HEAL consistently mitigates these errors, achieving an average gain of 3.02% and up to 18.83% in temporal sequence tasks, significantly enhancing model reliability. VIDHALLUC provides an indispensable tool for rigorous assessment, while DINO-HEAL offers an immediate, practical solution, paving the way for more trustworthy and robust MLLMs in complex video analysis.",
      "keywords": [
        "Multimodal Large Language Models (MLLMs)",
        "video understanding",
        "hallucinations",
        "temporal hallucinations",
        "VIDHALLUC benchmark",
        "DINO-HEAL algorithm",
        "training-free mitigation",
        "saliency-based feature reweighting",
        "visual encoders (CLIP",
        "DINOv2)",
        "adversarial video pairs",
        "dynamic and temporal content evaluation",
        "MLLM reliability",
        "semantic and visual similarity filtering"
      ],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/24a48ef14c8eb4e571e3f4ae9b37936060a3fb06.pdf",
      "citation_key": "li2024wyb",
      "metadata": {
        "title": "VidHalluc: Evaluating Temporal Hallucinations in Multimodal Large Language Models for Video Understanding",
        "authors": [
          "Chaoyu Li",
          "Eun Woo Im",
          "Pooyan Fazli"
        ],
        "published_date": "2024",
        "abstract": "Multimodal large language models (MLLMs) have recently shown significant advancements in video understanding, excelling in content reasoning and instruction-following tasks. However, hallucination, where models generate inaccurate or misleading content, remains underexplored in the video domain. Building on the observation that MLLM visual encoders often fail to distinguish visually different yet semantically similar video pairs, we introduce VIDHALLUC, the largest benchmark designed to examine hallucinations in MLLMs for video understanding. It consists of 5,002 videos, paired to highlight cases prone to hallucinations. VIDHALLUC assesses hallucinations across three critical dimensions: (1) action, (2) temporal sequence, and (3) scene transition. Comprehensive testing shows that most MLLMs are vulnerable to hallucinations across these dimensions. Furthermore, we propose DINO-HEAL, a training-free method that reduces hallucinations by incorporating spatial saliency from DINOv2 to reweight visual features during inference. Our results show that DINO-HEAL consistently improves performance on VIDHALLUC, achieving an average improvement of 3.02% in mitigating hallucinations across all tasks. Both the VIDHALLUC benchmark and DINO-HEAL code are available at https://peoplerobots.github.io/vidhalluc.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/24a48ef14c8eb4e571e3f4ae9b37936060a3fb06.pdf",
        "venue": "Computer Vision and Pattern Recognition",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the technical paper \\cite{li2024wyb} for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Multimodal Large Language Models (MLLMs) for video understanding are prone to \"hallucinations\" â€“ generating inaccurate or misleading content â€“ particularly when distinguishing visually different yet semantically similar video pairs. This issue is underexplored in the video domain.\n    *   **Importance and Challenge**: Hallucinations undermine the reliability of MLLMs in practical applications. Existing benchmarks for MLLM hallucinations are limited in scale (typically <1,000 videos), primarily focus on static elements (objects, attributes, spatial relationships), and offer restricted question types (e.g., binary QA), thus failing to comprehensively evaluate dynamic and temporal content crucial for video understanding. Furthermore, current mitigation strategies often require computationally expensive fine-tuning.\n\n*   **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: \\cite{li2024wyb} builds upon advancements in MLLMs for video understanding (e.g., Video-ChatGPT, Video-LLaVA) and existing hallucination benchmarks (e.g., HallusionBench, VideoHallucer).\n    *   **Limitations of Previous Solutions**:\n        *   **Benchmarks**: Existing hallucination benchmarks are small-scale, focus on static video elements, and use limited question types, making them insufficient for evaluating dynamic and temporal aspects like action, temporal sequence, and scene transitions.\n        *   **Mitigation Strategies**: Prior methods for reducing MLLM hallucinations (e.g., instruction-tuning, RLHF) typically require fine-tuning, incurring significant computational costs and necessitating additional dataset creation. \\cite{li2024wyb} observes that MLLMs' vulnerability stems from visual encoders (like CLIP series) over-relying on contextual scenes due to image-text contrastive learning, leading to a discrepancy with the language model's need for both semantic and vision-only representations.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method (VIDHALLUC Benchmark)**:\n        *   **Semi-Automated Data Collection**: A pipeline that identifies potential hallucination video pairs from existing datasets (ActivityNet, YouCook2, VALOR32K).\n        *   **Semantic and Visual Similarity Filtering**: A novel filtering mechanism using CLIP/SigLIP for high semantic similarity (threshold $\\lambda_{sem}=0.9$) and DINOv2 for low visual similarity (threshold $\\lambda_{vis}=0.6$). This identifies adversarial pairs where models might over-rely on semantic cues despite visual differences.\n        *   **Targeted Hallucination Types**: Specifically designed to evaluate three dynamic hallucination types: Action Hallucination (ACH), Temporal Sequence Hallucination (TSH), and Scene Transition Hallucination (STH).\n        *   **Diverse Question Formats**: Employs binary QA and Multiple-Choice Questions (MCQs) for ACH, sorting questions for TSH, and open-ended questions for STH, providing a comprehensive assessment.\n    *   **Core Technical Method (DINO-HEAL Algorithm)**:\n        *   **Training-Free Mitigation**: A novel, inference-time algorithm designed to mitigate MLLM hallucinations without requiring any additional training or fine-tuning.\n        *   **Saliency-Based Feature Reweighting**: Incorporates spatial saliency maps from DINOv2 to reweight visual features extracted by CLIP-series vision encoders. This enhances the visual encoder's focus on critical, salient spatial regions within video frames, thereby reducing reliance on potentially misleading contextual cues.\n\n*   **Key Technical Contributions**\n    *   **Novel Benchmark (VIDHALLUC)**: The largest benchmark for evaluating temporal hallucinations in MLLMs for video understanding, comprising 5,002 videos and 9,295 QA pairs, specifically targeting action, temporal sequence, and scene transition hallucinations with diverse question formats.\n    *   **Novel Algorithm (DINO-HEAL)**: A training-free method that leverages DINOv2 saliency to reweight visual features during inference, enhancing the visual encoder's focus on critical regions and improving MLLM robustness against hallucinations.\n    *   **Comprehensive Evaluation Framework**: Provides a rigorous framework for assessing MLLM reliability by curating adversarial video pairs and diverse question types that expose model vulnerabilities to dynamic and temporal inconsistencies.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted**:\n        *   Comprehensive testing of ten state-of-the-art MLLMs (e.g., Video-LLaVA, Video-ChatGPT, Video-LLaMA2) on the VIDHALLUC benchmark across all three hallucination categories (ACH, TSH, STH).\n        *   Evaluation of the DINO-HEAL algorithm's effectiveness in mitigating hallucinations across various MLLMs.\n    *   **Key Performance Metrics and Comparison Results**:\n        *   Initial results showed that most MLLMs are highly vulnerable to hallucinations across all three dimensions on VIDHALLUC.\n        *   DINO-HEAL consistently improved performance, achieving an average gain of **3.02%** in mitigating hallucinations across all tasks and five evaluated models.\n        *   Specific improvements include:\n            *   **Action Hallucination**: +5.01% for Video-LLaVA and +4.77% for Video-ChatGPT.\n            *   **Temporal Sequence Hallucination**: +11.67% for Video-ChatGPT and +18.83% for VideoLLaMA2.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The paper primarily focuses on addressing the limitations of *existing* benchmarks and mitigation strategies. It does not explicitly detail specific technical limitations or assumptions of the VIDHALLUC benchmark or the DINO-HEAL method itself, beyond the scope of its targeted hallucination types and the reliance on DINOv2 for saliency.\n    *   **Scope of Applicability**: VIDHALLUC is designed for evaluating temporal and dynamic hallucinations in MLLMs for video understanding. DINO-HEAL is applicable to any MLLM utilizing CLIP-series vision encoders.\n\n*   **Technical Significance**\n    *   **Advancement of State-of-the-Art**: \\cite{li2024wyb} significantly advances the technical state-of-the-art by introducing the largest and most comprehensive benchmark specifically designed to evaluate dynamic and temporal hallucinations in MLLMs for video understanding. It moves beyond static content evaluation to address critical temporal reasoning failures.\n    *   **Potential Impact on Future Research**:\n        *   Provides a robust and scalable tool (VIDHALLUC) for researchers to rigorously assess and compare the reliability of future MLLMs in handling complex video content.\n        *   Offers an efficient, training-free mitigation strategy (DINO-HEAL) that can be readily integrated into existing MLLMs, paving the way for more reliable and trustworthy video understanding systems.\n        *   Highlights the importance of spatial saliency in visual encoders for reducing hallucinations, suggesting new directions for architectural improvements or inference-time optimizations in MLLMs.",
        "keywords": [
          "Multimodal Large Language Models (MLLMs)",
          "video understanding",
          "hallucinations",
          "temporal hallucinations",
          "VIDHALLUC benchmark",
          "DINO-HEAL algorithm",
          "training-free mitigation",
          "saliency-based feature reweighting",
          "visual encoders (CLIP",
          "DINOv2)",
          "adversarial video pairs",
          "dynamic and temporal content evaluation",
          "MLLM reliability",
          "semantic and visual similarity filtering"
        ],
        "paper_type": "the paper introduces a new benchmark (vidhalluc) and proposes a new method (dino-heal). it then evaluates existing mllms using this benchmark and demonstrates the effectiveness of its proposed method.\n\n*   **abstract mentions:** \"we introduce vidhalluc, the largest benchmark designed to examine hallucinations...\", \"we propose dino-heal, a training-free method that reduces hallucinations...\"\n*   **introduction discusses:** the technical problem of hallucinations in mllms for video understanding and the need for better evaluation and mitigation strategies, which the paper addresses with its proposed solutions.\n\nthis clearly aligns with the **technical** classification, as it presents new methods, algorithms, and systems (a benchmark and a mitigation method). while it also includes empirical evaluation, the primary contribution is the development of these new technical artifacts.\n\n**classification: technical**"
      },
      "file_name": "24a48ef14c8eb4e571e3f4ae9b37936060a3fb06.pdf"
    },
    {
      "success": true,
      "doc_id": "a99899cb48553b406305aad18d1f2709",
      "summary": "Here's a focused summary of the paper \"\\cite{ji20243j6}\" for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem**: The paper addresses the critical need for comprehensive and fine-grained measurement of hallucination in Large Language Models (LLMs), particularly in generative question answering.\n    *   **Importance and Challenge**: Hallucinations significantly hinder the wide application of LLMs. Their fluent and convincing nature makes detection difficult, impeding deep analysis and effective mitigation. Existing benchmarks are often coarse-grained (only indicating if an entire response contains hallucinations) or outdated, failing to provide the detailed insights required to trace triggers and develop targeted solutions.\n\n*   **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: The work acknowledges prior efforts in detecting and evaluating hallucination.\n    *   **Limitations of Previous Solutions**: Most benchmarks were developed before the advent of modern LLMs and targeted specific English tasks, making them less challenging for current models. More recent LLM benchmarks typically offer only coarse-grained categorization of hallucination at the entire response level, lacking explanations or references, which prevents tracing the exact triggers and obstructs further mitigation efforts.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper introduces ANAH (ANalytical Annotation of Hallucinations), a novel, large-scale, bilingual (Chinese-English) dataset designed for sentence-level, analytical annotation of LLM hallucinations in knowledge-based generative question answering.\n    *   **Novelty**: The core innovation lies in its fine-grained annotation process for *each sentence* in an LLM-generated answer. This process involves:\n        1.  Retrieving a specific reference fragment from source documents.\n        2.  Judging the hallucination type (No Hallucination, Contradictory Hallucination, Unverifiable Hallucination, or No Fact).\n        3.  Providing a correction for hallucinated content based on the reference.\n    *   **Human-in-the-loop Pipeline**: A key aspect of its construction is a human-in-the-loop pipeline that leverages GPT-4 for preliminary annotation, followed by rigorous human verification and refinement to ensure accuracy and scalability.\n\n*   **Key Technical Contributions**\n    *   **Novel Dataset (ANAH)**: Creation of ANAH, comprising approximately 12k sentence-level annotations across 4.3k LLM responses covering over 700 topics, providing an unprecedented level of detail for hallucination analysis.\n    *   **Fine-grained Annotation Methodology**: Development of a structured, multi-step annotation scheme that moves beyond simple detection to analytical understanding of hallucination types and their specific locations within generated text.\n    *   **Quantitative Confirmation of Hallucination Accumulation**: Using the fine granularity of ANAH, the authors quantitatively confirm that hallucinations progressively accumulate in LLM responses, providing empirical evidence for the \"snowball effect.\"\n    *   **Enabling Training of Hallucination Annotators**: ANAH serves as a robust dataset for training and evaluating both generative and discriminative models specifically designed to annotate LLM hallucinations.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted**:\n        *   Verification of GPT-4's annotation quality by comparing its output with human annotations.\n        *   Analysis of hallucination proportions in answers generated by LLMs with and without reference documents.\n        *   Quantitative analysis of the hallucination accumulation effect across sentences.\n        *   Extensive experiments on training and evaluating generative and discriminative hallucination annotators using ANAH.\n        *   Comparison of trained annotators against various open-source LLMs and proprietary models like GPT-3.5 and GPT-4.\n    *   **Key Performance Metrics and Results**:\n        *   GPT-4 showed high consistency with human annotations: 86.97% for hallucination type, 85.37% for reference, and 78.98% for correction.\n        *   The probability of hallucination in a sentence given previous hallucinations (P(Ht|H[0:tâˆ’1])) was significantly higher (58.51% for English, 52.54% for Chinese) than the overall hallucination probability, confirming the accumulation effect.\n        *   A generative annotator trained with ANAH achieved 81.01% accuracy, outperforming all tested open-source LLMs and GPT-3.5, and demonstrating competitive performance with GPT-4 (86.97%) while exhibiting better generalization on unseen questions.\n        *   Generative annotators were found to be superior to discriminative annotators in handling the imbalance of hallucination types.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations**: GPT-4, while effective, showed a tendency to misclassify sentences with referential ambiguity or summary discussion as 'No Fact', though this category represents a small proportion of the dataset.\n    *   **Scope of Applicability**: The dataset and analysis are primarily focused on knowledge-based generative question answering and cover specific domains and topics curated during the dataset construction.\n\n*   **Technical Significance**\n    *   **Advancement of State-of-the-Art**: ANAH significantly advances the technical state-of-the-art by providing the first comprehensive, fine-grained, and bilingual benchmark for analytical hallucination annotation, moving beyond superficial detection.\n    *   **Potential Impact on Future Research**:\n        *   Enables deeper, more nuanced understanding of LLM hallucination mechanisms, such as the confirmed accumulation effect.\n        *   Provides a crucial resource for developing and rigorously evaluating advanced hallucination detection, explanation, and mitigation techniques.\n        *   Offers a pathway to create smaller, more cost-effective hallucination annotators that can rival the performance of large proprietary models like GPT-4.\n        *   Guides future data collection efforts by suggesting that prioritizing broader topic coverage is more beneficial for generalization than simply increasing the number of questions within existing topics.",
      "intriguing_abstract": "The pervasive issue of hallucination in Large Language Models (LLMs) critically impedes their reliability, yet existing detection methods lack the granularity for deep analysis and effective mitigation. We introduce ANAH (ANalytical Annotation of Hallucinations), a novel, large-scale, **bilingual (Chinese-English)** dataset designed for **sentence-level, analytical annotation** of LLM hallucinations in knowledge-based generative question answering. Unlike coarse-grained benchmarks, ANAH meticulously categorizes hallucination types (e.g., Contradictory, Unverifiable) and provides specific corrections for each problematic sentence, built via a robust human-in-the-loop pipeline leveraging GPT-4.\n\nOur fine-grained approach quantitatively confirms the **hallucination accumulation effect**, revealing how errors progressively compound within responses. Comprising ~12k annotations across 4.3k LLM responses, ANAH offers unprecedented detail. This resource is pivotal for training and evaluating specialized **generative and discriminative hallucination annotators**, demonstrating that models trained on ANAH can outperform open-source LLMs and rival proprietary systems like GPT-4 in accuracy and generalization. ANAH provides a critical foundation for developing advanced hallucination mitigation strategies and fostering a more profound understanding of LLM reliability.",
      "keywords": [
        "LLM hallucination",
        "generative question answering",
        "ANAH dataset",
        "fine-grained annotation",
        "sentence-level hallucination",
        "hallucination accumulation effect",
        "bilingual (Chinese-English) dataset",
        "human-in-the-loop annotation",
        "generative hallucination annotators",
        "knowledge-based QA",
        "hallucination detection and mitigation",
        "analytical annotation"
      ],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/3c3f5af1aee19bf0093c40f35a120744d099723e.pdf",
      "citation_key": "ji20243j6",
      "metadata": {
        "title": "ANAH: Analytical Annotation of Hallucinations in Large Language Models",
        "authors": [
          "Ziwei Ji",
          "Yuzhe Gu",
          "Wenwei Zhang",
          "Chengqi Lyu",
          "Dahua Lin",
          "Kai Chen"
        ],
        "published_date": "2024",
        "abstract": "Reducing the `$\\textit{hallucination}$' problem of Large Language Models (LLMs) is crucial for their wide applications. A comprehensive and fine-grained measurement of the hallucination is the first key step for the governance of this issue but is under-explored in the community. Thus, we present $\\textbf{ANAH}$, a bilingual dataset that offers $\\textbf{AN}$alytical $\\textbf{A}$nnotation of $\\textbf{H}$allucinations in LLMs within Generative Question Answering. Each answer sentence in our dataset undergoes rigorous annotation, involving the retrieval of a reference fragment, the judgment of the hallucination type, and the correction of hallucinated content. ANAH consists of ~12k sentence-level annotations for ~4.3k LLM responses covering over 700 topics, constructed by a human-in-the-loop pipeline. Thanks to the fine granularity of the hallucination annotations, we can quantitatively confirm that the hallucinations of LLMs progressively accumulate in the answer and use ANAH to train and evaluate hallucination annotators. We conduct extensive experiments on studying generative and discriminative annotators and show that, although current open-source LLMs have difficulties in fine-grained hallucination annotation, the generative annotator trained with ANAH can surpass all open-source LLMs and GPT-3.5, obtain performance competitive with GPT-4, and exhibits better generalization ability on unseen questions.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/3c3f5af1aee19bf0093c40f35a120744d099723e.pdf",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the paper \"\\cite{ji20243j6}\" for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem**: The paper addresses the critical need for comprehensive and fine-grained measurement of hallucination in Large Language Models (LLMs), particularly in generative question answering.\n    *   **Importance and Challenge**: Hallucinations significantly hinder the wide application of LLMs. Their fluent and convincing nature makes detection difficult, impeding deep analysis and effective mitigation. Existing benchmarks are often coarse-grained (only indicating if an entire response contains hallucinations) or outdated, failing to provide the detailed insights required to trace triggers and develop targeted solutions.\n\n*   **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: The work acknowledges prior efforts in detecting and evaluating hallucination.\n    *   **Limitations of Previous Solutions**: Most benchmarks were developed before the advent of modern LLMs and targeted specific English tasks, making them less challenging for current models. More recent LLM benchmarks typically offer only coarse-grained categorization of hallucination at the entire response level, lacking explanations or references, which prevents tracing the exact triggers and obstructs further mitigation efforts.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper introduces ANAH (ANalytical Annotation of Hallucinations), a novel, large-scale, bilingual (Chinese-English) dataset designed for sentence-level, analytical annotation of LLM hallucinations in knowledge-based generative question answering.\n    *   **Novelty**: The core innovation lies in its fine-grained annotation process for *each sentence* in an LLM-generated answer. This process involves:\n        1.  Retrieving a specific reference fragment from source documents.\n        2.  Judging the hallucination type (No Hallucination, Contradictory Hallucination, Unverifiable Hallucination, or No Fact).\n        3.  Providing a correction for hallucinated content based on the reference.\n    *   **Human-in-the-loop Pipeline**: A key aspect of its construction is a human-in-the-loop pipeline that leverages GPT-4 for preliminary annotation, followed by rigorous human verification and refinement to ensure accuracy and scalability.\n\n*   **Key Technical Contributions**\n    *   **Novel Dataset (ANAH)**: Creation of ANAH, comprising approximately 12k sentence-level annotations across 4.3k LLM responses covering over 700 topics, providing an unprecedented level of detail for hallucination analysis.\n    *   **Fine-grained Annotation Methodology**: Development of a structured, multi-step annotation scheme that moves beyond simple detection to analytical understanding of hallucination types and their specific locations within generated text.\n    *   **Quantitative Confirmation of Hallucination Accumulation**: Using the fine granularity of ANAH, the authors quantitatively confirm that hallucinations progressively accumulate in LLM responses, providing empirical evidence for the \"snowball effect.\"\n    *   **Enabling Training of Hallucination Annotators**: ANAH serves as a robust dataset for training and evaluating both generative and discriminative models specifically designed to annotate LLM hallucinations.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted**:\n        *   Verification of GPT-4's annotation quality by comparing its output with human annotations.\n        *   Analysis of hallucination proportions in answers generated by LLMs with and without reference documents.\n        *   Quantitative analysis of the hallucination accumulation effect across sentences.\n        *   Extensive experiments on training and evaluating generative and discriminative hallucination annotators using ANAH.\n        *   Comparison of trained annotators against various open-source LLMs and proprietary models like GPT-3.5 and GPT-4.\n    *   **Key Performance Metrics and Results**:\n        *   GPT-4 showed high consistency with human annotations: 86.97% for hallucination type, 85.37% for reference, and 78.98% for correction.\n        *   The probability of hallucination in a sentence given previous hallucinations (P(Ht|H[0:tâˆ’1])) was significantly higher (58.51% for English, 52.54% for Chinese) than the overall hallucination probability, confirming the accumulation effect.\n        *   A generative annotator trained with ANAH achieved 81.01% accuracy, outperforming all tested open-source LLMs and GPT-3.5, and demonstrating competitive performance with GPT-4 (86.97%) while exhibiting better generalization on unseen questions.\n        *   Generative annotators were found to be superior to discriminative annotators in handling the imbalance of hallucination types.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations**: GPT-4, while effective, showed a tendency to misclassify sentences with referential ambiguity or summary discussion as 'No Fact', though this category represents a small proportion of the dataset.\n    *   **Scope of Applicability**: The dataset and analysis are primarily focused on knowledge-based generative question answering and cover specific domains and topics curated during the dataset construction.\n\n*   **Technical Significance**\n    *   **Advancement of State-of-the-Art**: ANAH significantly advances the technical state-of-the-art by providing the first comprehensive, fine-grained, and bilingual benchmark for analytical hallucination annotation, moving beyond superficial detection.\n    *   **Potential Impact on Future Research**:\n        *   Enables deeper, more nuanced understanding of LLM hallucination mechanisms, such as the confirmed accumulation effect.\n        *   Provides a crucial resource for developing and rigorously evaluating advanced hallucination detection, explanation, and mitigation techniques.\n        *   Offers a pathway to create smaller, more cost-effective hallucination annotators that can rival the performance of large proprietary models like GPT-4.\n        *   Guides future data collection efforts by suggesting that prioritizing broader topic coverage is more beneficial for generalization than simply increasing the number of questions within existing topics.",
        "keywords": [
          "LLM hallucination",
          "generative question answering",
          "ANAH dataset",
          "fine-grained annotation",
          "sentence-level hallucination",
          "hallucination accumulation effect",
          "bilingual (Chinese-English) dataset",
          "human-in-the-loop annotation",
          "generative hallucination annotators",
          "knowledge-based QA",
          "hallucination detection and mitigation",
          "analytical annotation"
        ],
        "paper_type": "the paper should be classified as **technical**.\n\nhere's why:\n\n1.  **presents new methods, algorithms, or systems:** the abstract explicitly states, \"thus, we present anah, a bilingual dataset that offers analytical annotation of hallucinations in llms...\" and describes its construction (\"constructed by a human-in-the-loop pipeline\"). it also discusses using anah to \"train and evaluate hallucination annotators,\" and then presents the performance of these \"generative and discriminative annotators.\" this involves developing a new resource (the dataset) and new models/methods (the trained annotators).\n2.  **introduction discusses technical problem, proposed solution:** the introduction immediately highlights the \"hallucination\" problem in llms and then presents figure 1, which illustrates the \"fine-grained annotation for sentencei\" as part of the anah methodology, directly showcasing the proposed solution.\n3.  **abstract mentions keywords:** \"present\" (anah), \"train and evaluate hallucination annotators\" (implying new methods/models).\n\nwhile the paper also involves \"experiments\" and \"data\" (which are keywords for \"empirical\"), the core contribution is the *creation* of the anah dataset and the *development and evaluation* of annotator models using this dataset. the experiments serve to validate the utility and effectiveness of these new technical contributions. therefore, \"technical\" is the primary classification."
      },
      "file_name": "3c3f5af1aee19bf0093c40f35a120744d099723e.pdf"
    },
    {
      "success": true,
      "doc_id": "2b3a840180049a43a9a3d6691d7faaa9",
      "summary": "This paper, \"Retrieval-Augmented Generation for Large Language Models: A Survey\" \\cite{gao20232zb}, offers a comprehensive review of Retrieval-Augmented Generation (RAG) for Large Language Models (LLMs).\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem:** Large Language Models (LLMs) exhibit impressive capabilities but are plagued by issues such as hallucination (generating factually incorrect content), reliance on outdated knowledge, and a lack of transparency or traceability in their reasoning processes. These limitations are particularly pronounced in knowledge-intensive or domain-specific tasks.\n    *   **Importance & Challenge:** The inability of LLMs to consistently provide accurate, up-to-date, and verifiable information restricts their reliability and widespread adoption in real-world applications. The challenge lies in developing a mechanism that allows LLMs to dynamically access and integrate external, up-to-date knowledge without requiring continuous and costly retraining, thereby enhancing their accuracy, credibility, and adaptability.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches:** RAG is presented as a synergistic solution that combines the intrinsic knowledge of LLMs with vast, dynamic external knowledge bases. The paper traces RAG's evolution from early efforts to enhance pre-training models (PTMs) in the Transformer era to its rapid development post-ChatGPT, where it primarily focused on improving LLM inference and later integrated with fine-tuning techniques.\n    *   **Limitations of Previous Solutions:**\n        *   **Native LLMs:** Inherently limited by their training data, leading to \"hallucinations\" and an inability to address queries requiring current or domain-specific information.\n        *   **Naive RAG:** The initial RAG paradigm suffered from significant drawbacks, including poor retrieval precision and recall (leading to irrelevant or missing context), generation difficulties (e.g., hallucination, irrelevance, bias), and challenges in augmenting LLM outputs coherently (e.g., redundancy, lack of synthesis).\n        *   **Fine-tuning (FT):** While effective for customizing model behavior and style, FT is static (requiring retraining for updates), computationally intensive, and empirically shown to be less effective than RAG for learning new factual information, especially in dynamic knowledge environments.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method/Algorithm:** The paper systematically categorizes the evolution of RAG into three paradigms:\n        *   **Naive RAG:** A foundational \"Retrieve-Read\" framework comprising indexing (data cleaning, chunking, embedding, vector storage), retrieval (semantic similarity search for top-K relevant chunks), and generation (LLM synthesizes query and retrieved context).\n        *   **Advanced RAG:** Builds upon Naive RAG by introducing specific optimizations. These include pre-retrieval strategies (e.g., refined indexing techniques like sliding windows and metadata incorporation, and query optimization methods like rewriting, transformation, and expansion) and post-retrieval processes (e.g., re-ranking retrieved chunks and context compression).\n        *   **Modular RAG:** Represents the state-of-the-art, offering enhanced adaptability and versatility. It introduces specialized functional modules (e.g., Search, RAG-Fusion, Memory, Routing, Predict, Task Adapter) and flexible interaction patterns (e.g., Rewrite-Retrieve-Read, Generate-Read, Recite-Read, hybrid retrieval, iterative/adaptive retrieval flows like FLARE and Self-RAG).\n    *   **Novelty/Difference:** The primary innovation of this work is its comprehensive and structured survey of the RAG landscape. It highlights the progression from a fixed, sequential RAG process to a highly flexible, modular architecture. This modularity allows for dynamic component substitution, adaptive retrieval strategies based on scenario needs, and easier integration with other LLM optimization techniques such as fine-tuning and reinforcement learning, addressing the limitations of earlier RAG approaches.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms, Methods, or Techniques (Surveyed):**\n        *   **Advanced RAG Optimizations:** Detailed methods for improving indexing (e.g., data granularity, metadata), query processing (e.g., rewriting, expansion), and post-retrieval context handling (e.g., re-ranking, compression).\n        *   **Modular RAG Modules:** Identification and analysis of specialized modules like Search (for diverse data sources), RAG-Fusion (multi-query and re-ranking), Memory (LLM-guided retrieval), Routing (optimal data source selection), Predict (LLM-generated context), and Task Adapter (task-specific retrieval).\n        *   **Modular RAG Patterns:** Exploration of flexible interaction patterns such as Rewrite-Retrieve-Read, Generate-Read, Recite-Read, hybrid retrieval strategies, sub-queries, Hypothetical Document Embeddings (HyDE), and iterative/adaptive retrieval flows (e.g., DSP, ITER-RETGEN, FLARE, Self-RAG).\n    *   **System Design or Architectural Innovations (Surveyed):** The paper charts the architectural evolution from a simple \"Retrieve-Read\" chain (Naive RAG) to a more sophisticated, chain-like structure with pre/post-retrieval enhancements (Advanced RAG), culminating in a highly flexible, component-based architecture that supports dynamic module interaction and integrated end-to-end training (Modular RAG).\n    *   **Theoretical Insights or Analysis:** It provides a structured understanding of the \"tripartite foundation\" of RAG (retrieval, generation, and augmentation techniques) and analyzes their interdependencies. The paper also offers a comparative analysis of RAG against fine-tuning and prompt engineering, elucidating their distinct characteristics and complementary roles in LLM optimization.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted:** As a survey paper, \\cite{gao20232zb} does not present new experimental results. Instead, it synthesizes and summarizes the existing empirical validation landscape for RAG.\n    *   **Key Performance Metrics & Comparison Results:** The paper outlines the current assessment methods for RAG, detailing:\n        *   **Downstream Tasks:** A summary of 26 relevant tasks where RAG is applied.\n        *   **Datasets:** An overview of nearly 50 datasets commonly used for RAG evaluation.\n        *   **Evaluation Objectives & Metrics:** Discussion of the criteria and metrics employed to assess RAG system performance.\n        *   **Benchmarks & Tools:** A review of current evaluation benchmarks and tools.\n        *   **Empirical Finding (from cited work):** The survey highlights that existing research (e.g., [28] cited in \\cite{gao20232zb}) demonstrates RAG's consistent outperformance of unsupervised fine-tuning for both existing and novel knowledge, noting LLMs' difficulty in learning new factual information through unsupervised fine-tuning alone.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations (of RAG, as identified by the survey):**\n        *   **Naive RAG:** Prone to selecting irrelevant or missing crucial information, leading to hallucinations and incoherent augmented outputs.\n        *   **General RAG:** Can introduce higher latency compared to native LLMs and raises ethical considerations regarding data retrieval. There's also a risk that generation models might over-rely on retrieved information, producing outputs that merely echo content without insightful synthesis.\n    *   **Scope of Applicability:** The survey focuses on RAG's role in augmenting Large Language Models, particularly for enhancing factual accuracy, enabling real-time knowledge updates, and improving credibility across knowledge-intensive tasks. It covers RAG's influence and integration across the pre-training, fine-tuning, and inference stages of LLMs.\n\n7.  **Technical Significance**\n    *   **Advancement of Technical State-of-the-Art:** This survey significantly advances the technical state-of-the-art by providing the first systematic and comprehensive synthesis of the rapidly evolving RAG field. It meticulously categorizes RAG's paradigms, dissects its core technical components (retrieval, generation, augmentation), and summarizes the state-of-the-art within each, offering a clear and structured understanding of this complex domain.\n    *   **Potential Impact on Future Research:**\n        *   **Research Roadmap:** By delineating current challenges and identifying prospective avenues for research and development, the paper serves as a crucial roadmap for future RAG innovations.\n        *   **System Design Guidance:** The detailed analysis of modular RAG and its flexible patterns will inspire the design of more adaptable, robust, and high-performing RAG systems.\n        *   **Standardized Evaluation:** The comprehensive review of evaluation frameworks, benchmarks, and metrics will contribute to standardizing and improving the rigor of RAG system assessment, fostering more meaningful comparative studies.\n        *   **Hybrid Optimization:** The discussion on RAG's complementarity with fine-tuning encourages the exploration of powerful hybrid optimization strategies for LLMs, combining their respective strengths.",
      "intriguing_abstract": "The remarkable capabilities of Large Language Models (LLMs) are often undermined by persistent challenges: factual hallucination, reliance on static training data, and a lack of transparency. Retrieval-Augmented Generation (RAG) emerges as a pivotal paradigm, dynamically integrating external knowledge to anchor LLM responses in verifiable, up-to-date information.\n\nThis comprehensive survey meticulously charts the evolution of RAG, moving beyond foundational \"Retrieve-Read\" frameworks to advanced optimizations and the cutting-edge **Modular RAG** architecture. We dissect how **Modular RAG** revolutionizes the field through specialized functional modules and flexible interaction patternsâ€”such as query rewriting, iterative retrieval, and adaptive context generation (e.g., Self-RAG)â€”to overcome limitations of earlier approaches like poor retrieval precision and generation coherence. By systematically categorizing the tripartite foundation of RAG and analyzing its synergy with other LLM optimization techniques, this paper provides a critical roadmap for researchers. It illuminates how RAG enhances LLM accuracy, credibility, and adaptability, offering a robust solution for knowledge-intensive tasks and paving the way for more reliable and transparent AI systems.",
      "keywords": [
        "Retrieval-Augmented Generation (RAG)",
        "Large Language Models (LLMs)",
        "LLM hallucination",
        "knowledge-intensive tasks",
        "Naive RAG",
        "Advanced RAG",
        "Modular RAG",
        "indexing",
        "retrieval",
        "generation",
        "augmentation",
        "query optimization",
        "re-ranking",
        "context compression",
        "iterative and adaptive retrieval flows"
      ],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/46f9f7b8f88f72e12cbdb21e3311f995eb6e65c5.pdf",
      "citation_key": "gao20232zb",
      "metadata": {
        "title": "Retrieval-Augmented Generation for Large Language Models: A Survey",
        "authors": [
          "Yunfan Gao",
          "Yun Xiong",
          "Xinyu Gao",
          "Kangxiang Jia",
          "Jinliu Pan",
          "Yuxi Bi",
          "Yi Dai",
          "Jiawei Sun",
          "Qianyu Guo",
          "Meng Wang",
          "Haofen Wang"
        ],
        "published_date": "2023",
        "abstract": "Large Language Models (LLMs) showcase impressive capabilities but encounter challenges like hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has emerged as a promising solution by incorporating knowledge from external databases. This enhances the accuracy and credibility of the generation, particularly for knowledge-intensive tasks, and allows for continuous knowledge updates and integration of domain-specific information. RAG synergistically merges LLMs' intrinsic knowledge with the vast, dynamic repositories of external databases. This comprehensive review paper offers a detailed examination of the progression of RAG paradigms, encompassing the Naive RAG, the Advanced RAG, and the Modular RAG. It meticulously scrutinizes the tripartite foundation of RAG frameworks, which includes the retrieval, the generation and the augmentation techniques. The paper highlights the state-of-the-art technologies embedded in each of these critical components, providing a profound understanding of the advancements in RAG systems. Furthermore, this paper introduces up-to-date evaluation framework and benchmark. At the end, this article delineates the challenges currently faced and points out prospective avenues for research and development.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/46f9f7b8f88f72e12cbdb21e3311f995eb6e65c5.pdf",
        "venue": "arXiv.org",
        "citationCount": 0,
        "score": 0,
        "summary": "This paper, \"Retrieval-Augmented Generation for Large Language Models: A Survey\" \\cite{gao20232zb}, offers a comprehensive review of Retrieval-Augmented Generation (RAG) for Large Language Models (LLMs).\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem:** Large Language Models (LLMs) exhibit impressive capabilities but are plagued by issues such as hallucination (generating factually incorrect content), reliance on outdated knowledge, and a lack of transparency or traceability in their reasoning processes. These limitations are particularly pronounced in knowledge-intensive or domain-specific tasks.\n    *   **Importance & Challenge:** The inability of LLMs to consistently provide accurate, up-to-date, and verifiable information restricts their reliability and widespread adoption in real-world applications. The challenge lies in developing a mechanism that allows LLMs to dynamically access and integrate external, up-to-date knowledge without requiring continuous and costly retraining, thereby enhancing their accuracy, credibility, and adaptability.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches:** RAG is presented as a synergistic solution that combines the intrinsic knowledge of LLMs with vast, dynamic external knowledge bases. The paper traces RAG's evolution from early efforts to enhance pre-training models (PTMs) in the Transformer era to its rapid development post-ChatGPT, where it primarily focused on improving LLM inference and later integrated with fine-tuning techniques.\n    *   **Limitations of Previous Solutions:**\n        *   **Native LLMs:** Inherently limited by their training data, leading to \"hallucinations\" and an inability to address queries requiring current or domain-specific information.\n        *   **Naive RAG:** The initial RAG paradigm suffered from significant drawbacks, including poor retrieval precision and recall (leading to irrelevant or missing context), generation difficulties (e.g., hallucination, irrelevance, bias), and challenges in augmenting LLM outputs coherently (e.g., redundancy, lack of synthesis).\n        *   **Fine-tuning (FT):** While effective for customizing model behavior and style, FT is static (requiring retraining for updates), computationally intensive, and empirically shown to be less effective than RAG for learning new factual information, especially in dynamic knowledge environments.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method/Algorithm:** The paper systematically categorizes the evolution of RAG into three paradigms:\n        *   **Naive RAG:** A foundational \"Retrieve-Read\" framework comprising indexing (data cleaning, chunking, embedding, vector storage), retrieval (semantic similarity search for top-K relevant chunks), and generation (LLM synthesizes query and retrieved context).\n        *   **Advanced RAG:** Builds upon Naive RAG by introducing specific optimizations. These include pre-retrieval strategies (e.g., refined indexing techniques like sliding windows and metadata incorporation, and query optimization methods like rewriting, transformation, and expansion) and post-retrieval processes (e.g., re-ranking retrieved chunks and context compression).\n        *   **Modular RAG:** Represents the state-of-the-art, offering enhanced adaptability and versatility. It introduces specialized functional modules (e.g., Search, RAG-Fusion, Memory, Routing, Predict, Task Adapter) and flexible interaction patterns (e.g., Rewrite-Retrieve-Read, Generate-Read, Recite-Read, hybrid retrieval, iterative/adaptive retrieval flows like FLARE and Self-RAG).\n    *   **Novelty/Difference:** The primary innovation of this work is its comprehensive and structured survey of the RAG landscape. It highlights the progression from a fixed, sequential RAG process to a highly flexible, modular architecture. This modularity allows for dynamic component substitution, adaptive retrieval strategies based on scenario needs, and easier integration with other LLM optimization techniques such as fine-tuning and reinforcement learning, addressing the limitations of earlier RAG approaches.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms, Methods, or Techniques (Surveyed):**\n        *   **Advanced RAG Optimizations:** Detailed methods for improving indexing (e.g., data granularity, metadata), query processing (e.g., rewriting, expansion), and post-retrieval context handling (e.g., re-ranking, compression).\n        *   **Modular RAG Modules:** Identification and analysis of specialized modules like Search (for diverse data sources), RAG-Fusion (multi-query and re-ranking), Memory (LLM-guided retrieval), Routing (optimal data source selection), Predict (LLM-generated context), and Task Adapter (task-specific retrieval).\n        *   **Modular RAG Patterns:** Exploration of flexible interaction patterns such as Rewrite-Retrieve-Read, Generate-Read, Recite-Read, hybrid retrieval strategies, sub-queries, Hypothetical Document Embeddings (HyDE), and iterative/adaptive retrieval flows (e.g., DSP, ITER-RETGEN, FLARE, Self-RAG).\n    *   **System Design or Architectural Innovations (Surveyed):** The paper charts the architectural evolution from a simple \"Retrieve-Read\" chain (Naive RAG) to a more sophisticated, chain-like structure with pre/post-retrieval enhancements (Advanced RAG), culminating in a highly flexible, component-based architecture that supports dynamic module interaction and integrated end-to-end training (Modular RAG).\n    *   **Theoretical Insights or Analysis:** It provides a structured understanding of the \"tripartite foundation\" of RAG (retrieval, generation, and augmentation techniques) and analyzes their interdependencies. The paper also offers a comparative analysis of RAG against fine-tuning and prompt engineering, elucidating their distinct characteristics and complementary roles in LLM optimization.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted:** As a survey paper, \\cite{gao20232zb} does not present new experimental results. Instead, it synthesizes and summarizes the existing empirical validation landscape for RAG.\n    *   **Key Performance Metrics & Comparison Results:** The paper outlines the current assessment methods for RAG, detailing:\n        *   **Downstream Tasks:** A summary of 26 relevant tasks where RAG is applied.\n        *   **Datasets:** An overview of nearly 50 datasets commonly used for RAG evaluation.\n        *   **Evaluation Objectives & Metrics:** Discussion of the criteria and metrics employed to assess RAG system performance.\n        *   **Benchmarks & Tools:** A review of current evaluation benchmarks and tools.\n        *   **Empirical Finding (from cited work):** The survey highlights that existing research (e.g., [28] cited in \\cite{gao20232zb}) demonstrates RAG's consistent outperformance of unsupervised fine-tuning for both existing and novel knowledge, noting LLMs' difficulty in learning new factual information through unsupervised fine-tuning alone.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations (of RAG, as identified by the survey):**\n        *   **Naive RAG:** Prone to selecting irrelevant or missing crucial information, leading to hallucinations and incoherent augmented outputs.\n        *   **General RAG:** Can introduce higher latency compared to native LLMs and raises ethical considerations regarding data retrieval. There's also a risk that generation models might over-rely on retrieved information, producing outputs that merely echo content without insightful synthesis.\n    *   **Scope of Applicability:** The survey focuses on RAG's role in augmenting Large Language Models, particularly for enhancing factual accuracy, enabling real-time knowledge updates, and improving credibility across knowledge-intensive tasks. It covers RAG's influence and integration across the pre-training, fine-tuning, and inference stages of LLMs.\n\n7.  **Technical Significance**\n    *   **Advancement of Technical State-of-the-Art:** This survey significantly advances the technical state-of-the-art by providing the first systematic and comprehensive synthesis of the rapidly evolving RAG field. It meticulously categorizes RAG's paradigms, dissects its core technical components (retrieval, generation, augmentation), and summarizes the state-of-the-art within each, offering a clear and structured understanding of this complex domain.\n    *   **Potential Impact on Future Research:**\n        *   **Research Roadmap:** By delineating current challenges and identifying prospective avenues for research and development, the paper serves as a crucial roadmap for future RAG innovations.\n        *   **System Design Guidance:** The detailed analysis of modular RAG and its flexible patterns will inspire the design of more adaptable, robust, and high-performing RAG systems.\n        *   **Standardized Evaluation:** The comprehensive review of evaluation frameworks, benchmarks, and metrics will contribute to standardizing and improving the rigor of RAG system assessment, fostering more meaningful comparative studies.\n        *   **Hybrid Optimization:** The discussion on RAG's complementarity with fine-tuning encourages the exploration of powerful hybrid optimization strategies for LLMs, combining their respective strengths.",
        "keywords": [
          "Retrieval-Augmented Generation (RAG)",
          "Large Language Models (LLMs)",
          "LLM hallucination",
          "knowledge-intensive tasks",
          "Naive RAG",
          "Advanced RAG",
          "Modular RAG",
          "indexing",
          "retrieval",
          "generation",
          "augmentation",
          "query optimization",
          "re-ranking",
          "context compression",
          "iterative and adaptive retrieval flows"
        ],
        "paper_type": "**survey**"
      },
      "file_name": "46f9f7b8f88f72e12cbdb21e3311f995eb6e65c5.pdf"
    },
    {
      "success": true,
      "doc_id": "42ce3de9c4edb36d3832ef3b8173ebe9",
      "summary": "Here's a focused summary of the paper \\cite{ji20227ii} for a literature review:\n\n### Technical Paper Analysis: RHO: Reducing Hallucination in Open-domain Dialogues with Knowledge Grounding \\cite{ji20227ii}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Addressing the \"hallucination problem\" in knowledge-grounded dialogue (KGD) systems, where generated responses are nonsensical, unfaithful, or not supported by the input source (dialogue history or external knowledge). This includes both intrinsic (contradicted by source) and extrinsic (unverifiable) hallucinations.\n    *   **Importance & Challenge**: Hallucinations severely hinder the application of dialogue systems, raising safety concerns and undermining performance (e.g., observed in large foundation models like ChatGPT). The core challenge lies in the \"heterogeneity between external knowledge and textual dialogue content,\" which makes representation learning and source integration difficult for neural response generation models, leading to unfaithful outputs.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: Previous efforts to reduce hallucination in KGD include retrieved knowledge augmentation, control codes during decoding, and response post-processing (e.g., NPH \\cite{dziri2021neural}, Shuster et al. \\cite{shuster2021retrieval}, Rashkin et al. \\cite{rashkin2021towards}, Wu et al. \\cite{wu2021faithfulness}).\n    *   **Limitations of Previous Solutions**: These prior works \"do not emphasize handling the discrepancy between lexical dialogue and structured knowledge information for the harmony of their fusion.\" They also lack clarity on the interaction mechanism between external knowledge and dialogue context. \\cite{ji20227ii} positions its work as improving this fusion and interaction through enhanced knowledge groundings and reasoning techniques.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes RHO (Reducing Hallucination in Open-domain dialogue systems), an encoder-decoder model (based on BART) that integrates structured knowledge from Knowledge Graphs (KGs) into dialogue generation. It employs a multi-stage approach:\n        *   **Knowledgeable Task Guidance**: Linearizes KG triples into \"meta-knowledge\" and concatenates them with dialogue history using a specific prompt template to guide the language model.\n        *   **Local Knowledge Grounding**: Fuses KG embeddings of linked entities and relation predicates directly into the textual embeddings of corresponding tokens in the dialogue history.\n        *   **Global Knowledge Grounding**: Utilizes an attention mechanism to allow each token in the dialogue history to attentively learn from the entire context-related knowledge sub-graph stored in a memory bank, enabling multi-hop reasoning.\n        *   **Response Re-ranking**: A post-processing step that employs a conversational reasoning model (extending KG-CRUSE \\cite{sarkar2022kg}) to re-rank candidate responses. It selects the response that can be most faithfully reasoned backward to the source knowledge graph via learned traversal paths.\n    *   **Novelty/Differentiation**:\n        *   **Harmonized Knowledge Fusion**: Explicitly addresses the heterogeneity challenge by introducing both local (token-level) and global (sub-graph level) knowledge grounding techniques, ensuring a more comprehensive and integrated use of structured knowledge.\n        *   **Multi-hop Reasoning**: Global knowledge grounding equips the model with multi-hop reasoning abilities by dynamically weighting knowledge triples via attention.\n        *   **Faithfulness-driven Re-ranking**: The re-ranking mechanism is novel in its emphasis on conversational reasoning over KG sub-graphs to explicitly reward responses that are verifiable and faithful to the source knowledge, acting as an output constraint.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**:\n        *   Introduction of **RHO**, a comprehensive model for hallucination reduction in KGD leveraging structured KG knowledge.\n        *   **Local Knowledge Grounding**: A technique to fuse token-level textual embeddings with corresponding KG embeddings of linked entities and relations.\n        *   **Global Knowledge Grounding**: An attention-based mechanism for integrating knowledge from the entire context-related KG sub-graph, facilitating multi-hop reasoning.\n        *   **Response Re-ranking**: A conversational reasoning model that traverses KG sub-graphs to evaluate and select the most faithful generated response.\n    *   **System Design/Architectural Innovations**: The integration of these grounding techniques within an encoder-decoder architecture (BART) and the subsequent re-ranking module form a robust framework for faithful dialogue generation.\n    *   **Theoretical Insights/Analysis**: The work implicitly demonstrates that explicit knowledge grounding at multiple levels (local and global) and a faithfulness-driven re-ranking process can significantly improve the verifiability and consistency of generated dialogue responses.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**: Experiments were performed on the OpenDialKG dataset \\cite{moon2019opendialkg}. The evaluation included both automatic metrics and human evaluation.\n    *   **Key Performance Metrics**:\n        *   **Automatic Evaluation**: FeQA (Faithfulness Evaluation with Question Answering) \\cite{durmus2020feqa} was used to quantify faithfulness.\n        *   **Human Evaluation**: Assessed hallucination reduction, conversational abilities, and overall quality.\n    *   **Comparison Results**:\n        *   RHO significantly **outperformed state-of-the-art (SOTA)** methods (e.g., NPH \\cite{dziri2021neural}) by a large margin.\n        *   Achieved a **17.54% improvement in FeQA** scores, indicating substantially higher faithfulness.\n        *   Demonstrated a **32.93% reduction in hallucinations** according to human evaluation.\n        *   The generated responses showed **broader coverage of entities and relations** from the KG, further validating higher faithfulness without sacrificing conversational quality.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The approach relies on the accuracy of entity/relation linking tools (e.g., FLAIR) and the quality of pre-trained KG embeddings (e.g., TransE). While other KG embedding algorithms were explored, TransE was chosen for its effectiveness and simplicity, suggesting potential for further exploration with more advanced KG representation learning. The re-ranker extends KG-CRUSE, inheriting its underlying principles.\n    *   **Scope of Applicability**: The work primarily focuses on \"factoid knowledge paths\" from KGs for open-domain dialogue systems. Its direct applicability to other types of knowledge (e.g., commonsense knowledge not explicitly structured in a KG) or highly specialized domains might require adaptation.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: \\cite{ji20227ii} significantly advances the state-of-the-art in knowledge-grounded dialogue generation by providing a robust framework that effectively mitigates the hallucination problem. It demonstrates that explicit, multi-level knowledge grounding combined with a faithfulness-aware re-ranking mechanism can lead to substantially more verifiable and consistent responses.\n    *   **Potential Impact on Future Research**: This work lays a strong foundation for future research in:\n        *   Developing more sophisticated methods for integrating heterogeneous knowledge sources into language models.\n        *   Designing intrinsic mechanisms for multi-hop reasoning within dialogue systems.\n        *   Creating more effective and automated evaluation metrics for faithfulness and hallucination.\n        *   Improving the safety and reliability of large language models by reducing their propensity for generating unfaithful information.",
      "intriguing_abstract": "The pervasive problem of hallucinationâ€”where dialogue systems generate nonsensical or unfaithful responsesâ€”severely undermines the reliability and safety of knowledge-grounded dialogue (KGD) systems, including large foundation models. This challenge is exacerbated by the inherent heterogeneity between structured external knowledge and textual dialogue content, making effective knowledge integration elusive. We introduce RHO (Reducing Hallucination in Open-domain dialogue systems), a novel encoder-decoder framework designed to robustly mitigate this issue.\n\nRHO innovates with a multi-stage approach, featuring both **Local Knowledge Grounding**, which fuses Knowledge Graph (KG) embeddings directly into textual tokens, and **Global Knowledge Grounding**, an attention-based mechanism enabling multi-hop reasoning across context-related KG sub-graphs. This harmonized fusion explicitly addresses knowledge discrepancy. Furthermore, a unique faithfulness-driven **Response Re-ranking** module leverages conversational reasoning over KGs to select verifiable outputs. Evaluated on OpenDialKG, RHO dramatically outperforms state-of-the-art methods, achieving a **17.54% improvement in FeQA scores** and a **32.93% reduction in hallucinations** via human evaluation. Our work marks a significant leap towards building truly reliable and verifiable open-domain dialogue systems.",
      "keywords": [
        "hallucination problem",
        "knowledge-grounded dialogue (KGD)",
        "RHO model",
        "Knowledge Graphs (KGs)",
        "multi-level knowledge grounding",
        "multi-hop reasoning",
        "faithfulness-driven re-ranking",
        "conversational reasoning",
        "heterogeneous knowledge fusion",
        "hallucination reduction",
        "improved faithfulness",
        "state-of-the-art advancement",
        "open-domain dialogue systems"
      ],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/4e53b481beabba42aac027e5a8c69fed26ab4062.pdf",
      "citation_key": "ji20227ii",
      "metadata": {
        "title": "RHO ($Ï$): Reducing Hallucination in Open-domain Dialogues with Knowledge Grounding",
        "authors": [
          "Ziwei Ji",
          "Zihan Liu",
          "Nayeon Lee",
          "Tiezheng Yu",
          "Bryan Wilie",
          "Mini Zeng",
          "Pascale Fung"
        ],
        "published_date": "2022",
        "abstract": "Dialogue systems can leverage large pre-trained language models and knowledge to generate fluent and informative responses. However, these models are still prone to produce hallucinated responses not supported by the input source, which greatly hinders their application. The heterogeneity between external knowledge and dialogue context challenges representation learning and source integration, and further contributes to unfaithfulness. To handle this challenge and generate more faithful responses, this paper presents RHO ($\\rho$) utilizing the representations of linked entities and relation predicates from a knowledge graph (KG). We propose (1) local knowledge grounding to combine textual embeddings with the corresponding KG embeddings; and (2) global knowledge grounding to equip RHO with multi-hop reasoning abilities via the attention mechanism. In addition, we devise a response re-ranking technique based on walks over KG sub-graphs for better conversational reasoning. Experimental results on OpenDialKG show that our approach significantly outperforms state-of-the-art methods on both automatic and human evaluation by a large margin, especially in hallucination reduction (17.54% in FeQA).",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/4e53b481beabba42aac027e5a8c69fed26ab4062.pdf",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the paper \\cite{ji20227ii} for a literature review:\n\n### Technical Paper Analysis: RHO: Reducing Hallucination in Open-domain Dialogues with Knowledge Grounding \\cite{ji20227ii}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Addressing the \"hallucination problem\" in knowledge-grounded dialogue (KGD) systems, where generated responses are nonsensical, unfaithful, or not supported by the input source (dialogue history or external knowledge). This includes both intrinsic (contradicted by source) and extrinsic (unverifiable) hallucinations.\n    *   **Importance & Challenge**: Hallucinations severely hinder the application of dialogue systems, raising safety concerns and undermining performance (e.g., observed in large foundation models like ChatGPT). The core challenge lies in the \"heterogeneity between external knowledge and textual dialogue content,\" which makes representation learning and source integration difficult for neural response generation models, leading to unfaithful outputs.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: Previous efforts to reduce hallucination in KGD include retrieved knowledge augmentation, control codes during decoding, and response post-processing (e.g., NPH \\cite{dziri2021neural}, Shuster et al. \\cite{shuster2021retrieval}, Rashkin et al. \\cite{rashkin2021towards}, Wu et al. \\cite{wu2021faithfulness}).\n    *   **Limitations of Previous Solutions**: These prior works \"do not emphasize handling the discrepancy between lexical dialogue and structured knowledge information for the harmony of their fusion.\" They also lack clarity on the interaction mechanism between external knowledge and dialogue context. \\cite{ji20227ii} positions its work as improving this fusion and interaction through enhanced knowledge groundings and reasoning techniques.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes RHO (Reducing Hallucination in Open-domain dialogue systems), an encoder-decoder model (based on BART) that integrates structured knowledge from Knowledge Graphs (KGs) into dialogue generation. It employs a multi-stage approach:\n        *   **Knowledgeable Task Guidance**: Linearizes KG triples into \"meta-knowledge\" and concatenates them with dialogue history using a specific prompt template to guide the language model.\n        *   **Local Knowledge Grounding**: Fuses KG embeddings of linked entities and relation predicates directly into the textual embeddings of corresponding tokens in the dialogue history.\n        *   **Global Knowledge Grounding**: Utilizes an attention mechanism to allow each token in the dialogue history to attentively learn from the entire context-related knowledge sub-graph stored in a memory bank, enabling multi-hop reasoning.\n        *   **Response Re-ranking**: A post-processing step that employs a conversational reasoning model (extending KG-CRUSE \\cite{sarkar2022kg}) to re-rank candidate responses. It selects the response that can be most faithfully reasoned backward to the source knowledge graph via learned traversal paths.\n    *   **Novelty/Differentiation**:\n        *   **Harmonized Knowledge Fusion**: Explicitly addresses the heterogeneity challenge by introducing both local (token-level) and global (sub-graph level) knowledge grounding techniques, ensuring a more comprehensive and integrated use of structured knowledge.\n        *   **Multi-hop Reasoning**: Global knowledge grounding equips the model with multi-hop reasoning abilities by dynamically weighting knowledge triples via attention.\n        *   **Faithfulness-driven Re-ranking**: The re-ranking mechanism is novel in its emphasis on conversational reasoning over KG sub-graphs to explicitly reward responses that are verifiable and faithful to the source knowledge, acting as an output constraint.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**:\n        *   Introduction of **RHO**, a comprehensive model for hallucination reduction in KGD leveraging structured KG knowledge.\n        *   **Local Knowledge Grounding**: A technique to fuse token-level textual embeddings with corresponding KG embeddings of linked entities and relations.\n        *   **Global Knowledge Grounding**: An attention-based mechanism for integrating knowledge from the entire context-related KG sub-graph, facilitating multi-hop reasoning.\n        *   **Response Re-ranking**: A conversational reasoning model that traverses KG sub-graphs to evaluate and select the most faithful generated response.\n    *   **System Design/Architectural Innovations**: The integration of these grounding techniques within an encoder-decoder architecture (BART) and the subsequent re-ranking module form a robust framework for faithful dialogue generation.\n    *   **Theoretical Insights/Analysis**: The work implicitly demonstrates that explicit knowledge grounding at multiple levels (local and global) and a faithfulness-driven re-ranking process can significantly improve the verifiability and consistency of generated dialogue responses.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**: Experiments were performed on the OpenDialKG dataset \\cite{moon2019opendialkg}. The evaluation included both automatic metrics and human evaluation.\n    *   **Key Performance Metrics**:\n        *   **Automatic Evaluation**: FeQA (Faithfulness Evaluation with Question Answering) \\cite{durmus2020feqa} was used to quantify faithfulness.\n        *   **Human Evaluation**: Assessed hallucination reduction, conversational abilities, and overall quality.\n    *   **Comparison Results**:\n        *   RHO significantly **outperformed state-of-the-art (SOTA)** methods (e.g., NPH \\cite{dziri2021neural}) by a large margin.\n        *   Achieved a **17.54% improvement in FeQA** scores, indicating substantially higher faithfulness.\n        *   Demonstrated a **32.93% reduction in hallucinations** according to human evaluation.\n        *   The generated responses showed **broader coverage of entities and relations** from the KG, further validating higher faithfulness without sacrificing conversational quality.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The approach relies on the accuracy of entity/relation linking tools (e.g., FLAIR) and the quality of pre-trained KG embeddings (e.g., TransE). While other KG embedding algorithms were explored, TransE was chosen for its effectiveness and simplicity, suggesting potential for further exploration with more advanced KG representation learning. The re-ranker extends KG-CRUSE, inheriting its underlying principles.\n    *   **Scope of Applicability**: The work primarily focuses on \"factoid knowledge paths\" from KGs for open-domain dialogue systems. Its direct applicability to other types of knowledge (e.g., commonsense knowledge not explicitly structured in a KG) or highly specialized domains might require adaptation.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: \\cite{ji20227ii} significantly advances the state-of-the-art in knowledge-grounded dialogue generation by providing a robust framework that effectively mitigates the hallucination problem. It demonstrates that explicit, multi-level knowledge grounding combined with a faithfulness-aware re-ranking mechanism can lead to substantially more verifiable and consistent responses.\n    *   **Potential Impact on Future Research**: This work lays a strong foundation for future research in:\n        *   Developing more sophisticated methods for integrating heterogeneous knowledge sources into language models.\n        *   Designing intrinsic mechanisms for multi-hop reasoning within dialogue systems.\n        *   Creating more effective and automated evaluation metrics for faithfulness and hallucination.\n        *   Improving the safety and reliability of large language models by reducing their propensity for generating unfaithful information.",
        "keywords": [
          "hallucination problem",
          "knowledge-grounded dialogue (KGD)",
          "RHO model",
          "Knowledge Graphs (KGs)",
          "multi-level knowledge grounding",
          "multi-hop reasoning",
          "faithfulness-driven re-ranking",
          "conversational reasoning",
          "heterogeneous knowledge fusion",
          "hallucination reduction",
          "improved faithfulness",
          "state-of-the-art advancement",
          "open-domain dialogue systems"
        ],
        "paper_type": "based on the abstract and introduction:\n\n*   the abstract explicitly states: \"this paper **presents rho**\", \"we **propose** (1) local knowledge grounding...\", \"and (2) global knowledge grounding...\", \"we **devise** a response re-ranking technique\". these phrases directly align with the criteria for a **technical** paper, which \"presents new methods, algorithms, or systems.\"\n*   the introduction discusses a \"hallucination problem\" in \"knowledge-grounded dialogue (kgd) systems,\" which is a clear technical problem that the proposed rho system aims to solve.\n*   while \"experimental results\" are mentioned, they serve to validate the effectiveness of the *proposed new approach* (rho), rather than being the sole or primary contribution of a data-driven study (which would be an empirical paper). the core contribution is the creation of rho and its components.\n\ntherefore, the paper type is: **technical**"
      },
      "file_name": "4e53b481beabba42aac027e5a8c69fed26ab4062.pdf"
    },
    {
      "success": true,
      "doc_id": "0e6c58f28f6885b07d75f6c25cdf6be4",
      "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **CITATION**: \\cite{adams202289x}\n\n---\n\n### Technical Paper Analysis: Learning to Revise References for Faithful Summarization\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Abstractive summarization models frequently produce unfaithful summaries (hallucinations), a problem exacerbated by noisy, naturally occurring reference summaries in training datasets \\cite{adams202289x}. These datasets often lack \"inherent quality guarantees\" \\cite{adams202289x}.\n    *   **Importance and Challenge**: Noisy training data is detrimental to developing faithful summarization models \\cite{adams202289x}. While filtering low-quality samples works for large, clean corpora, it significantly degrades performance on smaller, noisier datasets (e.g., clinical notes) by discarding valuable data \\cite{adams202289x}. The paper highlights clinical summarization from MIMIC-III, where 60% of reference summary entities are unsupported by the source, demonstrating the severity of the noise problem \\cite{adams202289x}.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: Previous efforts to improve faithfulness have focused on smarter models, better metrics, content-plan editing, and post-hoc interventions (ranking, editing) \\cite{adams202289x}. For training data noise, the common approach is *filtering* low-quality samples based on entailment or entity overlap \\cite{adams202289x}.\n    *   **Limitations of Previous Solutions**: Filtering methods, while sometimes improving faithfulness, often degrade informativeness and are \"insufficient to train a competitive model\" when high-quality data is limited \\cite{adams202289x}. They discard data rather than improving it. Synthetic error generation is used but can produce disfluent text or require extensive annotations \\cite{adams202289x}. For clinical summarization, faithfulness in abstractive models is less explored, with most work focusing on extractive methods or finer temporal granularities \\cite{adams202289x}. This work positions itself by proposing to *revise* noisy references rather than remove them, thereby leveraging all available data.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes \"reference revision\" as a data pre-processing step. This involves selectively rewriting unsupported reference sentences to align better with the source text \\cite{adams202289x}.\n        *   **Alignment and Classification**: Each reference sentence is aligned to a small subset (1-5) of source sentences using BERTScore precision and entity coverage. Sentences are then classified as \"supported\" (no unsupported entities, high BERTScore) or \"unsupported\" \\cite{adams202289x}.\n        *   **Synthetic Data Generation (ReDRESS)**: To train a revision model without a gold standard, a novel method called ReDRESS (Reference Distractor Entity Set Swapping) is introduced. ReDRESS takes supported reference sentences and corrupts them to create diverse, realistic \"unsupported\" alternatives. This is achieved by combining BART's span deletion with controlled entity swaps using topically related entities \\cite{adams202289x}. The intensity of corruption (`k` entity swaps) is a controllable attribute \\cite{adams202289x}.\n        *   **Contrastive Learning**: The mix of real (supported) and synthetically generated (unsupported) sentences forms positive and negative revision examples, which are then used to train a revision model via contrastive learning \\cite{adams202289x}. The model learns to transform unsupported sentences into supported ones.\n        *   **Inference**: During inference, diverse revision candidates can be over-generated and then re-scored to balance faithfulness and abstraction \\cite{adams202289x}.\n    *   **Novelty/Difference**: The approach is novel in its focus on *revising* noisy references rather than simply filtering them, especially for low-resource, noisy domains \\cite{adams202289x}. The automatic generation of diverse, realistic synthetic hallucinations via controlled entity swaps (ReDRESS) for contrastive learning is a key innovation, allowing training without human-annotated revisions \\cite{adams202289x}.\n\n4.  **Key Technical Contributions**\n    *   **Novel Method**: Introduction of \"reference revision\" as a model-agnostic data pre-processing step to improve reference quality for faithful summarization \\cite{adams202289x}.\n    *   **Algorithm for Synthetic Data**: ReDRESS (Reference Distractor Entity Set Swapping), a BART-based denoising autoencoder that generates diverse and realistic synthetic hallucinations by combining span deletion with controlled entity swaps, enabling training without direct supervision for revision \\cite{adams202289x}.\n    *   **Learning Objective**: Application of contrastive learning to train the revision model using the synthetically generated positive and negative revision examples \\cite{adams202289x}.\n    *   **System Design**: A two-stage pipeline involving robust source-reference alignment and classification, followed by a revision model trained on synthetic data \\cite{adams202289x}.\n    *   **Versatility**: Demonstrates that reference revision can also function as a post-hoc editor and a pre-training objective for faithfulness, extending its utility beyond data pre-processing \\cite{adams202289x}.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**: The methods were tested on the task of hospital-course summarization using the publicly available MIMIC-III dataset \\cite{adams202289x}. Summarization models were trained on three different data preparations: original noisy references, filtered references (using existing methods), and references revised by the proposed method \\cite{adams202289x}.\n    *   **Key Performance Metrics**: Performance was evaluated using both automatic metrics and human evaluation for faithfulness, informativeness, and fluency \\cite{adams202289x}.\n    *   **Comparison Results**: Models trained on the *revised* clinical references significantly outperformed models trained on original or filtered data across all evaluated metrics (faithfulness, informativeness, and fluency), as confirmed by both automatic scores and human judgment \\cite{adams202289x}. This empirically validates the effectiveness of the reference revision approach.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The method relies on the accuracy of entity extraction (e.g., Amazon Comprehend Medical) and similarity scoring for determining entity support \\cite{adams202289x}. The definition of \"supported\" sentences uses heuristic thresholds (e.g., BERTScore precision >= 0.75) \\cite{adams202289x}. The realism of synthetically generated hallucinations by ReDRESS is crucial for effective training.\n    *   **Scope of Applicability**: The approach is primarily validated in the context of clinical summarization (MIMIC-III), a domain characterized by high noise and limited resources \\cite{adams202289x}. However, as a data pre-processing step, it is designed to be model-agnostic and potentially applicable to other domains with noisy reference data \\cite{adams202289x}.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: This work significantly advances the state-of-the-art in faithful summarization by providing a novel, data-centric solution to the problem of noisy training references \\cite{adams202289x}. Instead of discarding data, it actively improves its quality, which is particularly critical for low-resource and noisy domains where filtering is detrimental \\cite{adams202289x}.\n    *   **Potential Impact on Future Research**: The proposed reference revision framework, especially the ReDRESS algorithm for synthetic data generation and the use of contrastive learning, opens new avenues for research in data augmentation and quality improvement for various NLP tasks. It encourages a shift towards \"data-centric AI\" in summarization. The release of code, pre-processed datasets, and models will facilitate further research in clinical summarization and the broader challenge of faithfulness in abstractive models \\cite{adams202289x}.",
      "intriguing_abstract": "Abstractive summarization models frequently generate unfaithful content, or \"hallucinations,\" a critical problem exacerbated by noisy reference summaries in training datasets, particularly in high-stakes domains like clinical summarization. Traditional filtering methods, while attempting to mitigate this, often discard valuable data, severely degrading performance on smaller, noisy corpora. We propose a novel, data-centric paradigm: **reference revision**, a pre-processing step that actively transforms noisy reference sentences into faithful alternatives rather than simply removing them.\n\nOur core innovation is **ReDRESS (Reference Distractor Entity Set Swapping)**, a sophisticated algorithm that automatically generates diverse and realistic synthetic hallucinations by combining BART's span deletion with controlled entity swaps. This synthetic data, coupled with contrastive learning, enables training a robust revision model without requiring human-annotated revisions. Evaluated on the challenging MIMIC-III clinical summarization dataset, models trained on our revised references significantly outperform those trained on original or filtered data across faithfulness, informativeness, and fluency. This work offers a powerful, model-agnostic solution to the hallucination crisis, paving the way for more reliable abstractive summarization and advancing data-centric AI in low-resource, high-stakes applications.",
      "keywords": [
        "Faithful summarization",
        "hallucinations",
        "noisy training data",
        "reference revision",
        "data pre-processing",
        "ReDRESS (Reference Distractor Entity Set Swapping)",
        "synthetic data generation",
        "controlled entity swaps",
        "contrastive learning",
        "clinical summarization",
        "MIMIC-III dataset",
        "improved faithfulness",
        "informativeness and fluency",
        "data-centric AI"
      ],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/933d1d4f18e721160ddbf8dab25c33f8e3d2cec7.pdf",
      "citation_key": "adams202289x",
      "metadata": {
        "title": "Learning to Revise References for Faithful Summarization",
        "authors": [
          "Griffin Adams",
          "Han-Chin Shing",
          "Q. Sun",
          "C. Winestock",
          "K. McKeown",
          "NoÃ©mie Elhadad"
        ],
        "published_date": "2022",
        "abstract": "In real-world scenarios with naturally occurring datasets, reference summaries are noisy and may contain information that cannot be inferred from the source text. On large news corpora, removing low quality samples has been shown to reduce model hallucinations. Yet, for smaller, and/or noisier corpora, filtering is detrimental to performance. To improve reference quality while retaining all data, we propose a new approach: to selectively re-write unsupported reference sentences to better reflect source data. We automatically generate a synthetic dataset of positive and negative revisions by corrupting supported sentences and learn to revise reference sentences with contrastive learning. The intensity of revisions is treated as a controllable attribute so that, at inference, diverse candidates can be over-generated-then-rescored to balance faithfulness and abstraction. To test our methods, we extract noisy references from publicly available MIMIC-III discharge summaries for the task of hospital-course summarization, and vary the data on which models are trained. According to metrics and human evaluation, models trained on revised clinical references are much more faithful, informative, and fluent than models trained on original or filtered data.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/933d1d4f18e721160ddbf8dab25c33f8e3d2cec7.pdf",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **CITATION**: \\cite{adams202289x}\n\n---\n\n### Technical Paper Analysis: Learning to Revise References for Faithful Summarization\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Abstractive summarization models frequently produce unfaithful summaries (hallucinations), a problem exacerbated by noisy, naturally occurring reference summaries in training datasets \\cite{adams202289x}. These datasets often lack \"inherent quality guarantees\" \\cite{adams202289x}.\n    *   **Importance and Challenge**: Noisy training data is detrimental to developing faithful summarization models \\cite{adams202289x}. While filtering low-quality samples works for large, clean corpora, it significantly degrades performance on smaller, noisier datasets (e.g., clinical notes) by discarding valuable data \\cite{adams202289x}. The paper highlights clinical summarization from MIMIC-III, where 60% of reference summary entities are unsupported by the source, demonstrating the severity of the noise problem \\cite{adams202289x}.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: Previous efforts to improve faithfulness have focused on smarter models, better metrics, content-plan editing, and post-hoc interventions (ranking, editing) \\cite{adams202289x}. For training data noise, the common approach is *filtering* low-quality samples based on entailment or entity overlap \\cite{adams202289x}.\n    *   **Limitations of Previous Solutions**: Filtering methods, while sometimes improving faithfulness, often degrade informativeness and are \"insufficient to train a competitive model\" when high-quality data is limited \\cite{adams202289x}. They discard data rather than improving it. Synthetic error generation is used but can produce disfluent text or require extensive annotations \\cite{adams202289x}. For clinical summarization, faithfulness in abstractive models is less explored, with most work focusing on extractive methods or finer temporal granularities \\cite{adams202289x}. This work positions itself by proposing to *revise* noisy references rather than remove them, thereby leveraging all available data.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes \"reference revision\" as a data pre-processing step. This involves selectively rewriting unsupported reference sentences to align better with the source text \\cite{adams202289x}.\n        *   **Alignment and Classification**: Each reference sentence is aligned to a small subset (1-5) of source sentences using BERTScore precision and entity coverage. Sentences are then classified as \"supported\" (no unsupported entities, high BERTScore) or \"unsupported\" \\cite{adams202289x}.\n        *   **Synthetic Data Generation (ReDRESS)**: To train a revision model without a gold standard, a novel method called ReDRESS (Reference Distractor Entity Set Swapping) is introduced. ReDRESS takes supported reference sentences and corrupts them to create diverse, realistic \"unsupported\" alternatives. This is achieved by combining BART's span deletion with controlled entity swaps using topically related entities \\cite{adams202289x}. The intensity of corruption (`k` entity swaps) is a controllable attribute \\cite{adams202289x}.\n        *   **Contrastive Learning**: The mix of real (supported) and synthetically generated (unsupported) sentences forms positive and negative revision examples, which are then used to train a revision model via contrastive learning \\cite{adams202289x}. The model learns to transform unsupported sentences into supported ones.\n        *   **Inference**: During inference, diverse revision candidates can be over-generated and then re-scored to balance faithfulness and abstraction \\cite{adams202289x}.\n    *   **Novelty/Difference**: The approach is novel in its focus on *revising* noisy references rather than simply filtering them, especially for low-resource, noisy domains \\cite{adams202289x}. The automatic generation of diverse, realistic synthetic hallucinations via controlled entity swaps (ReDRESS) for contrastive learning is a key innovation, allowing training without human-annotated revisions \\cite{adams202289x}.\n\n4.  **Key Technical Contributions**\n    *   **Novel Method**: Introduction of \"reference revision\" as a model-agnostic data pre-processing step to improve reference quality for faithful summarization \\cite{adams202289x}.\n    *   **Algorithm for Synthetic Data**: ReDRESS (Reference Distractor Entity Set Swapping), a BART-based denoising autoencoder that generates diverse and realistic synthetic hallucinations by combining span deletion with controlled entity swaps, enabling training without direct supervision for revision \\cite{adams202289x}.\n    *   **Learning Objective**: Application of contrastive learning to train the revision model using the synthetically generated positive and negative revision examples \\cite{adams202289x}.\n    *   **System Design**: A two-stage pipeline involving robust source-reference alignment and classification, followed by a revision model trained on synthetic data \\cite{adams202289x}.\n    *   **Versatility**: Demonstrates that reference revision can also function as a post-hoc editor and a pre-training objective for faithfulness, extending its utility beyond data pre-processing \\cite{adams202289x}.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**: The methods were tested on the task of hospital-course summarization using the publicly available MIMIC-III dataset \\cite{adams202289x}. Summarization models were trained on three different data preparations: original noisy references, filtered references (using existing methods), and references revised by the proposed method \\cite{adams202289x}.\n    *   **Key Performance Metrics**: Performance was evaluated using both automatic metrics and human evaluation for faithfulness, informativeness, and fluency \\cite{adams202289x}.\n    *   **Comparison Results**: Models trained on the *revised* clinical references significantly outperformed models trained on original or filtered data across all evaluated metrics (faithfulness, informativeness, and fluency), as confirmed by both automatic scores and human judgment \\cite{adams202289x}. This empirically validates the effectiveness of the reference revision approach.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The method relies on the accuracy of entity extraction (e.g., Amazon Comprehend Medical) and similarity scoring for determining entity support \\cite{adams202289x}. The definition of \"supported\" sentences uses heuristic thresholds (e.g., BERTScore precision >= 0.75) \\cite{adams202289x}. The realism of synthetically generated hallucinations by ReDRESS is crucial for effective training.\n    *   **Scope of Applicability**: The approach is primarily validated in the context of clinical summarization (MIMIC-III), a domain characterized by high noise and limited resources \\cite{adams202289x}. However, as a data pre-processing step, it is designed to be model-agnostic and potentially applicable to other domains with noisy reference data \\cite{adams202289x}.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: This work significantly advances the state-of-the-art in faithful summarization by providing a novel, data-centric solution to the problem of noisy training references \\cite{adams202289x}. Instead of discarding data, it actively improves its quality, which is particularly critical for low-resource and noisy domains where filtering is detrimental \\cite{adams202289x}.\n    *   **Potential Impact on Future Research**: The proposed reference revision framework, especially the ReDRESS algorithm for synthetic data generation and the use of contrastive learning, opens new avenues for research in data augmentation and quality improvement for various NLP tasks. It encourages a shift towards \"data-centric AI\" in summarization. The release of code, pre-processed datasets, and models will facilitate further research in clinical summarization and the broader challenge of faithfulness in abstractive models \\cite{adams202289x}.",
        "keywords": [
          "Faithful summarization",
          "hallucinations",
          "noisy training data",
          "reference revision",
          "data pre-processing",
          "ReDRESS (Reference Distractor Entity Set Swapping)",
          "synthetic data generation",
          "controlled entity swaps",
          "contrastive learning",
          "clinical summarization",
          "MIMIC-III dataset",
          "improved faithfulness",
          "informativeness and fluency",
          "data-centric AI"
        ],
        "paper_type": "the paper proposes a \"new approach\" to selectively rewrite unsupported reference sentences, detailing the methodology involving synthetic dataset generation, contrastive learning, and controllable attributes for revisions. it then describes testing these \"methods\" and presenting findings from metrics and human evaluation.\n\n**keywords and phrases from the abstract/introduction:**\n\n*   \"we propose a new approach\"\n*   \"automatically generate a synthetic dataset\"\n*   \"learn to revise reference sentences with contrastive learning\"\n*   \"to test our methods\"\n*   \"according to metrics and human evaluation, models trained on revised clinical references are much more faithful...\"\n*   \"the tendency of abstractive systems to produce unfaithful summaries is well-studied... yet less attention is paid to the role of the data on which the models are trained. this is problematic...\" (identifies a problem that the proposed method aims to solve)\n\n**matching to classification criteria:**\n\n*   **technical**: this criterion states \"presents new methods, algorithms, or systems.\" the abstract explicitly states \"we propose a new approach\" and describes the specific techniques (synthetic dataset, contrastive learning). the introduction sets up the technical problem (noisy data leading to unfaithful summaries) that the proposed solution addresses. this is a strong match.\n*   **empirical**: this criterion states \"data-driven studies with statistical analysis.\" the abstract mentions \"to test our methods, we extract noisy references from publicly available mimic-iii discharge summaries... and vary the data on which models are trained. according to metrics and human evaluation, models trained on revised clinical references are much more faithful...\". this clearly describes an empirical evaluation of the proposed method.\n\nwhile the paper contains significant empirical work to validate its claims, its core contribution is the *development and proposal of a novel method/approach*. the empirical evaluation serves to demonstrate the effectiveness of this technical contribution. therefore, the primary classification leans towards **technical**.\n\nthe final classification is **technical**."
      },
      "file_name": "933d1d4f18e721160ddbf8dab25c33f8e3d2cec7.pdf"
    },
    {
      "success": true,
      "doc_id": "c3cc97ef516f92050c2a627ecb170471",
      "summary": "Here's a focused summary of the paper for a literature review:\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem:** Large Language Models (LLMs) frequently generate \"hallucinations\"â€”coherent but factually inaccurate responsesâ€”leading to user distrust in LLM-based information retrieval systems \\cite{su2024gnz}.\n    *   **Importance & Challenge:** While Retrieval-Augmented Generation (RAG) is a popular mitigation strategy, existing dynamic RAG approaches lack explicit mechanisms to verify the *optimal timing* for retrieval. They often trigger retrieval indiscriminately (e.g., based on token count or general low probability), leading to inefficiencies (increased inference time, computational cost) and potential ineffectiveness (introduction of irrelevant or noisy data) when hallucinations are not actually present \\cite{su2024gnz}. The challenge is to precisely identify *when* and *where* hallucinations occur to trigger targeted and efficient retrieval.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches:** This work builds upon and improves dynamic RAG methods by integrating real-time hallucination detection. It also relates to existing hallucination detection techniques \\cite{su2024gnz}.\n    *   **Limitations of Previous Solutions:**\n        *   **Hallucination Detection:** Previous methods often require generating multiple outputs (e.g., SelfCheckGPT) or introducing external models (e.g., MIND, fine-tuned BERT/RoBERTa) for detection, which can be computationally expensive or require additional training \\cite{su2024gnz}.\n        *   **Retrieval-Augmented Generation (RAG):**\n            *   **Single-round RAG:** Insufficient for complex tasks requiring continuously changing information (e.g., long-form generation, multi-hop QA) \\cite{su2024gnz}.\n            *   **Multi-round RAG (Dynamic RAG):** Existing methods (e.g., RETRO, IC-RALM, FLARE, DRAGIN) trigger retrieval based on predefined intervals, general token probabilities, or uncertainty, but *without explicitly verifying if retrieval is triggered at an optimal timing coinciding with hallucination occurrence* \\cite{su2024gnz}. This leads to suboptimal efficiency and effectiveness.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method:** The paper proposes **Dynamic Retrieval Augmentation based on hallucination Detection (DRAD)**, a framework that synchronizes retrieval augmentation with real-time hallucination detection during the LLM's text generation process \\cite{su2024gnz}. DRAD operates on a \"Detect-Retrieve-Revise\" paradigm \\cite{su2024gnz}.\n    *   **Novelty/Difference:**\n        *   **Real-time, Model-agnostic Hallucination Detection:** DRAD introduces **Real-time Hallucination Detection (RHD)**, which identifies potential hallucinations by analyzing the uncertainty of *output entities* (specifically, those with low predictive probability and high entropy) *without relying on external models or generating multiple responses* \\cite{su2024gnz}. This is a key differentiator from prior detection methods.\n        *   **Conditional Retrieval:** Retrieval is triggered *only when RHD detects a potential hallucination*, making the RAG process more efficient and targeted compared to indiscriminate or interval-based retrieval in existing dynamic RAG methods \\cite{su2024gnz}.\n        *   **Context-aware Self-correction:** The **Self-correction based on External Knowledge (SEK)** module formulates search queries from the context *surrounding* the detected hallucinated entity, retrieves relevant external knowledge, and then truncates the LLM's output at the hallucination point to regenerate the content using the retrieved information \\cite{su2024gnz}.\n\n4.  **Key Technical Contributions**\n    *   **Novel Framework:** Introduction of DRAD, a novel retrieval-augmented framework that integrates real-time hallucination detection to conditionally trigger RAG \\cite{su2024gnz}.\n    *   **Novel Detection Method:** Proposal of RHD, a real-time, model-agnostic hallucination detection method based on entity-level probability and entropy, achieving state-of-the-art performance \\cite{su2024gnz}.\n    *   **Adaptive Self-Correction Mechanism:** Development of SEK, which formulates context-specific queries for external knowledge retrieval and enables targeted regeneration of hallucinated content \\cite{su2024gnz}.\n    *   **Theoretical Insight:** The connection established between LLM output uncertainty (low probability, high entropy) at the entity level and the occurrence of hallucinations, forming the basis for RHD \\cite{su2024gnz}.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted:** Experiments were conducted on multiple complex QA benchmark datasets \\cite{su2024gnz}.\n    *   **Key Performance Metrics & Results:**\n        *   RHD achieved state-of-the-art (SOTA) performance in hallucination detection \\cite{su2024gnz}.\n        *   DRAD significantly outperformed existing single-round and multi-round retrieval augmentation methods in mitigating hallucinations \\cite{su2024gnz}.\n        *   The framework demonstrated a reduction in hallucinations across three diverse text generation benchmarks \\cite{su2024gnz}.\n        *   A detailed experiment was conducted to explore the impact of the hallucination detection thresholds (`theta_1`, `theta_2`) on efficiency and effectiveness \\cite{su2024gnz}.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions:** The effectiveness of RHD relies on the assumption that LLM uncertainty (low probability, high entropy) at the entity level reliably correlates with hallucination. The choice of aggregation function `f` for entity probability and entropy, and the specific thresholds (`theta_1`, `theta_2`), are hyperparameters that need tuning \\cite{su2024gnz}. The quality of external knowledge retrieval also impacts the self-correction module \\cite{su2024gnz}.\n    *   **Scope of Applicability:** DRAD is designed for mitigating *entity-level* hallucinations in LLM text generation, particularly in tasks like long-form generation and multi-hop question-answering where continuous information needs arise \\cite{su2024gnz}.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art:** DRAD significantly advances the state-of-the-art in hallucination mitigation by introducing a more intelligent and efficient dynamic RAG strategy. It moves beyond indiscriminate retrieval by precisely identifying and targeting hallucinated content \\cite{su2024gnz}.\n    *   **Potential Impact on Future Research:** This work opens avenues for future research in:\n        *   Developing more sophisticated, real-time, and model-agnostic hallucination detection methods beyond entity-level analysis \\cite{su2024gnz}.\n        *   Optimizing the interplay between detection confidence and retrieval triggering frequency \\cite{su2024gnz}.\n        *   Exploring adaptive query formulation and self-correction strategies based on the *type* of hallucination detected \\cite{su2024gnz}.\n        *   Improving the trustworthiness and factual accuracy of LLMs for critical applications by providing a robust framework for error correction \\cite{su2024gnz}.",
      "intriguing_abstract": "Large Language Models (LLMs) frequently generate convincing but factually incorrect \"hallucinations,\" severely undermining user trust in information retrieval systems. While Retrieval-Augmented Generation (RAG) offers mitigation, current dynamic RAG approaches suffer from inefficient, indiscriminate retrieval, often triggering augmentation without verifying the actual presence or optimal timing of a hallucination. We introduce **Dynamic Retrieval Augmentation based on hallucination Detection (DRAD)**, a novel framework that precisely synchronizes retrieval with real-time hallucination detection. DRAD features **Real-time Hallucination Detection (RHD)**, a model-agnostic method that identifies potential hallucinations by analyzing *entity-level uncertainty* (low predictive probability and high entropy) without external models or multiple generations. This enables *conditional retrieval*, where external knowledge is fetched only when a hallucination is detected, followed by **Self-correction based on External Knowledge (SEK)** for targeted regeneration. DRAD significantly outperforms existing RAG methods in mitigating hallucinations across diverse benchmarks, with RHD achieving state-of-the-art detection. This work dramatically enhances the factual accuracy and trustworthiness of LLMs, paving the way for more reliable AI assistants.",
      "keywords": [
        "Large Language Models (LLMs)",
        "Hallucinations",
        "Retrieval-Augmented Generation (RAG)",
        "Dynamic RAG",
        "DRAD (Dynamic Retrieval Augmentation based on hallucination Detection)",
        "Real-time Hallucination Detection (RHD)",
        "Model-agnostic hallucination detection",
        "Output entity uncertainty",
        "Conditional retrieval",
        "Context-aware Self-correction (SEK)",
        "Mitigating hallucinations",
        "Factual accuracy",
        "State-of-the-art performance"
      ],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/45ffc7928a358ff6567d8420b58d509fc3b7dbd1.pdf",
      "citation_key": "su2024gnz",
      "metadata": {
        "title": "Mitigating Entity-Level Hallucination in Large Language Models",
        "authors": [
          "Weihang Su",
          "Yichen Tang",
          "Qingyao Ai",
          "Changyue Wang",
          "Zhijing Wu",
          "Yiqun Liu"
        ],
        "published_date": "2024",
        "abstract": "The emergence of Large Language Models (LLMs) has revolutionized how users access information, shifting from traditional search engines to direct question-and-answer interactions with LLMs. However, the widespread adoption of LLMs has revealed a significant challenge known as hallucination, wherein LLMs generate coherent yet factually inaccurate responses. This hallucination phenomenon has led to users' distrust in information retrieval systems based on LLMs. To tackle this challenge, this paper proposes Dynamic Retrieval Augmentation based on hallucination Detection (DRAD) as a novel method to detect and mitigate hallucinations in LLMs. DRAD improves upon traditional retrieval augmentation by dynamically adapting the retrieval process based on real-time hallucination detection. It features two main components: Real-time Hallucination Detection (RHD) for identifying potential hallucinations without external models, and Self-correction based on External Knowledge (SEK) for correcting these errors using external knowledge. Experiment results show that DRAD demonstrates superior performance in both detecting and mitigating hallucinations in LLMs. All of our code and data are open-sourced at https://github.com/oneal2000/EntityHallucination.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/45ffc7928a358ff6567d8420b58d509fc3b7dbd1.pdf",
        "venue": "SIGIR-AP",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the paper for a literature review:\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem:** Large Language Models (LLMs) frequently generate \"hallucinations\"â€”coherent but factually inaccurate responsesâ€”leading to user distrust in LLM-based information retrieval systems \\cite{su2024gnz}.\n    *   **Importance & Challenge:** While Retrieval-Augmented Generation (RAG) is a popular mitigation strategy, existing dynamic RAG approaches lack explicit mechanisms to verify the *optimal timing* for retrieval. They often trigger retrieval indiscriminately (e.g., based on token count or general low probability), leading to inefficiencies (increased inference time, computational cost) and potential ineffectiveness (introduction of irrelevant or noisy data) when hallucinations are not actually present \\cite{su2024gnz}. The challenge is to precisely identify *when* and *where* hallucinations occur to trigger targeted and efficient retrieval.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches:** This work builds upon and improves dynamic RAG methods by integrating real-time hallucination detection. It also relates to existing hallucination detection techniques \\cite{su2024gnz}.\n    *   **Limitations of Previous Solutions:**\n        *   **Hallucination Detection:** Previous methods often require generating multiple outputs (e.g., SelfCheckGPT) or introducing external models (e.g., MIND, fine-tuned BERT/RoBERTa) for detection, which can be computationally expensive or require additional training \\cite{su2024gnz}.\n        *   **Retrieval-Augmented Generation (RAG):**\n            *   **Single-round RAG:** Insufficient for complex tasks requiring continuously changing information (e.g., long-form generation, multi-hop QA) \\cite{su2024gnz}.\n            *   **Multi-round RAG (Dynamic RAG):** Existing methods (e.g., RETRO, IC-RALM, FLARE, DRAGIN) trigger retrieval based on predefined intervals, general token probabilities, or uncertainty, but *without explicitly verifying if retrieval is triggered at an optimal timing coinciding with hallucination occurrence* \\cite{su2024gnz}. This leads to suboptimal efficiency and effectiveness.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method:** The paper proposes **Dynamic Retrieval Augmentation based on hallucination Detection (DRAD)**, a framework that synchronizes retrieval augmentation with real-time hallucination detection during the LLM's text generation process \\cite{su2024gnz}. DRAD operates on a \"Detect-Retrieve-Revise\" paradigm \\cite{su2024gnz}.\n    *   **Novelty/Difference:**\n        *   **Real-time, Model-agnostic Hallucination Detection:** DRAD introduces **Real-time Hallucination Detection (RHD)**, which identifies potential hallucinations by analyzing the uncertainty of *output entities* (specifically, those with low predictive probability and high entropy) *without relying on external models or generating multiple responses* \\cite{su2024gnz}. This is a key differentiator from prior detection methods.\n        *   **Conditional Retrieval:** Retrieval is triggered *only when RHD detects a potential hallucination*, making the RAG process more efficient and targeted compared to indiscriminate or interval-based retrieval in existing dynamic RAG methods \\cite{su2024gnz}.\n        *   **Context-aware Self-correction:** The **Self-correction based on External Knowledge (SEK)** module formulates search queries from the context *surrounding* the detected hallucinated entity, retrieves relevant external knowledge, and then truncates the LLM's output at the hallucination point to regenerate the content using the retrieved information \\cite{su2024gnz}.\n\n4.  **Key Technical Contributions**\n    *   **Novel Framework:** Introduction of DRAD, a novel retrieval-augmented framework that integrates real-time hallucination detection to conditionally trigger RAG \\cite{su2024gnz}.\n    *   **Novel Detection Method:** Proposal of RHD, a real-time, model-agnostic hallucination detection method based on entity-level probability and entropy, achieving state-of-the-art performance \\cite{su2024gnz}.\n    *   **Adaptive Self-Correction Mechanism:** Development of SEK, which formulates context-specific queries for external knowledge retrieval and enables targeted regeneration of hallucinated content \\cite{su2024gnz}.\n    *   **Theoretical Insight:** The connection established between LLM output uncertainty (low probability, high entropy) at the entity level and the occurrence of hallucinations, forming the basis for RHD \\cite{su2024gnz}.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted:** Experiments were conducted on multiple complex QA benchmark datasets \\cite{su2024gnz}.\n    *   **Key Performance Metrics & Results:**\n        *   RHD achieved state-of-the-art (SOTA) performance in hallucination detection \\cite{su2024gnz}.\n        *   DRAD significantly outperformed existing single-round and multi-round retrieval augmentation methods in mitigating hallucinations \\cite{su2024gnz}.\n        *   The framework demonstrated a reduction in hallucinations across three diverse text generation benchmarks \\cite{su2024gnz}.\n        *   A detailed experiment was conducted to explore the impact of the hallucination detection thresholds (`theta_1`, `theta_2`) on efficiency and effectiveness \\cite{su2024gnz}.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions:** The effectiveness of RHD relies on the assumption that LLM uncertainty (low probability, high entropy) at the entity level reliably correlates with hallucination. The choice of aggregation function `f` for entity probability and entropy, and the specific thresholds (`theta_1`, `theta_2`), are hyperparameters that need tuning \\cite{su2024gnz}. The quality of external knowledge retrieval also impacts the self-correction module \\cite{su2024gnz}.\n    *   **Scope of Applicability:** DRAD is designed for mitigating *entity-level* hallucinations in LLM text generation, particularly in tasks like long-form generation and multi-hop question-answering where continuous information needs arise \\cite{su2024gnz}.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art:** DRAD significantly advances the state-of-the-art in hallucination mitigation by introducing a more intelligent and efficient dynamic RAG strategy. It moves beyond indiscriminate retrieval by precisely identifying and targeting hallucinated content \\cite{su2024gnz}.\n    *   **Potential Impact on Future Research:** This work opens avenues for future research in:\n        *   Developing more sophisticated, real-time, and model-agnostic hallucination detection methods beyond entity-level analysis \\cite{su2024gnz}.\n        *   Optimizing the interplay between detection confidence and retrieval triggering frequency \\cite{su2024gnz}.\n        *   Exploring adaptive query formulation and self-correction strategies based on the *type* of hallucination detected \\cite{su2024gnz}.\n        *   Improving the trustworthiness and factual accuracy of LLMs for critical applications by providing a robust framework for error correction \\cite{su2024gnz}.",
        "keywords": [
          "Large Language Models (LLMs)",
          "Hallucinations",
          "Retrieval-Augmented Generation (RAG)",
          "Dynamic RAG",
          "DRAD (Dynamic Retrieval Augmentation based on hallucination Detection)",
          "Real-time Hallucination Detection (RHD)",
          "Model-agnostic hallucination detection",
          "Output entity uncertainty",
          "Conditional retrieval",
          "Context-aware Self-correction (SEK)",
          "Mitigating hallucinations",
          "Factual accuracy",
          "State-of-the-art performance"
        ],
        "paper_type": "based on the abstract and introduction:\n\n1.  **\"this paper proposes dynamic retrieval augmentation based on hallucination detection (drad) as a novel method to detect and mitigate hallucinations in llms.\"** this explicitly states the proposal of a \"novel method.\"\n2.  **\"drad improves upon traditional retrieval augmentation by dynamically adapting the retrieval process...\"** this describes the technical innovation.\n3.  **\"it features two main components: real-time hallucination detection (rhd)... and self-correction based on external knowledge (sek)...\"** this details the architecture and components of the proposed system/method.\n4.  **\"experiment results show that drad demonstrates superior performance...\"** while empirical results are mentioned, they serve to validate the *proposed method*, making the core contribution technical.\n\nthe paper clearly presents a new method/system to solve a technical problem (llm hallucination).\n\ntherefore, the most appropriate classification is **technical**."
      },
      "file_name": "45ffc7928a358ff6567d8420b58d509fc3b7dbd1.pdf"
    },
    {
      "success": true,
      "doc_id": "4db8754c54200a5d8788d639f0571ae0",
      "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem:** Accurately quantifying the hallucination level of Large Language Models (LLMs) and attributing the underlying reasons for these hallucinations.\n    *   **Importance and Challenge:** Hallucination severely threatens the reliability and trustworthiness of LLMs, limiting their application in critical domains. Existing methods for quantifying hallucination (e.g., hallucination rates) are easily distorted by confounding factors (like dataset distribution) and fail to explain *why* hallucinations occur, as similar phenomena can stem from different sources.\n\n*   **Related Work & Positioning**\n    *   **Existing Approaches:**\n        *   Previous research on task-specific generative models categorized hallucination by phenomena (e.g., intrinsic vs. extrinsic) and tasks (e.g., summarization, QA).\n        *   Pioneer research on LLMs focused on specific types of hallucination in confined scenarios (e.g., using benchmarks like TruthfulQA or examining commonsense recall).\n    *   **Limitations of Previous Solutions:**\n        *   Hallucination rates are easily influenced by confounders (e.g., data distribution), leading to biased quantification.\n        *   Previous work did not quantify the specific contributions of different sources to hallucination.\n        *   The multitasking nature of LLMs makes it laborious to investigate hallucination across every possible task.\n        *   Traditional definitions of hallucination (untruthful, illogical) are insufficient for LLMs that might be instructed to generate counterfactual or fictional content.\n    *   **Positioning:** This work addresses these limitations by proposing a unified framework that simultaneously quantifies hallucination in an unbiased manner and attributes it to specific model capability deficiencies, moving beyond phenomenon-based categorization \\cite{du2023qu7}.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method:** Proposes an association analysis framework to combine hallucination level quantification with attribution. This involves modeling the relationship between the probability of hallucinating and a set of potential risk factors using a logistic regression function: $Y_{k,M} = 1 / (1 + \\exp(\\beta_{0,M} + \\sum_i \\beta_{i,M} r_{i,k} + \\sum_j \\gamma_{i,M} c_{i,k}))$, where $r_{i,k}$ are risk factors and $c_{j,k}$ are confounders \\cite{du2023qu7}.\n    *   **Novelty/Difference:**\n        *   **Unbiased Quantification:** The association analysis allows for unbiased quantification of hallucination by controlling for the effects of potential confounders and other risk factors.\n        *   **Attribution:** The regression coefficients ($\\beta_{i,M}$) quantify the contribution and statistical significance of each risk factor, directly revealing the reasons for hallucination.\n        *   **Capability-Based Risk Factors:** Identifies risk factors based on a novel taxonomy of model capability deficiencies (inspired by cognitive psychology), rather than just surface-level phenomena. This includes deficiencies in commonsense memorization, relational reasoning, and instruction following \\cite{du2023qu7}.\n        *   **Refined Hallucination Definition:** Defines hallucination for LLMs as \"generations of the model that violate human instructions,\" which is more appropriate for their diverse capabilities than definitions based on untruthfulness or illogicality \\cite{du2023qu7}.\n\n*   **Key Technical Contributions**\n    *   **Novel Algorithms/Methods:**\n        *   An association analysis framework using logistic regression for simultaneous, unbiased quantification and attribution of LLM hallucination \\cite{du2023qu7}.\n        *   A new, instruction-following-centric definition of hallucination tailored for LLMs \\cite{du2023qu7}.\n    *   **Theoretical Insights/Analysis:**\n        *   Introduces a novel hallucination taxonomy based on deficiencies in fundamental model capabilities (commonsense memorization, relational reasoning, instruction following), drawing parallels to cognitive psychology concepts like Crystallized and Fluid Intelligence \\cite{du2023qu7}.\n        *   Systematically identifies specific risk factors associated with each capability deficiency, such as term frequency/complexity for commonsense, number of arguments/statements for relational reasoning, and conflicts between pretraining and fine-tuning objectives for instruction following \\cite{du2023qu7}.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted:**\n        *   **Commonsense QA Task:** A de novo dataset was constructed from Wikitext titles, focusing on low-frequency terms, to probe commonsense memorization. LLMs were prompted to describe/explain terms, and human annotators assessed correctness \\cite{du2023qu7}.\n        *   **Relational Reasoning Task:** Experiments were conducted on the Natural Language Satisfiability (NLSat) task, which involves deductive reasoning from explicitly stated rules without external world knowledge. Instances were constructed by controlling the number of arguments, predicates, and statements \\cite{du2023qu7}.\n        *   **Instruction Following Task:** Investigated the impact of conflicts between the pretraining objective (generating subsequent sequences) and supervised fine-tuning objectives (e.g., generating counterfactuals) on hallucination \\cite{du2023qu7}.\n    *   **Key Performance Metrics and Comparison Results:**\n        *   **Commonsense QA:** LLMs exhibited higher hallucination rates when dealing with entities or relational knowledge that appeared with lower frequency in the corpus and had greater complexity \\cite{du2023qu7}.\n        *   **Relational Reasoning:** The hallucination rate was found to be significantly and positively related to the number of arguments and statements in the reasoning task \\cite{du2023qu7}.\n        *   **Instruction Following:** Conflicts between human instructions and the inherent language modeling objective from pretraining were significantly related to the hallucination rate \\cite{du2023qu7}.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations/Assumptions:** The study primarily focuses on three specific model capabilities (commonsense, relational reasoning, instruction following), assuming strong linguistic knowledge. The use of a linear logistic regression model might simplify complex underlying relationships \\cite{du2023qu7}.\n    *   **Scope of Applicability:** The findings provide a framework for understanding and mitigating hallucination in LLMs by identifying specific deficiencies. This approach can guide improvements in pretraining and supervised fine-tuning processes across various NLP tasks \\cite{du2023qu7}.\n\n*   **Technical Significance**\n    *   **Advances State-of-the-Art:** This work significantly advances the understanding of LLM hallucination by providing a principled, quantitative, and attributable framework, moving beyond descriptive hallucination rates to explain *why* models hallucinate \\cite{du2023qu7}.\n    *   **Potential Impact on Future Research:**\n        *   Offers concrete, actionable guidance for mitigating hallucination by pinpointing specific model capability deficiencies that can be targeted during pretraining and fine-tuning \\cite{du2023qu7}.\n        *   Enables more robust and unbiased comparisons of hallucination levels across different LLMs by controlling for confounding factors \\cite{du2023qu7}.\n        *   The capability-based hallucination taxonomy provides a new lens for evaluating and improving LLMs, fostering research into the fundamental cognitive abilities of these models \\cite{du2023qu7}.",
      "intriguing_abstract": "The pervasive issue of hallucination in Large Language Models (LLMs) severely undermines their reliability, yet current quantification methods are often biased and fail to explain *why* these errors occur. We introduce a novel **association analysis framework** that fundamentally shifts how LLM hallucination is understood and addressed. Our method employs **logistic regression** to simultaneously provide unbiased **quantification** of hallucination levels while precisely **attributing** them to specific underlying model **capability deficiencies**.\n\nMoving beyond surface-level phenomena, we propose a new taxonomy of **risk factors** rooted in cognitive psychology, identifying deficiencies in **commonsense memorization, relational reasoning, and instruction following**. This framework, validated across diverse tasks, reveals that factors like term frequency, reasoning complexity, and **pretraining-fine-tuning** objective conflicts are significant drivers of hallucination. By offering an instruction-following-centric definition and a principled way to pinpoint the *causes* of untruthful generations, our work provides actionable guidance for mitigating hallucination, paving the way for more trustworthy and robust LLMs. This advances the state-of-the-art by enabling targeted improvements in model design and training, fostering a new era of reliable AI.",
      "keywords": [
        "LLM hallucination quantification",
        "hallucination attribution",
        "association analysis framework",
        "logistic regression model",
        "unbiased hallucination quantification",
        "capability-based hallucination taxonomy",
        "model capability deficiencies",
        "instruction-following hallucination definition",
        "commonsense memorization",
        "relational reasoning",
        "pretraining/fine-tuning conflicts",
        "hallucination risk factors",
        "LLM reliability",
        "hallucination mitigation"
      ],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/5e060f23914aff74d8c7b6973df44e5af8d97db5.pdf",
      "citation_key": "du2023qu7",
      "metadata": {
        "title": "Quantifying and Attributing the Hallucination of Large Language Models via Association Analysis",
        "authors": [
          "LI DU",
          "Yequan Wang",
          "Xingrun Xing",
          "Yiqun Ya",
          "Xiang Li",
          "Xin Jiang",
          "Xuezhi Fang"
        ],
        "published_date": "2023",
        "abstract": "Although demonstrating superb performance on various NLP tasks, large language models (LLMs) still suffer from the hallucination problem, which threatens the reliability of LLMs. To measure the level of hallucination of LLMs, previous works first categorize the hallucination according to the phenomenon similarity, then quantify the proportion that model outputs contain hallucinatory contents. However, such hallucination rates could easily be distorted by confounders. Moreover, such hallucination rates could not reflect the reasons for the hallucination, as similar hallucinatory phenomena may originate from different sources. To address these issues, we propose to combine the hallucination level quantification and hallucination reason investigation through an association analysis, which builds the relationship between the hallucination rate of LLMs with a set of risk factors. In this way, we are able to observe the hallucination level under each value of each risk factor, examining the contribution and statistical significance of each risk factor, meanwhile excluding the confounding effect of other factors. Additionally, by recognizing the risk factors according to a taxonomy of model capability, we reveal a set of potential deficiencies in commonsense memorization, relational reasoning, and instruction following, which may further provide guidance for the pretraining and supervised fine-tuning process of LLMs to mitigate the hallucination.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/5e060f23914aff74d8c7b6973df44e5af8d97db5.pdf",
        "venue": "arXiv.org",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem:** Accurately quantifying the hallucination level of Large Language Models (LLMs) and attributing the underlying reasons for these hallucinations.\n    *   **Importance and Challenge:** Hallucination severely threatens the reliability and trustworthiness of LLMs, limiting their application in critical domains. Existing methods for quantifying hallucination (e.g., hallucination rates) are easily distorted by confounding factors (like dataset distribution) and fail to explain *why* hallucinations occur, as similar phenomena can stem from different sources.\n\n*   **Related Work & Positioning**\n    *   **Existing Approaches:**\n        *   Previous research on task-specific generative models categorized hallucination by phenomena (e.g., intrinsic vs. extrinsic) and tasks (e.g., summarization, QA).\n        *   Pioneer research on LLMs focused on specific types of hallucination in confined scenarios (e.g., using benchmarks like TruthfulQA or examining commonsense recall).\n    *   **Limitations of Previous Solutions:**\n        *   Hallucination rates are easily influenced by confounders (e.g., data distribution), leading to biased quantification.\n        *   Previous work did not quantify the specific contributions of different sources to hallucination.\n        *   The multitasking nature of LLMs makes it laborious to investigate hallucination across every possible task.\n        *   Traditional definitions of hallucination (untruthful, illogical) are insufficient for LLMs that might be instructed to generate counterfactual or fictional content.\n    *   **Positioning:** This work addresses these limitations by proposing a unified framework that simultaneously quantifies hallucination in an unbiased manner and attributes it to specific model capability deficiencies, moving beyond phenomenon-based categorization \\cite{du2023qu7}.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method:** Proposes an association analysis framework to combine hallucination level quantification with attribution. This involves modeling the relationship between the probability of hallucinating and a set of potential risk factors using a logistic regression function: $Y_{k,M} = 1 / (1 + \\exp(\\beta_{0,M} + \\sum_i \\beta_{i,M} r_{i,k} + \\sum_j \\gamma_{i,M} c_{i,k}))$, where $r_{i,k}$ are risk factors and $c_{j,k}$ are confounders \\cite{du2023qu7}.\n    *   **Novelty/Difference:**\n        *   **Unbiased Quantification:** The association analysis allows for unbiased quantification of hallucination by controlling for the effects of potential confounders and other risk factors.\n        *   **Attribution:** The regression coefficients ($\\beta_{i,M}$) quantify the contribution and statistical significance of each risk factor, directly revealing the reasons for hallucination.\n        *   **Capability-Based Risk Factors:** Identifies risk factors based on a novel taxonomy of model capability deficiencies (inspired by cognitive psychology), rather than just surface-level phenomena. This includes deficiencies in commonsense memorization, relational reasoning, and instruction following \\cite{du2023qu7}.\n        *   **Refined Hallucination Definition:** Defines hallucination for LLMs as \"generations of the model that violate human instructions,\" which is more appropriate for their diverse capabilities than definitions based on untruthfulness or illogicality \\cite{du2023qu7}.\n\n*   **Key Technical Contributions**\n    *   **Novel Algorithms/Methods:**\n        *   An association analysis framework using logistic regression for simultaneous, unbiased quantification and attribution of LLM hallucination \\cite{du2023qu7}.\n        *   A new, instruction-following-centric definition of hallucination tailored for LLMs \\cite{du2023qu7}.\n    *   **Theoretical Insights/Analysis:**\n        *   Introduces a novel hallucination taxonomy based on deficiencies in fundamental model capabilities (commonsense memorization, relational reasoning, instruction following), drawing parallels to cognitive psychology concepts like Crystallized and Fluid Intelligence \\cite{du2023qu7}.\n        *   Systematically identifies specific risk factors associated with each capability deficiency, such as term frequency/complexity for commonsense, number of arguments/statements for relational reasoning, and conflicts between pretraining and fine-tuning objectives for instruction following \\cite{du2023qu7}.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted:**\n        *   **Commonsense QA Task:** A de novo dataset was constructed from Wikitext titles, focusing on low-frequency terms, to probe commonsense memorization. LLMs were prompted to describe/explain terms, and human annotators assessed correctness \\cite{du2023qu7}.\n        *   **Relational Reasoning Task:** Experiments were conducted on the Natural Language Satisfiability (NLSat) task, which involves deductive reasoning from explicitly stated rules without external world knowledge. Instances were constructed by controlling the number of arguments, predicates, and statements \\cite{du2023qu7}.\n        *   **Instruction Following Task:** Investigated the impact of conflicts between the pretraining objective (generating subsequent sequences) and supervised fine-tuning objectives (e.g., generating counterfactuals) on hallucination \\cite{du2023qu7}.\n    *   **Key Performance Metrics and Comparison Results:**\n        *   **Commonsense QA:** LLMs exhibited higher hallucination rates when dealing with entities or relational knowledge that appeared with lower frequency in the corpus and had greater complexity \\cite{du2023qu7}.\n        *   **Relational Reasoning:** The hallucination rate was found to be significantly and positively related to the number of arguments and statements in the reasoning task \\cite{du2023qu7}.\n        *   **Instruction Following:** Conflicts between human instructions and the inherent language modeling objective from pretraining were significantly related to the hallucination rate \\cite{du2023qu7}.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations/Assumptions:** The study primarily focuses on three specific model capabilities (commonsense, relational reasoning, instruction following), assuming strong linguistic knowledge. The use of a linear logistic regression model might simplify complex underlying relationships \\cite{du2023qu7}.\n    *   **Scope of Applicability:** The findings provide a framework for understanding and mitigating hallucination in LLMs by identifying specific deficiencies. This approach can guide improvements in pretraining and supervised fine-tuning processes across various NLP tasks \\cite{du2023qu7}.\n\n*   **Technical Significance**\n    *   **Advances State-of-the-Art:** This work significantly advances the understanding of LLM hallucination by providing a principled, quantitative, and attributable framework, moving beyond descriptive hallucination rates to explain *why* models hallucinate \\cite{du2023qu7}.\n    *   **Potential Impact on Future Research:**\n        *   Offers concrete, actionable guidance for mitigating hallucination by pinpointing specific model capability deficiencies that can be targeted during pretraining and fine-tuning \\cite{du2023qu7}.\n        *   Enables more robust and unbiased comparisons of hallucination levels across different LLMs by controlling for confounding factors \\cite{du2023qu7}.\n        *   The capability-based hallucination taxonomy provides a new lens for evaluating and improving LLMs, fostering research into the fundamental cognitive abilities of these models \\cite{du2023qu7}.",
        "keywords": [
          "LLM hallucination quantification",
          "hallucination attribution",
          "association analysis framework",
          "logistic regression model",
          "unbiased hallucination quantification",
          "capability-based hallucination taxonomy",
          "model capability deficiencies",
          "instruction-following hallucination definition",
          "commonsense memorization",
          "relational reasoning",
          "pretraining/fine-tuning conflicts",
          "hallucination risk factors",
          "LLM reliability",
          "hallucination mitigation"
        ],
        "paper_type": "based on the abstract and introduction, this paper is best classified as **technical**.\n\nhere's why:\n\n*   **abstract:** explicitly states \"we propose to combine the hallucination level quantification and hallucination reason investigation through an association analysis, which builds the relationship between the hallucination rate of llms with a set of risk factors.\" this is a clear proposal of a new method or approach. it then describes what this method enables (\"observe the hallucination level under each value of each risk factor, examining the contribution and statistical significance of each risk factor\").\n*   **introduction:** sets up a technical problem (the hallucination problem in llms and the limitations of previous quantification methods) that the proposed solution aims to address.\n*   **keywords matching \"technical\" criteria:** \"propose\", \"association analysis\", \"builds the relationship\", \"examine the contribution and statistical significance\" all point to presenting a new method or system for analysis."
      },
      "file_name": "5e060f23914aff74d8c7b6973df44e5af8d97db5.pdf"
    },
    {
      "success": true,
      "doc_id": "a5f2f4eb0463b398d6a5a67cd447bd20",
      "summary": "Here's a focused summary of the technical paper for a literature review:\n\n**1. Research Problem & Motivation**\n*   **Specific Technical Problem:** The paper addresses the critical issue of \"hallucination\" in Large Language Models (LLMs), where models generate plausible-sounding but factually incorrect, unfaithful, or nonsensical information \\cite{ji2023vhv}.\n*   **Importance and Challenge:** This problem is particularly acute and challenging in the medical domain due to:\n    *   The severe social and patient care risks associated with inaccurate or misleading medical information.\n    *   The complexity and specialized nature of professional medical concepts, which exacerbate the difficulty of ensuring factual accuracy in generative question-answering (GQA) tasks \\cite{ji2023vhv}.\n    *   A recognized gap in the current understanding of the prevalence and nature of hallucination in medical answers generated by LLMs.\n\n**2. Related Work & Positioning**\n*   **Relation to Existing Approaches:** The work acknowledges the progress in medical QA systems and faithful GQA, which aim to ground answers in source text or valid external knowledge \\cite{ji2023vhv}. It also notes efforts to leverage LLMs' parametric knowledge for knowledge-intensive tasks.\n*   **Limitations of Previous Solutions:**\n    *   Traditional n-gram similarity metrics (e.g., F1, ROUGE-L) commonly used in GQA often fail to effectively discriminate hallucinated or incorrect answers and show weak correlation with human judgments \\cite{ji2023vhv}.\n    *   While approaches like Rationale-Enriched Answer Generator (REAG) and Read-before-Generate aim to improve faithfulness, the fundamental challenge of hallucination persists, especially in complex, high-stakes domains like medicine \\cite{ji2023vhv}.\n    *   Despite their impressive capabilities, LLMs still face significant challenges regarding control, bias, and reliability, with hallucination being an increasingly visible issue \\cite{ji2023vhv}.\n\n**3. Technical Approach & Innovation**\n*   **Core Technical Method:** The paper introduces an **iterative self-reflection methodology** that harnesses the multi-turn interactivity and multi-task abilities inherent in LLMs \\cite{ji2023vhv}. This method operates in two main stages:\n    *   **Knowledge Acquisition Stage:** For a given question, the LLM first generates relevant background knowledge. This generated knowledge is then subjected to a factuality evaluation (scoring). If factual discrepancies are detected, the LLM is prompted to self-correct and refine the knowledge. This cyclical process continues until a satisfactory level of factuality is achieved.\n    *   **Answer Generation Stage:** Based on the refined background knowledge, the LLM generates an answer. This answer is subsequently scored for consistency with the acquired knowledge and for entailment with the original question. If the generated answer does not meet the required standards, it undergoes refinement; if still problematic, the process can revert to the Knowledge Acquisition Stage \\cite{ji2023vhv}.\n*   **Novelty/Difference:** The core innovation lies in its **dynamic, introspective feedback loop** \\cite{ji2023vhv}. Unlike static generation or purely retrieval-augmented methods, this approach enables the LLM to actively evaluate its own generated knowledge and answers, identify shortcomings, and iteratively refine them. This self-correction mechanism allows for a progressive enhancement of factuality, consistency, and entailment directly within the LLM's generative process.\n\n**4. Key Technical Contributions**\n*   **Novel Algorithms/Methods:** A novel **self-reflection method** designed to mitigate hallucination in LLMs, characterized by an iterative feedback loop that systematically generates, scores, and refines both background knowledge and answers until they achieve satisfactory levels of accuracy and reliability \\cite{ji2023vhv}.\n*   **Theoretical Insights/Analysis:** A comprehensive analysis and categorization of hallucination phenomena in medical GQA systems, identifying \"Fact Inconsistency,\" \"Query Inconsistency\" (both considered hallucination), and \"Tangentiality\" as distinct types of problematic answers \\cite{ji2023vhv}. The study also explores potential contributing factors, such as the impact of medical domain fine-tuning and a correlation between low keyword frequency and increased hallucination incidence.\n\n**5. Experimental Validation**\n*   **Experiments Conducted:**\n    *   An initial comprehensive analysis of hallucination occurrence across five diverse LLMs (Vicuna, Alpaca-LoRA, ChatGPT, MedAlpaca, Robin-medical) on five widely used medical GQA datasets (PubMedQA, MedQuAD, MEDIQA2019, LiveMedQA2017, MASH-QA) \\cite{ji2023vhv}.\n    *   Comparative evaluation of the proposed self-reflection method (denoted with `_L`) against the baseline LLMs on the same five datasets.\n*   **Key Performance Metrics:**\n    *   **Traditional GQA Metrics:** Unigram F1 and ROUGE-L.\n    *   **Novel Hallucination-focused Metrics:** Med-NLI (Medical Natural Language Inference) at both sample and sentence levels to assess logical consistency and entailment, and CTRL-Eval (an unsupervised, reference-free metric) specifically for consistency \\cite{ji2023vhv}.\n*   **Comparison Results:**\n    *   The self-reflection method (`_L`) consistently demonstrated **superior performance** in hallucination reduction across all tested LLMs (ranging from 7B to 175B parameters) and all five medical datasets \\cite{ji2023vhv}.\n    *   Significant improvements were observed across MedNLI, CtrlEval, F1, and ROUGE-L scores when the self-reflection loop was applied, indicating enhanced factuality, consistency, and entailment.\n    *   The results highlighted the **effectiveness, generalizability, and scalability** of the approach, showcasing its ability to improve LLM responses in medical GQA tasks without explicit dataset-specific training \\cite{ji2023vhv}.\n    *   Initial analysis also revealed that while medical fine-tuning can be beneficial, its effectiveness varies (e.g., MedAlpaca outperformed Robin-medical), suggesting that instruction learning is more suitable for GQA tasks \\cite{ji2023vhv}.\n\n**6. Limitations & Scope**\n*   **Technical Limitations/Assumptions:**\n    *   The paper explicitly notes the limitations of traditional n-gram metrics for evaluating hallucination, necessitating the development and use of more nuanced metrics like Med-NLI and CTRL-Eval \\cite{ji2023vhv}.\n    *   The analysis of keyword frequency as a potential cause of hallucination uses Google N-grams as a proxy for pre-training data distribution, acknowledging this as an approximation \\cite{ji2023vhv}.\n    *   The method relies on the LLM's inherent capabilities for multi-turn interaction, multi-task processing, and self-reflection, assuming these are sufficiently robust for effective self-correction \\cite{ji2023vhv}.\n*   **Scope of Applicability:** The method is specifically validated and shown to be effective for **medical generative question-answering systems** \\cite{ji2023vhv}. While its generalizability across different LLM sizes and medical datasets is demonstrated, its direct applicability and performance in other domains or for different types of generative tasks (e.g., creative content generation) would require further dedicated investigation.\n\n**7. Technical Significance**\n*   **Advancement of State-of-the-Art:** This work significantly advances the technical state-of-the-art in mitigating LLM hallucination by introducing an **innovative, iterative self-reflection paradigm** \\cite{ji2023vhv}. It moves beyond passive generation or simple retrieval-augmented methods by empowering LLMs to actively evaluate, critique, and refine their own generated knowledge and answers, leading to demonstrably more factual, consistent, and reliable outputs.\n*   **Potential Impact on Future Research:**\n    *   Provides a robust and generalizable framework for enhancing the reliability and trustworthiness of AI-enabled medical services, which is crucial for their safe and effective deployment \\cite{ji2023vhv}.\n    *   Opens new avenues for research into deeper LLM introspection, self-correction, and autonomous reasoning capabilities, potentially leading to more robust and less error-prone AI systems.\n    *   The introduction of specialized evaluation metrics like Med-NLI and the use of CTRL-Eval for consistency offers improved tools for assessing faithfulness, which can guide future research in both hallucination detection and mitigation \\cite{ji2023vhv}.\n    *   The methodology is designed to be extensible, allowing for future integration with other techniques, such as external knowledge bases or more powerful LLMs, to further enhance the development of highly reliable application systems \\cite{ji2023vhv}.",
      "intriguing_abstract": "The pervasive challenge of hallucination in Large Language Models (LLMs) poses severe risks, particularly in high-stakes medical generative question answering (GQA) where factual inaccuracies can compromise patient care. Traditional evaluation metrics and existing mitigation strategies often fall short, failing to adequately address the complex nature of medical misinformation.\n\nThis paper introduces a novel **iterative self-reflection methodology** that empowers LLMs to dynamically evaluate and refine their own generated knowledge and answers. Our approach employs a sophisticated feedback loop: an LLM first acquires and self-corrects background knowledge for factuality, then generates and refines answers for consistency and entailment with the original query.\n\nValidated across five diverse LLMs and five medical GQA datasets, our method significantly reduces hallucination. Utilizing specialized metrics like **Med-NLI** and **CTRL-Eval**, we demonstrate superior performance in enhancing factuality, consistency, and entailment. This robust, generalizable framework not only advances the state-of-the-art in LLM reliability but also offers a critical step towards trustworthy AI in healthcare, opening new frontiers for autonomous reasoning and self-correction in complex domains.",
      "keywords": [
        "LLM Hallucination",
        "Iterative Self-Reflection",
        "Medical Generative Question-Answering (GQA)",
        "Factuality and Consistency",
        "Self-Correction Mechanism",
        "Dynamic Introspective Feedback Loop",
        "Hallucination Reduction",
        "Med-NLI",
        "AI-enabled Medical Services",
        "Knowledge Acquisition and Refinement",
        "Generalizability and Scalability",
        "Problematic Answer Categorization"
      ],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/cd2e04598909158494e556823d9de8baa692cee2.pdf",
      "citation_key": "ji2023vhv",
      "metadata": {
        "title": "Towards Mitigating Hallucination in Large Language Models via Self-Reflection",
        "authors": [
          "Ziwei Ji",
          "Tiezheng Yu",
          "Yan Xu",
          "Nayeon Lee",
          "Etsuko Ishii",
          "Pascale Fung"
        ],
        "published_date": "2023",
        "abstract": "Large language models (LLMs) have shown promise for generative and knowledge-intensive tasks including question-answering (QA) tasks. However, the practical deployment still faces challenges, notably the issue of\"hallucination\", where models generate plausible-sounding but unfaithful or nonsensical information. This issue becomes particularly critical in the medical domain due to the uncommon professional concepts and potential social risks involved. This paper analyses the phenomenon of hallucination in medical generative QA systems using widely adopted LLMs and datasets. Our investigation centers on the identification and comprehension of common problematic answers, with a specific emphasis on hallucination. To tackle this challenge, we present an interactive self-reflection methodology that incorporates knowledge acquisition and answer generation. Through this feedback process, our approach steadily enhances the factuality, consistency, and entailment of the generated answers. Consequently, we harness the interactivity and multitasking ability of LLMs and produce progressively more precise and accurate answers. Experimental results on both automatic and human evaluation demonstrate the superiority of our approach in hallucination reduction compared to baselines.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/cd2e04598909158494e556823d9de8baa692cee2.pdf",
        "venue": "arXiv.org",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the technical paper for a literature review:\n\n**1. Research Problem & Motivation**\n*   **Specific Technical Problem:** The paper addresses the critical issue of \"hallucination\" in Large Language Models (LLMs), where models generate plausible-sounding but factually incorrect, unfaithful, or nonsensical information \\cite{ji2023vhv}.\n*   **Importance and Challenge:** This problem is particularly acute and challenging in the medical domain due to:\n    *   The severe social and patient care risks associated with inaccurate or misleading medical information.\n    *   The complexity and specialized nature of professional medical concepts, which exacerbate the difficulty of ensuring factual accuracy in generative question-answering (GQA) tasks \\cite{ji2023vhv}.\n    *   A recognized gap in the current understanding of the prevalence and nature of hallucination in medical answers generated by LLMs.\n\n**2. Related Work & Positioning**\n*   **Relation to Existing Approaches:** The work acknowledges the progress in medical QA systems and faithful GQA, which aim to ground answers in source text or valid external knowledge \\cite{ji2023vhv}. It also notes efforts to leverage LLMs' parametric knowledge for knowledge-intensive tasks.\n*   **Limitations of Previous Solutions:**\n    *   Traditional n-gram similarity metrics (e.g., F1, ROUGE-L) commonly used in GQA often fail to effectively discriminate hallucinated or incorrect answers and show weak correlation with human judgments \\cite{ji2023vhv}.\n    *   While approaches like Rationale-Enriched Answer Generator (REAG) and Read-before-Generate aim to improve faithfulness, the fundamental challenge of hallucination persists, especially in complex, high-stakes domains like medicine \\cite{ji2023vhv}.\n    *   Despite their impressive capabilities, LLMs still face significant challenges regarding control, bias, and reliability, with hallucination being an increasingly visible issue \\cite{ji2023vhv}.\n\n**3. Technical Approach & Innovation**\n*   **Core Technical Method:** The paper introduces an **iterative self-reflection methodology** that harnesses the multi-turn interactivity and multi-task abilities inherent in LLMs \\cite{ji2023vhv}. This method operates in two main stages:\n    *   **Knowledge Acquisition Stage:** For a given question, the LLM first generates relevant background knowledge. This generated knowledge is then subjected to a factuality evaluation (scoring). If factual discrepancies are detected, the LLM is prompted to self-correct and refine the knowledge. This cyclical process continues until a satisfactory level of factuality is achieved.\n    *   **Answer Generation Stage:** Based on the refined background knowledge, the LLM generates an answer. This answer is subsequently scored for consistency with the acquired knowledge and for entailment with the original question. If the generated answer does not meet the required standards, it undergoes refinement; if still problematic, the process can revert to the Knowledge Acquisition Stage \\cite{ji2023vhv}.\n*   **Novelty/Difference:** The core innovation lies in its **dynamic, introspective feedback loop** \\cite{ji2023vhv}. Unlike static generation or purely retrieval-augmented methods, this approach enables the LLM to actively evaluate its own generated knowledge and answers, identify shortcomings, and iteratively refine them. This self-correction mechanism allows for a progressive enhancement of factuality, consistency, and entailment directly within the LLM's generative process.\n\n**4. Key Technical Contributions**\n*   **Novel Algorithms/Methods:** A novel **self-reflection method** designed to mitigate hallucination in LLMs, characterized by an iterative feedback loop that systematically generates, scores, and refines both background knowledge and answers until they achieve satisfactory levels of accuracy and reliability \\cite{ji2023vhv}.\n*   **Theoretical Insights/Analysis:** A comprehensive analysis and categorization of hallucination phenomena in medical GQA systems, identifying \"Fact Inconsistency,\" \"Query Inconsistency\" (both considered hallucination), and \"Tangentiality\" as distinct types of problematic answers \\cite{ji2023vhv}. The study also explores potential contributing factors, such as the impact of medical domain fine-tuning and a correlation between low keyword frequency and increased hallucination incidence.\n\n**5. Experimental Validation**\n*   **Experiments Conducted:**\n    *   An initial comprehensive analysis of hallucination occurrence across five diverse LLMs (Vicuna, Alpaca-LoRA, ChatGPT, MedAlpaca, Robin-medical) on five widely used medical GQA datasets (PubMedQA, MedQuAD, MEDIQA2019, LiveMedQA2017, MASH-QA) \\cite{ji2023vhv}.\n    *   Comparative evaluation of the proposed self-reflection method (denoted with `_L`) against the baseline LLMs on the same five datasets.\n*   **Key Performance Metrics:**\n    *   **Traditional GQA Metrics:** Unigram F1 and ROUGE-L.\n    *   **Novel Hallucination-focused Metrics:** Med-NLI (Medical Natural Language Inference) at both sample and sentence levels to assess logical consistency and entailment, and CTRL-Eval (an unsupervised, reference-free metric) specifically for consistency \\cite{ji2023vhv}.\n*   **Comparison Results:**\n    *   The self-reflection method (`_L`) consistently demonstrated **superior performance** in hallucination reduction across all tested LLMs (ranging from 7B to 175B parameters) and all five medical datasets \\cite{ji2023vhv}.\n    *   Significant improvements were observed across MedNLI, CtrlEval, F1, and ROUGE-L scores when the self-reflection loop was applied, indicating enhanced factuality, consistency, and entailment.\n    *   The results highlighted the **effectiveness, generalizability, and scalability** of the approach, showcasing its ability to improve LLM responses in medical GQA tasks without explicit dataset-specific training \\cite{ji2023vhv}.\n    *   Initial analysis also revealed that while medical fine-tuning can be beneficial, its effectiveness varies (e.g., MedAlpaca outperformed Robin-medical), suggesting that instruction learning is more suitable for GQA tasks \\cite{ji2023vhv}.\n\n**6. Limitations & Scope**\n*   **Technical Limitations/Assumptions:**\n    *   The paper explicitly notes the limitations of traditional n-gram metrics for evaluating hallucination, necessitating the development and use of more nuanced metrics like Med-NLI and CTRL-Eval \\cite{ji2023vhv}.\n    *   The analysis of keyword frequency as a potential cause of hallucination uses Google N-grams as a proxy for pre-training data distribution, acknowledging this as an approximation \\cite{ji2023vhv}.\n    *   The method relies on the LLM's inherent capabilities for multi-turn interaction, multi-task processing, and self-reflection, assuming these are sufficiently robust for effective self-correction \\cite{ji2023vhv}.\n*   **Scope of Applicability:** The method is specifically validated and shown to be effective for **medical generative question-answering systems** \\cite{ji2023vhv}. While its generalizability across different LLM sizes and medical datasets is demonstrated, its direct applicability and performance in other domains or for different types of generative tasks (e.g., creative content generation) would require further dedicated investigation.\n\n**7. Technical Significance**\n*   **Advancement of State-of-the-Art:** This work significantly advances the technical state-of-the-art in mitigating LLM hallucination by introducing an **innovative, iterative self-reflection paradigm** \\cite{ji2023vhv}. It moves beyond passive generation or simple retrieval-augmented methods by empowering LLMs to actively evaluate, critique, and refine their own generated knowledge and answers, leading to demonstrably more factual, consistent, and reliable outputs.\n*   **Potential Impact on Future Research:**\n    *   Provides a robust and generalizable framework for enhancing the reliability and trustworthiness of AI-enabled medical services, which is crucial for their safe and effective deployment \\cite{ji2023vhv}.\n    *   Opens new avenues for research into deeper LLM introspection, self-correction, and autonomous reasoning capabilities, potentially leading to more robust and less error-prone AI systems.\n    *   The introduction of specialized evaluation metrics like Med-NLI and the use of CTRL-Eval for consistency offers improved tools for assessing faithfulness, which can guide future research in both hallucination detection and mitigation \\cite{ji2023vhv}.\n    *   The methodology is designed to be extensible, allowing for future integration with other techniques, such as external knowledge bases or more powerful LLMs, to further enhance the development of highly reliable application systems \\cite{ji2023vhv}.",
        "keywords": [
          "LLM Hallucination",
          "Iterative Self-Reflection",
          "Medical Generative Question-Answering (GQA)",
          "Factuality and Consistency",
          "Self-Correction Mechanism",
          "Dynamic Introspective Feedback Loop",
          "Hallucination Reduction",
          "Med-NLI",
          "AI-enabled Medical Services",
          "Knowledge Acquisition and Refinement",
          "Generalizability and Scalability",
          "Problematic Answer Categorization"
        ],
        "paper_type": "based on the abstract and introduction:\n\nthe paper clearly states: \"to tackle this challenge, we present an interactive self-reflection methodology that incorporates knowledge acquisition and answer generation.\" this indicates the development and presentation of a new method or system. the abstract also mentions \"our approach\" multiple times.\n\nfurthermore, it includes \"experimental results on both automatic and human evaluation demonstrate the superiority of our approach in hallucination reduction compared to baselines.\" this indicates an empirical evaluation of the proposed method.\n\nwhile it has strong empirical components, the primary contribution described is the **presentation of a new methodology** to address a technical problem (hallucination in llms). the empirical results serve to validate this proposed technical solution.\n\ntherefore, the most fitting classification is **technical**."
      },
      "file_name": "cd2e04598909158494e556823d9de8baa692cee2.pdf"
    },
    {
      "success": true,
      "doc_id": "100959a270f5631e12e0a50d05ae2693",
      "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Large Language Models (LLMs) frequently exhibit undesired and inconsistent behaviors, including hallucination, unfaithful reasoning, generation of toxic content, and failure to adhere to specified rules and constraints \\cite{pan2023mwu}.\n    *   **Importance and Challenge**: These flaws significantly undermine trust in LLMs and pose substantial hurdles to their practical, real-world deployment. While human feedback can address some issues, it is costly, resource-intensive, and lacks real-time capabilities, necessitating automated solutions \\cite{pan2023mwu}.\n\n*   **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: This work positions itself within the broader paradigm of \"learning from feedback\" for LLM improvement, mirroring human learning strategies \\cite{pan2023mwu}.\n    *   **Limitations of Previous Solutions**: It specifically highlights the limitations of human feedback-dependent methods (e.g., RLHF), which are expensive, require significant manual labor, and cannot provide real-time corrections \\cite{pan2023mwu}. The paper focuses on *automated feedback* as a promising alternative to overcome these drawbacks.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper surveys and taxonomizes \"self-correction\" strategies, where the LLM itself is prompted or guided to identify and fix problems in its own output, primarily leveraging *automated feedback* \\cite{pan2023mwu}.\n    *   **Novelty**: The primary innovation of this paper is its comprehensive *survey and taxonomy* of diverse self-correction techniques. It introduces a conceptual framework (Language Model as \"Patient,\" Critic Model as \"Doctor/Diagnosis,\" Refine Model as \"Treatment\") and classifies existing works along five key dimensions:\n        1.  **What gets corrected**: Hallucination, unfaithful reasoning, toxic/biased content, flawed code.\n        2.  **Source of feedback**: Self-feedback (from the LLM itself) or external feedback (from other models, tools, knowledge sources, or metrics).\n        3.  **Format of feedback**: Scalar value (e.g., scores) or natural language.\n        4.  **When feedback is used**: Training-time, generation-time, or post-hoc correction.\n        5.  **How to correct**: Refinement strategy and learning method.\n\n*   **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**: As a survey, the paper's core contribution is not a new algorithm but a structured, comprehensive review and classification system for the rapidly evolving field of automated LLM self-correction \\cite{pan2023mwu}.\n    *   **System Design/Architectural Innovations**: It proposes a clear conceptual framework (Patient-Doctor-Treatment) that helps to understand and categorize the components and interactions within self-correcting LLM systems \\cite{pan2023mwu}.\n    *   **Theoretical Insights/Analysis**: Provides an analytical overview of the trade-offs and characteristics of different self-correction approaches, such as the expressivity versus ease of collection for feedback formats, and the feasibility and implications of applying corrections at different stages (training, generation, post-hoc) \\cite{pan2023mwu}.\n\n*   **Experimental Validation**\n    *   This paper is a survey and does not present its own experimental validation. Instead, it summarizes the empirical successes and applications of the *surveyed self-correction techniques* across various tasks, including question answering, reasoning, code generation, and toxicity detection \\cite{pan2023mwu}. The paper highlights how these methods have demonstrated effectiveness in mitigating issues like hallucination and unfaithful reasoning.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The paper discusses inherent limitations of the *surveyed methods*, such as the infeasibility of fine-tuning giant closed-source LLMs for training-time correction, the potential for scalar feedback to be less informative than natural language feedback, and the challenges of obtaining \"optimizable\" feedback for certain correction strategies \\cite{pan2023mwu}.\n    *   **Scope of Applicability**: The survey specifically focuses on self-correction strategies that utilize *automated feedback*, distinguishing itself from methods heavily reliant on human intervention \\cite{pan2023mwu}.\n\n*   **Technical Significance**\n    *   **Advances State-of-the-Art**: By providing a comprehensive taxonomy and analysis, the paper significantly advances the understanding of the technical landscape of automated self-correction for LLMs, a critical area for improving model reliability and trustworthiness \\cite{pan2023mwu}.\n    *   **Potential Impact on Future Research**: It serves as a valuable resource for researchers, helping to navigate the diverse array of existing techniques, identify gaps, and guide future research directions towards more robust, autonomous, and deployable LLM solutions with minimal human oversight \\cite{pan2023mwu}.",
      "intriguing_abstract": "The pervasive challenge of Large Language Models (LLMs) exhibiting undesirable behaviorsâ€”from debilitating hallucination and unfaithful reasoning to toxic content generationâ€”critically undermines their trustworthiness and impedes real-world deployment. While human feedback offers some recourse, its prohibitive cost and lack of real-time adaptability demand autonomous solutions. This paper addresses this urgent need by presenting the first comprehensive *survey and taxonomy* of **automated self-correction** strategies, where LLMs are empowered to identify and rectify their own deficiencies.\n\nWe introduce a novel conceptual framework, the \"Patient-Doctor-Treatment\" model, to systematically categorize diverse self-correction techniques. Our taxonomy meticulously classifies existing methods across five crucial dimensions: the specific issues corrected (e.g., hallucination, reasoning flaws), the source and format of feedback (self-feedback, external; scalar, natural language), and the timing and mechanism of correction (training-time, generation-time, post-hoc refinement). This analytical overview provides critical insights into the trade-offs and effectiveness of various approaches, serving as an indispensable guide for researchers. By illuminating the landscape of autonomous LLM improvement, this work paves the way for developing more robust, reliable, and trustworthy AI systems, significantly advancing the state-of-the-art in responsible LLM deployment.",
      "keywords": [
        "Large Language Models (LLMs)",
        "self-correction strategies",
        "automated feedback",
        "hallucination",
        "unfaithful reasoning",
        "survey and taxonomy",
        "Patient-Doctor-Treatment framework",
        "LLM undesired behaviors",
        "model reliability",
        "feedback dimensions",
        "training-time correction",
        "generation-time correction",
        "post-hoc correction"
      ],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/ee19d5c943f1ebcd1a9e52a7bf494a88255b8e04.pdf",
      "citation_key": "pan2023mwu",
      "metadata": {
        "title": "Automatically Correcting Large Language Models: Surveying the landscape of diverse self-correction strategies",
        "authors": [
          "Liangming Pan",
          "Michael Stephen Saxon",
          "Wenda Xu",
          "Deepak Nathani",
          "Xinyi Wang",
          "William Yang Wang"
        ],
        "published_date": "2023",
        "abstract": "Large language models (LLMs) have demonstrated remarkable performance across a wide array of NLP tasks. However, their efficacy is undermined by undesired and inconsistent behaviors, including hallucination, unfaithful reasoning, and toxic content. A promising approach to rectify these flaws is self-correction, where the LLM itself is prompted or guided to fix problems in its own output. Techniques leveraging automated feedback -- either produced by the LLM itself or some external system -- are of particular interest as they are a promising way to make LLM-based solutions more practical and deployable with minimal human feedback. This paper presents a comprehensive review of this emerging class of techniques. We analyze and taxonomize a wide array of recent work utilizing these strategies, including training-time, generation-time, and post-hoc correction. We also summarize the major applications of this strategy and conclude by discussing future directions and challenges.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/ee19d5c943f1ebcd1a9e52a7bf494a88255b8e04.pdf",
        "venue": "arXiv.org",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Large Language Models (LLMs) frequently exhibit undesired and inconsistent behaviors, including hallucination, unfaithful reasoning, generation of toxic content, and failure to adhere to specified rules and constraints \\cite{pan2023mwu}.\n    *   **Importance and Challenge**: These flaws significantly undermine trust in LLMs and pose substantial hurdles to their practical, real-world deployment. While human feedback can address some issues, it is costly, resource-intensive, and lacks real-time capabilities, necessitating automated solutions \\cite{pan2023mwu}.\n\n*   **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: This work positions itself within the broader paradigm of \"learning from feedback\" for LLM improvement, mirroring human learning strategies \\cite{pan2023mwu}.\n    *   **Limitations of Previous Solutions**: It specifically highlights the limitations of human feedback-dependent methods (e.g., RLHF), which are expensive, require significant manual labor, and cannot provide real-time corrections \\cite{pan2023mwu}. The paper focuses on *automated feedback* as a promising alternative to overcome these drawbacks.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper surveys and taxonomizes \"self-correction\" strategies, where the LLM itself is prompted or guided to identify and fix problems in its own output, primarily leveraging *automated feedback* \\cite{pan2023mwu}.\n    *   **Novelty**: The primary innovation of this paper is its comprehensive *survey and taxonomy* of diverse self-correction techniques. It introduces a conceptual framework (Language Model as \"Patient,\" Critic Model as \"Doctor/Diagnosis,\" Refine Model as \"Treatment\") and classifies existing works along five key dimensions:\n        1.  **What gets corrected**: Hallucination, unfaithful reasoning, toxic/biased content, flawed code.\n        2.  **Source of feedback**: Self-feedback (from the LLM itself) or external feedback (from other models, tools, knowledge sources, or metrics).\n        3.  **Format of feedback**: Scalar value (e.g., scores) or natural language.\n        4.  **When feedback is used**: Training-time, generation-time, or post-hoc correction.\n        5.  **How to correct**: Refinement strategy and learning method.\n\n*   **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**: As a survey, the paper's core contribution is not a new algorithm but a structured, comprehensive review and classification system for the rapidly evolving field of automated LLM self-correction \\cite{pan2023mwu}.\n    *   **System Design/Architectural Innovations**: It proposes a clear conceptual framework (Patient-Doctor-Treatment) that helps to understand and categorize the components and interactions within self-correcting LLM systems \\cite{pan2023mwu}.\n    *   **Theoretical Insights/Analysis**: Provides an analytical overview of the trade-offs and characteristics of different self-correction approaches, such as the expressivity versus ease of collection for feedback formats, and the feasibility and implications of applying corrections at different stages (training, generation, post-hoc) \\cite{pan2023mwu}.\n\n*   **Experimental Validation**\n    *   This paper is a survey and does not present its own experimental validation. Instead, it summarizes the empirical successes and applications of the *surveyed self-correction techniques* across various tasks, including question answering, reasoning, code generation, and toxicity detection \\cite{pan2023mwu}. The paper highlights how these methods have demonstrated effectiveness in mitigating issues like hallucination and unfaithful reasoning.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The paper discusses inherent limitations of the *surveyed methods*, such as the infeasibility of fine-tuning giant closed-source LLMs for training-time correction, the potential for scalar feedback to be less informative than natural language feedback, and the challenges of obtaining \"optimizable\" feedback for certain correction strategies \\cite{pan2023mwu}.\n    *   **Scope of Applicability**: The survey specifically focuses on self-correction strategies that utilize *automated feedback*, distinguishing itself from methods heavily reliant on human intervention \\cite{pan2023mwu}.\n\n*   **Technical Significance**\n    *   **Advances State-of-the-Art**: By providing a comprehensive taxonomy and analysis, the paper significantly advances the understanding of the technical landscape of automated self-correction for LLMs, a critical area for improving model reliability and trustworthiness \\cite{pan2023mwu}.\n    *   **Potential Impact on Future Research**: It serves as a valuable resource for researchers, helping to navigate the diverse array of existing techniques, identify gaps, and guide future research directions towards more robust, autonomous, and deployable LLM solutions with minimal human oversight \\cite{pan2023mwu}.",
        "keywords": [
          "Large Language Models (LLMs)",
          "self-correction strategies",
          "automated feedback",
          "hallucination",
          "unfaithful reasoning",
          "survey and taxonomy",
          "Patient-Doctor-Treatment framework",
          "LLM undesired behaviors",
          "model reliability",
          "feedback dimensions",
          "training-time correction",
          "generation-time correction",
          "post-hoc correction"
        ],
        "paper_type": "based on the abstract and introduction, this paper is a **survey**.\n\nhere's why:\n\n*   **title:** \"automatically correcting large language models: **surveying** the landscape of diverse self-correction strategies\" - the word \"surveying\" is a direct indicator.\n*   **abstract:**\n    *   \"this paper presents a **comprehensive review** of this emerging class of techniques.\" - directly matches the \"survey\" criteria: \"reviews existing literature comprehensively\".\n    *   \"we **analyze and taxonomize** a wide array of recent work utilizing these strategies...\" - aligns with the \"survey\" criteria: \"introduction discusses: literature organization, classification schemes\".\n    *   \"...summarize the major applications of this strategy and conclude by discussing future directions and challenges.\" - common elements of a survey paper."
      },
      "file_name": "ee19d5c943f1ebcd1a9e52a7bf494a88255b8e04.pdf"
    },
    {
      "success": true,
      "doc_id": "13dc157f92ffd4ba67118f09ca465248",
      "summary": "This paper \\cite{kang20238j0} provides an empirical examination of hallucination behaviors in Large Language Models (LLMs) within financial tasks and evaluates various mitigation methods.\n\n*   **1. Research Problem & Motivation**\n    *   **Specific Technical Problem**: The paper addresses the fundamental deficiency of hallucination in LLMs, which manifests as generating plausible but factually incorrect or unsupported content, particularly when applied to high-stakes domains like finance.\n    *   **Importance & Challenge**: Hallucination in financial applications poses significant risks, including substantial monetary losses and erosion of trust. The problem is challenging due to the difficulty in accurately measuring hallucinations amidst intricate financial concepts and ensuring pinpoint accuracy for real-world tasks such as querying historical stock prices.\n\n*   **2. Related Work & Positioning**\n    *   **Relation to Existing Approaches**: The work relates to prior research on LLM hallucination, which attributes it to factors like imperfect learning, decoding methods, and knowledge gaps. It also connects to proposed mitigation techniques, including factuality-enhanced decoding (e.g., DoLa) and leveraging external tools (e.g., RAG, API calls).\n    *   **Limitations of Previous Solutions**: Despite the emergence of specialized financial LLMs (e.g., FinBERT, BloombergGPT), a comprehensive empirical investigation into their hallucination tendencies in finance has been lacking. Existing general-purpose LLMs often struggle with factual accuracy in dynamic, domain-specific contexts, and some domain-specific fine-tuning approaches may inadvertently degrade general instruction-following abilities.\n\n*   **3. Technical Approach & Innovation**\n    *   **Core Technical Method/Algorithm**: The paper establishes an empirical framework to assess LLM hallucination across three financial tasks:\n        1.  **Financial Abbreviation Recognition**: Identifying full names for financial acronyms and stock symbols.\n        2.  **Financial Term Explanations**: Providing definitions for less common financial terminologies.\n        3.  **Stock Price Query**: Retrieving historical stock prices for specific dates and tickers.\n    *   **Novelty/Difference**: The core innovation is the *systematic empirical examination* of hallucination in the financial domain, quantifying its extent across diverse LLMs (Llama2, GPT models, FinMA) and tasks. It rigorously *evaluates four practical mitigation methods*:\n        *   **Few-shot prompting**: Providing examples to guide model generation.\n        *   **Decoding by Contrasting Layers (DoLa)**: A decoding technique designed to enhance factual accuracy by contrasting outputs from different model layers.\n        *   **Retrieval Augmentation Generation (RAG)**: Integrating external knowledge from Wikipedia via a FAISS vector store to ground generated content.\n        *   **Prompt-based tool learning**: Enabling LLMs to generate precise Python function calls for external APIs (e.g., Alpha Vantage API) to fetch real-time or historical data.\n\n*   **4. Key Technical Contributions**\n    *   **Novel Algorithms, Methods, or Techniques**:\n        *   A novel empirical framework for quantifying LLM hallucination specifically tailored for financial tasks, covering knowledge recall, explanation generation, and time-sensitive data querying.\n        *   A comprehensive comparative analysis of four distinct hallucination mitigation strategies (few-shot, DoLa, RAG, prompt-based tool learning) within the financial domain.\n    *   **Theoretical Insights or Analysis**:\n        *   Empirically demonstrates that off-the-shelf LLMs exhibit severe hallucination in financial tasks, often providing outdated or incorrect information.\n        *   Reveals that multi-task domain-specific fine-tuning (e.g., FinMA-7B) can surprisingly diminish a model's general instruction-following abilities, potentially increasing instruction-inconsistent hallucinations.\n        *   Highlights the critical importance of external knowledge integration (RAG) and tool-use capabilities for achieving factual accuracy and reliability in dynamic and time-sensitive financial data.\n\n*   **5. Experimental Validation**\n    *   **Experiments Conducted**: The study evaluated various LLMs, including Llama1-7B, Llama2-7B/13B (and their chat versions), GPT3.5-turbo, GPT4, and the domain-specific FinMA-7B-NLP. Experiments were conducted across three financial tasks: Abbreviation Recognition (192 acronyms, 1215 stock symbols), Term Explanations (160 terms), and Stock Price Query (560 examples). Mitigation methods were tested under zero-shot and few-shot settings, and with DoLa, RAG, and prompt-based tool learning.\n    *   **Key Performance Metrics & Comparison Results**:\n        *   **Abbreviation Recognition & Term Explanations**: Measured by Accuracy (substring match) and FactScore (ChatGPT+Retrieval), respectively. While GPT4 showed decent zero-shot performance (82.5% / 90.4% accuracy, 81.11% FactScore), RAG consistently and significantly improved Llama2 models (e.g., Llama2-7B-chat FactScore increased from 64.68% to 90.48%).\n        *   **Stock Price Query**: Assessed by Accuracy (exact match), Mean Absolute Error (MAE), and integer part match. Without external tools, Llama2 models exhibited alarmingly high MAE (e.g., Llama2-7B-chat: 6380.5 USD), and GPT models often abstained from answering.\n        *   **Mitigation Efficacy**: RAG proved highly effective for knowledge-based tasks. Prompt-based tool learning was crucial for time-sensitive tasks, boosting Llama2-7B-chat's stock price query accuracy from negligible to 98.6%. Few-shot prompting and DoLa showed only marginal or mixed improvements. FinMA-7B underperformed its base model, suggesting limitations of its fine-tuning approach.\n\n*   **6. Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The study primarily focuses on factual inconsistency. The selection of financial tasks, while representative, is not exhaustive. The accuracy metric for abbreviation recognition uses substring matching, which might be a lenient criterion. The FactScore metric relies on another LLM. The stock price query task was designed considering Llama2's pretraining data cutoff.\n    *   **Scope of Applicability**: The findings are highly relevant for the deployment of LLMs in finance, education, and law, where factual accuracy is paramount. The evaluated mitigation strategies, particularly RAG and tool learning, are broadly applicable to other domains requiring up-to-date and verifiable information.\n\n*   **7. Technical Significance**\n    *   **Advancement of State-of-the-Art**: This paper provides the first comprehensive empirical quantification of LLM hallucination in the critical financial domain, establishing a crucial baseline. It rigorously demonstrates the severe limitations of off-the-shelf LLMs for factual financial tasks, highlighting the urgent need for specialized solutions.\n    *   **Potential Impact on Future Research**: The findings underscore the indispensable role of external knowledge integration (RAG) and tool-use capabilities for achieving reliable performance in dynamic, fact-intensive fields. It offers a cautionary note regarding the potential drawbacks of multi-task domain-specific fine-tuning, guiding future model development. The released code and data will serve as a valuable resource for further research into robust hallucination mitigation strategies for domain-specific LLMs.",
      "intriguing_abstract": "The integration of Large Language Models (LLMs) into high-stakes financial applications promises transformative efficiency but introduces a critical vulnerability: hallucination. Generating plausible but factually incorrect information can lead to severe monetary losses and erode trust. This paper presents the *first comprehensive empirical examination* of LLM hallucination within the financial domain, quantifying its extent across diverse models and tasks.\n\nWe establish a rigorous framework to assess hallucination in financial abbreviation recognition, term explanations, and historical stock price queries. Critically, we evaluate four practical mitigation strategies: few-shot prompting, Decoding by Contrasting Layers (DoLa), Retrieval Augmented Generation (RAG), and prompt-based tool learning. Our findings reveal that off-the-shelf LLMs exhibit alarming hallucination rates, often providing outdated or erroneous financial data. Paradoxically, some multi-task domain-specific fine-tuning can degrade general instruction-following. Crucially, we demonstrate that **RAG** and **prompt-based tool learning** are indispensable for achieving factual accuracy and reliability, particularly for dynamic, time-sensitive financial information. This work provides a vital baseline, offering critical insights for developing robust, trustworthy LLMs for finance and other fact-intensive domains, guiding future research towards truly grounded AI.",
      "keywords": [
        "Large Language Models (LLMs)",
        "Hallucination behavior",
        "Financial tasks",
        "Empirical examination",
        "Hallucination mitigation",
        "Retrieval Augmented Generation (RAG)",
        "Prompt-based tool learning",
        "Factual accuracy",
        "External knowledge integration",
        "Time-sensitive data",
        "Domain-specific fine-tuning limitations",
        "Quantification of hallucination",
        "High-stakes domains"
      ],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/5838b56f2c7ca3dd946428dae07bdc26a9265c67.pdf",
      "citation_key": "kang20238j0",
      "metadata": {
        "title": "Deficiency of Large Language Models in Finance: An Empirical Examination of Hallucination",
        "authors": [
          "Haoqiang Kang",
          "Xiao-Yang Liu"
        ],
        "published_date": "2023",
        "abstract": "The hallucination issue is recognized as a fundamental deficiency of large language models (LLMs), especially when applied to fields such as finance, education, and law. Despite the growing concerns, there has been a lack of empirical investigation. In this paper, we provide an empirical examination of LLMs' hallucination behaviors in financial tasks. First, we empirically investigate LLM model's ability of explaining financial concepts and terminologies. Second, we assess LLM models' capacity of querying historical stock prices. Third, to alleviate the hallucination issue, we evaluate the efficacy of four practical methods, including few-shot learning, Decoding by Contrasting Layers (DoLa), the Retrieval Augmentation Generation (RAG) method and the prompt-based tool learning method for a function to generate a query command. Finally, our major finding is that off-the-shelf LLMs experience serious hallucination behaviors in financial tasks. Therefore, there is an urgent need to call for research efforts in mitigating LLMs' hallucination.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/5838b56f2c7ca3dd946428dae07bdc26a9265c67.pdf",
        "venue": "arXiv.org",
        "citationCount": 0,
        "score": 0,
        "summary": "This paper \\cite{kang20238j0} provides an empirical examination of hallucination behaviors in Large Language Models (LLMs) within financial tasks and evaluates various mitigation methods.\n\n*   **1. Research Problem & Motivation**\n    *   **Specific Technical Problem**: The paper addresses the fundamental deficiency of hallucination in LLMs, which manifests as generating plausible but factually incorrect or unsupported content, particularly when applied to high-stakes domains like finance.\n    *   **Importance & Challenge**: Hallucination in financial applications poses significant risks, including substantial monetary losses and erosion of trust. The problem is challenging due to the difficulty in accurately measuring hallucinations amidst intricate financial concepts and ensuring pinpoint accuracy for real-world tasks such as querying historical stock prices.\n\n*   **2. Related Work & Positioning**\n    *   **Relation to Existing Approaches**: The work relates to prior research on LLM hallucination, which attributes it to factors like imperfect learning, decoding methods, and knowledge gaps. It also connects to proposed mitigation techniques, including factuality-enhanced decoding (e.g., DoLa) and leveraging external tools (e.g., RAG, API calls).\n    *   **Limitations of Previous Solutions**: Despite the emergence of specialized financial LLMs (e.g., FinBERT, BloombergGPT), a comprehensive empirical investigation into their hallucination tendencies in finance has been lacking. Existing general-purpose LLMs often struggle with factual accuracy in dynamic, domain-specific contexts, and some domain-specific fine-tuning approaches may inadvertently degrade general instruction-following abilities.\n\n*   **3. Technical Approach & Innovation**\n    *   **Core Technical Method/Algorithm**: The paper establishes an empirical framework to assess LLM hallucination across three financial tasks:\n        1.  **Financial Abbreviation Recognition**: Identifying full names for financial acronyms and stock symbols.\n        2.  **Financial Term Explanations**: Providing definitions for less common financial terminologies.\n        3.  **Stock Price Query**: Retrieving historical stock prices for specific dates and tickers.\n    *   **Novelty/Difference**: The core innovation is the *systematic empirical examination* of hallucination in the financial domain, quantifying its extent across diverse LLMs (Llama2, GPT models, FinMA) and tasks. It rigorously *evaluates four practical mitigation methods*:\n        *   **Few-shot prompting**: Providing examples to guide model generation.\n        *   **Decoding by Contrasting Layers (DoLa)**: A decoding technique designed to enhance factual accuracy by contrasting outputs from different model layers.\n        *   **Retrieval Augmentation Generation (RAG)**: Integrating external knowledge from Wikipedia via a FAISS vector store to ground generated content.\n        *   **Prompt-based tool learning**: Enabling LLMs to generate precise Python function calls for external APIs (e.g., Alpha Vantage API) to fetch real-time or historical data.\n\n*   **4. Key Technical Contributions**\n    *   **Novel Algorithms, Methods, or Techniques**:\n        *   A novel empirical framework for quantifying LLM hallucination specifically tailored for financial tasks, covering knowledge recall, explanation generation, and time-sensitive data querying.\n        *   A comprehensive comparative analysis of four distinct hallucination mitigation strategies (few-shot, DoLa, RAG, prompt-based tool learning) within the financial domain.\n    *   **Theoretical Insights or Analysis**:\n        *   Empirically demonstrates that off-the-shelf LLMs exhibit severe hallucination in financial tasks, often providing outdated or incorrect information.\n        *   Reveals that multi-task domain-specific fine-tuning (e.g., FinMA-7B) can surprisingly diminish a model's general instruction-following abilities, potentially increasing instruction-inconsistent hallucinations.\n        *   Highlights the critical importance of external knowledge integration (RAG) and tool-use capabilities for achieving factual accuracy and reliability in dynamic and time-sensitive financial data.\n\n*   **5. Experimental Validation**\n    *   **Experiments Conducted**: The study evaluated various LLMs, including Llama1-7B, Llama2-7B/13B (and their chat versions), GPT3.5-turbo, GPT4, and the domain-specific FinMA-7B-NLP. Experiments were conducted across three financial tasks: Abbreviation Recognition (192 acronyms, 1215 stock symbols), Term Explanations (160 terms), and Stock Price Query (560 examples). Mitigation methods were tested under zero-shot and few-shot settings, and with DoLa, RAG, and prompt-based tool learning.\n    *   **Key Performance Metrics & Comparison Results**:\n        *   **Abbreviation Recognition & Term Explanations**: Measured by Accuracy (substring match) and FactScore (ChatGPT+Retrieval), respectively. While GPT4 showed decent zero-shot performance (82.5% / 90.4% accuracy, 81.11% FactScore), RAG consistently and significantly improved Llama2 models (e.g., Llama2-7B-chat FactScore increased from 64.68% to 90.48%).\n        *   **Stock Price Query**: Assessed by Accuracy (exact match), Mean Absolute Error (MAE), and integer part match. Without external tools, Llama2 models exhibited alarmingly high MAE (e.g., Llama2-7B-chat: 6380.5 USD), and GPT models often abstained from answering.\n        *   **Mitigation Efficacy**: RAG proved highly effective for knowledge-based tasks. Prompt-based tool learning was crucial for time-sensitive tasks, boosting Llama2-7B-chat's stock price query accuracy from negligible to 98.6%. Few-shot prompting and DoLa showed only marginal or mixed improvements. FinMA-7B underperformed its base model, suggesting limitations of its fine-tuning approach.\n\n*   **6. Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The study primarily focuses on factual inconsistency. The selection of financial tasks, while representative, is not exhaustive. The accuracy metric for abbreviation recognition uses substring matching, which might be a lenient criterion. The FactScore metric relies on another LLM. The stock price query task was designed considering Llama2's pretraining data cutoff.\n    *   **Scope of Applicability**: The findings are highly relevant for the deployment of LLMs in finance, education, and law, where factual accuracy is paramount. The evaluated mitigation strategies, particularly RAG and tool learning, are broadly applicable to other domains requiring up-to-date and verifiable information.\n\n*   **7. Technical Significance**\n    *   **Advancement of State-of-the-Art**: This paper provides the first comprehensive empirical quantification of LLM hallucination in the critical financial domain, establishing a crucial baseline. It rigorously demonstrates the severe limitations of off-the-shelf LLMs for factual financial tasks, highlighting the urgent need for specialized solutions.\n    *   **Potential Impact on Future Research**: The findings underscore the indispensable role of external knowledge integration (RAG) and tool-use capabilities for achieving reliable performance in dynamic, fact-intensive fields. It offers a cautionary note regarding the potential drawbacks of multi-task domain-specific fine-tuning, guiding future model development. The released code and data will serve as a valuable resource for further research into robust hallucination mitigation strategies for domain-specific LLMs.",
        "keywords": [
          "Large Language Models (LLMs)",
          "Hallucination behavior",
          "Financial tasks",
          "Empirical examination",
          "Hallucination mitigation",
          "Retrieval Augmented Generation (RAG)",
          "Prompt-based tool learning",
          "Factual accuracy",
          "External knowledge integration",
          "Time-sensitive data",
          "Domain-specific fine-tuning limitations",
          "Quantification of hallucination",
          "High-stakes domains"
        ],
        "paper_type": "based on the abstract and introduction:\n\n*   the abstract explicitly states: \"we provide an **empirical examination** of llmsâ€™ hallucination behaviors in financial tasks.\"\n*   it further details: \"first, we **empirically investigate** llm modelâ€™s ability...\", \"second, we **assess** llm modelsâ€™ capacity...\", \"third, to alleviate the hallucination issue, we **evaluate the efficacy** of four practical methods...\", \"finally, our **major finding** is that...\"\n*   the title itself is \"deficiency of large language models in finance: an **empirical examination** of hallucination\".\n\nthese phrases directly align with the criteria for an **empirical** paper, which involves data-driven studies, experiments, assessments, and findings.\n\n**classification: empirical**"
      },
      "file_name": "5838b56f2c7ca3dd946428dae07bdc26a9265c67.pdf"
    },
    {
      "success": true,
      "doc_id": "c751b264e23905c995223bdd72aa8068",
      "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **CITATION REQUIREMENTS**: Always use \"\\cite{kang202378c}\" when referencing this paper.\n\n---\n\n*   **1. Research Problem & Motivation**\n    *   Large Language Models (LLMs) frequently generate inaccurate or \"hallucinated\" content, a challenge present in both non-retrieval and retrieval-augmented generation \\cite{kang202378c}.\n    *   Existing post-hoc rectification methods are insufficient because they fail to address the \"snowballing\" issue, where initial factual errors accumulate and propagate throughout the generation, especially in complex reasoning tasks \\cite{kang202378c}.\n    *   The problem is important because hallucinations undermine the trustworthiness and factual accuracy of LLM outputs, limiting their reliability in critical applications \\cite{kang202378c}.\n\n*   **2. Related Work & Positioning**\n    *   **Pre-generation methods** (e.g., optimizing retrieved content) may still lead to detailed factual errors in long-form generation without subsequent checks \\cite{kang202378c}.\n    *   **Post-generation methods** (e.g., enhancing attribution or editing after generation) do not account for the \"snowballing\" issue, requiring increasingly complex revisions for accumulated errors \\cite{kang202378c}.\n    *   **Positioning:** \\cite{kang202378c} introduces EVER as a novel real-time, step-wise approach that mitigates hallucinations *during* the generation process, directly addressing the \"snowballing\" problem and complementing existing RAG methods by adding a verification layer.\n\n*   **3. Technical Approach & Innovation**\n    *   **Core Method:** Real-time Verification and Rectification (EVER) \\cite{kang202378c}, which employs a step-wise generation and hallucination rectification strategy.\n    *   **Three-Stage Process (per sentence):**\n        1.  **Generation:** An LLM generates an initial sentence (can be non-retrieval or Retrieval-Augmented Generation (RAG) based) \\cite{kang202378c}.\n        2.  **Validation (Concept-Level):**\n            *   **Key Concepts Identification:** The model extracts factual-related concepts (e.g., dates, numbers, locations) from the generated sentence \\cite{kang202378c}.\n            *   **Validation Question Generation:** Yes/No questions are generated for each identified concept to verify its accuracy \\cite{kang202378c}.\n            *   **Support Checking:** Using few-shot Chain of Thought (CoT) prompting, the model assigns a flag (`True`, `False` for intrinsic hallucination, or `Not Enough Information (NEI)` for extrinsic hallucination) based on evidence, which can be obtained via `Self-query` or `Evidence Retrieval` \\cite{kang202378c}.\n        3.  **Rectification:**\n            *   **Intrinsic Hallucination Revision:** Contradictory facts are revised based on retrieved evidence \\cite{kang202378c}.\n            *   **Extrinsic Hallucination Rewrite:** Sentences with unverified content are rewritten, using retrieved evidence as a reference \\cite{kang202378c}.\n    *   **Handling Remaining Extrinsic Hallucinations:** After rectification, if extrinsic hallucinations persist, the sentence is either flagged with a warning (\"not sure\") or the model abstains from answering, enhancing trustworthiness \\cite{kang202378c}.\n    *   **Secondary Innovation:** EVER-rectified responses can be used as preferred data in preference tuning (e.g., Direct Preference Optimization - DPO) to further enhance the LLM's factuality \\cite{kang202378c}.\n\n*   **4. Key Technical Contributions**\n    *   **Novel Real-Time Framework:** EVER \\cite{kang202378c} introduces a pioneering real-time, step-wise verification and rectification framework to mitigate LLM hallucinations during generation.\n    *   **Concept-Level Granularity:** The approach validates and rectifies at a fine-grained, concept-level within each sentence, enabling precise error identification and correction \\cite{kang202378c}.\n    *   **Proactive Snowballing Mitigation:** By addressing errors immediately, EVER effectively prevents the propagation and accumulation of hallucinations, a significant advancement over post-hoc methods \\cite{kang202378c}.\n    *   **Dynamic Rectification Strategy:** Differentiates between intrinsic (contradictory) and extrinsic (unverifiable) hallucinations, applying tailored revision or rewriting strategies \\cite{kang202378c}.\n    *   **Factuality Enhancement via Preference Tuning:** Demonstrates a novel method to leverage EVER-generated data to create high-quality preference pairs, which can then be used to fine-tune LLMs for improved factual alignment \\cite{kang202378c}.\n\n*   **5. Experimental Validation**\n    *   **Tasks:** Evaluated on long-form biography generation and reasoning tasks, where hallucination snowballing is prevalent \\cite{kang202378c}.\n    *   **Metrics:** FACTSCORE \\cite{min2023factscore} (a retrieval-augmented metric aligning with human evaluation) was used for biography generation \\cite{kang202378c}.\n    *   **Baselines:** Compared against various non-retrieval (e.g., Zero-Shot LLMs, Dola, CoVE), retrieval-augmented rectification (RRAR), and RAG-like (Vanilla RAG, Self-RAG) methods \\cite{kang202378c}.\n    *   **EVER Variants:** Tested with `EVER (NRG+SQ)`, `EVER (NRG+ER)`, and `EVER (RAG+ER)` to assess different generation and validation strategies \\cite{kang202378c}.\n    *   **Key Results:**\n        *   EVER consistently achieved significant improvements in factual accuracy across all scenarios (non-retrieval, retrieval-augmented rectification, and RAG-like) compared to baselines \\cite{kang202378c}.\n        *   `EVER (NRG+SQ)` outperformed the post-hoc verification method CoVE on Llama 65B \\cite{kang202378c}.\n        *   `EVER (NRG+ER)` demonstrated superior and more stable factual precision than RRAR, particularly for rare subjects, highlighting its robustness across varying data rarity \\cite{kang202378c}.\n        *   `EVER (RAG+ER)` significantly improved upon conventional RAG, showcasing its effectiveness as a complementary method for refining retrieved information \\cite{kang202378c}.\n        *   The paper also demonstrates that EVER-generated data can lead to better preference data for enhancing LLM factuality through preference tuning \\cite{kang202378c}.\n\n*   **6. Limitations & Scope**\n    *   **Technical Limitations:** Completely rectifying all extrinsic hallucinations remains challenging; EVER addresses this by transparently flagging unverified content with a \"not sure\" warning rather than always achieving full correction \\cite{kang202378c}. The effectiveness relies on the LLM's ability to accurately perform concept identification, question generation, and support checking.\n    *   **Scope of Applicability:** Primarily applicable to tasks requiring high factual accuracy and where long-form generation or reasoning can lead to error propagation, such as biography generation and multi-hop reasoning \\cite{kang202378c}. It is compatible with both non-retrieval and retrieval-augmented generation paradigms \\cite{kang202378c}.\n\n*   **7. Technical Significance**\n    *   **Advances State-of-the-Art:** EVER \\cite{kang202378c} significantly advances the technical state-of-the-art in LLM hallucination mitigation by introducing a robust real-time, step-wise verification and rectification framework that effectively tackles the \"snowballing\" problem.\n    *   **Enhanced Trustworthiness:** By proactively identifying and correcting errors and transparently flagging unverified information, it substantially improves the trustworthiness and factual accuracy of LLM outputs.\n    *   **Versatile Integration:** Its compatibility with existing RAG methods and its utility in generating high-quality preference data for model alignment make it a versatile and impactful contribution to LLM development \\cite{kang202378c}.\n    *   **Future Research Impact:** This work opens new avenues for research into more sophisticated real-time control mechanisms for LLM generation, fine-grained factual verification, and dynamic self-correction architectures.",
      "intriguing_abstract": "Large Language Models (LLMs) are frequently undermined by pervasive factual inaccuracies and \"hallucinations,\" a critical challenge compounded by the \"snowballing\" effect where initial errors propagate and accumulate in complex, long-form generation. Existing post-hoc rectification methods are insufficient, failing to address this cumulative problem effectively. We introduce **EVER (Real-time Verification and Rectification)**, a novel framework that proactively mitigates hallucinations *during* the generation process. EVER employs a pioneering step-wise, concept-level validation strategy: it extracts factual concepts from each generated sentence, generates verification questions, and dynamically rectifies errors based on retrieved evidence. This real-time approach precisely prevents the snowballing of inaccuracies, differentiating between intrinsic (contradictory) and extrinsic (unverifiable) hallucinations for tailored revision. Crucially, EVER-rectified outputs can be leveraged for **preference tuning**, significantly enhancing LLM factuality through methods like Direct Preference Optimization (DPO). Our experiments demonstrate EVER's superior factual accuracy across non-retrieval and **retrieval-augmented generation (RAG)** scenarios, outperforming state-of-the-art baselines. EVER represents a critical advancement in building trustworthy and factually robust LLMs, offering a versatile solution to a pervasive challenge.",
      "keywords": [
        "LLM hallucinations",
        "snowballing issue",
        "Real-time Verification and Rectification (EVER)",
        "step-wise generation",
        "concept-level validation",
        "intrinsic and extrinsic hallucinations",
        "Retrieval-Augmented Generation (RAG)",
        "factual accuracy",
        "preference tuning",
        "long-form generation",
        "trustworthiness",
        "FACTSCORE metric",
        "proactive hallucination mitigation"
      ],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/b10482ab3dd1d340c3c926d92c3e617c24ee3949.pdf",
      "citation_key": "kang202378c",
      "metadata": {
        "title": "Ever: Mitigating Hallucination in Large Language Models through Real-Time Verification and Rectification",
        "authors": [
          "Haoqiang Kang",
          "Juntong Ni",
          "Huaxiu Yao"
        ],
        "published_date": "2023",
        "abstract": "Large Language Models (LLMs) have demonstrated remarkable proficiency in generating fluent text. However, they often encounter the challenge of generating inaccurate or hallucinated content. This issue is common in both non-retrieval-based generation and retrieval-augmented generation approaches, and existing post-hoc rectification methods may not address the accumulated hallucination errors that may be caused by the\"snowballing\"issue, especially in reasoning tasks. To tackle these challenges, we introduce a novel approach called Real-time Verification and Rectification (Ever). Instead of waiting until the end of the generation process to rectify hallucinations, Ever employs a real-time, step-wise generation and hallucination rectification strategy. The primary objective is to detect and rectify hallucinations as they occur during the text generation process. When compared to both retrieval-based and non-retrieval-based baselines, Ever demonstrates a significant improvement in generating trustworthy and factually accurate text across a diverse range of tasks, including short-form QA, biography generation, and multi-hop reasoning.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/b10482ab3dd1d340c3c926d92c3e617c24ee3949.pdf",
        "venue": "arXiv.org",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **CITATION REQUIREMENTS**: Always use \"\\cite{kang202378c}\" when referencing this paper.\n\n---\n\n*   **1. Research Problem & Motivation**\n    *   Large Language Models (LLMs) frequently generate inaccurate or \"hallucinated\" content, a challenge present in both non-retrieval and retrieval-augmented generation \\cite{kang202378c}.\n    *   Existing post-hoc rectification methods are insufficient because they fail to address the \"snowballing\" issue, where initial factual errors accumulate and propagate throughout the generation, especially in complex reasoning tasks \\cite{kang202378c}.\n    *   The problem is important because hallucinations undermine the trustworthiness and factual accuracy of LLM outputs, limiting their reliability in critical applications \\cite{kang202378c}.\n\n*   **2. Related Work & Positioning**\n    *   **Pre-generation methods** (e.g., optimizing retrieved content) may still lead to detailed factual errors in long-form generation without subsequent checks \\cite{kang202378c}.\n    *   **Post-generation methods** (e.g., enhancing attribution or editing after generation) do not account for the \"snowballing\" issue, requiring increasingly complex revisions for accumulated errors \\cite{kang202378c}.\n    *   **Positioning:** \\cite{kang202378c} introduces EVER as a novel real-time, step-wise approach that mitigates hallucinations *during* the generation process, directly addressing the \"snowballing\" problem and complementing existing RAG methods by adding a verification layer.\n\n*   **3. Technical Approach & Innovation**\n    *   **Core Method:** Real-time Verification and Rectification (EVER) \\cite{kang202378c}, which employs a step-wise generation and hallucination rectification strategy.\n    *   **Three-Stage Process (per sentence):**\n        1.  **Generation:** An LLM generates an initial sentence (can be non-retrieval or Retrieval-Augmented Generation (RAG) based) \\cite{kang202378c}.\n        2.  **Validation (Concept-Level):**\n            *   **Key Concepts Identification:** The model extracts factual-related concepts (e.g., dates, numbers, locations) from the generated sentence \\cite{kang202378c}.\n            *   **Validation Question Generation:** Yes/No questions are generated for each identified concept to verify its accuracy \\cite{kang202378c}.\n            *   **Support Checking:** Using few-shot Chain of Thought (CoT) prompting, the model assigns a flag (`True`, `False` for intrinsic hallucination, or `Not Enough Information (NEI)` for extrinsic hallucination) based on evidence, which can be obtained via `Self-query` or `Evidence Retrieval` \\cite{kang202378c}.\n        3.  **Rectification:**\n            *   **Intrinsic Hallucination Revision:** Contradictory facts are revised based on retrieved evidence \\cite{kang202378c}.\n            *   **Extrinsic Hallucination Rewrite:** Sentences with unverified content are rewritten, using retrieved evidence as a reference \\cite{kang202378c}.\n    *   **Handling Remaining Extrinsic Hallucinations:** After rectification, if extrinsic hallucinations persist, the sentence is either flagged with a warning (\"not sure\") or the model abstains from answering, enhancing trustworthiness \\cite{kang202378c}.\n    *   **Secondary Innovation:** EVER-rectified responses can be used as preferred data in preference tuning (e.g., Direct Preference Optimization - DPO) to further enhance the LLM's factuality \\cite{kang202378c}.\n\n*   **4. Key Technical Contributions**\n    *   **Novel Real-Time Framework:** EVER \\cite{kang202378c} introduces a pioneering real-time, step-wise verification and rectification framework to mitigate LLM hallucinations during generation.\n    *   **Concept-Level Granularity:** The approach validates and rectifies at a fine-grained, concept-level within each sentence, enabling precise error identification and correction \\cite{kang202378c}.\n    *   **Proactive Snowballing Mitigation:** By addressing errors immediately, EVER effectively prevents the propagation and accumulation of hallucinations, a significant advancement over post-hoc methods \\cite{kang202378c}.\n    *   **Dynamic Rectification Strategy:** Differentiates between intrinsic (contradictory) and extrinsic (unverifiable) hallucinations, applying tailored revision or rewriting strategies \\cite{kang202378c}.\n    *   **Factuality Enhancement via Preference Tuning:** Demonstrates a novel method to leverage EVER-generated data to create high-quality preference pairs, which can then be used to fine-tune LLMs for improved factual alignment \\cite{kang202378c}.\n\n*   **5. Experimental Validation**\n    *   **Tasks:** Evaluated on long-form biography generation and reasoning tasks, where hallucination snowballing is prevalent \\cite{kang202378c}.\n    *   **Metrics:** FACTSCORE \\cite{min2023factscore} (a retrieval-augmented metric aligning with human evaluation) was used for biography generation \\cite{kang202378c}.\n    *   **Baselines:** Compared against various non-retrieval (e.g., Zero-Shot LLMs, Dola, CoVE), retrieval-augmented rectification (RRAR), and RAG-like (Vanilla RAG, Self-RAG) methods \\cite{kang202378c}.\n    *   **EVER Variants:** Tested with `EVER (NRG+SQ)`, `EVER (NRG+ER)`, and `EVER (RAG+ER)` to assess different generation and validation strategies \\cite{kang202378c}.\n    *   **Key Results:**\n        *   EVER consistently achieved significant improvements in factual accuracy across all scenarios (non-retrieval, retrieval-augmented rectification, and RAG-like) compared to baselines \\cite{kang202378c}.\n        *   `EVER (NRG+SQ)` outperformed the post-hoc verification method CoVE on Llama 65B \\cite{kang202378c}.\n        *   `EVER (NRG+ER)` demonstrated superior and more stable factual precision than RRAR, particularly for rare subjects, highlighting its robustness across varying data rarity \\cite{kang202378c}.\n        *   `EVER (RAG+ER)` significantly improved upon conventional RAG, showcasing its effectiveness as a complementary method for refining retrieved information \\cite{kang202378c}.\n        *   The paper also demonstrates that EVER-generated data can lead to better preference data for enhancing LLM factuality through preference tuning \\cite{kang202378c}.\n\n*   **6. Limitations & Scope**\n    *   **Technical Limitations:** Completely rectifying all extrinsic hallucinations remains challenging; EVER addresses this by transparently flagging unverified content with a \"not sure\" warning rather than always achieving full correction \\cite{kang202378c}. The effectiveness relies on the LLM's ability to accurately perform concept identification, question generation, and support checking.\n    *   **Scope of Applicability:** Primarily applicable to tasks requiring high factual accuracy and where long-form generation or reasoning can lead to error propagation, such as biography generation and multi-hop reasoning \\cite{kang202378c}. It is compatible with both non-retrieval and retrieval-augmented generation paradigms \\cite{kang202378c}.\n\n*   **7. Technical Significance**\n    *   **Advances State-of-the-Art:** EVER \\cite{kang202378c} significantly advances the technical state-of-the-art in LLM hallucination mitigation by introducing a robust real-time, step-wise verification and rectification framework that effectively tackles the \"snowballing\" problem.\n    *   **Enhanced Trustworthiness:** By proactively identifying and correcting errors and transparently flagging unverified information, it substantially improves the trustworthiness and factual accuracy of LLM outputs.\n    *   **Versatile Integration:** Its compatibility with existing RAG methods and its utility in generating high-quality preference data for model alignment make it a versatile and impactful contribution to LLM development \\cite{kang202378c}.\n    *   **Future Research Impact:** This work opens new avenues for research into more sophisticated real-time control mechanisms for LLM generation, fine-grained factual verification, and dynamic self-correction architectures.",
        "keywords": [
          "LLM hallucinations",
          "snowballing issue",
          "Real-time Verification and Rectification (EVER)",
          "step-wise generation",
          "concept-level validation",
          "intrinsic and extrinsic hallucinations",
          "Retrieval-Augmented Generation (RAG)",
          "factual accuracy",
          "preference tuning",
          "long-form generation",
          "trustworthiness",
          "FACTSCORE metric",
          "proactive hallucination mitigation"
        ],
        "paper_type": "based on the abstract and introduction:\n\n*   the abstract explicitly states: \"we introduce a novel approach called real-time verification and rectification (ever).\" it then describes this approach (\"ever employs a real-time, step-wise generation and hallucination rectification strategy\") and its benefits.\n*   the introduction discusses a technical problem (llm hallucination) and the limitations of existing methods, setting the stage for a new solution.\n*   keywords like \"propose\", \"develop\", \"present\", \"algorithm\", \"method\" are strongly implied by \"introduce a novel approach\" and the description of \"ever\".\n*   while the abstract also mentions \"demonstrates a significant improvement... across a diverse range of tasks,\" indicating empirical evaluation, the primary contribution is the *new method* itself.\n\nthis aligns best with the **technical** classification."
      },
      "file_name": "b10482ab3dd1d340c3c926d92c3e617c24ee3949.pdf"
    },
    {
      "success": true,
      "doc_id": "1e3d4eb92519c779b8d875f74fabb47e",
      "summary": "Here's a focused summary of the technical paper for a literature review:\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Current abstractive summarization models frequently suffer from content hallucinations, generating text that is irrelevant or contradictory to the source document \\cite{dong20223yz}.\n    *   **Nuance**: Prior work often assumes any generated facts not explicitly in the source are undesired hallucinations, leading to methods focused solely on \"faithfulness\" to the source document \\cite{dong20223yz}. However, a significant portion of entities in gold reference summaries are not directly in the source but are factually correct and require external world knowledge for comprehension \\cite{dong20223yz}.\n    *   **Importance & Challenge**: Factual errors, particularly entity-based ones, are highly prevalent (e.g., 92% of XSUM summaries contain at least one factual error) and undermine the trustworthiness and utility of generated summaries \\cite{dong20223yz}. The challenge lies in enabling models to leverage external, abstractive knowledge to produce factually consistent summaries without simply making them more extractive \\cite{dong20223yz}.\n\n2.  **Related Work & Positioning**\n    *   **Limitations of Previous Solutions**: Almost all prior work considers out-of-article entities generated by models as factually incorrect, focusing on improving factual consistency by filtering training examples to contain only extractive entities or by enforcing strict source faithfulness \\cite{dong20223yz}.\n    *   **Positioning**: This work challenges the prevailing assumption by demonstrating that a large portion of out-of-article entities in gold references are factually correct and supported by external world knowledge \\cite{dong20223yz}. It positions itself as improving faithfulness from an *abstractive perspective* by providing additional, relevant facts, rather than solely focusing on extractiveness \\cite{dong20223yz}.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper explores two main methods to incorporate external knowledge:\n        1.  **Direct Fact Concatenation**: Linearizing facts from a knowledge subgraph (constructed by identifying one-hop Wikidata links from entities in the source document) and appending them to the source input of a T5-3B sequence-to-sequence model \\cite{dong20223yz}.\n        2.  **Fact-based Revision Model (Generate-and-Revise)**: A two-stage pipeline:\n            *   An initial T5-3B model generates a summary \\cite{dong20223yz}.\n            *   An entity linker identifies and masks entities in the generated summary, creating a \"skeleton summary\" \\cite{dong20223yz}.\n            *   A **Fact Injected Language Model (FILM)** \\cite{dong20223yz} then revises and corrects the masked entities by predicting new ones, leveraging a separate \"fact memory\" that stores millions of facts and learns to retrieve relevant subsets \\cite{dong20223yz}.\n    *   **Novelty/Difference**:\n        *   **Redefining Faithfulness**: Introduces a dual concept of faithfulness: to the source document (extractiveness) and to external world knowledge (abstractiveness) \\cite{dong20223yz}.\n        *   **Knowledge Subgraph Construction**: Systematically builds a one-hop knowledge subgraph from Wikidata, linked from source entities, to provide relevant external facts \\cite{dong20223yz}.\n        *   **FILM for Revision**: The use of FILM is a key innovation, addressing the scalability issues of direct concatenation by storing facts in a separate memory and learning to retrieve them, rather than appending them to the input \\cite{dong20223yz}.\n\n4.  **Key Technical Contributions**\n    *   **Empirical Study on Knowledge Gaps**: Provides a comprehensive study showing that 60-79% of target entities in XSUM are out-of-article, and a significant portion (20.6-57.1% improvement in coverage) can be found in a one-hop knowledge subgraph linked from source entities \\cite{dong20223yz}.\n    *   **Methods for Knowledge Integration**: Proposes and evaluates two distinct methods for incorporating external knowledge into abstractive summarization models: direct concatenation and a novel generate-and-revise pipeline using a fact-aware model (FILM) \\cite{dong20223yz}.\n    *   **System Design for Entity Correction**: The two-stage revision architecture, particularly the integration of FILM for targeted entity correction, offers a robust approach to mitigating entity-based hallucinations \\cite{dong20223yz}.\n    *   **Novel Evaluation Metrics**: Introduces entity-based metrics (entity correctness and entity consistency) that match predicted entities to target/source/KB entities via Wikidata IDs, enabling a more accurate evaluation of factual consistency that accounts for abstractive, factually correct entities \\cite{dong20223yz}.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**:\n        *   A case study analyzed entity coverage in gold reference summaries of XSUM and an abstractive subset of CNN/Daily Mail (CNNDM abs) by linking source entities to Wikidata \\cite{dong20223yz}.\n        *   Experiments evaluated the direct fact concatenation method using a T5-3B model on XSUM, comparing performance with source-only input, random word concatenation, and random fact concatenation \\cite{dong20223yz}.\n        *   The fact-based revision model (FILM) was evaluated for entity correction, including oracle correction on gold-reference summaries and revision of system-generated summaries \\cite{dong20223yz}.\n    *   **Key Performance Metrics**:\n        *   Standard ROUGE scores (ROUGE-1, ROUGE-L) for summary quality \\cite{dong20223yz}.\n        *   FactCC for factual consistency \\cite{dong20223yz}.\n        *   Novel entity-based metrics: \"entity correctness\" (matching predicted to target entities by ID) and \"entity consistency\" (matching predicted to source/KB entities by ID) \\cite{dong20223yz}.\n    *   **Comparison Results (from provided text)**:\n        *   The case study showed that including a one-hop knowledge subgraph significantly increased target entity coverage (e.g., 20.6-57.1% improvement over source-only entities), with diminishing returns for additional hops \\cite{dong20223yz}.\n        *   Directly appending linked location facts to the source input improved both ROUGE and FactCC scores compared to source-only input or appending random information \\cite{dong20223yz}.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations**:\n        *   **Scalability of Concatenation**: Direct concatenation of knowledge subgraph facts is limited by the input length constraints of Transformer models (e.g., 1024 tokens for T5), making it impractical for large KBs or many entity types \\cite{dong20223yz}.\n        *   **KB Incompleteness & Temporal Misalignment**: The underlying Wikidata KB is incomplete, leading to uncovered target entities. It can also suffer from temporal misalignment, providing outdated facts for news articles \\cite{dong20223yz}.\n        *   **Limited KB Traversal**: The current approach primarily uses one-hop facts, which may not capture all necessary multi-hop reasoning paths \\cite{dong20223yz}.\n    *   **Scope of Applicability**: The methods primarily focus on mitigating entity-based hallucinations using structured external knowledge. While effective for entities, they may not directly address other types of factual errors or non-entity-based inconsistencies \\cite{dong20223yz}.\n\n7.  **Technical Significance**\n    *   **Advancing State-of-the-Art**: This work significantly advances the technical state-of-the-art by redefining \"faithfulness\" in abstractive summarization to include external world knowledge, moving beyond a purely extractive paradigm \\cite{dong20223yz}. It demonstrates a viable path to generating more abstractive yet factually consistent summaries.\n    *   **Novel Hallucination Mitigation Paradigm**: It introduces a novel and effective paradigm for mitigating entity-based hallucinations through explicit knowledge base integration, particularly via the innovative generate-and-revise framework with the FILM model \\cite{dong20223yz}.\n    *   **Improved Evaluation Framework**: The introduction of entity-based evaluation metrics that account for abstractive but factually correct entities provides a more nuanced and accurate way to assess factual consistency, which is crucial for guiding future research and development in abstractive summarization \\cite{dong20223yz}.\n    *   **Potential Impact**: This research opens new avenues for exploring more sophisticated knowledge integration techniques, dynamic KB construction, and multi-hop reasoning in summarization, paving the way for more robust and human-like abstractive systems \\cite{dong20223yz}.",
      "intriguing_abstract": "Abstractive summarization models are plagued by factual hallucinations, particularly entity-based errors, which severely undermine their trustworthiness. While prior work often equates faithfulness with strict source extractiveness, we uncover a critical nuance: a significant portion of entities in high-quality gold summaries are factually correct but originate from external world knowledge, not the source document itself.\n\nThis paper challenges the prevailing paradigm by redefining faithfulness to encompass both source adherence and external factual consistency. We present an empirical study demonstrating that 60-79% of target entities in XSUM are out-of-article, with a substantial portion recoverable from a one-hop Wikidata knowledge subgraph. To leverage this, we propose two novel methods for integrating external knowledge: direct fact concatenation and a sophisticated **generate-and-revise pipeline**. Our key innovation is the **Fact Injected Language Model (FILM)**, which efficiently revises and corrects masked entities using a vast, retrievable fact memory, overcoming the scalability limits of direct concatenation. We also introduce novel entity-based metrics for a more accurate assessment of factual correctness. Our approach significantly improves factual consistency and summary quality, paving the way for more robust, abstractive, and trustworthy summarization systems.",
      "keywords": [
        "abstractive summarization",
        "content hallucinations",
        "factual consistency",
        "external world knowledge",
        "knowledge subgraph",
        "generate-and-revise",
        "Fact Injected Language Model (FILM)",
        "entity-based hallucinations",
        "Wikidata",
        "direct fact concatenation",
        "entity-based evaluation metrics",
        "knowledge integration",
        "redefining faithfulness"
      ],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/ae1e48a74cb2f313e8e99c82f0aa4487b0805002.pdf",
      "citation_key": "dong20223yz",
      "metadata": {
        "title": "Faithful to the Document or to the World? Mitigating Hallucinations via Entity-linked Knowledge in Abstractive Summarization",
        "authors": [
          "Yue Dong",
          "J. Wieting",
          "Pat Verga"
        ],
        "published_date": "2022",
        "abstract": "Despite recent advances in abstractive summarization, current summarization systems still suffer from content hallucinations where models generate text that is either irrelevant or contradictory to the source document. However, prior work has been predicated on the assumption that any generated facts not appearing explicitly in the source are undesired hallucinations. Methods have been proposed to address this scenario by ultimately improving `faithfulness' to the source document, but in reality, there is a large portion of entities in the gold reference targets that are not directly in the source. In this work, we show that these entities are not aberrations, but they instead require utilizing external world knowledge to infer reasoning paths from entities in the source. We show that by utilizing an external knowledge base, we can improve the faithfulness of summaries without simply making them more extractive, and additionally, we show that external knowledge bases linked from the source can benefit the factuality of generated summaries.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/ae1e48a74cb2f313e8e99c82f0aa4487b0805002.pdf",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the technical paper for a literature review:\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Current abstractive summarization models frequently suffer from content hallucinations, generating text that is irrelevant or contradictory to the source document \\cite{dong20223yz}.\n    *   **Nuance**: Prior work often assumes any generated facts not explicitly in the source are undesired hallucinations, leading to methods focused solely on \"faithfulness\" to the source document \\cite{dong20223yz}. However, a significant portion of entities in gold reference summaries are not directly in the source but are factually correct and require external world knowledge for comprehension \\cite{dong20223yz}.\n    *   **Importance & Challenge**: Factual errors, particularly entity-based ones, are highly prevalent (e.g., 92% of XSUM summaries contain at least one factual error) and undermine the trustworthiness and utility of generated summaries \\cite{dong20223yz}. The challenge lies in enabling models to leverage external, abstractive knowledge to produce factually consistent summaries without simply making them more extractive \\cite{dong20223yz}.\n\n2.  **Related Work & Positioning**\n    *   **Limitations of Previous Solutions**: Almost all prior work considers out-of-article entities generated by models as factually incorrect, focusing on improving factual consistency by filtering training examples to contain only extractive entities or by enforcing strict source faithfulness \\cite{dong20223yz}.\n    *   **Positioning**: This work challenges the prevailing assumption by demonstrating that a large portion of out-of-article entities in gold references are factually correct and supported by external world knowledge \\cite{dong20223yz}. It positions itself as improving faithfulness from an *abstractive perspective* by providing additional, relevant facts, rather than solely focusing on extractiveness \\cite{dong20223yz}.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper explores two main methods to incorporate external knowledge:\n        1.  **Direct Fact Concatenation**: Linearizing facts from a knowledge subgraph (constructed by identifying one-hop Wikidata links from entities in the source document) and appending them to the source input of a T5-3B sequence-to-sequence model \\cite{dong20223yz}.\n        2.  **Fact-based Revision Model (Generate-and-Revise)**: A two-stage pipeline:\n            *   An initial T5-3B model generates a summary \\cite{dong20223yz}.\n            *   An entity linker identifies and masks entities in the generated summary, creating a \"skeleton summary\" \\cite{dong20223yz}.\n            *   A **Fact Injected Language Model (FILM)** \\cite{dong20223yz} then revises and corrects the masked entities by predicting new ones, leveraging a separate \"fact memory\" that stores millions of facts and learns to retrieve relevant subsets \\cite{dong20223yz}.\n    *   **Novelty/Difference**:\n        *   **Redefining Faithfulness**: Introduces a dual concept of faithfulness: to the source document (extractiveness) and to external world knowledge (abstractiveness) \\cite{dong20223yz}.\n        *   **Knowledge Subgraph Construction**: Systematically builds a one-hop knowledge subgraph from Wikidata, linked from source entities, to provide relevant external facts \\cite{dong20223yz}.\n        *   **FILM for Revision**: The use of FILM is a key innovation, addressing the scalability issues of direct concatenation by storing facts in a separate memory and learning to retrieve them, rather than appending them to the input \\cite{dong20223yz}.\n\n4.  **Key Technical Contributions**\n    *   **Empirical Study on Knowledge Gaps**: Provides a comprehensive study showing that 60-79% of target entities in XSUM are out-of-article, and a significant portion (20.6-57.1% improvement in coverage) can be found in a one-hop knowledge subgraph linked from source entities \\cite{dong20223yz}.\n    *   **Methods for Knowledge Integration**: Proposes and evaluates two distinct methods for incorporating external knowledge into abstractive summarization models: direct concatenation and a novel generate-and-revise pipeline using a fact-aware model (FILM) \\cite{dong20223yz}.\n    *   **System Design for Entity Correction**: The two-stage revision architecture, particularly the integration of FILM for targeted entity correction, offers a robust approach to mitigating entity-based hallucinations \\cite{dong20223yz}.\n    *   **Novel Evaluation Metrics**: Introduces entity-based metrics (entity correctness and entity consistency) that match predicted entities to target/source/KB entities via Wikidata IDs, enabling a more accurate evaluation of factual consistency that accounts for abstractive, factually correct entities \\cite{dong20223yz}.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**:\n        *   A case study analyzed entity coverage in gold reference summaries of XSUM and an abstractive subset of CNN/Daily Mail (CNNDM abs) by linking source entities to Wikidata \\cite{dong20223yz}.\n        *   Experiments evaluated the direct fact concatenation method using a T5-3B model on XSUM, comparing performance with source-only input, random word concatenation, and random fact concatenation \\cite{dong20223yz}.\n        *   The fact-based revision model (FILM) was evaluated for entity correction, including oracle correction on gold-reference summaries and revision of system-generated summaries \\cite{dong20223yz}.\n    *   **Key Performance Metrics**:\n        *   Standard ROUGE scores (ROUGE-1, ROUGE-L) for summary quality \\cite{dong20223yz}.\n        *   FactCC for factual consistency \\cite{dong20223yz}.\n        *   Novel entity-based metrics: \"entity correctness\" (matching predicted to target entities by ID) and \"entity consistency\" (matching predicted to source/KB entities by ID) \\cite{dong20223yz}.\n    *   **Comparison Results (from provided text)**:\n        *   The case study showed that including a one-hop knowledge subgraph significantly increased target entity coverage (e.g., 20.6-57.1% improvement over source-only entities), with diminishing returns for additional hops \\cite{dong20223yz}.\n        *   Directly appending linked location facts to the source input improved both ROUGE and FactCC scores compared to source-only input or appending random information \\cite{dong20223yz}.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations**:\n        *   **Scalability of Concatenation**: Direct concatenation of knowledge subgraph facts is limited by the input length constraints of Transformer models (e.g., 1024 tokens for T5), making it impractical for large KBs or many entity types \\cite{dong20223yz}.\n        *   **KB Incompleteness & Temporal Misalignment**: The underlying Wikidata KB is incomplete, leading to uncovered target entities. It can also suffer from temporal misalignment, providing outdated facts for news articles \\cite{dong20223yz}.\n        *   **Limited KB Traversal**: The current approach primarily uses one-hop facts, which may not capture all necessary multi-hop reasoning paths \\cite{dong20223yz}.\n    *   **Scope of Applicability**: The methods primarily focus on mitigating entity-based hallucinations using structured external knowledge. While effective for entities, they may not directly address other types of factual errors or non-entity-based inconsistencies \\cite{dong20223yz}.\n\n7.  **Technical Significance**\n    *   **Advancing State-of-the-Art**: This work significantly advances the technical state-of-the-art by redefining \"faithfulness\" in abstractive summarization to include external world knowledge, moving beyond a purely extractive paradigm \\cite{dong20223yz}. It demonstrates a viable path to generating more abstractive yet factually consistent summaries.\n    *   **Novel Hallucination Mitigation Paradigm**: It introduces a novel and effective paradigm for mitigating entity-based hallucinations through explicit knowledge base integration, particularly via the innovative generate-and-revise framework with the FILM model \\cite{dong20223yz}.\n    *   **Improved Evaluation Framework**: The introduction of entity-based evaluation metrics that account for abstractive but factually correct entities provides a more nuanced and accurate way to assess factual consistency, which is crucial for guiding future research and development in abstractive summarization \\cite{dong20223yz}.\n    *   **Potential Impact**: This research opens new avenues for exploring more sophisticated knowledge integration techniques, dynamic KB construction, and multi-hop reasoning in summarization, paving the way for more robust and human-like abstractive systems \\cite{dong20223yz}.",
        "keywords": [
          "abstractive summarization",
          "content hallucinations",
          "factual consistency",
          "external world knowledge",
          "knowledge subgraph",
          "generate-and-revise",
          "Fact Injected Language Model (FILM)",
          "entity-based hallucinations",
          "Wikidata",
          "direct fact concatenation",
          "entity-based evaluation metrics",
          "knowledge integration",
          "redefining faithfulness"
        ],
        "paper_type": "the paper type is **technical**.\n\n**reasoning:**\n\n*   **abstract:**\n    *   identifies a technical problem: \"current summarization systems still suffer from content hallucinations.\"\n    *   critiques prior work's assumptions and limitations.\n    *   proposes a new approach/method: \"we show that these entities... require utilizing external world knowledge to infer reasoning paths from entities in the source.\"\n    *   describes the proposed solution: \"by utilizing an external knowledge base, we can improve the faithfulness of summaries...\"\n    *   mentions the benefits of this method.\n*   **introduction:**\n    *   further elaborates on the technical problem of hallucinations in state-of-the-art summarization models, providing specific examples and statistics from other research to contextualize the problem.\n    *   the overall tone and content focus on identifying a problem in an existing system/algorithm (abstractive summarization) and proposing a novel method (using entity-linked external knowledge) to address it.\n\nthis aligns perfectly with the \"technical\" classification criteria: \"presents new methods, algorithms, or systems\" and \"introduction discusses: technical problem, proposed solution.\" while the paper will undoubtedly include empirical results to validate its proposed method, the core contribution described in the abstract and introduction is the *method* itself."
      },
      "file_name": "ae1e48a74cb2f313e8e99c82f0aa4487b0805002.pdf"
    },
    {
      "success": true,
      "doc_id": "e0373d2fa87b2a138d4de737460680bf",
      "summary": "Here's a focused summary of the paper for a literature review:\n\n### Focused Summary for Literature Review: Mitigating Multilingual Hallucination in Large Vision-Language Models \\cite{qu20240f7}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Large Vision-Language Models (LVLMs) suffer from hallucination (generating plausible but incorrect answers), which is significantly *more severe* when querying images in non-English languages. Existing hallucination mitigation methods exclusively focus on English.\n    *   **Importance & Challenge**: Hallucination poses a considerable challenge to the practical application of LVLMs. The multilingual aspect is critical for global deployment, yet unaddressed. The authors identify multilingual hallucination as a systemic problem stemming from deficiencies in multilingual capabilities or inadequate multimodal abilities.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: Previous works (e.g., SFT, RLHF, DPO-based methods like HADPO) have focused on mitigating hallucination in LVLMs.\n    *   **Limitations of Previous Solutions**: All existing methods concentrate on building *English* datasets and addressing hallucination *only in English*. They do not account for multilingual hallucination, and manually constructing hallucination-aware datasets for numerous non-English languages is time-consuming and labor-intensive.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes a two-stage Multilingual Hallucination Removal (MHR) framework for LVLMs.\n        *   **Stage 1: Multilingual Supervised Fine-tuning (SFT)**: Addresses the foundational issue that most non-English languages struggle to follow instructions, leading to \"non-sense\" answers. It fine-tunes the LVLM using a multilingual instruction-following dataset (PALO) to improve robust query understanding across different languages.\n        *   **Stage 2: Hallucination-Enhanced Preference Optimization**: Aims to improve the model's ability to resist hallucinations.\n    *   **Novelty/Difference**:\n        *   **Cross-lingual Alignment Method**: Instead of manual annotation, the paper introduces a novel method to *automatically generate* multilingual hallucination-aware data pairs. This leverages the inherent capabilities of the LVLM itself.\n        *   **Data Generation Process**: The LVLM generates multiple responses for each image-query in various non-English languages. These responses are then aligned with existing English hallucination/non-hallucination answers using semantic distance metrics (scoring by cross-entropy loss or BLEU after translation). This process identifies hallucination-aware pairs for each language.\n        *   **Direct Preference Optimization (DPO)**: The automatically generated multilingual hallucination-aware pairs are then used for DPO to prompt LVLMs to favor non-hallucinating responses.\n\n4.  **Key Technical Contributions**\n    *   **Novel Problem Formulation**: First work to systematically address and mitigate multilingual hallucinations in LVLMs.\n    *   **Empirical Analysis**: Identifies two primary causes of multilingual hallucination: poor multilingual instruction following and lack of hallucination-aware training data for non-English languages.\n    *   **Novel Framework**: Proposes the two-stage MHR framework, combining multilingual SFT with hallucination-enhanced preference optimization.\n    *   **Automatic Data Generation**: Introduces a novel **cross-lingual alignment method** for automatically constructing multilingual hallucination-aware datasets, significantly reducing manual effort and aligning reasoning processes across languages.\n    *   **Benchmark Extension**: Extends traditional English hallucination benchmarks (POPE, MME, AMBER) into multilingual versions (POPE MUL, MME MUL, AMBER MUL) for comprehensive evaluation.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**: Evaluated the MHR framework on recent LVLMs, specifically LLaVA 1.5 and CogVLM.\n    *   **Key Performance Metrics & Comparison Results**:\n        *   Multilingual SFT significantly reduced the \"unknown prop\" (ratio of invalid answers) across all tested languages (e.g., Japanese from 9.6% to 0.8%, Ukrainian from over 35% to 6.4%).\n        *   The MHR framework achieved a substantial reduction in hallucination generation.\n        *   On the extended multilingual POPE benchmark, MHR delivered an average increase of **19.0% in accuracy** across 13 different languages.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations**: While multilingual SFT significantly improves instruction following, some invalid responses still persist for non-English languages, particularly low-resource ones (e.g., Ukrainian), potentially due to scarce training corpora during the base LLM's (LLaMA) initial training.\n    *   **Scope of Applicability**: The study focuses on 13 languages, categorized into high-resource and low-resource based on user population. The cross-lingual alignment relies on the performance of an off-the-shelf translation model.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: This work is the first to tackle the critical and previously unaddressed problem of multilingual hallucination in LVLMs, significantly advancing their reliability and applicability in diverse linguistic contexts.\n    *   **Impact on Future Research**:\n        *   Provides a systematic analysis and a robust framework for mitigating multilingual hallucination.\n        *   The novel automatic data generation method (cross-lingual alignment) offers a scalable solution for creating multilingual hallucination-aware datasets, paving the way for more efficient research in this domain.\n        *   The introduction of multilingual hallucination benchmarks enables standardized evaluation and fosters further research into improving LVLM performance across languages.\n        *   The findings highlight the need for further exploration into fully addressing instruction-following issues in multilingual LVLMs, especially for low-resource languages.",
      "intriguing_abstract": "Large Vision-Language Models (LVLMs) promise transformative AI, yet their global potential is severely undermined by persistent hallucinationâ€”a problem that intensifies dramatically and remains largely unaddressed in non-English languages. This critical barrier to reliable deployment demands a systemic solution. We introduce the first such framework: a novel two-stage Multilingual Hallucination Removal (MHR) approach.\n\nOur MHR framework first employs Multilingual Supervised Fine-tuning (SFT) to establish robust instruction following across diverse languages. Crucially, we present an innovative **automatic cross-lingual alignment method** that leverages the LVLM itself to generate high-quality, multilingual hallucination-aware data pairs, circumventing laborious manual annotation. These automatically generated pairs then fuel Hallucination-Enhanced **Direct Preference Optimization (DPO)**, effectively steering models away from incorrect responses. Validated on our newly extended multilingual benchmarks (POPE MUL, MME MUL, AMBER MUL), MHR significantly reduces invalid answers and boosts accuracy by an average of **19.0% on POPE MUL** across 13 languages. This work not only provides a scalable framework for mitigating multilingual hallucination but also paves the way for truly reliable and globally applicable LVLMs.",
      "keywords": [
        "Multilingual Hallucination",
        "Large Vision-Language Models (LVLMs)",
        "Hallucination Mitigation",
        "Multilingual Hallucination Removal (MHR) framework",
        "Multilingual Supervised Fine-tuning (SFT)",
        "Hallucination-Enhanced Preference Optimization",
        "Automatic Data Generation",
        "Cross-lingual Alignment Method",
        "Direct Preference Optimization (DPO)",
        "Multilingual Hallucination Benchmarks",
        "Poor Multilingual Instruction Following",
        "Low-resource Languages"
      ],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/88e52de2320e06c7556795be43b38c85a9800e5a.pdf",
      "citation_key": "qu20240f7",
      "metadata": {
        "title": "Mitigating Multilingual Hallucination in Large Vision-Language Models",
        "authors": [
          "Xiaoye Qu",
          "Mingyang Song",
          "Wei Wei",
          "Jianfeng Dong",
          "Yu Cheng"
        ],
        "published_date": "2024",
        "abstract": "While Large Vision-Language Models (LVLMs) have exhibited remarkable capabilities across a wide range of tasks, they suffer from hallucination problems, where models generate plausible yet incorrect answers given the input image-query pair. This hallucination phenomenon is even more severe when querying the image in non-English languages, while existing methods for mitigating hallucinations in LVLMs only consider the English scenarios. In this paper, we make the first attempt to mitigate this important multilingual hallucination in LVLMs. With thorough experiment analysis, we found that multilingual hallucination in LVLMs is a systemic problem that could arise from deficiencies in multilingual capabilities or inadequate multimodal abilities. To this end, we propose a two-stage Multilingual Hallucination Removal (MHR) framework for LVLMs, aiming to improve resistance to hallucination for both high-resource and low-resource languages. Instead of relying on the intricate manual annotations of multilingual resources, we fully leverage the inherent capabilities of the LVLM and propose a novel cross-lingual alignment method, which generates multiple responses for each image-query input and then identifies the hallucination-aware pairs for each language. These data pairs are finally used for direct preference optimization to prompt the LVLMs to favor non-hallucinating responses. Experimental results show that our MHR achieves a substantial reduction in hallucination generation for LVLMs. Notably, on our extended multilingual POPE benchmark, our framework delivers an average increase of 19.0% in accuracy across 13 different languages. Our code and model weights are available at https://github.com/ssmisya/MHR",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/88e52de2320e06c7556795be43b38c85a9800e5a.pdf",
        "venue": "arXiv.org",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the paper for a literature review:\n\n### Focused Summary for Literature Review: Mitigating Multilingual Hallucination in Large Vision-Language Models \\cite{qu20240f7}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Large Vision-Language Models (LVLMs) suffer from hallucination (generating plausible but incorrect answers), which is significantly *more severe* when querying images in non-English languages. Existing hallucination mitigation methods exclusively focus on English.\n    *   **Importance & Challenge**: Hallucination poses a considerable challenge to the practical application of LVLMs. The multilingual aspect is critical for global deployment, yet unaddressed. The authors identify multilingual hallucination as a systemic problem stemming from deficiencies in multilingual capabilities or inadequate multimodal abilities.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: Previous works (e.g., SFT, RLHF, DPO-based methods like HADPO) have focused on mitigating hallucination in LVLMs.\n    *   **Limitations of Previous Solutions**: All existing methods concentrate on building *English* datasets and addressing hallucination *only in English*. They do not account for multilingual hallucination, and manually constructing hallucination-aware datasets for numerous non-English languages is time-consuming and labor-intensive.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes a two-stage Multilingual Hallucination Removal (MHR) framework for LVLMs.\n        *   **Stage 1: Multilingual Supervised Fine-tuning (SFT)**: Addresses the foundational issue that most non-English languages struggle to follow instructions, leading to \"non-sense\" answers. It fine-tunes the LVLM using a multilingual instruction-following dataset (PALO) to improve robust query understanding across different languages.\n        *   **Stage 2: Hallucination-Enhanced Preference Optimization**: Aims to improve the model's ability to resist hallucinations.\n    *   **Novelty/Difference**:\n        *   **Cross-lingual Alignment Method**: Instead of manual annotation, the paper introduces a novel method to *automatically generate* multilingual hallucination-aware data pairs. This leverages the inherent capabilities of the LVLM itself.\n        *   **Data Generation Process**: The LVLM generates multiple responses for each image-query in various non-English languages. These responses are then aligned with existing English hallucination/non-hallucination answers using semantic distance metrics (scoring by cross-entropy loss or BLEU after translation). This process identifies hallucination-aware pairs for each language.\n        *   **Direct Preference Optimization (DPO)**: The automatically generated multilingual hallucination-aware pairs are then used for DPO to prompt LVLMs to favor non-hallucinating responses.\n\n4.  **Key Technical Contributions**\n    *   **Novel Problem Formulation**: First work to systematically address and mitigate multilingual hallucinations in LVLMs.\n    *   **Empirical Analysis**: Identifies two primary causes of multilingual hallucination: poor multilingual instruction following and lack of hallucination-aware training data for non-English languages.\n    *   **Novel Framework**: Proposes the two-stage MHR framework, combining multilingual SFT with hallucination-enhanced preference optimization.\n    *   **Automatic Data Generation**: Introduces a novel **cross-lingual alignment method** for automatically constructing multilingual hallucination-aware datasets, significantly reducing manual effort and aligning reasoning processes across languages.\n    *   **Benchmark Extension**: Extends traditional English hallucination benchmarks (POPE, MME, AMBER) into multilingual versions (POPE MUL, MME MUL, AMBER MUL) for comprehensive evaluation.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**: Evaluated the MHR framework on recent LVLMs, specifically LLaVA 1.5 and CogVLM.\n    *   **Key Performance Metrics & Comparison Results**:\n        *   Multilingual SFT significantly reduced the \"unknown prop\" (ratio of invalid answers) across all tested languages (e.g., Japanese from 9.6% to 0.8%, Ukrainian from over 35% to 6.4%).\n        *   The MHR framework achieved a substantial reduction in hallucination generation.\n        *   On the extended multilingual POPE benchmark, MHR delivered an average increase of **19.0% in accuracy** across 13 different languages.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations**: While multilingual SFT significantly improves instruction following, some invalid responses still persist for non-English languages, particularly low-resource ones (e.g., Ukrainian), potentially due to scarce training corpora during the base LLM's (LLaMA) initial training.\n    *   **Scope of Applicability**: The study focuses on 13 languages, categorized into high-resource and low-resource based on user population. The cross-lingual alignment relies on the performance of an off-the-shelf translation model.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: This work is the first to tackle the critical and previously unaddressed problem of multilingual hallucination in LVLMs, significantly advancing their reliability and applicability in diverse linguistic contexts.\n    *   **Impact on Future Research**:\n        *   Provides a systematic analysis and a robust framework for mitigating multilingual hallucination.\n        *   The novel automatic data generation method (cross-lingual alignment) offers a scalable solution for creating multilingual hallucination-aware datasets, paving the way for more efficient research in this domain.\n        *   The introduction of multilingual hallucination benchmarks enables standardized evaluation and fosters further research into improving LVLM performance across languages.\n        *   The findings highlight the need for further exploration into fully addressing instruction-following issues in multilingual LVLMs, especially for low-resource languages.",
        "keywords": [
          "Multilingual Hallucination",
          "Large Vision-Language Models (LVLMs)",
          "Hallucination Mitigation",
          "Multilingual Hallucination Removal (MHR) framework",
          "Multilingual Supervised Fine-tuning (SFT)",
          "Hallucination-Enhanced Preference Optimization",
          "Automatic Data Generation",
          "Cross-lingual Alignment Method",
          "Direct Preference Optimization (DPO)",
          "Multilingual Hallucination Benchmarks",
          "Poor Multilingual Instruction Following",
          "Low-resource Languages"
        ],
        "paper_type": "based on the abstract and introduction, this paper is a **technical** paper.\n\nhere's why:\n\n*   **abstract mentions:** \"we make the first attempt to mitigate...\", \"we propose a two-stage multilingual hallucination removal (mhr) framework for lvlms\". these phrases clearly indicate the development and presentation of a new method or system.\n*   **introduction discusses:** \"we boost multilingual instruction following ability with a multilingual supervised fine-tuning\", \"propose a novel cross-lingual alignment method\", \"direct preference optimization\". these are specific technical details of the proposed solution.\n*   while it includes \"experimental results show...\" and \"average increase of 19.0% in accuracy\", these are presented as validation of the *proposed framework* rather than the sole focus of the paper being a data-driven study without a new method. the primary contribution is the mhr framework itself."
      },
      "file_name": "88e52de2320e06c7556795be43b38c85a9800e5a.pdf"
    },
    {
      "success": true,
      "doc_id": "eb581e205635e1da7ec09a062e7c2376",
      "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **1. Research Problem & Motivation**\n    *   **Specific Technical Problem**: Large Language Models (LLMs) are claimed to be capable of Natural Language Inference (NLI), a critical component for downstream tasks like question answering and summarization. However, they frequently exhibit \"hallucination,\" providing incorrect or inappropriate information presented as fact, particularly in NLI tasks. The underlying causes of this hallucination, especially false positives, are opaque due to LLMs' large scale and proprietary training data.\n    *   **Importance & Challenge**: Understanding the mechanisms behind LLM hallucination is crucial for improving their reliability, robustness, and trustworthiness in real-world applications. The challenge lies in probing the internal decision-making processes of black-box LLMs to identify specific biases originating from pretraining that lead to these errors.\n\n*   **2. Related Work & Positioning**\n    *   **Relation to Existing Approaches**: This work builds on previous research that identified dataset artifacts in NLI (e.g., Poliak et al., 2018) and generalization failures in smaller LMs (e.g., Talman and Chatzikyriakidis, 2019; Li et al., 2022). It also relates to studies on LLM memorization (Carlini et al., 2023; Tirumala et al., 2022) and factual recall (Weller et al., 2023; Kandpal et al., 2022).\n    *   **Limitations of Previous Solutions**: Previous work on dataset artifacts often focused on supervised models or smaller LMs. While some studies explored memorization, they didn't deeply connect it to NLI hallucination or the role of entities as \"indices.\" Claims of LLM understanding \"beyond memorization\" (e.g., Bubeck et al., 2023) are challenged by \\cite{mckenna2023pzc}'s findings of persistent biases even in advanced models like GPT-4. This paper extends the analysis to much larger, more robust LLMs and specifically probes the *sources* of hallucination in NLI rather than just observing it.\n\n*   **3. Technical Approach & Innovation**\n    *   **Core Technical Method**: \\cite{mckenna2023pzc} employs a behavioral study approach using controlled experiments on several LLM families (LLaMA, GPT-3.5, PaLM). They systematically modify existing NLI datasets (Levy/Holt, RTE-1) and measure changes in LLM predictions. The core method involves identifying and quantifying two specific biases:\n        1.  **Attestation Bias (Î›)**: LLMs' over-reliance on propositional memory, affirming entailment if the hypothesis is likely attested in their training data, regardless of the premise. This is measured by prompting the LLM to assess the truthfulness of the hypothesis independently.\n        2.  **Relative Frequency Bias (Î¦)**: LLMs' tendency to affirm entailment if the premise predicate is less frequent than the hypothesis predicate in training data, using corpus statistics (Google N-grams as a proxy).\n    *   **Novelty/Difference**:\n        *   **Controlled Dataset Transformations**: The paper introduces novel dataset transformations (Random Premise, Generic Argument, Random Argument with varying entity frequencies) to isolate and probe the impact of specific linguistic features (predicates, entities) on NLI decisions, without retraining models.\n        *   **Probing Memorization without Training**: Unlike previous hypothesis-only baselines, \\cite{mckenna2023pzc} uses this technique to directly probe LLM memory *without any training*, revealing inherent sensitivity to attestation.\n        *   **Entity as \"Index\" Discovery**: The study uniquely demonstrates that LLMs use specific named entities as \"indices\" to access memorized propositional knowledge, even when these entities are logically irrelevant to the predicate inference task.\n        *   **Quantifying Bias Impact**: It quantifies how these biases lead to misleadingly high performance on NLI samples consistent with the biases and severely degraded performance on adversarial samples.\n\n*   **4. Key Technical Contributions**\n    *   **Novel Algorithms, Methods, or Techniques**:\n        *   Introduction of controlled dataset transformations (IRandPrem, IGenArg, IRandArg â†“/â†‘) for behavioral studies of LLMs in NLI, allowing for precise measurement of bias sources.\n        *   A method for measuring \"attestedness\" of a hypothesis by directly querying the LLM, used to quantify the Attestation Bias.\n        *   A method for estimating \"relative frequency bias\" using external corpus statistics (Google N-grams) as a proxy for LLM training data.\n    *   **Theoretical Insights or Analysis**:\n        *   Demonstration that LLM hallucination in NLI is largely driven by two specific biases originating from pretraining: propositional memory (Attestation Bias) and statistical patterns of usage (Relative Frequency Bias).\n        *   The critical insight that named entities function as \"indices\" for accessing memorized propositional knowledge, rather than being processed for their logical role in inference.\n        *   Evidence that LLMs rely on superficial statistical patterns (memorization, frequency heuristics) instead of robust logical reasoning for NLI.\n\n*   **5. Experimental Validation**\n    *   **Experiments Conducted**: Behavioral studies were conducted on three major LLM families: LLaMA-65B, GPT-3.5 (text-davinci-003), and PaLM-540B. Experiments involved NLI tasks on the Levy/Holt dataset (for detailed behavioral analysis) and RTE-1 (for general NLI impact), using the original datasets and the custom-designed transformed datasets (IRandPrem, IGenArg, IRandArg â†“, IRandArg â†‘).\n    *   **Key Performance Metrics & Comparison Results**:\n        *   **Attestation Bias**: LLMs were significantly more likely to *wrongly* predict \"Entail\" when the hypothesis was attested in training data. LLaMA-65B was 1.9x, GPT-3.5 was 2.2x, and PaLM-540B was 2.0x more likely to do so.\n        *   **Entity Indexing**: Performance on the Random Premise task (IRandPrem) showed LLMs still predicted entailment for attested hypotheses, demonstrating reliance on memory over premise logic. Performance dropped significantly with generic arguments (IGenArg). Critically, performance was *better* with rare random entities (IRandArg â†“) than common ones (IRandArg â†‘), indicating common entities trigger memorized facts more readily.\n        *   **Relative Frequency Bias**: LLMs were 1.6x (LLaMA-65B), 1.8x (GPT-3.5), and 2.0x (PaLM-540B) more likely to wrongly affirm entailments if the premise predicate was less frequent than the hypothesis predicate.\n        *   **Overall NLI Performance Impact**: When NLI samples were adversarial to these biases (i.e., true label contradicted the bias), LLM performance severely degraded, often becoming poor or near-random classifiers, demonstrating that high scores on standard NLI benchmarks can be misleading.\n\n*   **6. Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The study uses Google N-grams as a proxy for LLM pre-training corpora, which may not perfectly reflect the exact distributions. The manual inspection of IRandPrem transformations found a small percentage of unintended true entailments (9.6%), though the majority were successfully non-entailing. The study focuses on directional entailments and predicate-argument structures, which might not cover all forms of NLI.\n    *   **Scope of Applicability**: The analysis is primarily focused on Natural Language Inference tasks and the specific types of hallucination (false positives) arising from the identified biases. It explicitly excludes NLI datasets that rely heavily on world knowledge (e.g., MMLU, Natural Questions) to isolate reasoning about language semantics. The findings are applicable to large, pre-trained language models, including those with instruction-tuning and RLHF.\n\n*   **7. Technical Significance**\n    *   **Advance State-of-the-Art**: \\cite{mckenna2023pzc} significantly advances the understanding of LLM behavior by identifying and quantifying two fundamental biases (attestation and relative frequency) as major sources of hallucination in NLI. It provides empirical evidence that even state-of-the-art LLMs often rely on superficial statistical patterns and memorized facts rather than robust logical reasoning. The discovery of entities acting as \"indices\" for propositional memory is a novel and crucial insight into LLM internal mechanisms.\n    *   **Potential Impact on Future Research**: The identified biases and the methodology for probing them offer valuable controls and diagnostic tools for future LLM evaluation. Researchers can design adversarial NLI benchmarks that specifically target these biases to more accurately assess true reasoning capabilities. This work highlights the need for developing LLMs that can perform NLI based on semantic understanding and logical inference, rather than statistical correlations or memorized data, paving the way for more robust and trustworthy AI systems.",
      "intriguing_abstract": "Large Language Models (LLMs) frequently exhibit problematic hallucination in Natural Language Inference (NLI) tasks, casting doubt on their reliability for critical downstream applications. This paper uncovers two fundamental, pretraining-induced biases driving these pervasive false positives: **Attestation Bias**, an over-reliance on memorized propositional knowledge, and **Relative Frequency Bias**, a tendency to affirm entailment based on statistical predicate frequency disparities. Through novel controlled dataset transformations and rigorous behavioral studies across LLaMA, GPT-3.5, and PaLM, we demonstrate that LLMs often use specific named entities as mere \"indices\" to access memorized facts, rather than engaging in robust logical reasoning. These biases lead to misleadingly high performance on standard NLI benchmarks but severely degrade accuracy on adversarial samples, exposing a reliance on superficial statistical patterns over genuine semantic understanding. Our findings challenge claims of LLM understanding \"beyond memorization,\" providing critical diagnostic tools and highlighting the urgent need for developing LLMs capable of true logical inference. This work is pivotal for building more reliable and trustworthy AI systems.",
      "keywords": [
        "Large Language Models (LLMs)",
        "Natural Language Inference (NLI)",
        "LLM Hallucination",
        "Attestation Bias",
        "Relative Frequency Bias",
        "Propositional Memory",
        "Entity Indexing",
        "Controlled Dataset Transformations",
        "Behavioral Study",
        "Pretraining Biases",
        "Superficial Statistical Patterns",
        "Logical Reasoning",
        "LLM Reliability",
        "Adversarial NLI Benchmarks"
      ],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/2c67ee597ed38f43ec0f123a3f1cce38cbd3b5b4.pdf",
      "citation_key": "mckenna2023pzc",
      "metadata": {
        "title": "Sources of Hallucination by Large Language Models on Inference Tasks",
        "authors": [
          "Nick McKenna",
          "Tianyi Li",
          "Liang Cheng",
          "Mohammad Javad Hosseini",
          "Mark Johnson",
          "Mark Steedman"
        ],
        "published_date": "2023",
        "abstract": "Large Language Models (LLMs) are claimed to be capable of Natural Language Inference (NLI), necessary for applied tasks like question answering and summarization. We present a series of behavioral studies on several LLM families (LLaMA, GPT-3.5, and PaLM) which probe their behavior using controlled experiments. We establish two biases originating from pretraining which predict much of their behavior, and show that these are major sources of hallucination in generative LLMs. First, memorization at the level of sentences: we show that, regardless of the premise, models falsely label NLI test samples as entailing when the hypothesis is attested in training data, and that entities are used as ``indices'' to access the memorized data. Second, statistical patterns of usage learned at the level of corpora: we further show a similar effect when the premise predicate is less frequent than that of the hypothesis in the training data, a bias following from previous studies. We demonstrate that LLMs perform significantly worse on NLI test samples which do not conform to these biases than those which do, and we offer these as valuable controls for future LLM evaluation.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/2c67ee597ed38f43ec0f123a3f1cce38cbd3b5b4.pdf",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **1. Research Problem & Motivation**\n    *   **Specific Technical Problem**: Large Language Models (LLMs) are claimed to be capable of Natural Language Inference (NLI), a critical component for downstream tasks like question answering and summarization. However, they frequently exhibit \"hallucination,\" providing incorrect or inappropriate information presented as fact, particularly in NLI tasks. The underlying causes of this hallucination, especially false positives, are opaque due to LLMs' large scale and proprietary training data.\n    *   **Importance & Challenge**: Understanding the mechanisms behind LLM hallucination is crucial for improving their reliability, robustness, and trustworthiness in real-world applications. The challenge lies in probing the internal decision-making processes of black-box LLMs to identify specific biases originating from pretraining that lead to these errors.\n\n*   **2. Related Work & Positioning**\n    *   **Relation to Existing Approaches**: This work builds on previous research that identified dataset artifacts in NLI (e.g., Poliak et al., 2018) and generalization failures in smaller LMs (e.g., Talman and Chatzikyriakidis, 2019; Li et al., 2022). It also relates to studies on LLM memorization (Carlini et al., 2023; Tirumala et al., 2022) and factual recall (Weller et al., 2023; Kandpal et al., 2022).\n    *   **Limitations of Previous Solutions**: Previous work on dataset artifacts often focused on supervised models or smaller LMs. While some studies explored memorization, they didn't deeply connect it to NLI hallucination or the role of entities as \"indices.\" Claims of LLM understanding \"beyond memorization\" (e.g., Bubeck et al., 2023) are challenged by \\cite{mckenna2023pzc}'s findings of persistent biases even in advanced models like GPT-4. This paper extends the analysis to much larger, more robust LLMs and specifically probes the *sources* of hallucination in NLI rather than just observing it.\n\n*   **3. Technical Approach & Innovation**\n    *   **Core Technical Method**: \\cite{mckenna2023pzc} employs a behavioral study approach using controlled experiments on several LLM families (LLaMA, GPT-3.5, PaLM). They systematically modify existing NLI datasets (Levy/Holt, RTE-1) and measure changes in LLM predictions. The core method involves identifying and quantifying two specific biases:\n        1.  **Attestation Bias (Î›)**: LLMs' over-reliance on propositional memory, affirming entailment if the hypothesis is likely attested in their training data, regardless of the premise. This is measured by prompting the LLM to assess the truthfulness of the hypothesis independently.\n        2.  **Relative Frequency Bias (Î¦)**: LLMs' tendency to affirm entailment if the premise predicate is less frequent than the hypothesis predicate in training data, using corpus statistics (Google N-grams as a proxy).\n    *   **Novelty/Difference**:\n        *   **Controlled Dataset Transformations**: The paper introduces novel dataset transformations (Random Premise, Generic Argument, Random Argument with varying entity frequencies) to isolate and probe the impact of specific linguistic features (predicates, entities) on NLI decisions, without retraining models.\n        *   **Probing Memorization without Training**: Unlike previous hypothesis-only baselines, \\cite{mckenna2023pzc} uses this technique to directly probe LLM memory *without any training*, revealing inherent sensitivity to attestation.\n        *   **Entity as \"Index\" Discovery**: The study uniquely demonstrates that LLMs use specific named entities as \"indices\" to access memorized propositional knowledge, even when these entities are logically irrelevant to the predicate inference task.\n        *   **Quantifying Bias Impact**: It quantifies how these biases lead to misleadingly high performance on NLI samples consistent with the biases and severely degraded performance on adversarial samples.\n\n*   **4. Key Technical Contributions**\n    *   **Novel Algorithms, Methods, or Techniques**:\n        *   Introduction of controlled dataset transformations (IRandPrem, IGenArg, IRandArg â†“/â†‘) for behavioral studies of LLMs in NLI, allowing for precise measurement of bias sources.\n        *   A method for measuring \"attestedness\" of a hypothesis by directly querying the LLM, used to quantify the Attestation Bias.\n        *   A method for estimating \"relative frequency bias\" using external corpus statistics (Google N-grams) as a proxy for LLM training data.\n    *   **Theoretical Insights or Analysis**:\n        *   Demonstration that LLM hallucination in NLI is largely driven by two specific biases originating from pretraining: propositional memory (Attestation Bias) and statistical patterns of usage (Relative Frequency Bias).\n        *   The critical insight that named entities function as \"indices\" for accessing memorized propositional knowledge, rather than being processed for their logical role in inference.\n        *   Evidence that LLMs rely on superficial statistical patterns (memorization, frequency heuristics) instead of robust logical reasoning for NLI.\n\n*   **5. Experimental Validation**\n    *   **Experiments Conducted**: Behavioral studies were conducted on three major LLM families: LLaMA-65B, GPT-3.5 (text-davinci-003), and PaLM-540B. Experiments involved NLI tasks on the Levy/Holt dataset (for detailed behavioral analysis) and RTE-1 (for general NLI impact), using the original datasets and the custom-designed transformed datasets (IRandPrem, IGenArg, IRandArg â†“, IRandArg â†‘).\n    *   **Key Performance Metrics & Comparison Results**:\n        *   **Attestation Bias**: LLMs were significantly more likely to *wrongly* predict \"Entail\" when the hypothesis was attested in training data. LLaMA-65B was 1.9x, GPT-3.5 was 2.2x, and PaLM-540B was 2.0x more likely to do so.\n        *   **Entity Indexing**: Performance on the Random Premise task (IRandPrem) showed LLMs still predicted entailment for attested hypotheses, demonstrating reliance on memory over premise logic. Performance dropped significantly with generic arguments (IGenArg). Critically, performance was *better* with rare random entities (IRandArg â†“) than common ones (IRandArg â†‘), indicating common entities trigger memorized facts more readily.\n        *   **Relative Frequency Bias**: LLMs were 1.6x (LLaMA-65B), 1.8x (GPT-3.5), and 2.0x (PaLM-540B) more likely to wrongly affirm entailments if the premise predicate was less frequent than the hypothesis predicate.\n        *   **Overall NLI Performance Impact**: When NLI samples were adversarial to these biases (i.e., true label contradicted the bias), LLM performance severely degraded, often becoming poor or near-random classifiers, demonstrating that high scores on standard NLI benchmarks can be misleading.\n\n*   **6. Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The study uses Google N-grams as a proxy for LLM pre-training corpora, which may not perfectly reflect the exact distributions. The manual inspection of IRandPrem transformations found a small percentage of unintended true entailments (9.6%), though the majority were successfully non-entailing. The study focuses on directional entailments and predicate-argument structures, which might not cover all forms of NLI.\n    *   **Scope of Applicability**: The analysis is primarily focused on Natural Language Inference tasks and the specific types of hallucination (false positives) arising from the identified biases. It explicitly excludes NLI datasets that rely heavily on world knowledge (e.g., MMLU, Natural Questions) to isolate reasoning about language semantics. The findings are applicable to large, pre-trained language models, including those with instruction-tuning and RLHF.\n\n*   **7. Technical Significance**\n    *   **Advance State-of-the-Art**: \\cite{mckenna2023pzc} significantly advances the understanding of LLM behavior by identifying and quantifying two fundamental biases (attestation and relative frequency) as major sources of hallucination in NLI. It provides empirical evidence that even state-of-the-art LLMs often rely on superficial statistical patterns and memorized facts rather than robust logical reasoning. The discovery of entities acting as \"indices\" for propositional memory is a novel and crucial insight into LLM internal mechanisms.\n    *   **Potential Impact on Future Research**: The identified biases and the methodology for probing them offer valuable controls and diagnostic tools for future LLM evaluation. Researchers can design adversarial NLI benchmarks that specifically target these biases to more accurately assess true reasoning capabilities. This work highlights the need for developing LLMs that can perform NLI based on semantic understanding and logical inference, rather than statistical correlations or memorized data, paving the way for more robust and trustworthy AI systems.",
        "keywords": [
          "Large Language Models (LLMs)",
          "Natural Language Inference (NLI)",
          "LLM Hallucination",
          "Attestation Bias",
          "Relative Frequency Bias",
          "Propositional Memory",
          "Entity Indexing",
          "Controlled Dataset Transformations",
          "Behavioral Study",
          "Pretraining Biases",
          "Superficial Statistical Patterns",
          "Logical Reasoning",
          "LLM Reliability",
          "Adversarial NLI Benchmarks"
        ],
        "paper_type": "**empirical**\n\n**reasoning:**\n\nthe abstract and introduction clearly indicate that the paper presents:\n*   \"a series of behavioral studies\"\n*   \"controlled experiments\"\n*   \"establish two biases\"\n*   \"show that\"\n*   \"demonstrate that llms perform significantly worse\"\n*   \"investigates two biases driving llm performance\"\n\nthese phrases directly align with the criteria for an **empirical** paper, which focuses on data-driven studies, experiments, and statistical analysis to uncover findings."
      },
      "file_name": "2c67ee597ed38f43ec0f123a3f1cce38cbd3b5b4.pdf"
    },
    {
      "success": true,
      "doc_id": "81ae0e211c44e252a6db7896efa8cb41",
      "summary": "Here's a focused summary of the technical paper for a literature review:\n\n### Focused Summary for Literature Review\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: The paper addresses the critical issue of declining information quality (IQ) and lack of trustworthiness in content generated by Large Language Models (LLMs) \\cite{rejeleene2024okw}.\n    *   **Importance and Challenge**: Information from LLMs is often unreliable, biased, and prone to hallucinations or fabricated content, stemming from issues in pre-training processes like tokenization and data quality \\cite{rejeleene2024okw}. This problem is crucial because unreliable LLM outputs can lead to flawed decisions in vital sectors (e.g., business, medicine, law), eroding user trust and negatively impacting economic activity and societal development \\cite{rejeleene2024okw}.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: The work reviews the landscape of state-of-the-art LLMs, including Transformer-based architectures like BERT, GPT series (GPT-2, GPT-3, GPT-4), and Llama/Llama 2, acknowledging their impressive capabilities and scaling trends \\cite{rejeleene2024okw}. It also discusses various tokenization methods and benchmark datasets used in NLP \\cite{rejeleene2024okw}.\n    *   **Limitations of Previous Solutions**: While existing LLMs excel in performance and scale, the paper implicitly positions its work by highlighting that current research and evaluation metrics often overlook a systematic and explicit framework for assessing the *information quality* of LLM outputs \\cite{rejeleene2024okw}. This gap leads to persistent issues like factual incorrectness, bias, and hallucination, which are not adequately captured or addressed by traditional performance metrics alone \\cite{rejeleene2024okw}.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper introduces a novel mathematical formulation for evaluating the Information Quality (IQ) of LLM-generated information \\cite{rejeleene2024okw}. This formulation defines IQ as a function of three key dimensions: consistency, relevance, and accuracy \\cite{rejeleene2024okw}.\n    *   **Novelty**: The proposed IQ model is a linear weighted combination: `IQ(L) = f((w1 * consistency * w2 * relevance * w3 * accuracy) / sum(w1 to w3))`, where `sum(w1 to w3) = 1` and `w1, w2, w3` are context-specific weights \\cite{rejeleene2024okw}. This approach is innovative due to its simplicity, tunability, normalization, and domain-agnostic nature, explicitly designed to align with human expectations and LLM alignment goals, offering a systematic pipeline for IQ evaluation in NLP \\cite{rejeleene2024okw}.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**: A new mathematical formulation for Information Quality (IQ) evaluation in LLMs, based on a weighted combination of consistency, relevance, and accuracy \\cite{rejeleene2024okw}.\n    *   **System Design/Architectural Innovations**: While not a system design in the traditional sense, the paper proposes a conceptual pipeline for integrating IQ evaluation into LLM development and deployment \\cite{rejeleene2024okw}.\n    *   **Theoretical Insights/Analysis**: A comprehensive analysis of the root causes of poor information quality in LLMs, including the impact of tokenization methods, lack of data diversity, inherent biases in training data, and the implications of scaling laws on quality \\cite{rejeleene2024okw}. The paper also discusses the economic significance of trustworthy LLM outputs \\cite{rejeleene2024okw}.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**: The paper primarily presents a conceptual framework and an analytical discussion of information quality challenges in LLMs. It *does not* present direct empirical experiments or validation of its proposed mathematical `IQ(L)` model with actual LLM outputs \\cite{rejeleene2024okw}.\n    *   **Key Performance Metrics and Comparison Results**: The paper references existing LLM performance metrics (e.g., perplexity, BLEU score) and discusses how poor data quality (e.g., SBNATION data leading to low BLEU scores) impacts these, but these are not results from new experiments conducted to validate their IQ metric \\cite{rejeleene2024okw}. A comparison table of state-of-the-art LLMs (BERT, GPT3, BART, ChatGPT, Llama2) is provided to contextualize the discussion on scaling and data quality, detailing their parameters, training data, and general performance, but this serves as background rather than direct validation of the proposed IQ framework \\cite{rejeleene2024okw}.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**:\n        *   LLMs are inherently limited by the quality, diversity, and bias present in their massive training datasets \\cite{rejeleene2024okw}.\n        *   Current tokenization methods can introduce information loss, \"token's glitching,\" and increased computational costs, affecting output quality \\cite{rejeleene2024okw}.\n        *   LLMs frequently exhibit hallucinations, factual inaccuracies, and inconsistencies, especially in complex reasoning or multimodal contexts (e.g., GPT-4V) \\cite{rejeleene2024okw}.\n        *   The computational cost and energy consumption associated with scaling LLMs pose challenges for simultaneously optimizing quality and efficiency \\cite{rejeleene2024okw}.\n    *   **Scope of Applicability**: The proposed IQ evaluation framework is designed to be domain-agnostic, aiming for broad applicability across various LLM applications where trust and reliability are critical \\cite{rejeleene2024okw}. However, the practical implementation and fine-tuning of the context-specific weights (`w1, w2, w3`) would require domain expertise \\cite{rejeleene2024okw}.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: This work significantly advances the technical state-of-the-art by proposing a structured, mathematical framework for evaluating *information quality* in LLM outputs, moving beyond traditional performance metrics \\cite{rejeleene2024okw}. This is crucial for the trustworthy deployment and adoption of LLMs in sensitive applications \\cite{rejeleene2024okw}.\n    *   **Potential Impact on Future Research**: By systematically analyzing and highlighting the technical challenges related to data quality, tokenization, bias, and scaling that undermine LLM trustworthiness, the paper provides clear and actionable directions for future research aimed at improving LLM reliability, reducing hallucinations, and fostering more transparent and accountable AI systems \\cite{rejeleene2024okw}.",
      "intriguing_abstract": "The pervasive issue of declining information quality (IQ) and trustworthiness in Large Language Model (LLM) outputs poses a critical threat to their adoption in vital sectors. Despite impressive advancements in LLM capabilities, persistent problems like hallucinations, bias, and factual inaccuracies erode user confidence and risk flawed decision-making. While performance metrics abound, a robust and systematic framework for assessing the intrinsic *information quality* of LLM-generated content remains elusive.\n\nThis paper introduces a novel mathematical formulation for evaluating LLM Information Quality, defining IQ as a weighted function of three fundamental dimensions: consistency, relevance, and accuracy. Our innovative, domain-agnostic model, `IQ(L) = f(w1 * consistency, w2 * relevance, w3 * accuracy)`, provides a simple, tunable, and normalized approach explicitly designed to align LLM outputs with human-centric quality expectations. We analyze the root causes of poor IQ, including tokenization issues and data biases, and propose a conceptual pipeline for integrating IQ evaluation into LLM development. This framework offers a crucial step towards building truly trustworthy and reliable LLM systems, fostering transparent AI, and mitigating the risks of misinformation in critical applications.",
      "keywords": [
        "Large Language Models (LLMs)",
        "Information Quality (IQ)",
        "LLM trustworthiness",
        "hallucinations",
        "bias",
        "novel mathematical formulation",
        "IQ evaluation framework",
        "consistency",
        "relevance",
        "accuracy",
        "weighted combination model",
        "tokenization methods",
        "data quality",
        "reliable LLM outputs."
      ],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/0422493dc3a70816bb5d327c4c67094f64a78c98.pdf",
      "citation_key": "rejeleene2024okw",
      "metadata": {
        "title": "Towards Trustable Language Models: Investigating Information Quality of Large Language Models",
        "authors": [
          "Rick Rejeleene",
          "Xiaowei Xu",
          "John R. Talburt"
        ],
        "published_date": "2024",
        "abstract": "Large language models (LLM) are generating information at a rapid pace, requiring users to increasingly rely and trust the data. Despite remarkable advances of LLM, Information generated by LLM is not completely trustworthy, due to challenges in information quality. Specifically, integrity of Information quality decreases due to unreliable, biased, tokenization during pre-training of LLM. Moreover, due to decreased information quality issues, has led towards hallucination, fabricated information. Unreliable information can lead towards flawed decisions in businesses, which impacts economic activity. In this work, we introduce novel mathematical information quality evaluation of LLM, we furthermore analyze and highlight information quality challenges, scaling laws to systematically scale language models.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/0422493dc3a70816bb5d327c4c67094f64a78c98.pdf",
        "venue": "arXiv.org",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the technical paper for a literature review:\n\n### Focused Summary for Literature Review\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: The paper addresses the critical issue of declining information quality (IQ) and lack of trustworthiness in content generated by Large Language Models (LLMs) \\cite{rejeleene2024okw}.\n    *   **Importance and Challenge**: Information from LLMs is often unreliable, biased, and prone to hallucinations or fabricated content, stemming from issues in pre-training processes like tokenization and data quality \\cite{rejeleene2024okw}. This problem is crucial because unreliable LLM outputs can lead to flawed decisions in vital sectors (e.g., business, medicine, law), eroding user trust and negatively impacting economic activity and societal development \\cite{rejeleene2024okw}.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: The work reviews the landscape of state-of-the-art LLMs, including Transformer-based architectures like BERT, GPT series (GPT-2, GPT-3, GPT-4), and Llama/Llama 2, acknowledging their impressive capabilities and scaling trends \\cite{rejeleene2024okw}. It also discusses various tokenization methods and benchmark datasets used in NLP \\cite{rejeleene2024okw}.\n    *   **Limitations of Previous Solutions**: While existing LLMs excel in performance and scale, the paper implicitly positions its work by highlighting that current research and evaluation metrics often overlook a systematic and explicit framework for assessing the *information quality* of LLM outputs \\cite{rejeleene2024okw}. This gap leads to persistent issues like factual incorrectness, bias, and hallucination, which are not adequately captured or addressed by traditional performance metrics alone \\cite{rejeleene2024okw}.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper introduces a novel mathematical formulation for evaluating the Information Quality (IQ) of LLM-generated information \\cite{rejeleene2024okw}. This formulation defines IQ as a function of three key dimensions: consistency, relevance, and accuracy \\cite{rejeleene2024okw}.\n    *   **Novelty**: The proposed IQ model is a linear weighted combination: `IQ(L) = f((w1 * consistency * w2 * relevance * w3 * accuracy) / sum(w1 to w3))`, where `sum(w1 to w3) = 1` and `w1, w2, w3` are context-specific weights \\cite{rejeleene2024okw}. This approach is innovative due to its simplicity, tunability, normalization, and domain-agnostic nature, explicitly designed to align with human expectations and LLM alignment goals, offering a systematic pipeline for IQ evaluation in NLP \\cite{rejeleene2024okw}.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**: A new mathematical formulation for Information Quality (IQ) evaluation in LLMs, based on a weighted combination of consistency, relevance, and accuracy \\cite{rejeleene2024okw}.\n    *   **System Design/Architectural Innovations**: While not a system design in the traditional sense, the paper proposes a conceptual pipeline for integrating IQ evaluation into LLM development and deployment \\cite{rejeleene2024okw}.\n    *   **Theoretical Insights/Analysis**: A comprehensive analysis of the root causes of poor information quality in LLMs, including the impact of tokenization methods, lack of data diversity, inherent biases in training data, and the implications of scaling laws on quality \\cite{rejeleene2024okw}. The paper also discusses the economic significance of trustworthy LLM outputs \\cite{rejeleene2024okw}.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**: The paper primarily presents a conceptual framework and an analytical discussion of information quality challenges in LLMs. It *does not* present direct empirical experiments or validation of its proposed mathematical `IQ(L)` model with actual LLM outputs \\cite{rejeleene2024okw}.\n    *   **Key Performance Metrics and Comparison Results**: The paper references existing LLM performance metrics (e.g., perplexity, BLEU score) and discusses how poor data quality (e.g., SBNATION data leading to low BLEU scores) impacts these, but these are not results from new experiments conducted to validate their IQ metric \\cite{rejeleene2024okw}. A comparison table of state-of-the-art LLMs (BERT, GPT3, BART, ChatGPT, Llama2) is provided to contextualize the discussion on scaling and data quality, detailing their parameters, training data, and general performance, but this serves as background rather than direct validation of the proposed IQ framework \\cite{rejeleene2024okw}.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**:\n        *   LLMs are inherently limited by the quality, diversity, and bias present in their massive training datasets \\cite{rejeleene2024okw}.\n        *   Current tokenization methods can introduce information loss, \"token's glitching,\" and increased computational costs, affecting output quality \\cite{rejeleene2024okw}.\n        *   LLMs frequently exhibit hallucinations, factual inaccuracies, and inconsistencies, especially in complex reasoning or multimodal contexts (e.g., GPT-4V) \\cite{rejeleene2024okw}.\n        *   The computational cost and energy consumption associated with scaling LLMs pose challenges for simultaneously optimizing quality and efficiency \\cite{rejeleene2024okw}.\n    *   **Scope of Applicability**: The proposed IQ evaluation framework is designed to be domain-agnostic, aiming for broad applicability across various LLM applications where trust and reliability are critical \\cite{rejeleene2024okw}. However, the practical implementation and fine-tuning of the context-specific weights (`w1, w2, w3`) would require domain expertise \\cite{rejeleene2024okw}.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: This work significantly advances the technical state-of-the-art by proposing a structured, mathematical framework for evaluating *information quality* in LLM outputs, moving beyond traditional performance metrics \\cite{rejeleene2024okw}. This is crucial for the trustworthy deployment and adoption of LLMs in sensitive applications \\cite{rejeleene2024okw}.\n    *   **Potential Impact on Future Research**: By systematically analyzing and highlighting the technical challenges related to data quality, tokenization, bias, and scaling that undermine LLM trustworthiness, the paper provides clear and actionable directions for future research aimed at improving LLM reliability, reducing hallucinations, and fostering more transparent and accountable AI systems \\cite{rejeleene2024okw}.",
        "keywords": [
          "Large Language Models (LLMs)",
          "Information Quality (IQ)",
          "LLM trustworthiness",
          "hallucinations",
          "bias",
          "novel mathematical formulation",
          "IQ evaluation framework",
          "consistency",
          "relevance",
          "accuracy",
          "weighted combination model",
          "tokenization methods",
          "data quality",
          "reliable LLM outputs."
        ],
        "paper_type": "the paper type is **theoretical**.\n\n**reasoning:**\n\nthe abstract explicitly states: \"in this work, we introduce **novel mathematical information quality evaluation** of llm, we furthermore analyze and highlight information quality challenges, scaling laws to systematically scale language models.\"\n\nthis directly aligns with the criteria for a **theoretical** paper: \"mathematical analysis, proofs, formal models\" and the abstract mentioning \"mathematical\" and \"analysis.\" the focus is on developing a formal, mathematical framework for evaluating information quality, rather than presenting an empirical study, a new system/algorithm (technical, though it could be a theoretical basis for one), or a comprehensive review."
      },
      "file_name": "0422493dc3a70816bb5d327c4c67094f64a78c98.pdf"
    },
    {
      "success": true,
      "doc_id": "3e8b85eab4d954ffc63acc141b23ca52",
      "summary": "This paper \\cite{liu2024p39} provides a comprehensive survey of the hallucination problem in Large Language Models (LLMs).\n\nHere's a focused summary for literature review:\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem**: LLMs frequently generate text that is inconsistent with real-world facts or user input, a phenomenon termed \"hallucinations.\"\n    *   **Importance and Challenge**: This problem severely compromises the reliability and trustworthiness of LLMs, particularly in critical domains like healthcare, finance, and law, where erroneous outputs can have severe consequences. Current methods for assessing hallucinations are labor-intensive, and there's a need for effective, low-cost, and flexible mitigation strategies. The paper also highlights the challenge of distinguishing hallucinations (honest errors) from intentional deception and \"jailbreak\" phenomena.\n\n*   **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: This work is a survey that synthesizes and categorizes existing research on LLM hallucinations. It builds upon prior classifications, adjusting them to \"factuality hallucination\" and \"faithfulness hallucination\" for broader applicability.\n    *   **Limitations of Previous Solutions (as identified by the survey)**: The paper notes that existing hallucination assessment methods are costly and require significant manual effort. It also points out the need for clearer distinctions between hallucinations, deceptive behaviors \\cite{liu2024p39} [3], and security issues like \"jailbreaking\" \\cite{liu2024p39} [4].\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method**: As a survey, the paper's core approach is to systematically analyze and categorize the definition, types, underlying causes, and proposed solutions for LLM hallucinations. It structures the causes into three main stages: data acquisition, model training, and inference decoding.\n    *   **Novelty/Difference**: The innovation lies in its comprehensive and structured synthesis of the current state of research, providing a unified framework for understanding the multifaceted nature of LLM hallucinations and their mitigation strategies. It doesn't propose a new algorithm but rather organizes and critically reviews existing technical efforts.\n\n*   **Key Technical Contributions (Summarized from existing literature)**\n    *   **Novel Algorithms, Methods, or Techniques (summarized)**:\n        *   **Data-related**: Strategies include managing training datasets (e.g., sampling factual data, removing repetitive/social biases), knowledge editing to directly modify model parameters \\cite{liu2024p39} [2], retrieval-augmented generation (RAG) to incorporate external knowledge \\cite{liu2024p39} [7], and confidence-based knowledge boundary expression (CoKE) to enable models to express ignorance \\cite{liu2024p39} [8].\n        *   **Training-related**: Methods like the bidirectional autoregressive method (BATGPT) to capture complex contextual dependencies \\cite{liu2024p39} [2], Faithful Finetuning using explicit loss functions to improve fidelity \\cite{liu2024p39} [9], and skepticism modeling to enhance uncertainty estimation \\cite{liu2024p39} [10].\n        *   **Inference-related**: Techniques such as fact enhancement, independent decoding using fact kernel sampling \\cite{liu2024p39} [2], inference time intervention to guide responses towards factual accuracy \\cite{liu2024p39} [2], DoLa for dynamic logic selection \\cite{liu2024p39} [2], post-editing decoding for self-correction, and the hardware-friendly ConSmax algorithm to replace Softmax and improve computational parallelism \\cite{liu2024p39} [6].\n    *   **Theoretical Insights/Analysis**: The paper provides a detailed breakdown of hallucination types (factuality, faithfulness) and their root causes across the LLM lifecycle, including data defects, architectural flaws (e.g., unidirectional representation, attention limitations), exposure bias, alignment issues, and inference-time likelihood traps.\n\n*   **Experimental Validation**\n    *   This paper is a survey and does not present its own experimental validation.\n    *   It references that some of the *discussed* mitigation strategies have been experimentally validated in their respective original works, for instance, the generalization ability of skepticism modeling has been validated through out-of-domain experiments \\cite{liu2024p39} [10]. The paper emphasizes the ongoing need for effective and low-cost validation strategies.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations (of LLMs discussed)**: The survey highlights limitations such as LLMs' reliance on experience over systematic data collection (leading to biases), knowledge boundary limitations (lack of up-to-date or domain-specific knowledge), architectural flaws (e.g., one-way representation, Softmax bottleneck), and issues during alignment (e.g., inconsistency between internal beliefs and outputs).\n    *   **Scope of Applicability**: The survey focuses broadly on LLM hallucinations, covering their manifestation, causes, and mitigation across various stages of LLM development and deployment. It also touches upon the implications for high-stakes applications.\n\n*   **Technical Significance**\n    *   **Advancement of State-of-the-Art**: By systematically categorizing and reviewing the complex problem of LLM hallucinations, their causes, and existing technical solutions, the paper significantly contributes to organizing the current understanding of this critical issue. It provides a valuable reference point for researchers.\n    *   **Potential Impact on Future Research**: The paper outlines crucial future research directions, including finer classification and evaluation of hallucinations, exploring multimodal strategies, enhancing model stability, and integrating human and artificial intelligence to jointly address these challenges. This guidance is vital for promoting continuous progress in LLM reliability and safety.",
      "intriguing_abstract": "The dazzling capabilities of Large Language Models (LLMs) are increasingly overshadowed by a critical flaw: persistent hallucinations. These fabrications, ranging from subtle factual inaccuracies to outright confabulations, severely undermine LLM reliability and trustworthiness, posing significant risks in sensitive applications like healthcare and finance. This comprehensive survey meticulously dissects the multifaceted problem of LLM hallucinations, offering a unified framework that clarifies their definition, distinct typesâ€”**factuality hallucination** and **faithfulness hallucination**â€”and underlying causes across the entire LLM lifecycle: **data acquisition**, **model training**, and **inference decoding**.\n\nWe systematically categorize and analyze state-of-the-art mitigation strategies, from **knowledge editing** and **Retrieval-Augmented Generation (RAG)** to advanced **skepticism modeling** and **confidence-based knowledge boundary expression (CoKE)**. By distinguishing hallucinations from intentional deception and jailbreaking, this paper provides crucial theoretical insights and practical guidance. This work is an indispensable resource for researchers and practitioners, charting a clear path towards developing more robust, verifiable, and ultimately trustworthy LLMs, essential for their safe and effective deployment.",
      "keywords": [
        "Large Language Models (LLMs)",
        "LLM hallucination",
        "factuality hallucination",
        "faithfulness hallucination",
        "causes of hallucination",
        "hallucination mitigation strategies",
        "retrieval-augmented generation (RAG)",
        "knowledge editing",
        "skepticism modeling",
        "hallucination assessment",
        "LLM reliability",
        "trustworthiness",
        "critical application domains",
        "comprehensive survey",
        "future research directions"
      ],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/e5f183ffafd74e2fba831420fa1f3e5f07b7ce2d.pdf",
      "citation_key": "liu2024p39",
      "metadata": {
        "title": "A Survey of Hallucination Problems Based on Large Language Models",
        "authors": [
          "Xinxin Liu"
        ],
        "published_date": "2024",
        "abstract": "Abstract. Large language models (LLM) have made significant achievements in the field of natural language processing, but the generated text often contains content that is inconsistent with the real world or user input, known as hallucinations. This article investigates the current situation of hallucinations in LLM, including the definition, types, causes, and solutions of hallucinations. Illusions are divided into different types such as factual and faithful, mainly caused by factors such as training data defects, low utilization of facts, and randomness in the decoding process. The phenomenon of hallucinations poses a threat to the reliability of LLM, especially in fields such as healthcare, finance, and law, which may lead to serious consequences. To address this issue, this article investigates methods such as managing training datasets, knowledge editing, and enhancing retrieval generation. Future research should classify and evaluate illusions more finely, explore multimodal strategies, enhance model stability, and integrate human intelligence and artificial intelligence to jointly address challenges, promoting the continuous progress of LLM.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/e5f183ffafd74e2fba831420fa1f3e5f07b7ce2d.pdf",
        "venue": "Applied and Computational Engineering",
        "citationCount": 0,
        "score": 0,
        "summary": "This paper \\cite{liu2024p39} provides a comprehensive survey of the hallucination problem in Large Language Models (LLMs).\n\nHere's a focused summary for literature review:\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem**: LLMs frequently generate text that is inconsistent with real-world facts or user input, a phenomenon termed \"hallucinations.\"\n    *   **Importance and Challenge**: This problem severely compromises the reliability and trustworthiness of LLMs, particularly in critical domains like healthcare, finance, and law, where erroneous outputs can have severe consequences. Current methods for assessing hallucinations are labor-intensive, and there's a need for effective, low-cost, and flexible mitigation strategies. The paper also highlights the challenge of distinguishing hallucinations (honest errors) from intentional deception and \"jailbreak\" phenomena.\n\n*   **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: This work is a survey that synthesizes and categorizes existing research on LLM hallucinations. It builds upon prior classifications, adjusting them to \"factuality hallucination\" and \"faithfulness hallucination\" for broader applicability.\n    *   **Limitations of Previous Solutions (as identified by the survey)**: The paper notes that existing hallucination assessment methods are costly and require significant manual effort. It also points out the need for clearer distinctions between hallucinations, deceptive behaviors \\cite{liu2024p39} [3], and security issues like \"jailbreaking\" \\cite{liu2024p39} [4].\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method**: As a survey, the paper's core approach is to systematically analyze and categorize the definition, types, underlying causes, and proposed solutions for LLM hallucinations. It structures the causes into three main stages: data acquisition, model training, and inference decoding.\n    *   **Novelty/Difference**: The innovation lies in its comprehensive and structured synthesis of the current state of research, providing a unified framework for understanding the multifaceted nature of LLM hallucinations and their mitigation strategies. It doesn't propose a new algorithm but rather organizes and critically reviews existing technical efforts.\n\n*   **Key Technical Contributions (Summarized from existing literature)**\n    *   **Novel Algorithms, Methods, or Techniques (summarized)**:\n        *   **Data-related**: Strategies include managing training datasets (e.g., sampling factual data, removing repetitive/social biases), knowledge editing to directly modify model parameters \\cite{liu2024p39} [2], retrieval-augmented generation (RAG) to incorporate external knowledge \\cite{liu2024p39} [7], and confidence-based knowledge boundary expression (CoKE) to enable models to express ignorance \\cite{liu2024p39} [8].\n        *   **Training-related**: Methods like the bidirectional autoregressive method (BATGPT) to capture complex contextual dependencies \\cite{liu2024p39} [2], Faithful Finetuning using explicit loss functions to improve fidelity \\cite{liu2024p39} [9], and skepticism modeling to enhance uncertainty estimation \\cite{liu2024p39} [10].\n        *   **Inference-related**: Techniques such as fact enhancement, independent decoding using fact kernel sampling \\cite{liu2024p39} [2], inference time intervention to guide responses towards factual accuracy \\cite{liu2024p39} [2], DoLa for dynamic logic selection \\cite{liu2024p39} [2], post-editing decoding for self-correction, and the hardware-friendly ConSmax algorithm to replace Softmax and improve computational parallelism \\cite{liu2024p39} [6].\n    *   **Theoretical Insights/Analysis**: The paper provides a detailed breakdown of hallucination types (factuality, faithfulness) and their root causes across the LLM lifecycle, including data defects, architectural flaws (e.g., unidirectional representation, attention limitations), exposure bias, alignment issues, and inference-time likelihood traps.\n\n*   **Experimental Validation**\n    *   This paper is a survey and does not present its own experimental validation.\n    *   It references that some of the *discussed* mitigation strategies have been experimentally validated in their respective original works, for instance, the generalization ability of skepticism modeling has been validated through out-of-domain experiments \\cite{liu2024p39} [10]. The paper emphasizes the ongoing need for effective and low-cost validation strategies.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations (of LLMs discussed)**: The survey highlights limitations such as LLMs' reliance on experience over systematic data collection (leading to biases), knowledge boundary limitations (lack of up-to-date or domain-specific knowledge), architectural flaws (e.g., one-way representation, Softmax bottleneck), and issues during alignment (e.g., inconsistency between internal beliefs and outputs).\n    *   **Scope of Applicability**: The survey focuses broadly on LLM hallucinations, covering their manifestation, causes, and mitigation across various stages of LLM development and deployment. It also touches upon the implications for high-stakes applications.\n\n*   **Technical Significance**\n    *   **Advancement of State-of-the-Art**: By systematically categorizing and reviewing the complex problem of LLM hallucinations, their causes, and existing technical solutions, the paper significantly contributes to organizing the current understanding of this critical issue. It provides a valuable reference point for researchers.\n    *   **Potential Impact on Future Research**: The paper outlines crucial future research directions, including finer classification and evaluation of hallucinations, exploring multimodal strategies, enhancing model stability, and integrating human and artificial intelligence to jointly address these challenges. This guidance is vital for promoting continuous progress in LLM reliability and safety.",
        "keywords": [
          "Large Language Models (LLMs)",
          "LLM hallucination",
          "factuality hallucination",
          "faithfulness hallucination",
          "causes of hallucination",
          "hallucination mitigation strategies",
          "retrieval-augmented generation (RAG)",
          "knowledge editing",
          "skepticism modeling",
          "hallucination assessment",
          "LLM reliability",
          "trustworthiness",
          "critical application domains",
          "comprehensive survey",
          "future research directions"
        ],
        "paper_type": "**survey**\n\n**reasoning:**\n\n1.  **title:** the title explicitly states \"a survey of hallucination problems based on large language models,\" which is a direct indicator.\n2.  **abstract:**\n    *   \"this article investigates the current situation of hallucinations in llm, including the definition, types, causes, and solutions of hallucinations.\" - this describes a comprehensive review of existing knowledge.\n    *   \"illusions are divided into different types...\" - indicates classification and organization of existing information.\n    *   \"this article investigates methods such as managing training datasets, knowledge editing, and enhancing retrieval generation.\" - this refers to reviewing existing methods, not proposing new ones.\n    *   \"future research should classify and evaluate illusions more finely, explore multimodal strategies...\" - this is a common component of surveys, identifying gaps and future directions based on the current state-of-the-art.\n3.  **introduction:**\n    *   it sets the stage by discussing the \"fatal problem - hallucinations\" and how it \"has attracted the attention of many people,\" justifying the need for a comprehensive review.\n    *   \"some literature suggests that the llm hallucination problem leads to a lack of reliability...\" - references existing literature, a characteristic of a survey.\n\nall these points align perfectly with the criteria for a **survey** paper."
      },
      "file_name": "e5f183ffafd74e2fba831420fa1f3e5f07b7ce2d.pdf"
    },
    {
      "success": true,
      "doc_id": "45529c965645c77a855693ee525fa25e",
      "summary": "This paper, \"\\cite{liu2024sn3}\", provides a comprehensive survey on the phenomenon of hallucination in Large Vision-Language Models (LVLMs).\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem**: The paper addresses \"hallucination\" in Large Vision-Language Models (LVLMs), defined as the misalignment between factual visual content and the corresponding textual generation.\n    *   **Importance and Challenge**: Hallucination poses a significant impediment to the practical application of LVLMs. Unlike traditional image captioning models, LVLMs' enhanced capabilities for detailed and fluent descriptions diversify and exacerbate hallucination, extending beyond mere object existence to include attribute and relation errors. The multimodal nature of LVLMs introduces unique challenges in detecting hallucinations, inferring their causes, and developing effective mitigation strategies.\n\n*   **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: As a survey, this work systematically reviews and categorizes existing research on LVLM hallucinations. It clarifies the concept, outlines current evaluation benchmarks and methodologies, investigates root causes, and critically reviews existing mitigation methods.\n    *   **Limitations of Previous Solutions (as discussed in the survey)**: The survey highlights that conventional hallucination evaluation methods (e.g., CHAIR for image captioning) struggle with the vast object categories and diverse hallucination types (attributes, relations) present in LVLMs. It also notes that LLM-targeted mitigation methods are often insufficient for LVLMs, necessitating multimodal-specific approaches.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method/Algorithm**: This paper is a survey and does not propose a new technical method or algorithm. Instead, its core \"approach\" is a structured, comprehensive analysis and synthesis of the current state of research on LVLM hallucinations.\n    *   **Novelty/Difference**: The innovation lies in providing the first comprehensive and structured overview of LVLM hallucinations. It offers a clear taxonomy of hallucination symptoms (judgment vs. description; object, attribute, relation), categorizes evaluation methods (non-hallucinatory generation vs. hallucination discrimination), identifies root causes (data bias, model architecture, modality misalignment), and reviews mitigation strategies. This structured analysis helps to clarify a complex and rapidly evolving field.\n\n*   **Key Technical Contributions**\n    *   **Novel Algorithms, Methods, or Techniques**: The paper does not introduce new algorithms but rather *categorizes and explains* existing ones for hallucination evaluation (e.g., handcrafted pipeline methods like CCEval, model-based end-to-end methods like LLM-based evaluation and hallucination data-driven models, and discrimination methods like POPE).\n    *   **System Design or Architectural Innovations**: Not applicable, as it is a survey.\n    *   **Theoretical Insights or Analysis**: Provides a detailed conceptual clarification of LVLM hallucinations, a multifaceted taxonomy of symptoms, and a thorough analysis of their root causes, encompassing training data issues (bias, irrelevance) and model component limitations (vision encoder grounding, modality misalignment, context attention).\n\n*   **Experimental Validation**\n    *   **Experiments Conducted**: The authors of this survey did not conduct new experiments.\n    *   **Key Performance Metrics and Comparison Results**: The paper *reviews* the experimental validation from other research, detailing various benchmarks (e.g., POPE, NOPE, CIEM for discriminative tasks; M-HalDetect, GAVIE, FAITHScore, MMHal-Bench for generative tasks) and metrics used in the field (e.g., Accuracy for object presence, Reward Model Score, FAITHScore, Rating Score for generative faithfulness). It highlights the trend towards more complex metrics for generative benchmarks.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations or Assumptions (of the field, as discussed)**: The survey discusses limitations of existing LVLMs and mitigation methods, such as the resource-intensive nature of training high-resolution visual encoders for hallucination reduction, and the persistent challenge of the significant gap between visual and textual tokens.\n    *   **Scope of Applicability**: The survey's scope is strictly focused on \"hallucination\" within \"Large Vision-Language Models,\" covering its definition, symptoms, evaluation, causes, and mitigation strategies.\n\n*   **Technical Significance**\n    *   **Advance the Technical State-of-the-Art**: This survey significantly advances the technical state-of-the-art by providing a much-needed, structured, and comprehensive overview of a critical and complex problem in LVLMs. It systematizes existing knowledge, clarifies terminology, and identifies key challenges and open questions.\n    *   **Potential Impact on Future Research**: By offering a clear conceptual framework, a review of current evaluation practices, an analysis of root causes, and a summary of mitigation strategies, the paper serves as a foundational reference. It is expected to guide future research and development efforts towards building more reliable, faithful, and efficient LVLMs. The accompanying GitHub repository ensures continuous updates, further enhancing its utility.",
      "intriguing_abstract": "The dazzling capabilities of Large Vision-Language Models (LVLMs) are often overshadowed by a critical flaw: hallucination â€“ the insidious misalignment between factual visual content and generated text. This pervasive issue, extending beyond simple object errors to intricate attribute and relation inaccuracies, severely impedes their real-world deployment. Addressing this urgent challenge, this paper presents the *first* comprehensive and structured survey of hallucination in LVLMs.\n\nWe meticulously define the phenomenon, propose a novel taxonomy of symptoms, and systematically categorize current evaluation benchmarks and methodologies. Crucially, we delve into the multifaceted root causes, from data bias and model architecture limitations to profound modality misalignment, and critically review existing mitigation strategies. By clarifying this complex landscape and systematizing fragmented knowledge, our work serves as a foundational reference, poised to guide future research towards developing truly reliable, faithful, and trustworthy multimodal AI systems. This survey is indispensable for researchers striving to unlock the full potential of LVLMs.",
      "keywords": [
        "Large Vision-Language Models (LVLMs)",
        "hallucination",
        "multimodal misalignment",
        "factual visual content",
        "textual generation",
        "hallucination symptoms taxonomy",
        "evaluation benchmarks",
        "root causes of hallucination",
        "mitigation strategies",
        "data bias",
        "vision encoder grounding",
        "comprehensive survey"
      ],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/fc4c380102d6f72657d1ab54dffd6be536bb01c7.pdf",
      "citation_key": "liu2024sn3",
      "metadata": {
        "title": "A Survey on Hallucination in Large Vision-Language Models",
        "authors": [
          "Hanchao Liu",
          "Wenyuan Xue",
          "Yifei Chen",
          "Dapeng Chen",
          "Xiutian Zhao",
          "Ke Wang",
          "Liping Hou",
          "Rong-Zhi Li",
          "Wei Peng"
        ],
        "published_date": "2024",
        "abstract": "Recent development of Large Vision-Language Models (LVLMs) has attracted growing attention within the AI landscape for its practical implementation potential. However, ``hallucination'', or more specifically, the misalignment between factual visual content and corresponding textual generation, poses a significant challenge of utilizing LVLMs. In this comprehensive survey, we dissect LVLM-related hallucinations in an attempt to establish an overview and facilitate future mitigation. Our scrutiny starts with a clarification of the concept of hallucinations in LVLMs, presenting a variety of hallucination symptoms and highlighting the unique challenges inherent in LVLM hallucinations. Subsequently, we outline the benchmarks and methodologies tailored specifically for evaluating hallucinations unique to LVLMs. Additionally, we delve into an investigation of the root causes of these hallucinations, encompassing insights from the training data and model components. We also critically review existing methods for mitigating hallucinations. The open questions and future directions pertaining to hallucinations within LVLMs are discussed to conclude this survey.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/fc4c380102d6f72657d1ab54dffd6be536bb01c7.pdf",
        "venue": "arXiv.org",
        "citationCount": 0,
        "score": 0,
        "summary": "This paper, \"\\cite{liu2024sn3}\", provides a comprehensive survey on the phenomenon of hallucination in Large Vision-Language Models (LVLMs).\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem**: The paper addresses \"hallucination\" in Large Vision-Language Models (LVLMs), defined as the misalignment between factual visual content and the corresponding textual generation.\n    *   **Importance and Challenge**: Hallucination poses a significant impediment to the practical application of LVLMs. Unlike traditional image captioning models, LVLMs' enhanced capabilities for detailed and fluent descriptions diversify and exacerbate hallucination, extending beyond mere object existence to include attribute and relation errors. The multimodal nature of LVLMs introduces unique challenges in detecting hallucinations, inferring their causes, and developing effective mitigation strategies.\n\n*   **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: As a survey, this work systematically reviews and categorizes existing research on LVLM hallucinations. It clarifies the concept, outlines current evaluation benchmarks and methodologies, investigates root causes, and critically reviews existing mitigation methods.\n    *   **Limitations of Previous Solutions (as discussed in the survey)**: The survey highlights that conventional hallucination evaluation methods (e.g., CHAIR for image captioning) struggle with the vast object categories and diverse hallucination types (attributes, relations) present in LVLMs. It also notes that LLM-targeted mitigation methods are often insufficient for LVLMs, necessitating multimodal-specific approaches.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method/Algorithm**: This paper is a survey and does not propose a new technical method or algorithm. Instead, its core \"approach\" is a structured, comprehensive analysis and synthesis of the current state of research on LVLM hallucinations.\n    *   **Novelty/Difference**: The innovation lies in providing the first comprehensive and structured overview of LVLM hallucinations. It offers a clear taxonomy of hallucination symptoms (judgment vs. description; object, attribute, relation), categorizes evaluation methods (non-hallucinatory generation vs. hallucination discrimination), identifies root causes (data bias, model architecture, modality misalignment), and reviews mitigation strategies. This structured analysis helps to clarify a complex and rapidly evolving field.\n\n*   **Key Technical Contributions**\n    *   **Novel Algorithms, Methods, or Techniques**: The paper does not introduce new algorithms but rather *categorizes and explains* existing ones for hallucination evaluation (e.g., handcrafted pipeline methods like CCEval, model-based end-to-end methods like LLM-based evaluation and hallucination data-driven models, and discrimination methods like POPE).\n    *   **System Design or Architectural Innovations**: Not applicable, as it is a survey.\n    *   **Theoretical Insights or Analysis**: Provides a detailed conceptual clarification of LVLM hallucinations, a multifaceted taxonomy of symptoms, and a thorough analysis of their root causes, encompassing training data issues (bias, irrelevance) and model component limitations (vision encoder grounding, modality misalignment, context attention).\n\n*   **Experimental Validation**\n    *   **Experiments Conducted**: The authors of this survey did not conduct new experiments.\n    *   **Key Performance Metrics and Comparison Results**: The paper *reviews* the experimental validation from other research, detailing various benchmarks (e.g., POPE, NOPE, CIEM for discriminative tasks; M-HalDetect, GAVIE, FAITHScore, MMHal-Bench for generative tasks) and metrics used in the field (e.g., Accuracy for object presence, Reward Model Score, FAITHScore, Rating Score for generative faithfulness). It highlights the trend towards more complex metrics for generative benchmarks.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations or Assumptions (of the field, as discussed)**: The survey discusses limitations of existing LVLMs and mitigation methods, such as the resource-intensive nature of training high-resolution visual encoders for hallucination reduction, and the persistent challenge of the significant gap between visual and textual tokens.\n    *   **Scope of Applicability**: The survey's scope is strictly focused on \"hallucination\" within \"Large Vision-Language Models,\" covering its definition, symptoms, evaluation, causes, and mitigation strategies.\n\n*   **Technical Significance**\n    *   **Advance the Technical State-of-the-Art**: This survey significantly advances the technical state-of-the-art by providing a much-needed, structured, and comprehensive overview of a critical and complex problem in LVLMs. It systematizes existing knowledge, clarifies terminology, and identifies key challenges and open questions.\n    *   **Potential Impact on Future Research**: By offering a clear conceptual framework, a review of current evaluation practices, an analysis of root causes, and a summary of mitigation strategies, the paper serves as a foundational reference. It is expected to guide future research and development efforts towards building more reliable, faithful, and efficient LVLMs. The accompanying GitHub repository ensures continuous updates, further enhancing its utility.",
        "keywords": [
          "Large Vision-Language Models (LVLMs)",
          "hallucination",
          "multimodal misalignment",
          "factual visual content",
          "textual generation",
          "hallucination symptoms taxonomy",
          "evaluation benchmarks",
          "root causes of hallucination",
          "mitigation strategies",
          "data bias",
          "vision encoder grounding",
          "comprehensive survey"
        ],
        "paper_type": "based on the abstract and introduction, this paper is a **survey**.\n\nhere's why:\n\n*   **title:** \"a survey on hallucination in large vision-language models\" explicitly states it's a survey.\n*   **abstract:**\n    *   directly states: \"in this comprehensive survey, we dissect lvlm-related hallucinations in an attempt to establish an overview and facilitate future mitigation.\"\n    *   mentions: \"clarification of the concept\", \"presenting a variety of hallucination symptoms\", \"outline the benchmarks and methodologies\", \"delve into an investigation of the root causes\", \"critically review existing methods\", \"open questions and future directions... to conclude this survey.\"\n    *   these phrases are all indicative of reviewing and organizing existing knowledge.\n*   **introduction:** sets the stage for the topic (lvlms and hallucination) that the survey will then comprehensively cover.\n\nthese points align perfectly with the classification criteria for a \"survey\" paper."
      },
      "file_name": "fc4c380102d6f72657d1ab54dffd6be536bb01c7.pdf"
    },
    {
      "success": true,
      "doc_id": "64d2c4984731d8e5ee9cf77ca5547d4f",
      "summary": "Here's a focused summary of the technical paper for a literature review:\n\n### Technical Paper Analysis: \"Detecting and Evaluating Medical Hallucinations in Large Vision Language Models\" \\cite{chen2024hfe}\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Large Vision Language Models (LVLMs) applied in healthcare (e.g., medical VQA, imaging report generation) are susceptible to \"hallucinations\" â€“ generating factually incorrect or irrelevant information. There is a critical lack of dedicated methods and benchmarks for detecting and evaluating these medical hallucinations.\n    *   **Importance and Challenge**: Hallucinations in medical contexts can lead to disastrous misdiagnosis or incorrect decision-making, where the margin for error is minimal. Existing general-domain hallucination detection methods and traditional NLP metrics (e.g., BLEU, METEOR) are insufficient as they fail to capture the multi-layered complexities and clinical severity of medical hallucinations, are often limited to specific object types, or struggle with the long, structured outputs typical of medical reports. Current medical LVLMs also suffer from evaluation on outdated benchmarks, leading to unreliable results due to data leakage.\n\n*   **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: The paper acknowledges the general problem of hallucinations in LVLMs, where even SOTA models exhibit significant hallucinatory text (e.g., 30% in general domains). Existing hallucination detection efforts fall into off-the-shelf tools or training-based models.\n    *   **Limitations of Previous Solutions**:\n        *   **General LVLM Hallucination Methods (e.g., CHAIR, POPE)**: Limited to \"object hallucinations\" in general domains, cannot accommodate the multi-layered complexities of medical hallucinations, and are often constrained to fixed benchmarks or specific question types.\n        *   **Traditional NLP Metrics (e.g., METEOR, BLEU)**: Primarily measure shallow similarities to ground truth, failing to directly reflect factual correctness or distinguish degrees of hallucination.\n        *   **Accuracy**: Evaluates at a coarse semantic level, unable to differentiate hallucination severity.\n        *   **LLM-based Detection (e.g., GPT-API)**: Lack appropriate medical domain knowledge and are typically based on textual evaluation only, without considering image inputs.\n        *   **Medical LVLMs**: Often evaluated on outdated benchmarks, leading to unreliable results due to potential data leakage, and these benchmarks often have short or unstructured answers incompatible with LVLM outputs.\n        *   **Overall**: No systematic investigation or dedicated methods/benchmarks for medical LVLM hallucinations exist.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes a three-pronged solution:\n        1.  **Med-HallMark Benchmark**: The first benchmark specifically for hallucination detection in the medical multimodal domain. It features multi-task hallucination support (Med-VQA, IRG), multifaceted hallucination data (GT, LVLM outputs, fine-grained annotations), and a novel hierarchical hallucination categorization.\n        2.  **MediHall Score**: A new medical evaluative metric designed to assess LVLM hallucinations through a hierarchical scoring system. It considers the severity and type of hallucination, enabling a granular assessment of potential clinical impacts.\n        3.  **MediHallDetector**: A novel Medical LVLM engineered for precise hallucination detection. It employs multitask training to classify hallucination levels based on input image, original prompt, LVLM answer, and ground truth.\n    *   **Novelty/Difference**:\n        *   **Med-HallMark**: Introduces the first dedicated medical hallucination benchmark with a unique hierarchical categorization tailored to clinical impact, moving beyond general object/attribute/relational hallucinations. It also supports diverse medical tasks and question types (conventional, confidence-weakening, counterfactual, image depiction).\n        *   **MediHall Score**: Provides a fine-grained, clinically relevant numerical representation of hallucination severity, which is a significant improvement over coarse accuracy or general NLP metrics. It differentiates between five levels of hallucination (Catastrophic, Critical, Attribute, Prompt-induced, Minor) and correct statements.\n        *   **MediHallDetector**: The first multimodal medical hallucination detection model, specifically designed to leverage both visual and textual information (image, prompt, LVLM answer, ground truth) for fine-grained hallucination classification, built upon the LLaVA architecture with multitask supervised fine-tuning.\n\n*   **Key Technical Contributions**\n    *   **Novel Benchmark**: Introduction of **Med-HallMark**, the first benchmark for hallucination detection in the medical multimodal domain, featuring multi-task support, multifaceted data, and a novel hierarchical categorization of medical hallucinations.\n    *   **Novel Evaluation Metric**: Proposal of **MediHall Score**, a hierarchical scoring system that quantifies hallucination severity based on clinical impact, offering a more nuanced assessment than traditional metrics.\n    *   **Novel Detection Model**: Development of **MediHallDetector**, the first multimodal medical hallucination detection model, utilizing multitask training to precisely classify hallucination types and severity.\n    *   **Hierarchical Hallucination Categorization**: A novel classification system for medical text hallucinations (Catastrophic, Critical, Attribute, Prompt-induced, Minor) based on their severity and impact on clinical diagnosis/decision-making.\n    *   **Benchmark Construction Methodology**: A detailed process for constructing Med-HallMark, including leveraging existing medical datasets, designing diverse question types, using SOTA LVLMs for initial responses, and employing GPT-based tools for data expansion and refinement, all while addressing data leakage and privacy concerns.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted**: Extensive experimental evaluations were performed to establish baselines for popular LVLMs using the Med-HallMark benchmark. The performance of MediHallDetector was assessed, and the effectiveness of MediHall Score was demonstrated through qualitative and quantitative analysis.\n    *   **Key Performance Metrics & Results**: The findings indicate that MediHall Score provides a more nuanced understanding of hallucination impacts compared to traditional metrics. MediHallDetector demonstrated enhanced performance in hallucination detection. (Specific numerical results are not provided in the abstract/introduction but are implied to be in the full paper's experimental section).\n\n*   **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The paper does not explicitly state technical limitations or assumptions within the provided abstract and introduction. However, the reliance on LLaVA as a base for MediHallDetector implies inheriting some characteristics or limitations of that architecture. The manual annotation process for the benchmark, while thorough, can be resource-intensive.\n    *   **Scope of Applicability**: The work is specifically focused on medical multimodal domains (Med-VQA, Imaging Report Generation). While the principles of hierarchical evaluation could be adapted, the benchmark and specific hallucination categories are tailored to medical texts and images.\n\n*   **Technical Significance**\n    *   **Advancement of State-of-the-Art**: This work significantly advances the technical state-of-the-art by providing the first dedicated tools (benchmark, metric, model) for systematically addressing the critical problem of hallucinations in medical LVLMs. It moves beyond general-domain approaches to offer clinically relevant, fine-grained evaluation and detection.\n    *   **Potential Impact on Future Research**: Med-HallMark will serve as a crucial resource for researchers to reliably evaluate and develop more robust and trustworthy LVLMs for healthcare. MediHall Score provides a standardized, clinically meaningful metric for comparison. MediHallDetector offers a foundational model for further advancements in medical hallucination mitigation, ultimately improving the reliability and safety of AI applications in clinical settings.",
      "intriguing_abstract": "The promise of Large Vision Language Models (LVLMs) in healthcare is immense, yet their susceptibility to generating \"medical hallucinations\"â€”factually incorrect or irrelevant informationâ€”poses a catastrophic risk to patient safety and clinical decision-making. Existing general-domain detection methods and traditional NLP metrics are critically inadequate, failing to capture the multi-layered complexities and severe clinical impact of these errors.\n\nWe address this urgent gap by introducing a pioneering three-pronged solution. First, **Med-HallMark**, the first dedicated multimodal benchmark for medical hallucination detection, featuring a novel hierarchical categorization of hallucinations based on clinical severity (e.g., Catastrophic, Critical) across tasks like Med-VQA and Imaging Report Generation. Second, the **MediHall Score**, a new hierarchical evaluation metric that quantifies hallucination severity with unprecedented granularity. Third, **MediHallDetector**, the first multimodal medical hallucination detection model, engineered for precise classification by leveraging both visual and textual inputs. Our work provides indispensable tools for systematically evaluating and mitigating medical hallucinations in LVLMs, paving the way for safer, more reliable, and trustworthy AI applications in clinical settings.",
      "keywords": [
        "Medical hallucinations",
        "Large Vision Language Models (LVLMs)",
        "hallucination detection and evaluation",
        "Med-HallMark benchmark",
        "MediHall Score",
        "MediHallDetector",
        "hierarchical hallucination categorization",
        "clinical impact",
        "medical multimodal domain",
        "multitask training",
        "trustworthy AI in healthcare",
        "medical VQA",
        "imaging report generation"
      ],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/c910c8f715d8231ed824caff13952d6946de1e59.pdf",
      "citation_key": "chen2024hfe",
      "metadata": {
        "title": "Detecting and Evaluating Medical Hallucinations in Large Vision Language Models",
        "authors": [
          "Jiawei Chen",
          "Dingkang Yang",
          "Tong Wu",
          "Yue Jiang",
          "Xiaolu Hou",
          "Mingcheng Li",
          "Shunli Wang",
          "Dongling Xiao",
          "Ke Li",
          "Lihua Zhang"
        ],
        "published_date": "2024",
        "abstract": "Large Vision Language Models (LVLMs) are increasingly integral to healthcare applications, including medical visual question answering and imaging report generation. While these models inherit the robust capabilities of foundational Large Language Models (LLMs), they also inherit susceptibility to hallucinations-a significant concern in high-stakes medical contexts where the margin for error is minimal. However, currently, there are no dedicated methods or benchmarks for hallucination detection and evaluation in the medical field. To bridge this gap, we introduce Med-HallMark, the first benchmark specifically designed for hallucination detection and evaluation within the medical multimodal domain. This benchmark provides multi-tasking hallucination support, multifaceted hallucination data, and hierarchical hallucination categorization. Furthermore, we propose the MediHall Score, a new medical evaluative metric designed to assess LVLMs' hallucinations through a hierarchical scoring system that considers the severity and type of hallucination, thereby enabling a granular assessment of potential clinical impacts. We also present MediHallDetector, a novel Medical LVLM engineered for precise hallucination detection, which employs multitask training for hallucination detection. Through extensive experimental evaluations, we establish baselines for popular LVLMs using our benchmark. The findings indicate that MediHall Score provides a more nuanced understanding of hallucination impacts compared to traditional metrics and demonstrate the enhanced performance of MediHallDetector. We hope this work can significantly improve the reliability of LVLMs in medical applications. All resources of this work will be released soon.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/c910c8f715d8231ed824caff13952d6946de1e59.pdf",
        "venue": "arXiv.org",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the technical paper for a literature review:\n\n### Technical Paper Analysis: \"Detecting and Evaluating Medical Hallucinations in Large Vision Language Models\" \\cite{chen2024hfe}\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Large Vision Language Models (LVLMs) applied in healthcare (e.g., medical VQA, imaging report generation) are susceptible to \"hallucinations\" â€“ generating factually incorrect or irrelevant information. There is a critical lack of dedicated methods and benchmarks for detecting and evaluating these medical hallucinations.\n    *   **Importance and Challenge**: Hallucinations in medical contexts can lead to disastrous misdiagnosis or incorrect decision-making, where the margin for error is minimal. Existing general-domain hallucination detection methods and traditional NLP metrics (e.g., BLEU, METEOR) are insufficient as they fail to capture the multi-layered complexities and clinical severity of medical hallucinations, are often limited to specific object types, or struggle with the long, structured outputs typical of medical reports. Current medical LVLMs also suffer from evaluation on outdated benchmarks, leading to unreliable results due to data leakage.\n\n*   **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: The paper acknowledges the general problem of hallucinations in LVLMs, where even SOTA models exhibit significant hallucinatory text (e.g., 30% in general domains). Existing hallucination detection efforts fall into off-the-shelf tools or training-based models.\n    *   **Limitations of Previous Solutions**:\n        *   **General LVLM Hallucination Methods (e.g., CHAIR, POPE)**: Limited to \"object hallucinations\" in general domains, cannot accommodate the multi-layered complexities of medical hallucinations, and are often constrained to fixed benchmarks or specific question types.\n        *   **Traditional NLP Metrics (e.g., METEOR, BLEU)**: Primarily measure shallow similarities to ground truth, failing to directly reflect factual correctness or distinguish degrees of hallucination.\n        *   **Accuracy**: Evaluates at a coarse semantic level, unable to differentiate hallucination severity.\n        *   **LLM-based Detection (e.g., GPT-API)**: Lack appropriate medical domain knowledge and are typically based on textual evaluation only, without considering image inputs.\n        *   **Medical LVLMs**: Often evaluated on outdated benchmarks, leading to unreliable results due to potential data leakage, and these benchmarks often have short or unstructured answers incompatible with LVLM outputs.\n        *   **Overall**: No systematic investigation or dedicated methods/benchmarks for medical LVLM hallucinations exist.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes a three-pronged solution:\n        1.  **Med-HallMark Benchmark**: The first benchmark specifically for hallucination detection in the medical multimodal domain. It features multi-task hallucination support (Med-VQA, IRG), multifaceted hallucination data (GT, LVLM outputs, fine-grained annotations), and a novel hierarchical hallucination categorization.\n        2.  **MediHall Score**: A new medical evaluative metric designed to assess LVLM hallucinations through a hierarchical scoring system. It considers the severity and type of hallucination, enabling a granular assessment of potential clinical impacts.\n        3.  **MediHallDetector**: A novel Medical LVLM engineered for precise hallucination detection. It employs multitask training to classify hallucination levels based on input image, original prompt, LVLM answer, and ground truth.\n    *   **Novelty/Difference**:\n        *   **Med-HallMark**: Introduces the first dedicated medical hallucination benchmark with a unique hierarchical categorization tailored to clinical impact, moving beyond general object/attribute/relational hallucinations. It also supports diverse medical tasks and question types (conventional, confidence-weakening, counterfactual, image depiction).\n        *   **MediHall Score**: Provides a fine-grained, clinically relevant numerical representation of hallucination severity, which is a significant improvement over coarse accuracy or general NLP metrics. It differentiates between five levels of hallucination (Catastrophic, Critical, Attribute, Prompt-induced, Minor) and correct statements.\n        *   **MediHallDetector**: The first multimodal medical hallucination detection model, specifically designed to leverage both visual and textual information (image, prompt, LVLM answer, ground truth) for fine-grained hallucination classification, built upon the LLaVA architecture with multitask supervised fine-tuning.\n\n*   **Key Technical Contributions**\n    *   **Novel Benchmark**: Introduction of **Med-HallMark**, the first benchmark for hallucination detection in the medical multimodal domain, featuring multi-task support, multifaceted data, and a novel hierarchical categorization of medical hallucinations.\n    *   **Novel Evaluation Metric**: Proposal of **MediHall Score**, a hierarchical scoring system that quantifies hallucination severity based on clinical impact, offering a more nuanced assessment than traditional metrics.\n    *   **Novel Detection Model**: Development of **MediHallDetector**, the first multimodal medical hallucination detection model, utilizing multitask training to precisely classify hallucination types and severity.\n    *   **Hierarchical Hallucination Categorization**: A novel classification system for medical text hallucinations (Catastrophic, Critical, Attribute, Prompt-induced, Minor) based on their severity and impact on clinical diagnosis/decision-making.\n    *   **Benchmark Construction Methodology**: A detailed process for constructing Med-HallMark, including leveraging existing medical datasets, designing diverse question types, using SOTA LVLMs for initial responses, and employing GPT-based tools for data expansion and refinement, all while addressing data leakage and privacy concerns.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted**: Extensive experimental evaluations were performed to establish baselines for popular LVLMs using the Med-HallMark benchmark. The performance of MediHallDetector was assessed, and the effectiveness of MediHall Score was demonstrated through qualitative and quantitative analysis.\n    *   **Key Performance Metrics & Results**: The findings indicate that MediHall Score provides a more nuanced understanding of hallucination impacts compared to traditional metrics. MediHallDetector demonstrated enhanced performance in hallucination detection. (Specific numerical results are not provided in the abstract/introduction but are implied to be in the full paper's experimental section).\n\n*   **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The paper does not explicitly state technical limitations or assumptions within the provided abstract and introduction. However, the reliance on LLaVA as a base for MediHallDetector implies inheriting some characteristics or limitations of that architecture. The manual annotation process for the benchmark, while thorough, can be resource-intensive.\n    *   **Scope of Applicability**: The work is specifically focused on medical multimodal domains (Med-VQA, Imaging Report Generation). While the principles of hierarchical evaluation could be adapted, the benchmark and specific hallucination categories are tailored to medical texts and images.\n\n*   **Technical Significance**\n    *   **Advancement of State-of-the-Art**: This work significantly advances the technical state-of-the-art by providing the first dedicated tools (benchmark, metric, model) for systematically addressing the critical problem of hallucinations in medical LVLMs. It moves beyond general-domain approaches to offer clinically relevant, fine-grained evaluation and detection.\n    *   **Potential Impact on Future Research**: Med-HallMark will serve as a crucial resource for researchers to reliably evaluate and develop more robust and trustworthy LVLMs for healthcare. MediHall Score provides a standardized, clinically meaningful metric for comparison. MediHallDetector offers a foundational model for further advancements in medical hallucination mitigation, ultimately improving the reliability and safety of AI applications in clinical settings.",
        "keywords": [
          "Medical hallucinations",
          "Large Vision Language Models (LVLMs)",
          "hallucination detection and evaluation",
          "Med-HallMark benchmark",
          "MediHall Score",
          "MediHallDetector",
          "hierarchical hallucination categorization",
          "clinical impact",
          "medical multimodal domain",
          "multitask training",
          "trustworthy AI in healthcare",
          "medical VQA",
          "imaging report generation"
        ],
        "paper_type": "based on the abstract and introduction, this paper should be classified as **technical**.\n\nhere's why:\n\n*   **abstract mentions:**\n    *   \"we introduce med-hallmark, the first benchmark specifically designed...\" (presents a new system/tool)\n    *   \"we propose the medihall score, a new medical evaluative metric...\" (presents a new method/algorithm)\n    *   \"we also present medihalldetector, a novel medical lvlm engineered for precise hallucination detection...\" (presents a new system/method)\n    *   \"through extensive experimental evaluations, we establish baselines...\" (indicates empirical validation of the proposed technical solutions)\n    *   \"the findings indicate that medihall score provides a more nuanced understanding... and demonstrate the enhanced performance of medihalldetector.\" (shows the effectiveness of the proposed technical solutions)\n\n*   **introduction discusses:**\n    *   the problem of medical hallucinations in lvlms and the lack of \"a specific method and benchmark for detecting hallucinations,\" which the paper aims to address by proposing new solutions.\n\nthe core contribution is the development and presentation of novel technical artifacts (a benchmark, a metric, and a detection model), followed by their empirical evaluation. this aligns perfectly with the \"technical\" classification criteria."
      },
      "file_name": "c910c8f715d8231ed824caff13952d6946de1e59.pdf"
    },
    {
      "success": true,
      "doc_id": "e67087c51b60ece066a68619790777ad",
      "summary": "Here's a focused summary of the paper for a literature review:\n\n### Focused Summary for Literature Review: Reference-free Hallucination Detection for Large Vision-Language Models\n\n**1. Research Problem & Motivation**\n*   **Specific Technical Problem:** Large Vision-Language Models (LVLMs) are prone to generating \"hallucinations\" â€“ descriptions of objects or events not present in the input image, or non-factual information \\cite{li2024hdc}.\n*   **Importance and Challenge:** Hallucinations undermine the reliability and trustworthiness of LVLMs, posing significant challenges for their practical application. Most existing hallucination detection methods are \"reference-based,\" relying on external tools or models (e.g., segmentation tools, visual entailment models), which increases computational cost and limits their applicability due to dependencies on these external components \\cite{li2024hdc}. The challenge is to develop efficient and practical detection methods that do not require external references.\n\n**2. Related Work & Positioning**\n*   **Relation to Existing Approaches:** Previous methods like POPE \\cite{li2024hdc} and Faithscore \\cite{li2024hdc} are reference-based, using external models for validation. This paper positions itself by exploring \"reference-free\" approaches, which utilize the LVLM's internal knowledge and states, similar to methods previously studied for Large Language Models (LLMs) \\cite{li2024hdc}.\n*   **Limitations of Previous Solutions:** Reference-based methods are computationally expensive and constrained by the capabilities and availability of external tools, complicating their practical deployment \\cite{li2024hdc}. The potential of reference-free methods for LVLMs, despite their promise in LLMs, has not been systematically studied due to the complexity of multimodal data \\cite{li2024hdc}.\n\n**3. Technical Approach & Innovation**\n*   **Core Technical Method:** The paper conducts an extensive exploratory study on three categories of reference-free hallucination detection techniques for LVLMs \\cite{li2024hdc}:\n    *   **Uncertainty-based methods:** These hypothesize that LVLMs are more uncertain about hallucinated information. They aggregate token-level uncertainty (e.g., probability, entropy) to measure sentence-level and passage-level uncertainty using metrics like AvgProb, AvgEnt, MaxProb, and MaxEnt \\cite{li2024hdc}.\n    *   **Consistency-based methods:** These assess the consistency of generated text, focusing on four variants of self-consistency: BERTScore, Question Answering (QA), Unigram, and Natural Language Inference (NLI) \\cite{li2024hdc}.\n    *   **Supervised Uncertainty Quantification (SUQ):** This method trains a classifier (a \"probe\") on labeled examples to predict the truthfulness of a statement by analyzing the hidden layer activations of the LVLM during text generation or reading \\cite{li2024hdc}.\n*   **Novelty/Difference:** The primary innovation is the systematic and comprehensive evaluation of these diverse reference-free approaches specifically in the context of LVLMs, which had not been thoroughly investigated before due to multimodal data complexities \\cite{li2024hdc}. The study compares their effectiveness across different LVLM architectures, tasks (Yes-or-No, Open-ended), and levels of granularity (sentence-level, passage-level) \\cite{li2024hdc}.\n\n**4. Key Technical Contributions**\n*   **Novel Algorithms/Methods/Techniques:** While the individual methods (uncertainty, consistency, SUQ) are known, their systematic application and comparative analysis for LVLM hallucination detection is a key contribution \\cite{li2024hdc}.\n*   **System Design/Architectural Innovations:** The paper doesn't propose new architectures but evaluates how existing internal states (probabilities, entropies, hidden embeddings) of LVLMs can be leveraged for hallucination detection without external tools \\cite{li2024hdc}.\n*   **Theoretical Insights/Analysis:** The study provides empirical insights into which types of reference-free methods are most effective for different tasks and LVLM types, demonstrating the viability of internal model states for detecting non-factual responses \\cite{li2024hdc}.\n*   **Dataset Contribution:** The paper introduces the Image-Hallucination Annotation Dataset (IHAD), a manually annotated, sentence-level dataset created by prompting LLaVA-v1.5-7b, which can be used for future research in this area \\cite{li2024hdc}.\n\n**5. Experimental Validation**\n*   **Experiments Conducted:**\n    *   Evaluated uncertainty-based, consistency-based, and SUQ methods.\n    *   Tested on five representative LVLMs: MiniGPT-4v, LLaVA-v1.5-7b, LLaVA-v1.6-vicuna-7b, LLaVA-v1.6-mistral-7b, and LLaVA-v1.6-vicuna-13b \\cite{li2024hdc}.\n    *   Experiments covered two types of tasks: Yes-or-No tasks (using POPE and GQA datasets) and Open-ended tasks (using M-HalDetect and the newly introduced IHAD dataset) \\cite{li2024hdc}.\n    *   Analysis included sentence-level and passage-level hallucination detection \\cite{li2024hdc}.\n    *   Investigated robustness of SUQ, impact of image clarity (using Gaussian blur), and impact of data source (self-generated vs. hand-crafted) \\cite{li2024hdc}.\n*   **Key Performance Metrics and Comparison Results:**\n    *   **Metric:** Area Under the Precision-Recall Curve (AUC-PR) \\cite{li2024hdc}.\n    *   **Overall Best:** Supervised Uncertainty Quantification (SUQ) consistently achieved the best performance across different settings and LVLMs \\cite{li2024hdc}. For Yes-or-No tasks, SUQ outperformed AvgProb by about 4.5% \\cite{li2024hdc}. For sentence-level Open-ended tasks, SUQ was approximately 15% better than consistency-based methods \\cite{li2024hdc}.\n    *   **Consistency vs. Uncertainty:** Consistency-based methods generally outperformed uncertainty-based methods for Open-ended tasks, with NLI being the most effective among them \\cite{li2024hdc}.\n    *   **Uncertainty Methods:** AvgProb showed strong performance for Yes-or-No tasks, nearly doubling the random baseline \\cite{li2024hdc}.\n    *   **Robustness:** SUQ demonstrated robustness, with a classifier trained on one dataset performing comparably on others \\cite{li2024hdc}.\n    *   **Image Clarity:** Reduced image clarity (Gaussian blur) significantly decreased the effectiveness of all tested methods \\cite{li2024hdc}.\n    *   **Data Source:** Reference-free methods were found to be insensitive to whether the data was self-generated or hand-crafted \\cite{li2024hdc}.\n\n**6. Limitations & Scope**\n*   **Technical Limitations/Assumptions:**\n    *   SUQ, while effective, relies on sufficient training data, which might limit its applicability in data-scarce scenarios \\cite{li2024hdc}.\n    *   SUQ was found to be ineffective in detecting subtle, manually crafted hallucinations where the difference in hidden states is negligible \\cite{li2024hdc}.\n    *   Consistency-based methods, while effective, require heavy computation, making them less suitable for passage-level detection \\cite{li2024hdc}.\n*   **Scope of Applicability:** The study primarily focuses on \"white-box\" LVLMs, where internal states are accessible \\cite{li2024hdc}. The findings are most directly applicable to scenarios where model internals can be probed. The study did not classify different types of hallucinations (e.g., object existence, attribute errors) due to resource limitations \\cite{li2024hdc}.\n\n**7. Technical Significance**\n*   **Advancement of State-of-the-Art:** This paper significantly advances the understanding of reference-free hallucination detection in LVLMs by providing the first systematic and comprehensive empirical evaluation of various internal-state-based methods \\cite{li2024hdc}. It demonstrates that effective hallucination detection is possible without external tools, paving the way for more practical and efficient solutions.\n*   **Potential Impact on Future Research:**\n    *   Highlights SUQ as a promising direction for supervised hallucination detection, encouraging further research into its training data requirements and robustness to subtle hallucinations \\cite{li2024hdc}.\n    *   Suggests that improving image clarity is a crucial strategy for boosting detection effectiveness \\cite{li2024hdc}.\n    *   Opens avenues for exploring more complex proxy models for black-box LVLMs, extending the applicability of reference-free methods \\cite{li2024hdc}.\n    *   The contributed IHAD dataset provides a valuable resource for future research and benchmarking in this domain \\cite{li2024hdc}.",
      "intriguing_abstract": "The pervasive challenge of \"hallucinations\" in Large Vision-Language Models (LVLMs) severely undermines their trustworthiness, yet current detection methods are often computationally expensive and reliant on external references. This paper pioneers the first systematic exploration of *reference-free* hallucination detection for LVLMs, leveraging only their internal states to identify non-factual generations.\n\nWe comprehensively evaluate uncertainty-based, consistency-based, and Supervised Uncertainty Quantification (SUQ) methods across diverse LVLMs and tasks. Our findings reveal that SUQ consistently achieves superior performance, significantly outperforming other approaches by up to 15% AUC-PR in open-ended tasks, by analyzing hidden layer activations. We also introduce the Image-Hallucination Annotation Dataset (IHAD) to foster future research. This seminal work demonstrates the profound potential of internal model states for robust hallucination detection, paving the way for more efficient, practical, and reliable LVLMs. By shifting away from external dependencies, we advance the state-of-the-art, enhancing the trustworthiness of multimodal AI and guiding future research into explainable and verifiable model outputs.",
      "keywords": [
        "Large Vision-Language Models (LVLMs)",
        "Hallucination detection",
        "Reference-free methods",
        "Uncertainty-based methods",
        "Consistency-based methods",
        "Supervised Uncertainty Quantification (SUQ)",
        "Internal model states",
        "Systematic empirical evaluation",
        "Image-Hallucination Annotation Dataset (IHAD)",
        "Reliability and trustworthiness",
        "Multimodal data complexities",
        "AUC-PR",
        "Image clarity impact"
      ],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/0ba76fbb7a4a2e6a221b4c31321e9846eca2fe92.pdf",
      "citation_key": "li2024hdc",
      "metadata": {
        "title": "Reference-free Hallucination Detection for Large Vision-Language Models",
        "authors": [
          "Qing Li",
          "Chenyang Lyu",
          "Jiahui Geng",
          "Derui Zhu",
          "Maxim Panov",
          "Fakhri Karray"
        ],
        "published_date": "2024",
        "abstract": "Large vision-language models (LVLMs) have made significant progress in recent years. While LVLMs exhibit excellent ability in language understanding, question answering, and conversations of visual inputs, they are prone to producing hallucinations. While several methods are proposed to evaluate the hallucinations in LVLMs, most are reference-based and depend on external tools, which complicates their practical application. To assess the viability of alternative methods, it is critical to understand whether the reference-free approaches, which do not rely on any external tools, can efficiently detect hallucinations. Therefore, we initiate an exploratory study to demonstrate the effectiveness of different reference-free solutions in detecting hallucinations in LVLMs. In particular, we conduct an extensive study on three kinds of techniques: uncertainty-based, consistency-based, and supervised uncertainty quantification methods on four representative LVLMs across two different tasks. The empirical results show that the reference-free approaches are capable of effectively detecting non-factual responses in LVLMs, with the supervised uncertainty quantification method outperforming the others, achieving the best performance across different settings.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/0ba76fbb7a4a2e6a221b4c31321e9846eca2fe92.pdf",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the paper for a literature review:\n\n### Focused Summary for Literature Review: Reference-free Hallucination Detection for Large Vision-Language Models\n\n**1. Research Problem & Motivation**\n*   **Specific Technical Problem:** Large Vision-Language Models (LVLMs) are prone to generating \"hallucinations\" â€“ descriptions of objects or events not present in the input image, or non-factual information \\cite{li2024hdc}.\n*   **Importance and Challenge:** Hallucinations undermine the reliability and trustworthiness of LVLMs, posing significant challenges for their practical application. Most existing hallucination detection methods are \"reference-based,\" relying on external tools or models (e.g., segmentation tools, visual entailment models), which increases computational cost and limits their applicability due to dependencies on these external components \\cite{li2024hdc}. The challenge is to develop efficient and practical detection methods that do not require external references.\n\n**2. Related Work & Positioning**\n*   **Relation to Existing Approaches:** Previous methods like POPE \\cite{li2024hdc} and Faithscore \\cite{li2024hdc} are reference-based, using external models for validation. This paper positions itself by exploring \"reference-free\" approaches, which utilize the LVLM's internal knowledge and states, similar to methods previously studied for Large Language Models (LLMs) \\cite{li2024hdc}.\n*   **Limitations of Previous Solutions:** Reference-based methods are computationally expensive and constrained by the capabilities and availability of external tools, complicating their practical deployment \\cite{li2024hdc}. The potential of reference-free methods for LVLMs, despite their promise in LLMs, has not been systematically studied due to the complexity of multimodal data \\cite{li2024hdc}.\n\n**3. Technical Approach & Innovation**\n*   **Core Technical Method:** The paper conducts an extensive exploratory study on three categories of reference-free hallucination detection techniques for LVLMs \\cite{li2024hdc}:\n    *   **Uncertainty-based methods:** These hypothesize that LVLMs are more uncertain about hallucinated information. They aggregate token-level uncertainty (e.g., probability, entropy) to measure sentence-level and passage-level uncertainty using metrics like AvgProb, AvgEnt, MaxProb, and MaxEnt \\cite{li2024hdc}.\n    *   **Consistency-based methods:** These assess the consistency of generated text, focusing on four variants of self-consistency: BERTScore, Question Answering (QA), Unigram, and Natural Language Inference (NLI) \\cite{li2024hdc}.\n    *   **Supervised Uncertainty Quantification (SUQ):** This method trains a classifier (a \"probe\") on labeled examples to predict the truthfulness of a statement by analyzing the hidden layer activations of the LVLM during text generation or reading \\cite{li2024hdc}.\n*   **Novelty/Difference:** The primary innovation is the systematic and comprehensive evaluation of these diverse reference-free approaches specifically in the context of LVLMs, which had not been thoroughly investigated before due to multimodal data complexities \\cite{li2024hdc}. The study compares their effectiveness across different LVLM architectures, tasks (Yes-or-No, Open-ended), and levels of granularity (sentence-level, passage-level) \\cite{li2024hdc}.\n\n**4. Key Technical Contributions**\n*   **Novel Algorithms/Methods/Techniques:** While the individual methods (uncertainty, consistency, SUQ) are known, their systematic application and comparative analysis for LVLM hallucination detection is a key contribution \\cite{li2024hdc}.\n*   **System Design/Architectural Innovations:** The paper doesn't propose new architectures but evaluates how existing internal states (probabilities, entropies, hidden embeddings) of LVLMs can be leveraged for hallucination detection without external tools \\cite{li2024hdc}.\n*   **Theoretical Insights/Analysis:** The study provides empirical insights into which types of reference-free methods are most effective for different tasks and LVLM types, demonstrating the viability of internal model states for detecting non-factual responses \\cite{li2024hdc}.\n*   **Dataset Contribution:** The paper introduces the Image-Hallucination Annotation Dataset (IHAD), a manually annotated, sentence-level dataset created by prompting LLaVA-v1.5-7b, which can be used for future research in this area \\cite{li2024hdc}.\n\n**5. Experimental Validation**\n*   **Experiments Conducted:**\n    *   Evaluated uncertainty-based, consistency-based, and SUQ methods.\n    *   Tested on five representative LVLMs: MiniGPT-4v, LLaVA-v1.5-7b, LLaVA-v1.6-vicuna-7b, LLaVA-v1.6-mistral-7b, and LLaVA-v1.6-vicuna-13b \\cite{li2024hdc}.\n    *   Experiments covered two types of tasks: Yes-or-No tasks (using POPE and GQA datasets) and Open-ended tasks (using M-HalDetect and the newly introduced IHAD dataset) \\cite{li2024hdc}.\n    *   Analysis included sentence-level and passage-level hallucination detection \\cite{li2024hdc}.\n    *   Investigated robustness of SUQ, impact of image clarity (using Gaussian blur), and impact of data source (self-generated vs. hand-crafted) \\cite{li2024hdc}.\n*   **Key Performance Metrics and Comparison Results:**\n    *   **Metric:** Area Under the Precision-Recall Curve (AUC-PR) \\cite{li2024hdc}.\n    *   **Overall Best:** Supervised Uncertainty Quantification (SUQ) consistently achieved the best performance across different settings and LVLMs \\cite{li2024hdc}. For Yes-or-No tasks, SUQ outperformed AvgProb by about 4.5% \\cite{li2024hdc}. For sentence-level Open-ended tasks, SUQ was approximately 15% better than consistency-based methods \\cite{li2024hdc}.\n    *   **Consistency vs. Uncertainty:** Consistency-based methods generally outperformed uncertainty-based methods for Open-ended tasks, with NLI being the most effective among them \\cite{li2024hdc}.\n    *   **Uncertainty Methods:** AvgProb showed strong performance for Yes-or-No tasks, nearly doubling the random baseline \\cite{li2024hdc}.\n    *   **Robustness:** SUQ demonstrated robustness, with a classifier trained on one dataset performing comparably on others \\cite{li2024hdc}.\n    *   **Image Clarity:** Reduced image clarity (Gaussian blur) significantly decreased the effectiveness of all tested methods \\cite{li2024hdc}.\n    *   **Data Source:** Reference-free methods were found to be insensitive to whether the data was self-generated or hand-crafted \\cite{li2024hdc}.\n\n**6. Limitations & Scope**\n*   **Technical Limitations/Assumptions:**\n    *   SUQ, while effective, relies on sufficient training data, which might limit its applicability in data-scarce scenarios \\cite{li2024hdc}.\n    *   SUQ was found to be ineffective in detecting subtle, manually crafted hallucinations where the difference in hidden states is negligible \\cite{li2024hdc}.\n    *   Consistency-based methods, while effective, require heavy computation, making them less suitable for passage-level detection \\cite{li2024hdc}.\n*   **Scope of Applicability:** The study primarily focuses on \"white-box\" LVLMs, where internal states are accessible \\cite{li2024hdc}. The findings are most directly applicable to scenarios where model internals can be probed. The study did not classify different types of hallucinations (e.g., object existence, attribute errors) due to resource limitations \\cite{li2024hdc}.\n\n**7. Technical Significance**\n*   **Advancement of State-of-the-Art:** This paper significantly advances the understanding of reference-free hallucination detection in LVLMs by providing the first systematic and comprehensive empirical evaluation of various internal-state-based methods \\cite{li2024hdc}. It demonstrates that effective hallucination detection is possible without external tools, paving the way for more practical and efficient solutions.\n*   **Potential Impact on Future Research:**\n    *   Highlights SUQ as a promising direction for supervised hallucination detection, encouraging further research into its training data requirements and robustness to subtle hallucinations \\cite{li2024hdc}.\n    *   Suggests that improving image clarity is a crucial strategy for boosting detection effectiveness \\cite{li2024hdc}.\n    *   Opens avenues for exploring more complex proxy models for black-box LVLMs, extending the applicability of reference-free methods \\cite{li2024hdc}.\n    *   The contributed IHAD dataset provides a valuable resource for future research and benchmarking in this domain \\cite{li2024hdc}.",
        "keywords": [
          "Large Vision-Language Models (LVLMs)",
          "Hallucination detection",
          "Reference-free methods",
          "Uncertainty-based methods",
          "Consistency-based methods",
          "Supervised Uncertainty Quantification (SUQ)",
          "Internal model states",
          "Systematic empirical evaluation",
          "Image-Hallucination Annotation Dataset (IHAD)",
          "Reliability and trustworthiness",
          "Multimodal data complexities",
          "AUC-PR",
          "Image clarity impact"
        ],
        "paper_type": "based on the abstract and introduction:\n\nthe abstract explicitly states:\n*   \"we initiate an **exploratory study** to demonstrate the effectiveness of different reference-free solutions...\"\n*   \"we **conduct an extensive study** on three kinds of techniques... on four representative lvlms across two different tasks.\"\n*   \"the **empirical results** show that the reference-free approaches are capable of effectively detecting non-factual responses...\"\n\nthese phrases directly match the criteria for an **empirical** paper, which involves data-driven studies with statistical analysis and findings. the paper describes a methodology (study on techniques, models, tasks) and presents results/findings.\n\n**classification: empirical**"
      },
      "file_name": "0ba76fbb7a4a2e6a221b4c31321e9846eca2fe92.pdf"
    },
    {
      "success": true,
      "doc_id": "1367e6db871d906d74f9d3cf93292a91",
      "summary": "Here's a focused summary of the technical paper for a literature review:\n\n### Analyzing \"Evaluating the Quality of Hallucination Benchmarks for Large Vision-Language Models\" \\cite{yan2024ux8}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Large Vision-Language Models (LVLMs) are significantly plagued by hallucination, where generated textual content is inconsistent with visual inputs \\cite{yan2024ux8}. Existing benchmarks designed to evaluate this hallucination suffer from quality issues.\n    *   **Importance and Challenge**: Hallucination can lead to harmful consequences, especially when users over-rely on models \\cite{yan2024ux8}. The quality issues in existing benchmarks include inconsistent evaluation results under repeated tests, misalignment with human evaluation, and limited coverage of hallucination types, which casts doubt on the trustworthiness of their evaluation results and hinders reliable progress in mitigating hallucination \\cite{yan2024ux8}.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: Previous studies have proposed various hallucination benchmarks for LVLMs, categorized into closed-ended tasks (e.g., yes-or-no questions like POPE, AMBER-d, HallusionBench) and open-ended tasks (e.g., image captioning, free-form VQA like AMBER-g, OpenCHAIR, MMHal, GAVIE) \\cite{yan2024ux8}.\n    *   **Limitations of Previous Solutions**:\n        *   **Closed-ended benchmarks**: While offering efficient automated evaluation, they exhibit poor reliability due to LVLM susceptibility to response biases (e.g., acquiescence, dissent, position bias) \\cite{yan2024ux8}.\n        *   **Open-ended benchmarks**: Although avoiding response bias, they primarily suffer from validity issues, showing significant misalignment with human evaluation \\cite{yan2024ux8}.\n        *   **Score-based metrics in open-ended tasks**: Metrics that use external LLMs (like GPT) to assign a specific hallucination score are often too challenging for current LLMs, leading to inconsistent scores across repeated/parallel tests and inaccurate scores misaligned with human judgment \\cite{yan2024ux8}.\n        *   **Limited coverage**: Many existing benchmarks do not comprehensively cover different types of hallucination \\cite{yan2024ux8}.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**:\n        *   **Hallucination benchmark Quality Measurement framework (HQM)**: Inspired by psychometrics, HQM systematically assesses benchmark quality based on reliability and validity \\cite{yan2024ux8}.\n            *   **Reliability**: Measured by *test-retest reliability* (Pearson correlation between original and retest results with different random seeds) and *parallel-forms reliability* (Pearson correlation between original and parallel-form results using equivalent prompts) \\cite{yan2024ux8}.\n            *   **Validity**: Assessed by *criterion validity* (Pearson correlation between automated benchmark results and human evaluation results) and *coverage of hallucination types* (examining inclusion of 8 predefined types: attribute, action, counting, environment, relation, comparison, OCR, existence) \\cite{yan2024ux8}.\n        *   **High-Quality Hallucination Benchmark (HQH)**: A new benchmark constructed using open-ended free-form VQA tasks to achieve a balance between reliability and validity \\cite{yan2024ux8}.\n            *   **Data**: Collects images from Visual Genome and designs image-instruction pairs covering comprehensive hallucination types, with manual review for quality \\cite{yan2024ux8}.\n            *   **Evaluation Metric**: Employs a simplified binary classification process where an external LLM (GPT) only determines if a response is hallucinated (yes/no), rather than assigning a score. This allows for the computation of a \"hallucination rate\" \\cite{yan2024ux8}.\n    *   **Novelty/Difference**:\n        *   HQM is the first framework to systematically quantify the quality of LVLM hallucination benchmarks using psychometric principles, providing a principled way to evaluate benchmark trustworthiness \\cite{yan2024ux8}.\n        *   HQH's simplified binary hallucination detection metric is a novel approach designed to significantly reduce the evaluation gap between external LLMs and human evaluators, thereby enhancing both reliability and validity compared to prior score-based methods \\cite{yan2024ux8}.\n        *   HQH offers comprehensive coverage of 8 distinct hallucination types, addressing the limited scope of many existing benchmarks \\cite{yan2024ux8}.\n\n4.  **Key Technical Contributions**\n    *   **Novel Framework**: Proposed the Hallucination benchmark Quality Measurement framework (HQM) for LVLMs, which leverages various indicators to assess benchmark reliability (test-retest, parallel-forms) and validity (criterion validity, coverage of hallucination types) \\cite{yan2024ux8}.\n    *   **High-Quality Benchmark**: Constructed a new High-Quality Hallucination Benchmark (HQH) that demonstrates superior reliability and validity under the HQM framework, featuring a simplified binary hallucination detection metric and comprehensive coverage of 8 hallucination types \\cite{yan2024ux8}.\n    *   **Extensive Evaluation**: Conducted a large-scale evaluation of over 10 representative LVLMs, including state-of-the-art models like GPT-4o and Gemini-1.5-Pro, using HQH to provide an in-depth analysis of hallucination issues \\cite{yan2024ux8}.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**:\n        *   Evaluated the quality of 6 representative existing hallucination benchmarks (POPE, AMBER-d, HallusionBench, AMBER-g, OpenCHAIR, MMHal, GAVIE) using the proposed HQM framework \\cite{yan2024ux8}.\n        *   Assessed the reliability and validity of the newly constructed HQH benchmark using HQM \\cite{yan2024ux8}.\n        *   Performed a large-scale evaluation of 9 mainstream open-source LVLMs (20 checkpoints) and several powerful closed-source models (e.g., GPT-4o, Gemini-1.5-Pro) on HQH and other benchmarks \\cite{yan2024ux8}.\n    *   **Key Performance Metrics and Comparison Results**:\n        *   **HQM Metrics**: Test-retest reliability, parallel-forms reliability, criterion validity (all quantified by Pearson correlation coefficient), and the number of hallucination types covered \\cite{yan2024ux8}.\n        *   **Benchmark Quality (Table 1)**:\n            *   **HQH (Ours)** achieved outstanding scores: Test-retest reliability (0.9962), Parallel-forms reliability (0.9943), Average reliability (0.9953), and Criterion validity (0.9347), while covering all 8 hallucination types \\cite{yan2024ux8}.\n            *   **Closed-ended benchmarks**: Showed high test-retest reliability (e.g., POPE: 0.9996) and criterion validity (e.g., POPE: 0.9634), but critically low parallel-forms reliability (e.g., POPE: 0.3563), confirming their susceptibility to response bias \\cite{yan2024ux8}.\n            *   **Open-ended benchmarks**: Generally had better parallel-forms reliability than closed-ended ones (e.g., MMHal: 0.8412), but often suffered from lower criterion validity (e.g., GAVIE: 0.3122) and sometimes lower test-retest reliability (e.g., MMHal: 0.8784) due to the inconsistency of GPT-based scoring \\cite{yan2024ux8}.\n        *   **Model Performance (Figure 2)**: The evaluation revealed considerable differences in model performance and rankings across various benchmarks, underscoring the necessity of benchmark quality measurement. For instance, Qwen-VL consistently appeared among the top performers on several benchmarks, including HQH \\cite{yan2024ux8}.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations**:\n        *   While HQH's simplified metric significantly improves reliability, the use of external LLMs for evaluation in free-form VQA benchmarks can still introduce a minor degree of external randomness and instability compared to fully deterministic metrics \\cite{yan2024ux8}.\n        *   Cost considerations limited the use of more advanced GPT models (e.g., GPT-4) for assisting some benchmark evaluations, with GPT-3.5 being used instead \\cite{yan2024ux8}.\n    *   **Scope of Applicability**: The HQM framework and HQH benchmark are specifically designed for evaluating hallucination in Large Vision-Language Models (LVLMs) \\cite{yan2024ux8}. The defined hallucination types focus on common perceptual inconsistencies.\n\n7.  **Technical Significance**\n    *   **Advances State-of-the-Art**: This work introduces the first systematic and quantitative framework (HQM) for assessing the quality of hallucination benchmarks for LVLMs, providing a principled, psychometrics-inspired methodology to evaluate benchmark trustworthiness \\cite{yan2024ux8}. It moves beyond anecdotal observations of benchmark flaws to provide concrete metrics.\n    *   **Potential Impact on Future Research**:\n        *   **Improved Benchmark Design**: HQM offers clear guidelines and metrics for developing future hallucination benchmarks that are more reliable, valid, and robust against biases, leading to more trustworthy evaluation tools \\cite{yan2024ux8}.\n        *   **More Accurate Model Evaluation**: By enabling the identification and selection of high-quality benchmarks, this work facilitates more accurate and consistent evaluation of LVLMs' hallucination capabilities, which is crucial for tracking progress and making informed comparisons \\cite{yan2024ux8}.\n        *   **Deeper Understanding of Hallucination**: The HQH benchmark, with its high quality and comprehensive coverage of hallucination types, provides a robust tool for researchers to conduct more in-depth analyses of how LVLMs hallucinate across different scenarios, guiding targeted model improvements \\cite{yan2024ux8}.\n        *   **Broader Application of Psychometrics**: Demonstrates the valuable application of psychometric principles in AI evaluation, potentially inspiring similar quality assessment frameworks for other AI tasks and benchmarks \\cite{yan2024ux8}.",
      "intriguing_abstract": "The pervasive problem of hallucination in Large Vision-Language Models (LVLMs) critically undermines their trustworthiness, yet current evaluation benchmarks suffer from profound quality issues, yielding inconsistent and unreliable results. We introduce a groundbreaking solution: the **Hallucination benchmark Quality Measurement framework (HQM)**, the first to systematically quantify benchmark reliability and validity using psychometric principles. HQM rigorously assesses existing benchmarks, revealing their susceptibility to response biases and significant misalignment with human judgment.\n\nLeveraging these insights, we present **HQH**, a novel High-Quality Hallucination Benchmark. HQH employs an innovative simplified binary classification metric for open-ended Visual Question Answering (VQA), drastically improving evaluation consistency and human alignment compared to prior score-based methods. Furthermore, HQH offers comprehensive coverage of eight distinct hallucination types, addressing a major gap in existing datasets. Our extensive evaluation of over 10 state-of-the-art LVLMs using HQH provides unprecedented insights into their hallucination tendencies. This work not only delivers a robust tool for accurate LVLM assessment but also establishes a principled methodology for designing truly trustworthy AI benchmarks, paving the way for safer and more reliable multimodal AI.",
      "keywords": [
        "Large Vision-Language Models (LVLMs)",
        "hallucination evaluation",
        "hallucination benchmarks",
        "benchmark quality assessment",
        "psychometric principles",
        "reliability and validity metrics",
        "HQM framework",
        "HQH benchmark",
        "simplified binary hallucination detection",
        "comprehensive hallucination types",
        "response bias",
        "open-ended VQA"
      ],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/fca2da71f3dce2f757aef39e561a572f68106603.pdf",
      "citation_key": "yan2024ux8",
      "metadata": {
        "title": "Evaluating the Quality of Hallucination Benchmarks for Large Vision-Language Models",
        "authors": [
          "Bei Yan",
          "Jie Zhang",
          "Zheng Yuan",
          "Shiguang Shan",
          "Xilin Chen"
        ],
        "published_date": "2024",
        "abstract": "Despite the rapid progress and outstanding performance of Large Vision-Language Models (LVLMs) in recent years, LVLMs have been plagued by the issue of hallucination, i.e., LVLMs tend to generate responses that are inconsistent with the corresponding visual inputs. To evaluate the degree of hallucination in LVLMs, previous works have proposed a series of benchmarks featuring different types of tasks and evaluation metrics. However, we find that the quality of the existing hallucination benchmarks varies, with some suffering from problems, e.g., inconsistent evaluation results under repeated tests, and misalignment with human evaluation. To this end, we propose a Hallucination benchmark Quality Measurement framework (HQM), which leverages various indicators to assess the reliability and validity of existing hallucination benchmarks separately. Specifically, for reliability we explore test-retest reliability and parallel-forms reliability, while for validity we examine criterion validity and coverage of hallucination types. Furthermore, based on the results of our quality measurement, we construct a High-Quality Hallucination Benchmark (HQH) for LVLMs, which demonstrates superior reliability and validity under our HQM framework. We conduct an extensive evaluation of over 10 representative LVLMs, including GPT-4o and Gemini-1.5-Pro, to provide an in-depth analysis of the hallucination issues in existing models. Our benchmark is publicly available at https://github.com/HQHBench/HQHBench.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/fca2da71f3dce2f757aef39e561a572f68106603.pdf",
        "venue": "arXiv.org",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the technical paper for a literature review:\n\n### Analyzing \"Evaluating the Quality of Hallucination Benchmarks for Large Vision-Language Models\" \\cite{yan2024ux8}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Large Vision-Language Models (LVLMs) are significantly plagued by hallucination, where generated textual content is inconsistent with visual inputs \\cite{yan2024ux8}. Existing benchmarks designed to evaluate this hallucination suffer from quality issues.\n    *   **Importance and Challenge**: Hallucination can lead to harmful consequences, especially when users over-rely on models \\cite{yan2024ux8}. The quality issues in existing benchmarks include inconsistent evaluation results under repeated tests, misalignment with human evaluation, and limited coverage of hallucination types, which casts doubt on the trustworthiness of their evaluation results and hinders reliable progress in mitigating hallucination \\cite{yan2024ux8}.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: Previous studies have proposed various hallucination benchmarks for LVLMs, categorized into closed-ended tasks (e.g., yes-or-no questions like POPE, AMBER-d, HallusionBench) and open-ended tasks (e.g., image captioning, free-form VQA like AMBER-g, OpenCHAIR, MMHal, GAVIE) \\cite{yan2024ux8}.\n    *   **Limitations of Previous Solutions**:\n        *   **Closed-ended benchmarks**: While offering efficient automated evaluation, they exhibit poor reliability due to LVLM susceptibility to response biases (e.g., acquiescence, dissent, position bias) \\cite{yan2024ux8}.\n        *   **Open-ended benchmarks**: Although avoiding response bias, they primarily suffer from validity issues, showing significant misalignment with human evaluation \\cite{yan2024ux8}.\n        *   **Score-based metrics in open-ended tasks**: Metrics that use external LLMs (like GPT) to assign a specific hallucination score are often too challenging for current LLMs, leading to inconsistent scores across repeated/parallel tests and inaccurate scores misaligned with human judgment \\cite{yan2024ux8}.\n        *   **Limited coverage**: Many existing benchmarks do not comprehensively cover different types of hallucination \\cite{yan2024ux8}.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**:\n        *   **Hallucination benchmark Quality Measurement framework (HQM)**: Inspired by psychometrics, HQM systematically assesses benchmark quality based on reliability and validity \\cite{yan2024ux8}.\n            *   **Reliability**: Measured by *test-retest reliability* (Pearson correlation between original and retest results with different random seeds) and *parallel-forms reliability* (Pearson correlation between original and parallel-form results using equivalent prompts) \\cite{yan2024ux8}.\n            *   **Validity**: Assessed by *criterion validity* (Pearson correlation between automated benchmark results and human evaluation results) and *coverage of hallucination types* (examining inclusion of 8 predefined types: attribute, action, counting, environment, relation, comparison, OCR, existence) \\cite{yan2024ux8}.\n        *   **High-Quality Hallucination Benchmark (HQH)**: A new benchmark constructed using open-ended free-form VQA tasks to achieve a balance between reliability and validity \\cite{yan2024ux8}.\n            *   **Data**: Collects images from Visual Genome and designs image-instruction pairs covering comprehensive hallucination types, with manual review for quality \\cite{yan2024ux8}.\n            *   **Evaluation Metric**: Employs a simplified binary classification process where an external LLM (GPT) only determines if a response is hallucinated (yes/no), rather than assigning a score. This allows for the computation of a \"hallucination rate\" \\cite{yan2024ux8}.\n    *   **Novelty/Difference**:\n        *   HQM is the first framework to systematically quantify the quality of LVLM hallucination benchmarks using psychometric principles, providing a principled way to evaluate benchmark trustworthiness \\cite{yan2024ux8}.\n        *   HQH's simplified binary hallucination detection metric is a novel approach designed to significantly reduce the evaluation gap between external LLMs and human evaluators, thereby enhancing both reliability and validity compared to prior score-based methods \\cite{yan2024ux8}.\n        *   HQH offers comprehensive coverage of 8 distinct hallucination types, addressing the limited scope of many existing benchmarks \\cite{yan2024ux8}.\n\n4.  **Key Technical Contributions**\n    *   **Novel Framework**: Proposed the Hallucination benchmark Quality Measurement framework (HQM) for LVLMs, which leverages various indicators to assess benchmark reliability (test-retest, parallel-forms) and validity (criterion validity, coverage of hallucination types) \\cite{yan2024ux8}.\n    *   **High-Quality Benchmark**: Constructed a new High-Quality Hallucination Benchmark (HQH) that demonstrates superior reliability and validity under the HQM framework, featuring a simplified binary hallucination detection metric and comprehensive coverage of 8 hallucination types \\cite{yan2024ux8}.\n    *   **Extensive Evaluation**: Conducted a large-scale evaluation of over 10 representative LVLMs, including state-of-the-art models like GPT-4o and Gemini-1.5-Pro, using HQH to provide an in-depth analysis of hallucination issues \\cite{yan2024ux8}.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**:\n        *   Evaluated the quality of 6 representative existing hallucination benchmarks (POPE, AMBER-d, HallusionBench, AMBER-g, OpenCHAIR, MMHal, GAVIE) using the proposed HQM framework \\cite{yan2024ux8}.\n        *   Assessed the reliability and validity of the newly constructed HQH benchmark using HQM \\cite{yan2024ux8}.\n        *   Performed a large-scale evaluation of 9 mainstream open-source LVLMs (20 checkpoints) and several powerful closed-source models (e.g., GPT-4o, Gemini-1.5-Pro) on HQH and other benchmarks \\cite{yan2024ux8}.\n    *   **Key Performance Metrics and Comparison Results**:\n        *   **HQM Metrics**: Test-retest reliability, parallel-forms reliability, criterion validity (all quantified by Pearson correlation coefficient), and the number of hallucination types covered \\cite{yan2024ux8}.\n        *   **Benchmark Quality (Table 1)**:\n            *   **HQH (Ours)** achieved outstanding scores: Test-retest reliability (0.9962), Parallel-forms reliability (0.9943), Average reliability (0.9953), and Criterion validity (0.9347), while covering all 8 hallucination types \\cite{yan2024ux8}.\n            *   **Closed-ended benchmarks**: Showed high test-retest reliability (e.g., POPE: 0.9996) and criterion validity (e.g., POPE: 0.9634), but critically low parallel-forms reliability (e.g., POPE: 0.3563), confirming their susceptibility to response bias \\cite{yan2024ux8}.\n            *   **Open-ended benchmarks**: Generally had better parallel-forms reliability than closed-ended ones (e.g., MMHal: 0.8412), but often suffered from lower criterion validity (e.g., GAVIE: 0.3122) and sometimes lower test-retest reliability (e.g., MMHal: 0.8784) due to the inconsistency of GPT-based scoring \\cite{yan2024ux8}.\n        *   **Model Performance (Figure 2)**: The evaluation revealed considerable differences in model performance and rankings across various benchmarks, underscoring the necessity of benchmark quality measurement. For instance, Qwen-VL consistently appeared among the top performers on several benchmarks, including HQH \\cite{yan2024ux8}.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations**:\n        *   While HQH's simplified metric significantly improves reliability, the use of external LLMs for evaluation in free-form VQA benchmarks can still introduce a minor degree of external randomness and instability compared to fully deterministic metrics \\cite{yan2024ux8}.\n        *   Cost considerations limited the use of more advanced GPT models (e.g., GPT-4) for assisting some benchmark evaluations, with GPT-3.5 being used instead \\cite{yan2024ux8}.\n    *   **Scope of Applicability**: The HQM framework and HQH benchmark are specifically designed for evaluating hallucination in Large Vision-Language Models (LVLMs) \\cite{yan2024ux8}. The defined hallucination types focus on common perceptual inconsistencies.\n\n7.  **Technical Significance**\n    *   **Advances State-of-the-Art**: This work introduces the first systematic and quantitative framework (HQM) for assessing the quality of hallucination benchmarks for LVLMs, providing a principled, psychometrics-inspired methodology to evaluate benchmark trustworthiness \\cite{yan2024ux8}. It moves beyond anecdotal observations of benchmark flaws to provide concrete metrics.\n    *   **Potential Impact on Future Research**:\n        *   **Improved Benchmark Design**: HQM offers clear guidelines and metrics for developing future hallucination benchmarks that are more reliable, valid, and robust against biases, leading to more trustworthy evaluation tools \\cite{yan2024ux8}.\n        *   **More Accurate Model Evaluation**: By enabling the identification and selection of high-quality benchmarks, this work facilitates more accurate and consistent evaluation of LVLMs' hallucination capabilities, which is crucial for tracking progress and making informed comparisons \\cite{yan2024ux8}.\n        *   **Deeper Understanding of Hallucination**: The HQH benchmark, with its high quality and comprehensive coverage of hallucination types, provides a robust tool for researchers to conduct more in-depth analyses of how LVLMs hallucinate across different scenarios, guiding targeted model improvements \\cite{yan2024ux8}.\n        *   **Broader Application of Psychometrics**: Demonstrates the valuable application of psychometric principles in AI evaluation, potentially inspiring similar quality assessment frameworks for other AI tasks and benchmarks \\cite{yan2024ux8}.",
        "keywords": [
          "Large Vision-Language Models (LVLMs)",
          "hallucination evaluation",
          "hallucination benchmarks",
          "benchmark quality assessment",
          "psychometric principles",
          "reliability and validity metrics",
          "HQM framework",
          "HQH benchmark",
          "simplified binary hallucination detection",
          "comprehensive hallucination types",
          "response bias",
          "open-ended VQA"
        ],
        "paper_type": "based on the abstract and introduction, this paper is best classified as **technical**.\n\nhere's why:\n\n*   **abstract keywords:** the abstract explicitly states, \"we **propose** a hallucination benchmark quality measurement **framework (hqm)**\" and \"we **construct** a high-quality hallucination **benchmark (hqh)**\". these phrases directly align with the \"technical\" criteria of presenting new methods, algorithms, or systems.\n*   **problem and solution:** the paper identifies a technical problem (\"the quality of the existing hallucination benchmarks varies, with some suffering from problems\") and then proposes a concrete technical solution (the hqm framework and the hqh benchmark).\n*   **empirical component:** while the paper also mentions \"we conduct an extensive evaluation of over 10 representative lvlms...\", this empirical study serves to validate and demonstrate the utility of their *proposed* framework and benchmark, rather than being the sole or primary contribution. the core contribution is the development of the new tools (framework and benchmark)."
      },
      "file_name": "fca2da71f3dce2f757aef39e561a572f68106603.pdf"
    },
    {
      "success": true,
      "doc_id": "4902dbe2da73a79d3842ffb075328f9b",
      "summary": "Here's a focused summary of the paper `\\cite{wang2024rta}` for a literature review, emphasizing technical innovations and empirical validation:\n\n### Technical Paper Analysis: `\\cite{wang2024rta}`\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Large Video-Language Models (LVLMs) frequently generate \"hallucinations\"â€”irrelevant or nonsensical content that deviates from the actual video context.\n    *   **Importance and Challenge**: This problem is critical because it undermines the reliability and trustworthiness of LVLMs. Existing hallucination benchmarks primarily focus on static images and basic visual attributes, overlooking the complexities of dynamic video content (actions, events, stories) and lacking a comprehensive evaluation framework for LVLMs.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: `\\cite{wang2024rta}` builds upon the concept of hallucination from Natural Language Generation (NLG) and extends existing vision hallucination benchmarks (e.g., POPE, CHAIR) to the video domain.\n    *   **Limitations of Previous Solutions**: Previous benchmarks are largely image-centric, focusing on static visual attributes (objects, relationships, attributes) and often lack a comprehensive taxonomy for video-specific hallucinations. They also do not adequately address dynamic content, temporal reasoning, or the distinction between intrinsic and extrinsic factual inconsistencies in video.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: `\\cite{wang2024rta}` introduces **VideoHallucer**, the first comprehensive benchmark for hallucination detection in LVLMs. It employs an **adversarial binary VideoQA method**, where each datapoint consists of a \"basic question\" (to test LVLM's fundamental understanding) and a strategically crafted \"hallucinated question\" (to evaluate hallucination detection).\n    *   **Novelty**:\n        *   **Comprehensive Taxonomy**: `VideoHallucer` establishes a novel and detailed taxonomy of hallucinations for LVLMs, categorizing them into:\n            *   **Intrinsic Hallucinations**: Content directly contradicting the video (Object-Relation, Temporal, Semantic Detail).\n            *   **Extrinsic Hallucinations**: Content not verifiable from the video (Extrinsic Factual, Extrinsic Non-factual).\n        *   **Video-Specific Evaluation**: It specifically addresses dynamic content, temporal reasoning, and semantic details within videos, which are largely overlooked by image-based benchmarks.\n        *   **Adversarial Paired Questions**: The use of paired basic and hallucinated questions in a binary VQA format provides a robust and rigorous evaluation, minimizing language biases by balancing 'yes'/'no' responses and providing explanations for clarity.\n        *   **Self-PEP Framework**: A plug-and-play framework designed to enhance models' self-improvement capabilities in hallucination resistance by integrating explanatory processes.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**:\n        *   The **VideoHallucer benchmark** itself, with its structured dataset construction for five distinct hallucination types (Object-Relation, Temporal, Semantic Detail, Extrinsic Factual, Extrinsic Non-factual).\n        *   The **adversarial binary VideoQA evaluation method** for robust hallucination detection.\n        *   The **Self-PEP (Self-Prompting with Explanations for Performance)** framework, a post-hoc method to improve hallucination resistance.\n    *   **System Design/Architectural Innovations**: The proposed **taxonomy of intrinsic and extrinsic hallucinations** with their subcategories provides a structured way to analyze and understand different types of LVLM failures.\n    *   **Theoretical Insights/Analysis**: The paper provides empirical insights into the nature of hallucinations in LVLMs, particularly regarding the limitations of scaling for extrinsic factual hallucinations and the disparity between fact recognition and hallucination detection.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**: `\\cite{wang2024rta}` comprehensively evaluated eleven state-of-the-art LVLMs on the `VideoHallucer` benchmark. Additionally, the Self-PEP framework was applied to these models to demonstrate its effectiveness.\n    *   **Key Performance Metrics and Comparison Results**:\n        *   **Widespread Hallucinations**: Revealed significant hallucination issues across the majority of current LVLMs, with substantial performance gaps between human and model capabilities in all `VideoHallucer` settings.\n        *   **Scaling Limitations**: While scaling datasets and parameters improved detection of basic visual cues and counterfactuals, it showed *limited benefit* for detecting extrinsic factual hallucinations.\n        *   **Fact vs. Hallucination Detection**: Current models are more proficient at recognizing facts than at identifying hallucinations, which requires discerning facts within the source context.\n        *   **Self-PEP Effectiveness**: The Self-PEP framework achieved an average of **5.38% improvement** in hallucination resistance across various model architectures, demonstrating its practical utility.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The benchmark focuses on hallucination detection via a binary VQA format, which might not cover all forms of generative hallucinations (e.g., in open-ended video captioning or summarization). The construction relies on semi-automated and human annotation, which can introduce biases, though efforts were made to mitigate this.\n    *   **Scope of Applicability**: `VideoHallucer` is specifically designed for evaluating hallucination detection in Large Video-Language Models (LVLMs) and provides a framework for understanding and mitigating these issues. Its direct applicability is primarily for VQA-based hallucination assessment.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: `\\cite{wang2024rta}` significantly advances the technical state-of-the-art by providing the *first comprehensive benchmark* specifically tailored for hallucination detection in LVLMs, moving beyond static image-based evaluations.\n    *   **Potential Impact on Future Research**:\n        *   Offers a standardized and rigorous tool (`VideoHallucer`) for evaluating and comparing future LVLMs' robustness against hallucinations.\n        *   The detailed taxonomy provides a conceptual framework for researchers to categorize and target specific types of hallucinations.\n        *   The empirical insights (e.g., scaling limitations for extrinsic factual hallucinations) highlight critical areas for future model development.\n        *   The Self-PEP framework offers a practical, plug-and-play method for improving hallucination resistance, encouraging further research into explainability-driven model self-improvement.",
      "intriguing_abstract": "The promise of Large Video-Language Models (LVLMs) is critically undermined by **hallucinations**, where generated content deviates from actual video context, eroding reliability. Existing image-centric benchmarks are inadequate for assessing these complex, dynamic inconsistencies. We introduce **VideoHallucer**, the first comprehensive benchmark specifically designed for rigorous hallucination detection in LVLMs.\n\nVideoHallucer pioneers a novel, fine-grained **taxonomy** categorizing hallucinations into intrinsic (e.g., Object-Relation, Temporal) and extrinsic (e.g., Factual, Non-factual) types. It employs an **adversarial binary VideoQA** method, using paired basic and hallucinated questions to robustly evaluate models across dynamic content, temporal reasoning, and semantic details. Our extensive evaluation of eleven state-of-the-art LVLMs reveals pervasive hallucination issues, showing that current scaling strategies offer limited benefit for detecting extrinsic factual inconsistencies. Intriguingly, models are more adept at fact recognition than hallucination identification. To combat this, we propose **Self-PEP**, a plug-and-play framework that significantly boosts hallucination resistance by an average of 5.38%. VideoHallucer provides a critical tool for developing more trustworthy LVLMs, offering both a standardized evaluation framework and a practical mitigation strategy to unlock their full potential.",
      "keywords": [
        "Large Video-Language Models (LVLMs)",
        "Hallucinations (LVLMs)",
        "VideoHallucer benchmark",
        "Comprehensive hallucination taxonomy",
        "Intrinsic and Extrinsic Hallucinations",
        "Adversarial binary VideoQA",
        "Self-PEP framework",
        "Temporal reasoning",
        "Widespread hallucination issues",
        "Scaling limitations (extrinsic factual)",
        "Fact recognition vs. hallucination detection",
        "LVLM reliability"
      ],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/58ee9e1c426166a5451a1ce13e1186f7d6baacfd.pdf",
      "citation_key": "wang2024rta",
      "metadata": {
        "title": "VideoHallucer: Evaluating Intrinsic and Extrinsic Hallucinations in Large Video-Language Models",
        "authors": [
          "Yuxuan Wang",
          "Yueqian Wang",
          "Dongyan Zhao",
          "Cihang Xie",
          "Zilong Zheng"
        ],
        "published_date": "2024",
        "abstract": "Recent advancements in Multimodal Large Language Models (MLLMs) have extended their capabilities to video understanding. Yet, these models are often plagued by\"hallucinations\", where irrelevant or nonsensical content is generated, deviating from the actual video context. This work introduces VideoHallucer, the first comprehensive benchmark for hallucination detection in large video-language models (LVLMs). VideoHallucer categorizes hallucinations into two main types: intrinsic and extrinsic, offering further subcategories for detailed analysis, including object-relation, temporal, semantic detail, extrinsic factual, and extrinsic non-factual hallucinations. We adopt an adversarial binary VideoQA method for comprehensive evaluation, where pairs of basic and hallucinated questions are crafted strategically. By evaluating eleven LVLMs on VideoHallucer, we reveal that i) the majority of current models exhibit significant issues with hallucinations; ii) while scaling datasets and parameters improves models' ability to detect basic visual cues and counterfactuals, it provides limited benefit for detecting extrinsic factual hallucinations; iii) existing models are more adept at detecting facts than identifying hallucinations. As a byproduct, these analyses further instruct the development of our self-PEP framework, achieving an average of 5.38% improvement in hallucination resistance across all model architectures.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/58ee9e1c426166a5451a1ce13e1186f7d6baacfd.pdf",
        "venue": "arXiv.org",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the paper `\\cite{wang2024rta}` for a literature review, emphasizing technical innovations and empirical validation:\n\n### Technical Paper Analysis: `\\cite{wang2024rta}`\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Large Video-Language Models (LVLMs) frequently generate \"hallucinations\"â€”irrelevant or nonsensical content that deviates from the actual video context.\n    *   **Importance and Challenge**: This problem is critical because it undermines the reliability and trustworthiness of LVLMs. Existing hallucination benchmarks primarily focus on static images and basic visual attributes, overlooking the complexities of dynamic video content (actions, events, stories) and lacking a comprehensive evaluation framework for LVLMs.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: `\\cite{wang2024rta}` builds upon the concept of hallucination from Natural Language Generation (NLG) and extends existing vision hallucination benchmarks (e.g., POPE, CHAIR) to the video domain.\n    *   **Limitations of Previous Solutions**: Previous benchmarks are largely image-centric, focusing on static visual attributes (objects, relationships, attributes) and often lack a comprehensive taxonomy for video-specific hallucinations. They also do not adequately address dynamic content, temporal reasoning, or the distinction between intrinsic and extrinsic factual inconsistencies in video.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: `\\cite{wang2024rta}` introduces **VideoHallucer**, the first comprehensive benchmark for hallucination detection in LVLMs. It employs an **adversarial binary VideoQA method**, where each datapoint consists of a \"basic question\" (to test LVLM's fundamental understanding) and a strategically crafted \"hallucinated question\" (to evaluate hallucination detection).\n    *   **Novelty**:\n        *   **Comprehensive Taxonomy**: `VideoHallucer` establishes a novel and detailed taxonomy of hallucinations for LVLMs, categorizing them into:\n            *   **Intrinsic Hallucinations**: Content directly contradicting the video (Object-Relation, Temporal, Semantic Detail).\n            *   **Extrinsic Hallucinations**: Content not verifiable from the video (Extrinsic Factual, Extrinsic Non-factual).\n        *   **Video-Specific Evaluation**: It specifically addresses dynamic content, temporal reasoning, and semantic details within videos, which are largely overlooked by image-based benchmarks.\n        *   **Adversarial Paired Questions**: The use of paired basic and hallucinated questions in a binary VQA format provides a robust and rigorous evaluation, minimizing language biases by balancing 'yes'/'no' responses and providing explanations for clarity.\n        *   **Self-PEP Framework**: A plug-and-play framework designed to enhance models' self-improvement capabilities in hallucination resistance by integrating explanatory processes.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**:\n        *   The **VideoHallucer benchmark** itself, with its structured dataset construction for five distinct hallucination types (Object-Relation, Temporal, Semantic Detail, Extrinsic Factual, Extrinsic Non-factual).\n        *   The **adversarial binary VideoQA evaluation method** for robust hallucination detection.\n        *   The **Self-PEP (Self-Prompting with Explanations for Performance)** framework, a post-hoc method to improve hallucination resistance.\n    *   **System Design/Architectural Innovations**: The proposed **taxonomy of intrinsic and extrinsic hallucinations** with their subcategories provides a structured way to analyze and understand different types of LVLM failures.\n    *   **Theoretical Insights/Analysis**: The paper provides empirical insights into the nature of hallucinations in LVLMs, particularly regarding the limitations of scaling for extrinsic factual hallucinations and the disparity between fact recognition and hallucination detection.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**: `\\cite{wang2024rta}` comprehensively evaluated eleven state-of-the-art LVLMs on the `VideoHallucer` benchmark. Additionally, the Self-PEP framework was applied to these models to demonstrate its effectiveness.\n    *   **Key Performance Metrics and Comparison Results**:\n        *   **Widespread Hallucinations**: Revealed significant hallucination issues across the majority of current LVLMs, with substantial performance gaps between human and model capabilities in all `VideoHallucer` settings.\n        *   **Scaling Limitations**: While scaling datasets and parameters improved detection of basic visual cues and counterfactuals, it showed *limited benefit* for detecting extrinsic factual hallucinations.\n        *   **Fact vs. Hallucination Detection**: Current models are more proficient at recognizing facts than at identifying hallucinations, which requires discerning facts within the source context.\n        *   **Self-PEP Effectiveness**: The Self-PEP framework achieved an average of **5.38% improvement** in hallucination resistance across various model architectures, demonstrating its practical utility.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The benchmark focuses on hallucination detection via a binary VQA format, which might not cover all forms of generative hallucinations (e.g., in open-ended video captioning or summarization). The construction relies on semi-automated and human annotation, which can introduce biases, though efforts were made to mitigate this.\n    *   **Scope of Applicability**: `VideoHallucer` is specifically designed for evaluating hallucination detection in Large Video-Language Models (LVLMs) and provides a framework for understanding and mitigating these issues. Its direct applicability is primarily for VQA-based hallucination assessment.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: `\\cite{wang2024rta}` significantly advances the technical state-of-the-art by providing the *first comprehensive benchmark* specifically tailored for hallucination detection in LVLMs, moving beyond static image-based evaluations.\n    *   **Potential Impact on Future Research**:\n        *   Offers a standardized and rigorous tool (`VideoHallucer`) for evaluating and comparing future LVLMs' robustness against hallucinations.\n        *   The detailed taxonomy provides a conceptual framework for researchers to categorize and target specific types of hallucinations.\n        *   The empirical insights (e.g., scaling limitations for extrinsic factual hallucinations) highlight critical areas for future model development.\n        *   The Self-PEP framework offers a practical, plug-and-play method for improving hallucination resistance, encouraging further research into explainability-driven model self-improvement.",
        "keywords": [
          "Large Video-Language Models (LVLMs)",
          "Hallucinations (LVLMs)",
          "VideoHallucer benchmark",
          "Comprehensive hallucination taxonomy",
          "Intrinsic and Extrinsic Hallucinations",
          "Adversarial binary VideoQA",
          "Self-PEP framework",
          "Temporal reasoning",
          "Widespread hallucination issues",
          "Scaling limitations (extrinsic factual)",
          "Fact recognition vs. hallucination detection",
          "LVLM reliability"
        ],
        "paper_type": "this paper introduces \"videohallucer,\" a new comprehensive benchmark and an \"adversarial binary videoqa method\" for detecting hallucinations in large video-language models. it also mentions the development of a \"self-pep framework.\" while the paper includes an empirical evaluation of eleven existing models using this new benchmark and method, the primary contribution highlighted is the *creation* of these new tools and methodologies.\n\nlet's break down the criteria:\n\n*   **survey**: not a survey; it introduces new work, not a review of existing literature.\n*   **technical**: the abstract explicitly states: \"this work introduces videohallucer, the first comprehensive benchmark...\", \"videohallucer categorizes hallucinations...\", \"we adopt an adversarial binary videoqa method...\", and \"further instruct the development of our self-pep framework...\". these are all descriptions of new systems, methods, and frameworks. this aligns perfectly with the \"technical\" criteria.\n*   **theoretical**: no mention of mathematical proofs, theorems, or formal models.\n*   **empirical**: the paper *does* conduct an empirical study (\"by evaluating eleven lvlms on videohallucer, we reveal that...\"). however, this empirical study is performed *using* the newly developed technical contributions (the benchmark and method). the creation of the benchmark and method is a foundational technical contribution that enables the empirical analysis. when a paper introduces a new system/method and then evaluates it, the core contribution is often classified as technical.\n*   **case_study**: not a detailed analysis of a specific application.\n*   **position**: not primarily arguing for a viewpoint or future direction, but presenting a concrete solution and its evaluation.\n*   **short**: while it's on arxiv and \"under review,\" the content described (comprehensive benchmark, evaluation of multiple models, new framework) suggests a substantial work, not just a brief communication. the classification should be based on the *type* of work, not just its current publication status.\n\ngiven that the paper's main contribution is the *introduction* and *development* of a new benchmark, a new evaluation method, and a new framework, it best fits the **technical** classification. the empirical evaluation serves to demonstrate and validate these technical contributions.\n\n**classification: technical**"
      },
      "file_name": "58ee9e1c426166a5451a1ce13e1186f7d6baacfd.pdf"
    },
    {
      "success": true,
      "doc_id": "bdfdcff7cb514b6a5ee85f7210ef620c",
      "summary": "Here's a focused summary of the paper for a literature review, emphasizing technical innovations and empirical validation:\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Large Vision-Language Models (LVLMs) suffer from \"hallucination,\" where generated textual responses are not factually grounded in the input visual content \\cite{xie2024l8a}. This is particularly challenging for unconventional images.\n    *   **Importance & Challenge**: Hallucination undermines the reliability and trustworthiness of LVLMs. The problem is attributed to LVLMs' over-reliance on their Large Language Model (LLM) backbone, leading to insufficient attention to visual inputs and prioritization of language priors \\cite{xie2024l8a}. Existing mitigation strategies, such as decoding optimization, often increase inference time and lack generalizability across diverse data domains \\cite{xie2024l8a}.\n\n*   **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: Previous work on hallucination mitigation falls into two categories: (1) post-processing methods (e.g., post-hoc corrections, specialized decoding) and (2) preference optimization using hallucination-aware data (e.g., adapting RLHF or DPO, data augmentation for contrastive tuning) \\cite{xie2024l8a}.\n    *   **Limitations of Previous Solutions**: Post-processing methods incur increased inference time and hinder generalizability \\cite{xie2024l8a}. While preference optimization is promising, existing DPO variants for LVLMs primarily focus on response-contrast data and may not explicitly enhance visual understanding or leverage nuanced visual differences effectively \\cite{xie2024l8a}.\n    *   **Positioning**: This work positions itself within preference optimization but uniquely addresses the over-reliance on language priors by introducing a *vision-specific optimization target* and leveraging *image-contrast preference data* to enhance visual understanding during training \\cite{xie2024l8a}.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes Vision-guided Direct Preference Optimization (V-DPO), a vision-specific variant of DPO, to enhance visual context learning during training \\cite{xie2024l8a}.\n    *   **Novelty**:\n        *   **Vision-Guided DPO Objective**: V-DPO integrates Classifier-Free Guidance (CFG) directly into the DPO reward maximization objective. It introduces an additional KL-divergence term, `+Î±DKL[Ï€(y|v, x)âˆ¥Ï€(y|x)]`, where `Î± > 0` controls the weight of visual guidance, explicitly encouraging the model to prioritize visual grounding over language-only priors \\cite{xie2024l8a}.\n        *   **Guidance Strength Control**: Unlike inference-time CFG, V-DPO decreases the guidance strength parameter `Î³ < 1` (by increasing `Î± > 0`) during training to strengthen the visual context guidance \\cite{xie2024l8a}.\n        *   **Normalized Visual Guidance**: To prevent the visual guidance effect from progressively decreasing as visual-conditioned and unconditioned distributions diverge during training, the CFG-modified probability distribution is normalized using a softmax function over modified logits \\cite{xie2024l8a}.\n        *   **Synthetic Image-Contrast Data Augmentation**: A novel pipeline is introduced to automatically construct *image-contrast preference pairs*. This involves using LVLMs and LLMs to propose replacements for objects in an image (e.g., replacing a \"cake\" with a \"pile of rocks\"), followed by image editing via Denoising Diffusion Probabilistic Models (DDPMs) and filtering using CLIPScore to ensure alignment with prompts \\cite{xie2024l8a}. This data specifically targets improving the model's ability to discern nuanced visual differences.\n\n*   **Key Technical Contributions**\n    *   **Novel Algorithm**: V-DPO, a modified DPO algorithm that incorporates a vision-specific guidance term derived from Classifier-Free Guidance, directly addressing insufficient visual context attention during preference learning \\cite{xie2024l8a}.\n    *   **Objective Function Innovation**: A new reward maximization objective for LVLMs that explicitly includes a KL-divergence term to align the vision-conditioned policy with a vision-guided distribution, thereby enhancing visual grounding \\cite{xie2024l8a}.\n    *   **Data Augmentation Strategy**: A fully automatic pipeline for generating *image-contrast preference data*, which complements traditional response-contrast data and is crucial for training models to understand subtle visual nuances and mitigate hallucination on unconventional content \\cite{xie2024l8a}.\n    *   **Implementation Detail**: A normalization technique for the visual guidance term to ensure its stable and effective influence throughout the training process \\cite{xie2024l8a}.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted**: V-DPO was evaluated on four hallucination benchmarks: POPE (object hallucination), AMBER (object, attribute, relation hallucination, generative & discriminative), HallusionBench (visual illusion & knowledge hallucination), and MMHal-Bench (various question types) \\cite{xie2024l8a}. Experiments used LLAVA-V1.5-7B as the base SFT model and compared training with a synthetic dataset (5K response- and image-contrast pairs) against human-annotated RLHF-V data (5K response-contrast pairs) \\cite{xie2024l8a}.\n    *   **Key Performance Metrics & Results**:\n        *   **POPE**: V-DPO significantly improved F1 scores (e.g., from 85.98 to 86.92 with synthetic data, and 87.12 to 87.22 with RLHF-V) and mitigated the \"yes\" bias compared to SFT and vanilla DPO baselines \\cite{xie2024l8a}.\n        *   **AMBER**: Achieved significant improvements on both generative (e.g., lower CHAIR scores, improved Cover, Hal, Cog) and discriminative tasks (improved F1 scores, e.g., 83.1 to 83.5 with synthetic data) \\cite{xie2024l8a}.\n        *   **HallusionBench**: Showed improvements in accuracy metrics (qAcc, fAcc, aAcc), particularly with synthetic data \\cite{xie2024l8a}.\n        *   **Overall**: V-DPO consistently outperformed SFT, vanilla DPO, and HA-DPO across various benchmarks.\n        *   **Analysis**: The paper highlights that V-DPO particularly excels in learning from *image-contrast preference data*, demonstrating its superior ability to elicit and understand nuances of visual context and mitigate over-reliance on language priors \\cite{xie2024l8a}.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations**: The paper acknowledges that imperfections in LLMs and generative models can introduce noise and bias into the synthetic data, which is addressed by a CLIPScore filtering mechanism \\cite{xie2024l8a}. The choice of the textual-only distribution `Ë†Ï€(Â· |x)` and the dynamics of visual guidance inflation are discussed as implementation considerations \\cite{xie2024l8a}.\n    *   **Scope of Applicability**: The method is primarily focused on mitigating hallucination in LVLMs for vision-conditioned text generation tasks, especially those involving unconventional visual content \\cite{xie2024l8a}.\n\n*   **Technical Significance**\n    *   **Advancement of State-of-the-Art**: V-DPO provides a novel and effective training-time strategy to directly combat LVLM hallucination by explicitly enhancing visual context learning through a modified preference optimization objective \\cite{xie2024l8a}. This offers a more generalizable solution than inference-time methods.\n    *   **Potential Impact**: The introduction and empirical validation of *image-contrast preference data* open new avenues for data augmentation strategies in multimodal learning. The integration of CFG into DPO for visual grounding could inspire similar vision-guided preference learning paradigms in other multimodal tasks, leading to more reliable and factually consistent LVLMs \\cite{xie2024l8a}.",
      "intriguing_abstract": "Large Vision-Language Models (LVLMs) are plagued by \"hallucination,\" generating text ungrounded in visual input, severely undermining their reliability and trustworthiness. This critical issue stems from an over-reliance on language priors, often neglecting nuanced visual context. We introduce **Vision-guided Direct Preference Optimization (V-DPO)**, a novel training-time framework that fundamentally reorients LVLMs towards robust visual grounding.\n\nV-DPO innovatively integrates **Classifier-Free Guidance (CFG)** directly into the **DPO reward maximization objective** via a unique KL-divergence term, explicitly compelling the model to prioritize visual evidence. Crucially, we present an automatic pipeline for generating **synthetic image-contrast preference data**. This novel dataset, created using **Denoising Diffusion Probabilistic Models (DDPMs)** and filtered by CLIPScore, trains models to discern subtle visual differences, a key weakness in existing approaches.\n\nEmpirical validation across four challenging hallucination benchmarks (POPE, AMBER, HallusionBench, MMHal-Bench) demonstrates V-DPO's superior performance, consistently outperforming SFT and vanilla DPO baselines. By enhancing visual context learning and mitigating language prior dominance, V-DPO offers a generalizable solution for building more trustworthy and factually consistent LVLMs, opening new frontiers in **multimodal learning** data augmentation.",
      "keywords": [
        "LVLM Hallucination",
        "Vision-guided Direct Preference Optimization (V-DPO)",
        "Image-contrast Preference Data",
        "Classifier-Free Guidance (CFG)",
        "Visual Grounding",
        "Language Priors Over-reliance",
        "Synthetic Data Augmentation",
        "KL-divergence Objective",
        "Denoising Diffusion Probabilistic Models (DDPMs)",
        "Hallucination Mitigation",
        "Preference Optimization",
        "Multimodal Learning"
      ],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/73020a07af4cfc42286e299097a0e35d2fe71a6c.pdf",
      "citation_key": "xie2024l8a",
      "metadata": {
        "title": "V-DPO: Mitigating Hallucination in Large Vision Language Models via Vision-Guided Direct Preference Optimization",
        "authors": [
          "Yuxi Xie",
          "Guanzhen Li",
          "Xiao Xu",
          "Min-Yen Kan"
        ],
        "published_date": "2024",
        "abstract": "Large vision-language models (LVLMs) suffer from hallucination, resulting in misalignment between the output textual response and the input visual content. Recent research indicates that the over-reliance on the Large Language Model (LLM) backbone, as one cause of the LVLM hallucination, inherently introduces bias from language priors, leading to insufficient context attention to the visual inputs. We tackle this issue of hallucination by mitigating such over-reliance through preference learning. We propose Vision-guided Direct Preference Optimization (V-DPO) to enhance visual context learning at training time. To interpret the effectiveness and generalizability of V-DPO on different types of training data, we construct a synthetic dataset containing both response- and image-contrast preference pairs, compared against existing human-annotated hallucination samples. Our approach achieves significant improvements compared with baseline methods across various hallucination benchmarks. Our analysis indicates that V-DPO excels in learning from image-contrast preference data, demonstrating its superior ability to elicit and understand nuances of visual context. Our code is publicly available at https://github.com/YuxiXie/V-DPO.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/73020a07af4cfc42286e299097a0e35d2fe71a6c.pdf",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the paper for a literature review, emphasizing technical innovations and empirical validation:\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Large Vision-Language Models (LVLMs) suffer from \"hallucination,\" where generated textual responses are not factually grounded in the input visual content \\cite{xie2024l8a}. This is particularly challenging for unconventional images.\n    *   **Importance & Challenge**: Hallucination undermines the reliability and trustworthiness of LVLMs. The problem is attributed to LVLMs' over-reliance on their Large Language Model (LLM) backbone, leading to insufficient attention to visual inputs and prioritization of language priors \\cite{xie2024l8a}. Existing mitigation strategies, such as decoding optimization, often increase inference time and lack generalizability across diverse data domains \\cite{xie2024l8a}.\n\n*   **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: Previous work on hallucination mitigation falls into two categories: (1) post-processing methods (e.g., post-hoc corrections, specialized decoding) and (2) preference optimization using hallucination-aware data (e.g., adapting RLHF or DPO, data augmentation for contrastive tuning) \\cite{xie2024l8a}.\n    *   **Limitations of Previous Solutions**: Post-processing methods incur increased inference time and hinder generalizability \\cite{xie2024l8a}. While preference optimization is promising, existing DPO variants for LVLMs primarily focus on response-contrast data and may not explicitly enhance visual understanding or leverage nuanced visual differences effectively \\cite{xie2024l8a}.\n    *   **Positioning**: This work positions itself within preference optimization but uniquely addresses the over-reliance on language priors by introducing a *vision-specific optimization target* and leveraging *image-contrast preference data* to enhance visual understanding during training \\cite{xie2024l8a}.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes Vision-guided Direct Preference Optimization (V-DPO), a vision-specific variant of DPO, to enhance visual context learning during training \\cite{xie2024l8a}.\n    *   **Novelty**:\n        *   **Vision-Guided DPO Objective**: V-DPO integrates Classifier-Free Guidance (CFG) directly into the DPO reward maximization objective. It introduces an additional KL-divergence term, `+Î±DKL[Ï€(y|v, x)âˆ¥Ï€(y|x)]`, where `Î± > 0` controls the weight of visual guidance, explicitly encouraging the model to prioritize visual grounding over language-only priors \\cite{xie2024l8a}.\n        *   **Guidance Strength Control**: Unlike inference-time CFG, V-DPO decreases the guidance strength parameter `Î³ < 1` (by increasing `Î± > 0`) during training to strengthen the visual context guidance \\cite{xie2024l8a}.\n        *   **Normalized Visual Guidance**: To prevent the visual guidance effect from progressively decreasing as visual-conditioned and unconditioned distributions diverge during training, the CFG-modified probability distribution is normalized using a softmax function over modified logits \\cite{xie2024l8a}.\n        *   **Synthetic Image-Contrast Data Augmentation**: A novel pipeline is introduced to automatically construct *image-contrast preference pairs*. This involves using LVLMs and LLMs to propose replacements for objects in an image (e.g., replacing a \"cake\" with a \"pile of rocks\"), followed by image editing via Denoising Diffusion Probabilistic Models (DDPMs) and filtering using CLIPScore to ensure alignment with prompts \\cite{xie2024l8a}. This data specifically targets improving the model's ability to discern nuanced visual differences.\n\n*   **Key Technical Contributions**\n    *   **Novel Algorithm**: V-DPO, a modified DPO algorithm that incorporates a vision-specific guidance term derived from Classifier-Free Guidance, directly addressing insufficient visual context attention during preference learning \\cite{xie2024l8a}.\n    *   **Objective Function Innovation**: A new reward maximization objective for LVLMs that explicitly includes a KL-divergence term to align the vision-conditioned policy with a vision-guided distribution, thereby enhancing visual grounding \\cite{xie2024l8a}.\n    *   **Data Augmentation Strategy**: A fully automatic pipeline for generating *image-contrast preference data*, which complements traditional response-contrast data and is crucial for training models to understand subtle visual nuances and mitigate hallucination on unconventional content \\cite{xie2024l8a}.\n    *   **Implementation Detail**: A normalization technique for the visual guidance term to ensure its stable and effective influence throughout the training process \\cite{xie2024l8a}.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted**: V-DPO was evaluated on four hallucination benchmarks: POPE (object hallucination), AMBER (object, attribute, relation hallucination, generative & discriminative), HallusionBench (visual illusion & knowledge hallucination), and MMHal-Bench (various question types) \\cite{xie2024l8a}. Experiments used LLAVA-V1.5-7B as the base SFT model and compared training with a synthetic dataset (5K response- and image-contrast pairs) against human-annotated RLHF-V data (5K response-contrast pairs) \\cite{xie2024l8a}.\n    *   **Key Performance Metrics & Results**:\n        *   **POPE**: V-DPO significantly improved F1 scores (e.g., from 85.98 to 86.92 with synthetic data, and 87.12 to 87.22 with RLHF-V) and mitigated the \"yes\" bias compared to SFT and vanilla DPO baselines \\cite{xie2024l8a}.\n        *   **AMBER**: Achieved significant improvements on both generative (e.g., lower CHAIR scores, improved Cover, Hal, Cog) and discriminative tasks (improved F1 scores, e.g., 83.1 to 83.5 with synthetic data) \\cite{xie2024l8a}.\n        *   **HallusionBench**: Showed improvements in accuracy metrics (qAcc, fAcc, aAcc), particularly with synthetic data \\cite{xie2024l8a}.\n        *   **Overall**: V-DPO consistently outperformed SFT, vanilla DPO, and HA-DPO across various benchmarks.\n        *   **Analysis**: The paper highlights that V-DPO particularly excels in learning from *image-contrast preference data*, demonstrating its superior ability to elicit and understand nuances of visual context and mitigate over-reliance on language priors \\cite{xie2024l8a}.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations**: The paper acknowledges that imperfections in LLMs and generative models can introduce noise and bias into the synthetic data, which is addressed by a CLIPScore filtering mechanism \\cite{xie2024l8a}. The choice of the textual-only distribution `Ë†Ï€(Â· |x)` and the dynamics of visual guidance inflation are discussed as implementation considerations \\cite{xie2024l8a}.\n    *   **Scope of Applicability**: The method is primarily focused on mitigating hallucination in LVLMs for vision-conditioned text generation tasks, especially those involving unconventional visual content \\cite{xie2024l8a}.\n\n*   **Technical Significance**\n    *   **Advancement of State-of-the-Art**: V-DPO provides a novel and effective training-time strategy to directly combat LVLM hallucination by explicitly enhancing visual context learning through a modified preference optimization objective \\cite{xie2024l8a}. This offers a more generalizable solution than inference-time methods.\n    *   **Potential Impact**: The introduction and empirical validation of *image-contrast preference data* open new avenues for data augmentation strategies in multimodal learning. The integration of CFG into DPO for visual grounding could inspire similar vision-guided preference learning paradigms in other multimodal tasks, leading to more reliable and factually consistent LVLMs \\cite{xie2024l8a}.",
        "keywords": [
          "LVLM Hallucination",
          "Vision-guided Direct Preference Optimization (V-DPO)",
          "Image-contrast Preference Data",
          "Classifier-Free Guidance (CFG)",
          "Visual Grounding",
          "Language Priors Over-reliance",
          "Synthetic Data Augmentation",
          "KL-divergence Objective",
          "Denoising Diffusion Probabilistic Models (DDPMs)",
          "Hallucination Mitigation",
          "Preference Optimization",
          "Multimodal Learning"
        ],
        "paper_type": "based on the abstract and introduction, this paper is best classified as **technical**.\n\nhere's why:\n\n*   **abstract mentions:** \"we **propose** vision-guided direct preference optimization (v-dpo) to enhance visual context learning at training time.\" the word \"propose\" is a strong indicator for the \"technical\" category, signifying the introduction of a new method or system.\n*   **introduction discusses:** the paper identifies a \"technical problem\" (hallucination in lvlms due to over-reliance on llm backbone) and immediately sets up the \"proposed solution\" (v-dpo).\n*   while the paper also includes significant \"empirical\" elements (constructing a dataset, achieving improvements on benchmarks, analysis of effectiveness), these are primarily to validate the *new method* being proposed. the core contribution is the development and presentation of v-dpo."
      },
      "file_name": "73020a07af4cfc42286e299097a0e35d2fe71a6c.pdf"
    },
    {
      "success": true,
      "doc_id": "0a1d7e745cbea231718fa9e66ab5da08",
      "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Large Vision-Language Models (LVLMs) suffer from significant reliability issues due to \"language hallucination\" and \"visual illusion\" when performing image-context reasoning. Language hallucination refers to drawing conclusions without visual input, relying on language priors. Visual illusion involves misinterpreting visual inputs, leading to overconfident yet erroneous assertions.\n    *   **Importance and Challenge**: This problem is critical because LVLMs like GPT-4V and LLaVA-1.5, despite their advanced capabilities, exhibit a pronounced language bias where knowledge priors often conflict with visual context. Existing benchmarks primarily focus on object hallucinations and lack the diagnostic capabilities to systematically dissect and understand these complex failure modes, hindering targeted improvements.\n\n*   **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: Previous research on hallucination in LVLMs primarily focuses on detecting/evaluating object hallucinations (e.g., POPE, GAVIE) or methods to reduce them. Traditional Visual Language (VL) benchmarks assess distinct skills like visual recognition or image description, while more recent multi-modal benchmarks (e.g., MME, MMBench, MM-Vet) evaluate complex tasks.\n    *   **Limitations of Previous Solutions**: Existing benchmarks fall short in providing a detailed ability assessment for advanced LVLMs, particularly in diagnosing the *types* of failures. They are limited in scope, focusing mainly on object hallucinations, and lack the structured approach needed for quantitative analysis of language hallucination and visual illusion. None provide a comprehensive diagnostic suite with control groups and human-edited images to systematically analyze these entangled failure modes.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method**: \\cite{guan2023z15} introduces **HALLUSION BENCH**, a comprehensive diagnostic benchmark for evaluating image-context reasoning in LVLMs. It comprises 346 images paired with 1129 meticulously human-crafted questions.\n    *   **Novelty**:\n        *   **Novel VQ Structure with Control Groups**: The benchmark features a unique visual-question (VQ) pair structure designed to establish control groups. This allows for quantitative analysis of models' response tendencies, logical consistency, and specific failure modes.\n        *   **Human-Edited Images**: A significant portion (45%) of the images are expertly modified by human professionals (e.g., flipping, order reversing, masking, object/color editing) to create scenarios that challenge LVLMs' robustness and expose specific vulnerabilities.\n        *   **Visual Question Taxonomy**: Questions are categorized into \"Visual Dependent\" (requiring visual context for an affirmative answer) and \"Visual Supplement\" (where visual input provides supplemental information or corrections, potentially conflicting with parametric memory). This taxonomy helps diagnose the balance between language priors and visual understanding.\n        *   **GPT-4 Assisted Evaluation**: A text-only GPT-4 judge is employed to evaluate model responses (Correct, Incorrect, Uncertain), demonstrating high alignment with human judgment.\n\n*   **Key Technical Contributions**\n    *   **Novel Diagnostic Suite**: HALLUSION BENCH is the first advanced diagnostic suite specifically designed to systematically dissect and analyze diverse failure modes, including both language hallucination and visual illusion, in LVLMs \\cite{guan2023z15}.\n    *   **Structured VQA for Quantitative Analysis**: The innovative structure of VQA pairs, including control groups and human-edited images, enables a quantitative analysis of specific dimensions and aspects where current models falter, moving beyond traditional accuracy metrics.\n    *   **Comprehensive Failure Mode Analysis**: The benchmark facilitates an in-depth exploration and diagnosis of various issues faced by LVLMs, providing insights into why state-of-the-art models fail.\n    *   **Diverse Content and Modalities**: It covers a wide range of topics (e.g., math, geography, illusions) and visual formats (e.g., charts, tables, maps, videos, consecutive images), offering a robust evaluation environment.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted**: \\cite{guan2023z15} benchmarked 15 different state-of-the-art LVLMs, including GPT-4V(ision), Gemini Pro Vision, Claude 3, and LLaVA-1.5, on HALLUSION BENCH.\n    *   **Key Performance Metrics**: The primary metric reported is \"Question Pair Accuracy\" (`qAcc`), alongside \"All accuracy\" (`aAcc`) and \"Figure Accuracy\" (`fAcc`). Analytical criteria like \"Yes/No Bias Test,\" \"Consistency Test,\" and \"Diagnostic Test\" are introduced for deeper failure diagnosis.\n    *   **Comparison Results**:\n        *   The state-of-the-art GPT-4V achieved a Question Pair Accuracy of merely **31.42%**.\n        *   All other evaluated models performed significantly worse, achieving accuracy below **16%**.\n        *   This highlights the formidable challenges HALLUSION BENCH presents to existing methods and the severe limitations of current LVLMs in nuanced image-context reasoning.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations**: The evaluation primarily uses Yes/No questions, which might limit the expressiveness of responses. While GPT-4 assisted evaluation aligns with human judgment, it still involves an LLM as a judge, which could introduce its own biases, though efforts were made to mitigate this (e.g., 3 evaluations, temperature 0).\n    *   **Scope of Applicability**: HALLUSION BENCH is specifically designed for diagnosing language hallucination and visual illusion in LVLMs, particularly focusing on image-context reasoning. While diverse, the dataset is human-curated and finite, and may not cover all possible failure modes or real-world scenarios.\n\n*   **Technical Significance**\n    *   **Advancement of State-of-the-Art**: \\cite{guan2023z15} significantly advances the technical state-of-the-art by providing the first dedicated diagnostic suite for systematically analyzing the complex and entangled failure modes of language hallucination and visual illusion in LVLMs. It moves beyond simple accuracy metrics to enable a deeper, quantitative understanding of model shortcomings.\n    *   **Potential Impact on Future Research**: The insights gained from HALLUSION BENCH, particularly the detailed analysis of failure modes and the low performance of even state-of-the-art models, lay crucial groundwork for future research. It suggests potential pathways for improving LVLMs, aiming to make them more robust, balanced (between language priors and visual context), and precise, ultimately leading to more reliable and trustworthy AI systems.",
      "intriguing_abstract": "Large Vision-Language Models (LVLMs) are increasingly powerful, yet their reliability is severely compromised by \"language hallucination\" and \"visual illusion,\" leading to overconfident, erroneous assertions in image-context reasoning. Existing benchmarks lack the diagnostic precision to dissect these entangled failure modes, hindering targeted improvements. We introduce **HALLUSION BENCH**, a novel and comprehensive diagnostic benchmark comprising 346 images and 1129 meticulously human-crafted questions.\n\nHALLUSION BENCH features an innovative visual-question (VQ) structure with control groups and 45% human-edited images, specifically designed to quantitatively analyze LVLMs' response tendencies and expose vulnerabilities to language priors versus visual input. Our unique Visual Question Taxonomy further distinguishes between visual dependency and supplementation, providing unprecedented insights into model biases. Benchmarking 15 state-of-the-art LVLMs, including GPT-4V, we reveal alarming limitations: GPT-4V achieves a mere 31.42% accuracy, with other models performing significantly worse. HALLUSION BENCH is the first diagnostic suite to systematically analyze these critical shortcomings, laying crucial groundwork for developing more robust, balanced, and trustworthy LVLMs.",
      "keywords": [
        "Large Vision-Language Models (LVLMs)",
        "language hallucination",
        "visual illusion",
        "image-context reasoning",
        "HALLUSION BENCH",
        "diagnostic benchmark",
        "novel VQ structure",
        "control groups",
        "human-edited images",
        "Visual Question Taxonomy",
        "quantitative failure analysis",
        "GPT-4 assisted evaluation",
        "low SOTA LVLM performance",
        "complex failure modes"
      ],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/0b395ed1c8b284e551172b728e83cf257e33729a.pdf",
      "citation_key": "guan2023z15",
      "metadata": {
        "title": "Hallusionbench: An Advanced Diagnostic Suite for Entangled Language Hallucination and Visual Illusion in Large Vision-Language Models",
        "authors": [
          "Tianrui Guan",
          "Fuxiao Liu",
          "Xiyang Wu",
          "Ruiqi Xian",
          "Zongxia Li",
          "Xiaoyu Liu",
          "Xijun Wang",
          "Lichang Chen",
          "Furong Huang",
          "Yaser Yacoob",
          "Dinesh Manocha",
          "Tianyi Zhou"
        ],
        "published_date": "2023",
        "abstract": "We introduce â€œHALLUSIONBENCH11â€œHallusionâ€ is a portmanteau of â€œhallucinationâ€ and â€œillusion.â€,â€ a comprehensive benchmark designed for the evaluation of image-context rea-soning. This benchmark presents significant challenges to advanced large visual-language models (LVLMs), such as GPT-4V(ision), Gemini Pro Vision, Claude 3, and LLaVA-1.5, by emphasizing nuanced understanding and interpre-tation of visual data. The benchmark comprises 346 images paired with 1129 questions, all meticulously crafted by human experts. We introduce a novel structure for these visual questions designed to establish control groups. This structure enables us to conduct a quantitative analysis of the models' response tendencies, logical consistency, and various failure modes. In our evaluation on Hallusion-bench, we benchmarked 15 different models, highlighting a 31.42% question-pair accuracy achieved by the state-of-the-art GPT-4V. Notably, all other evaluated models achieve accuracy below 16%. Moreover, our analysis not only high-lights the observed failure modes, including language hal-lucination and visual illusion but also deepens an under-standing of these pitfalls. Our comprehensive case studies within Hallusionbench shed light on the challenges of hallucination and illusion in LVLMs. Based on these in-sights, we suggest potential pathways for their future im-provement. The benchmark and codebase can be accessed at https://github.com/tianyi-labIHallusionBench.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/0b395ed1c8b284e551172b728e83cf257e33729a.pdf",
        "venue": "Computer Vision and Pattern Recognition",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Large Vision-Language Models (LVLMs) suffer from significant reliability issues due to \"language hallucination\" and \"visual illusion\" when performing image-context reasoning. Language hallucination refers to drawing conclusions without visual input, relying on language priors. Visual illusion involves misinterpreting visual inputs, leading to overconfident yet erroneous assertions.\n    *   **Importance and Challenge**: This problem is critical because LVLMs like GPT-4V and LLaVA-1.5, despite their advanced capabilities, exhibit a pronounced language bias where knowledge priors often conflict with visual context. Existing benchmarks primarily focus on object hallucinations and lack the diagnostic capabilities to systematically dissect and understand these complex failure modes, hindering targeted improvements.\n\n*   **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: Previous research on hallucination in LVLMs primarily focuses on detecting/evaluating object hallucinations (e.g., POPE, GAVIE) or methods to reduce them. Traditional Visual Language (VL) benchmarks assess distinct skills like visual recognition or image description, while more recent multi-modal benchmarks (e.g., MME, MMBench, MM-Vet) evaluate complex tasks.\n    *   **Limitations of Previous Solutions**: Existing benchmarks fall short in providing a detailed ability assessment for advanced LVLMs, particularly in diagnosing the *types* of failures. They are limited in scope, focusing mainly on object hallucinations, and lack the structured approach needed for quantitative analysis of language hallucination and visual illusion. None provide a comprehensive diagnostic suite with control groups and human-edited images to systematically analyze these entangled failure modes.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method**: \\cite{guan2023z15} introduces **HALLUSION BENCH**, a comprehensive diagnostic benchmark for evaluating image-context reasoning in LVLMs. It comprises 346 images paired with 1129 meticulously human-crafted questions.\n    *   **Novelty**:\n        *   **Novel VQ Structure with Control Groups**: The benchmark features a unique visual-question (VQ) pair structure designed to establish control groups. This allows for quantitative analysis of models' response tendencies, logical consistency, and specific failure modes.\n        *   **Human-Edited Images**: A significant portion (45%) of the images are expertly modified by human professionals (e.g., flipping, order reversing, masking, object/color editing) to create scenarios that challenge LVLMs' robustness and expose specific vulnerabilities.\n        *   **Visual Question Taxonomy**: Questions are categorized into \"Visual Dependent\" (requiring visual context for an affirmative answer) and \"Visual Supplement\" (where visual input provides supplemental information or corrections, potentially conflicting with parametric memory). This taxonomy helps diagnose the balance between language priors and visual understanding.\n        *   **GPT-4 Assisted Evaluation**: A text-only GPT-4 judge is employed to evaluate model responses (Correct, Incorrect, Uncertain), demonstrating high alignment with human judgment.\n\n*   **Key Technical Contributions**\n    *   **Novel Diagnostic Suite**: HALLUSION BENCH is the first advanced diagnostic suite specifically designed to systematically dissect and analyze diverse failure modes, including both language hallucination and visual illusion, in LVLMs \\cite{guan2023z15}.\n    *   **Structured VQA for Quantitative Analysis**: The innovative structure of VQA pairs, including control groups and human-edited images, enables a quantitative analysis of specific dimensions and aspects where current models falter, moving beyond traditional accuracy metrics.\n    *   **Comprehensive Failure Mode Analysis**: The benchmark facilitates an in-depth exploration and diagnosis of various issues faced by LVLMs, providing insights into why state-of-the-art models fail.\n    *   **Diverse Content and Modalities**: It covers a wide range of topics (e.g., math, geography, illusions) and visual formats (e.g., charts, tables, maps, videos, consecutive images), offering a robust evaluation environment.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted**: \\cite{guan2023z15} benchmarked 15 different state-of-the-art LVLMs, including GPT-4V(ision), Gemini Pro Vision, Claude 3, and LLaVA-1.5, on HALLUSION BENCH.\n    *   **Key Performance Metrics**: The primary metric reported is \"Question Pair Accuracy\" (`qAcc`), alongside \"All accuracy\" (`aAcc`) and \"Figure Accuracy\" (`fAcc`). Analytical criteria like \"Yes/No Bias Test,\" \"Consistency Test,\" and \"Diagnostic Test\" are introduced for deeper failure diagnosis.\n    *   **Comparison Results**:\n        *   The state-of-the-art GPT-4V achieved a Question Pair Accuracy of merely **31.42%**.\n        *   All other evaluated models performed significantly worse, achieving accuracy below **16%**.\n        *   This highlights the formidable challenges HALLUSION BENCH presents to existing methods and the severe limitations of current LVLMs in nuanced image-context reasoning.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations**: The evaluation primarily uses Yes/No questions, which might limit the expressiveness of responses. While GPT-4 assisted evaluation aligns with human judgment, it still involves an LLM as a judge, which could introduce its own biases, though efforts were made to mitigate this (e.g., 3 evaluations, temperature 0).\n    *   **Scope of Applicability**: HALLUSION BENCH is specifically designed for diagnosing language hallucination and visual illusion in LVLMs, particularly focusing on image-context reasoning. While diverse, the dataset is human-curated and finite, and may not cover all possible failure modes or real-world scenarios.\n\n*   **Technical Significance**\n    *   **Advancement of State-of-the-Art**: \\cite{guan2023z15} significantly advances the technical state-of-the-art by providing the first dedicated diagnostic suite for systematically analyzing the complex and entangled failure modes of language hallucination and visual illusion in LVLMs. It moves beyond simple accuracy metrics to enable a deeper, quantitative understanding of model shortcomings.\n    *   **Potential Impact on Future Research**: The insights gained from HALLUSION BENCH, particularly the detailed analysis of failure modes and the low performance of even state-of-the-art models, lay crucial groundwork for future research. It suggests potential pathways for improving LVLMs, aiming to make them more robust, balanced (between language priors and visual context), and precise, ultimately leading to more reliable and trustworthy AI systems.",
        "keywords": [
          "Large Vision-Language Models (LVLMs)",
          "language hallucination",
          "visual illusion",
          "image-context reasoning",
          "HALLUSION BENCH",
          "diagnostic benchmark",
          "novel VQ structure",
          "control groups",
          "human-edited images",
          "Visual Question Taxonomy",
          "quantitative failure analysis",
          "GPT-4 assisted evaluation",
          "low SOTA LVLM performance",
          "complex failure modes"
        ],
        "paper_type": "based on the abstract and introduction:\n\n*   the abstract explicitly states: \"we introduce 'hallusion bench,' a comprehensive benchmark designed for the evaluation of image-context reasoning.\"\n*   it further mentions: \"we introduce a novel structure for these visual questions designed to establish control groups.\"\n*   the paper then describes using this benchmark to evaluate 15 different models and presents quantitative results (e.g., \"31.42% question-pair accuracy\").\n*   the availability of the \"benchmark and codebase\" is also highlighted.\n\nthis strongly aligns with the \"technical\" classification criteria:\n*   **technical** - presents new methods, algorithms, or systems\n    *   abstract mentions: \"propose\", \"develop\", \"present\", \"algorithm\", \"method\"\n    *   introduction discusses: technical problem, proposed solution\n\nthe \"hallusion bench\" itself is a new system/method for evaluating lvlms, and the \"novel structure\" for questions is a new method. while the paper also conducts an empirical study *using* this benchmark, the primary contribution is the creation and introduction of the benchmark itself.\n\ntherefore, the paper is best classified as **technical**."
      },
      "file_name": "0b395ed1c8b284e551172b728e83cf257e33729a.pdf"
    },
    {
      "success": true,
      "doc_id": "a594f88dc018323d3854979df61e9fa7",
      "summary": "Here's a focused summary of the technical paper for a literature review, adhering to the specified citation and bullet format:\n\n### Technical Paper Analysis:\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: The paper addresses the critical issue of hallucination in Large Language Models (LLMs), specifically focusing on the lack of robust and realistic evaluation benchmarks for Chinese LLMs.\n    *   **Importance and Challenge**:\n        *   Hallucinated text compromises the practical utility and reliability of LLMs in professional contexts \\cite{liang20236sh}.\n        *   Existing hallucination benchmarks often rely on \"constrained generation\" techniques (e.g., directed hallucination induction, text perturbation), which do not accurately reflect the \"unrestricted text generation\" demanded by real-world applications \\cite{liang20236sh}. This makes it difficult to determine if observed hallucinations are genuine model errors or artifacts of the constrained generation process.\n        *   There is a significant lack of a well-established, comprehensive Chinese-language dataset and evaluation framework dedicated to assessing LLM hallucinations \\cite{liang20236sh}. Chinese presents unique challenges due to its extensive lexicon and complex word segmentation.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**:\n        *   The work positions itself against benchmarks that use Constrained Hallucination Generation (CHG), such as HaluEval \\cite{liang20236sh} and HaDes \\cite{liang20236sh}, which specify hallucination types or perturb text.\n        *   It acknowledges that most existing evaluation datasets are English-centric, with only two Chinese benchmarks (ChineseFactEval, HalluQA) noted as having very limited sizes (125 and 450 questions, respectively) \\cite{liang20236sh}.\n        *   It notes that previous dataset construction often relies on a single LLM for hallucination generation (e.g., ChatGPT for HaluEval and PHD, InstructGPT for FActScore and FACTOR) \\cite{liang20236sh}.\n    *   **Limitations of Previous Solutions**:\n        *   **Constrained Generation Bias**: Previous methods often introduce biases towards predefined hallucination types, hindering the simulation of real-world, spontaneous hallucinations \\cite{liang20236sh}. This limits in-depth analysis of how LLMs inherently produce errors.\n        *   **Language Gap**: The predominant focus on English benchmarks neglects the specific challenges and needs of Chinese LLM evaluation \\cite{liang20236sh}.\n        *   **Limited Scope of Evaluation Metrics**: Many existing metrics are rule-based and primarily classify hallucinations, failing to evaluate an LLM's ability to *generate content without hallucinations* in an unconstrained setting \\cite{liang20236sh}.\n        *   **Single Model Generation Bias**: Relying on a single LLM to generate hallucinated content can introduce a bias specific to that model, not reflecting the broader landscape of LLM behaviors \\cite{liang20236sh}.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper introduces **UHG Eval**, an Unconstrained Hallucination Generation Evaluation benchmark and framework \\cite{liang20236sh}.\n        *   **Unconstrained Generation**: LLMs are prompted with \"beginning text\" from real Chinese news articles to generate continuations without any restrictive instructions, aiming for \"organic results\" \\cite{liang20236sh}.\n        *   **Two-Stage Annotation**: To manage the complexity of annotating unconstrained generations, a two-stage process is employed: 1) Hallucination Ranking, and 2) Automatic Labeling and Human Rechecking \\cite{liang20236sh}.\n        *   **Hallucination Ranking**: This stage selects the most appropriate continuation from multiple candidates based on two metrics: *fluency* (assessed by a fine-tuned reward model) and *likelihood of hallucination occurrence* (estimated by lexical correlation with reference information) \\cite{liang20236sh}.\n    *   **Novelty/Difference**:\n        *   **Unconstrained Generation Paradigm**: Unlike most prior work, UHG Eval focuses on generating hallucinations in a free, unrestricted manner, mirroring real-world LLM usage \\cite{liang20236sh}.\n        *   **Multi-LLM Generation**: Instead of a single model, UHG Eval utilizes *five distinct Chinese LLMs* (ChatGLM2-6B, Baichuan2-13B, Qwen-14B, InternLM-20B, Xinyu-7B) to generate candidate continuations, mitigating model-specific biases and creating a more heterogeneous dataset \\cite{liang20236sh}.\n        *   **Novel `kwPrec` Metric**: The paper proposes `kwPrec` (keyword precision), which uses an LLM (e.g., GPT3.5-Turbo) to extract keywords from generated text and checks for exact matches in reference information. This focuses on *factual relevance* rather than mere expressional similarity (like BLEU/ROUGE), making it more effective for discovering factual hallucinations \\cite{liang20236sh}.\n        *   **Chinese-Centric Focus**: It specifically addresses the critical gap in Chinese hallucination evaluation, providing a much-needed large-scale dataset and framework for this language \\cite{liang20236sh}.\n\n4.  **Key Technical Contributions**\n    *   **Novel Dataset**: Development of an unconstrained hallucination evaluation dataset for Chinese LLMs, comprising over 5000 items derived from real Chinese news articles and multi-LLM generated continuations \\cite{liang20236sh}.\n    *   **Unified Evaluation Framework (UHG Eval)**: Establishment of a comprehensive framework that supports discriminative, selective, and generative evaluations, offering a diverse approach to hallucination assessment \\cite{liang20236sh}.\n    *   **`kwPrec` Metric**: Introduction of the keyword precision (`kwPrec`) metric, leveraging LLMs for keyword extraction to assess factual relevance and improve hallucination detection beyond traditional lexical overlap metrics \\cite{liang20236sh}.\n    *   **Multi-LLM Generation Strategy**: A novel approach to dataset creation by employing multiple diverse LLMs for unconstrained text generation, ensuring a broader and less biased representation of hallucination types \\cite{liang20236sh}.\n    *   **Two-Stage Annotation Pipeline**: A practical and cost-effective two-stage annotation process combining hallucination ranking (using fluency and likelihood metrics) with automatic labeling and human rechecking to efficiently create high-quality datasets from unconstrained generation \\cite{liang20236sh}.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**: The paper conducted a comprehensive empirical analysis by evaluating prominent Chinese LLMs and classic GPT series models \\cite{liang20236sh}.\n    *   **Models Evaluated**: Eight prominent Chinese LLMs (e.g., ChatGLM2-6B, Baichuan2-13B, Qwen-14B, InternLM-20B, Xinyu-7B) and three classic GPT series models (e.g., GPT-4, GPT-3.5 Turbo) were evaluated \\cite{liang20236sh}.\n    *   **Key Performance Metrics**: The evaluation framework encompasses discriminative, selective, and generative evaluations. Specific metrics include accuracy (Acc), keyword precision (kwPrec), BERTScore, BLEU, and ROUGE \\cite{liang20236sh}.\n    *   **Comparison Results**: The evaluation aimed to derive insights regarding hallucination across various LLMs, exploring their credibility. (Specific results are not detailed in the provided text but are implied to be part of the \"comprehensive empirical analysis\" and available on the project webpage).\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**:\n        *   The two-stage annotation process, while efficient, still involves human rechecking, which can be resource-intensive \\cite{liang20236sh}.\n        *   The `kwPrec` metric relies on an LLM (e.g., GPT3.5-Turbo) for keyword extraction, meaning its effectiveness is dependent on the quality of that LLM's extraction capabilities \\cite{liang20236sh}.\n        *   The fluency metric relies on a reward model, whose performance is tied to its training data and fine-tuning \\cite{liang20236sh}.\n    *   **Scope of Applicability**:\n        *   The dataset is primarily focused on Chinese news articles from a specific historical period (2015-2017), chosen to align with LLM training data \\cite{liang20236sh}. While news covers diverse topics, the domain specificity might limit direct applicability to highly specialized or conversational domains without further adaptation.\n        *   The benchmark is designed for evaluating factual hallucinations in text generation tasks, particularly continuations of news articles.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**:\n        *   **More Realistic Hallucination Assessment**: By pioneering an unconstrained generation paradigm and multi-LLM generation, UHG Eval provides a more realistic and less biased assessment of LLM hallucination behavior, moving beyond the limitations of constrained generation \\cite{liang20236sh}.\n        *   **Bridging the Chinese Language Gap**: It significantly advances the state-of-the-art for Chinese LLM evaluation by providing the first large-scale, unconstrained hallucination benchmark and a comprehensive framework, addressing a critical need in the field \\cite{liang20236sh}.\n        *   **Novel Evaluation Metric**: The introduction of `kwPrec` offers a more factually-oriented metric for hallucination detection, complementing existing lexical overlap metrics and providing deeper insights into factual inconsistencies \\cite{liang20236sh}.\n    *   **Potential Impact on Future Research**:\n        *   **Deeper Hallucination Analysis**: The unconstrained nature of the dataset enables future research to explore the underlying mechanisms of hallucination, such as differences in model hidden states between hallucinated and unhallucinated text spans \\cite{liang20236sh}.\n        *   **Improved LLM Reliability**: A more accurate and realistic evaluation benchmark can drive the development of more reliable and less hallucinatory Chinese LLMs, enhancing their utility in professional and real-world applications.\n        *   **Foundation for Multilingual Hallucination Research**: The methodology and framework established by UHG Eval could serve as a blueprint for developing similar unconstrained hallucination benchmarks in other languages.",
      "intriguing_abstract": "The pervasive issue of hallucination severely undermines the trustworthiness and practical utility of Large Language Models (LLMs), particularly in real-world, unconstrained text generation scenarios. Existing evaluation benchmarks often fall short, relying on artificial constrained generation and predominantly focusing on English, leaving a critical gap for robust assessment of Chinese LLMs. We introduce **UHG Eval**, a novel Unconstrained Hallucination Generation Evaluation benchmark and comprehensive framework designed specifically for Chinese LLMs.\n\nUHG Eval pioneers an *unconstrained generation paradigm*, prompting LLMs with real Chinese news articles to produce organic continuations, mirroring authentic usage. To overcome single-model bias, our dataset features diverse generations from *five distinct Chinese LLMs*. We propose `kwPrec` (keyword precision), an innovative LLM-driven metric that extracts keywords to robustly assess *factual consistency* beyond traditional lexical overlap. Comprising over 5000 meticulously annotated items, UHG Eval offers a unified framework for discriminative, selective, and generative evaluations. This work provides an unprecedented, realistic benchmark for Chinese LLMs, paving the way for deeper analysis into hallucination mechanisms and fostering the development of significantly more reliable and factually grounded LLM applications.",
      "keywords": [
        "Hallucination in LLMs",
        "Chinese LLMs",
        "Unconstrained text generation",
        "UHG Eval",
        "`kwPrec` metric",
        "Multi-LLM generation strategy",
        "Two-stage annotation pipeline",
        "Evaluation benchmarks",
        "Factual relevance",
        "LLM reliability",
        "Constrained generation bias",
        "Dataset construction"
      ],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/1146d40d3d01427a008a20530269667b8989750c.pdf",
      "citation_key": "liang20236sh",
      "metadata": {
        "title": "UHGEval: Benchmarking the Hallucination of Chinese Large Language Models via Unconstrained Generation",
        "authors": [
          "Xun Liang",
          "Shichao Song",
          "Simin Niu",
          "Zhiyu Li",
          "Feiyu Xiong",
          "Bo Tang",
          "Zhaohui Wy",
          "Dawei He",
          "Peng Cheng",
          "Zhonghao Wang",
          "Haiying Deng"
        ],
        "published_date": "2023",
        "abstract": "Large language models (LLMs) have emerged as pivotal contributors in contemporary natural language processing and are increasingly being applied across a diverse range of industries. However, these large-scale probabilistic statistical models cannot currently ensure the requisite quality in professional content generation. These models often produce hallucinated text, compromising their practical utility in professional contexts. To assess the authentic reliability of LLMs in text generation, numerous initiatives have developed benchmark evaluations for hallucination phenomena. Nevertheless, these benchmarks frequently utilize constrained generation techniques due to cost and temporal constraints. These techniques encompass the use of directed hallucination induction and strategies that deliberately alter authentic text to produce hallucinations. These approaches are not congruent with the unrestricted text generation demanded by real-world applications. Furthermore, a well-established Chinese-language dataset dedicated to the evaluation of hallucinations in text generation is presently lacking. Consequently, we have developed an Unconstrained Hallucination Generation Evaluation (UHGEval) benchmark, designed to compile outputs produced with minimal restrictions by LLMs. Concurrently, we have established a comprehensive benchmark evaluation framework to aid subsequent researchers in undertaking scalable and reproducible experiments. We have also executed extensive experiments, evaluating prominent Chinese language models and the GPT series models to derive professional performance insights regarding hallucination challenges.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/1146d40d3d01427a008a20530269667b8989750c.pdf",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the technical paper for a literature review, adhering to the specified citation and bullet format:\n\n### Technical Paper Analysis:\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: The paper addresses the critical issue of hallucination in Large Language Models (LLMs), specifically focusing on the lack of robust and realistic evaluation benchmarks for Chinese LLMs.\n    *   **Importance and Challenge**:\n        *   Hallucinated text compromises the practical utility and reliability of LLMs in professional contexts \\cite{liang20236sh}.\n        *   Existing hallucination benchmarks often rely on \"constrained generation\" techniques (e.g., directed hallucination induction, text perturbation), which do not accurately reflect the \"unrestricted text generation\" demanded by real-world applications \\cite{liang20236sh}. This makes it difficult to determine if observed hallucinations are genuine model errors or artifacts of the constrained generation process.\n        *   There is a significant lack of a well-established, comprehensive Chinese-language dataset and evaluation framework dedicated to assessing LLM hallucinations \\cite{liang20236sh}. Chinese presents unique challenges due to its extensive lexicon and complex word segmentation.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**:\n        *   The work positions itself against benchmarks that use Constrained Hallucination Generation (CHG), such as HaluEval \\cite{liang20236sh} and HaDes \\cite{liang20236sh}, which specify hallucination types or perturb text.\n        *   It acknowledges that most existing evaluation datasets are English-centric, with only two Chinese benchmarks (ChineseFactEval, HalluQA) noted as having very limited sizes (125 and 450 questions, respectively) \\cite{liang20236sh}.\n        *   It notes that previous dataset construction often relies on a single LLM for hallucination generation (e.g., ChatGPT for HaluEval and PHD, InstructGPT for FActScore and FACTOR) \\cite{liang20236sh}.\n    *   **Limitations of Previous Solutions**:\n        *   **Constrained Generation Bias**: Previous methods often introduce biases towards predefined hallucination types, hindering the simulation of real-world, spontaneous hallucinations \\cite{liang20236sh}. This limits in-depth analysis of how LLMs inherently produce errors.\n        *   **Language Gap**: The predominant focus on English benchmarks neglects the specific challenges and needs of Chinese LLM evaluation \\cite{liang20236sh}.\n        *   **Limited Scope of Evaluation Metrics**: Many existing metrics are rule-based and primarily classify hallucinations, failing to evaluate an LLM's ability to *generate content without hallucinations* in an unconstrained setting \\cite{liang20236sh}.\n        *   **Single Model Generation Bias**: Relying on a single LLM to generate hallucinated content can introduce a bias specific to that model, not reflecting the broader landscape of LLM behaviors \\cite{liang20236sh}.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper introduces **UHG Eval**, an Unconstrained Hallucination Generation Evaluation benchmark and framework \\cite{liang20236sh}.\n        *   **Unconstrained Generation**: LLMs are prompted with \"beginning text\" from real Chinese news articles to generate continuations without any restrictive instructions, aiming for \"organic results\" \\cite{liang20236sh}.\n        *   **Two-Stage Annotation**: To manage the complexity of annotating unconstrained generations, a two-stage process is employed: 1) Hallucination Ranking, and 2) Automatic Labeling and Human Rechecking \\cite{liang20236sh}.\n        *   **Hallucination Ranking**: This stage selects the most appropriate continuation from multiple candidates based on two metrics: *fluency* (assessed by a fine-tuned reward model) and *likelihood of hallucination occurrence* (estimated by lexical correlation with reference information) \\cite{liang20236sh}.\n    *   **Novelty/Difference**:\n        *   **Unconstrained Generation Paradigm**: Unlike most prior work, UHG Eval focuses on generating hallucinations in a free, unrestricted manner, mirroring real-world LLM usage \\cite{liang20236sh}.\n        *   **Multi-LLM Generation**: Instead of a single model, UHG Eval utilizes *five distinct Chinese LLMs* (ChatGLM2-6B, Baichuan2-13B, Qwen-14B, InternLM-20B, Xinyu-7B) to generate candidate continuations, mitigating model-specific biases and creating a more heterogeneous dataset \\cite{liang20236sh}.\n        *   **Novel `kwPrec` Metric**: The paper proposes `kwPrec` (keyword precision), which uses an LLM (e.g., GPT3.5-Turbo) to extract keywords from generated text and checks for exact matches in reference information. This focuses on *factual relevance* rather than mere expressional similarity (like BLEU/ROUGE), making it more effective for discovering factual hallucinations \\cite{liang20236sh}.\n        *   **Chinese-Centric Focus**: It specifically addresses the critical gap in Chinese hallucination evaluation, providing a much-needed large-scale dataset and framework for this language \\cite{liang20236sh}.\n\n4.  **Key Technical Contributions**\n    *   **Novel Dataset**: Development of an unconstrained hallucination evaluation dataset for Chinese LLMs, comprising over 5000 items derived from real Chinese news articles and multi-LLM generated continuations \\cite{liang20236sh}.\n    *   **Unified Evaluation Framework (UHG Eval)**: Establishment of a comprehensive framework that supports discriminative, selective, and generative evaluations, offering a diverse approach to hallucination assessment \\cite{liang20236sh}.\n    *   **`kwPrec` Metric**: Introduction of the keyword precision (`kwPrec`) metric, leveraging LLMs for keyword extraction to assess factual relevance and improve hallucination detection beyond traditional lexical overlap metrics \\cite{liang20236sh}.\n    *   **Multi-LLM Generation Strategy**: A novel approach to dataset creation by employing multiple diverse LLMs for unconstrained text generation, ensuring a broader and less biased representation of hallucination types \\cite{liang20236sh}.\n    *   **Two-Stage Annotation Pipeline**: A practical and cost-effective two-stage annotation process combining hallucination ranking (using fluency and likelihood metrics) with automatic labeling and human rechecking to efficiently create high-quality datasets from unconstrained generation \\cite{liang20236sh}.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**: The paper conducted a comprehensive empirical analysis by evaluating prominent Chinese LLMs and classic GPT series models \\cite{liang20236sh}.\n    *   **Models Evaluated**: Eight prominent Chinese LLMs (e.g., ChatGLM2-6B, Baichuan2-13B, Qwen-14B, InternLM-20B, Xinyu-7B) and three classic GPT series models (e.g., GPT-4, GPT-3.5 Turbo) were evaluated \\cite{liang20236sh}.\n    *   **Key Performance Metrics**: The evaluation framework encompasses discriminative, selective, and generative evaluations. Specific metrics include accuracy (Acc), keyword precision (kwPrec), BERTScore, BLEU, and ROUGE \\cite{liang20236sh}.\n    *   **Comparison Results**: The evaluation aimed to derive insights regarding hallucination across various LLMs, exploring their credibility. (Specific results are not detailed in the provided text but are implied to be part of the \"comprehensive empirical analysis\" and available on the project webpage).\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**:\n        *   The two-stage annotation process, while efficient, still involves human rechecking, which can be resource-intensive \\cite{liang20236sh}.\n        *   The `kwPrec` metric relies on an LLM (e.g., GPT3.5-Turbo) for keyword extraction, meaning its effectiveness is dependent on the quality of that LLM's extraction capabilities \\cite{liang20236sh}.\n        *   The fluency metric relies on a reward model, whose performance is tied to its training data and fine-tuning \\cite{liang20236sh}.\n    *   **Scope of Applicability**:\n        *   The dataset is primarily focused on Chinese news articles from a specific historical period (2015-2017), chosen to align with LLM training data \\cite{liang20236sh}. While news covers diverse topics, the domain specificity might limit direct applicability to highly specialized or conversational domains without further adaptation.\n        *   The benchmark is designed for evaluating factual hallucinations in text generation tasks, particularly continuations of news articles.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**:\n        *   **More Realistic Hallucination Assessment**: By pioneering an unconstrained generation paradigm and multi-LLM generation, UHG Eval provides a more realistic and less biased assessment of LLM hallucination behavior, moving beyond the limitations of constrained generation \\cite{liang20236sh}.\n        *   **Bridging the Chinese Language Gap**: It significantly advances the state-of-the-art for Chinese LLM evaluation by providing the first large-scale, unconstrained hallucination benchmark and a comprehensive framework, addressing a critical need in the field \\cite{liang20236sh}.\n        *   **Novel Evaluation Metric**: The introduction of `kwPrec` offers a more factually-oriented metric for hallucination detection, complementing existing lexical overlap metrics and providing deeper insights into factual inconsistencies \\cite{liang20236sh}.\n    *   **Potential Impact on Future Research**:\n        *   **Deeper Hallucination Analysis**: The unconstrained nature of the dataset enables future research to explore the underlying mechanisms of hallucination, such as differences in model hidden states between hallucinated and unhallucinated text spans \\cite{liang20236sh}.\n        *   **Improved LLM Reliability**: A more accurate and realistic evaluation benchmark can drive the development of more reliable and less hallucinatory Chinese LLMs, enhancing their utility in professional and real-world applications.\n        *   **Foundation for Multilingual Hallucination Research**: The methodology and framework established by UHG Eval could serve as a blueprint for developing similar unconstrained hallucination benchmarks in other languages.",
        "keywords": [
          "Hallucination in LLMs",
          "Chinese LLMs",
          "Unconstrained text generation",
          "UHG Eval",
          "`kwPrec` metric",
          "Multi-LLM generation strategy",
          "Two-stage annotation pipeline",
          "Evaluation benchmarks",
          "Factual relevance",
          "LLM reliability",
          "Constrained generation bias",
          "Dataset construction"
        ],
        "paper_type": "based on the abstract and introduction:\n\n1.  **\"we have developed an unconstrained hallucination generation evaluation (uhg eval) benchmark\"**\n2.  **\"we have established a comprehensive benchmark evaluation framework\"**\n3.  the paper identifies a problem with existing evaluation methods (constrained generation) and a gap (lack of chinese dataset).\n4.  it then proposes and develops a new solution: a new benchmark and an evaluation framework.\n5.  finally, it mentions using this new benchmark to evaluate models (\"we have also evaluated prominent chinese llms...\").\n\nthese points strongly align with the criteria for a **technical** paper, which \"presents new methods, algorithms, or systems\" and discusses a \"technical problem, proposed solution.\" the development of a new benchmark and evaluation framework constitutes a new system/method for assessing llm hallucinations. while it also includes an empirical evaluation, the primary contribution highlighted is the creation of this new tool.\n\n**classification: technical**"
      },
      "file_name": "1146d40d3d01427a008a20530269667b8989750c.pdf"
    },
    {
      "success": true,
      "doc_id": "593d42de6c8f8c30d4eede4702d3d198",
      "summary": "Here's a focused summary of the paper for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Large Language Models (LLMs) are prone to \"hallucination,\" generating plausible but nonfactual content \\cite{huang2023akj}.\n    *   **Importance and Challenge**: This phenomenon significantly undermines the reliability of LLMs in real-world information retrieval (IR) systems (e.g., chatbots, search engines, recommender systems) \\cite{huang2023akj}. LLMs' open-ended, general-purpose nature presents distinct challenges compared to prior task-specific models, necessitating a re-evaluation of hallucination concepts. The highly convincing, human-like nature of LLM outputs makes detecting these hallucinations particularly difficult, posing risks of spreading misinformation or causing harm \\cite{huang2023akj}.\n\n*   **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: This survey differentiates itself from previous works by offering a unique and comprehensive perspective on LLM hallucinations \\cite{huang2023akj}.\n    *   **Limitations of Previous Solutions (Other Surveys)**:\n        *   Prior surveys often focused on pre-trained models for conventional NLG tasks, not specifically LLMs \\cite{huang2023akj}.\n        *   Some concentrated solely on mitigation strategies or took a broader view of LLM trustworthiness without deep-diving into specific hallucination phenomena \\cite{huang2023akj}.\n        *   While some addressed factuality, this work extends the discussion to include \"faithfulness hallucinations\" \\cite{huang2023akj}.\n        *   Compared to the most closely aligned survey, this paper proposes a *unique taxonomy*, conducts a *more comprehensive analysis of hallucination causes*, and crucially, presents *mitigation strategies directly tied to these identified causes*, offering a more targeted and coherent framework \\cite{huang2023akj}.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method**: As a survey, the core \"method\" is its structured analysis and synthesis of the field. Key innovations include:\n        *   **Novel Taxonomy**: Proposes a redefined taxonomy of LLM hallucination tailored for LLMs, categorizing it into **factuality hallucination** (discrepancy with real-world facts) and **faithfulness hallucination** (divergence from user input/context or lack of self-consistency, further subdivided into instruction, context, and logical inconsistency) \\cite{huang2023akj}.\n        *   **Comprehensive Cause Analysis**: Identifies and categorizes the multifaceted sources of LLM hallucinations into three main aspects: data-related, training-related (pre-training, supervised fine-tuning, RLHF), and inference-related factors \\cite{huang2023akj}.\n        *   **Structured Mitigation Framework**: Presents mitigation strategies systematically, directly aligning them with their corresponding causes (data-related, training-related, inference-related approaches) \\cite{huang2023akj}.\n        *   **RAG System Analysis**: Provides an in-depth analysis of the current limitations and challenges faced by Retrieval-Augmented Generation (RAG) systems in effectively combating hallucinations \\cite{huang2023akj}.\n    *   **Novelty/Difference**: The primary novelty lies in its innovative, layered taxonomy and the coherent organizational structure that directly links the identified causes of hallucination to specific mitigation strategies, providing a more actionable framework for researchers \\cite{huang2023akj}.\n\n*   **Key Technical Contributions**\n    *   An innovative and refined taxonomy of LLM hallucinations, distinguishing between \"factuality hallucination\" and \"faithfulness hallucination\" with further subcategories \\cite{huang2023akj}.\n    *   A comprehensive categorization of the underlying causes of LLM hallucinations across data, training (pre-training, SFT, RLHF), and inference stages \\cite{huang2023akj}.\n    *   A systematic overview of effective detection methods and benchmarks specifically designed for LLM hallucinations \\cite{huang2023akj}.\n    *   A structured framework of mitigation strategies, directly correlating them with their root causes, spanning data filtering, model editing, RAG, pre-training/misalignment mitigation, and factuality/faithfulness enhanced decoding \\cite{huang2023akj}.\n    *   An in-depth analysis of the inherent limitations and challenges of current Retrieval-Augmented Generation (RAG) systems in addressing LLM hallucinations \\cite{huang2023akj}.\n\n*   **Experimental Validation**\n    *   As a survey paper, this work does not conduct its own original experiments. Instead, it provides a comprehensive review and synthesis of the experimental validation landscape within the field \\cite{huang2023akj}.\n    *   It outlines various **hallucination detection methods** (e.g., for factuality and faithfulness) and provides an exhaustive overview of **benchmarks** (e.g., TruthfulQA, HalluQA, HaluEval-2.0 for evaluation; SelfCheckGPT-Wikibio, HaluEval, FELM for detection) used to assess the extent of hallucinations and the efficacy of detection/mitigation methods in the literature \\cite{huang2023akj}.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The survey's primary focus is on factual and faithfulness hallucinations in LLMs, providing a deep dive into these specific aspects rather than a broader view of all LLM trustworthiness challenges \\cite{huang2023akj}.\n    *   **Scope of Applicability**: The survey's findings and frameworks are directly applicable to understanding, detecting, and mitigating hallucinations in general-purpose LLMs. It also highlights future research directions, including hallucinations in large vision-language models and understanding knowledge boundaries \\cite{huang2023akj}.\n\n*   **Technical Significance**\n    *   **Advance State-of-the-Art**: This survey significantly advances the technical state-of-the-art by providing a much-needed, structured, and comprehensive understanding of LLM hallucinations \\cite{huang2023akj}. Its innovative taxonomy and cause-aligned mitigation framework offer a clearer conceptual and practical roadmap for researchers and practitioners.\n    *   **Potential Impact**: It is expected to guide future research by clarifying definitions, identifying root causes, and systematically organizing detection and mitigation strategies \\cite{huang2023akj}. This will be crucial for developing more robust, reliable, and trustworthy LLM-based systems, particularly in critical applications like information retrieval, and for addressing emerging challenges such as hallucinations in multimodal LLMs \\cite{huang2023akj}.",
      "intriguing_abstract": "Large Language Models (LLMs) revolutionize information access, yet their pervasive tendency to \"hallucinate\"â€”generating plausible but nonfactual or unfaithful contentâ€”critically undermines their reliability in real-world **Information Retrieval (IR)** systems like chatbots and search engines. This survey offers a groundbreaking, comprehensive framework to demystify LLM hallucinations, moving beyond prior broad analyses.\n\nWe introduce a novel **taxonomy**, meticulously distinguishing between **factuality hallucination** (discrepancy with real-world facts) and **faithfulness hallucination** (divergence from input or self-consistency, including instruction, context, and logical inconsistencies). Our work provides an exhaustive categorization of underlying causes, spanning data-related, training-related (**pre-training, SFT, RLHF**), and **inference-related** factors. Crucially, we present a systematic framework of **mitigation strategies** directly aligned with these identified causes, offering a targeted roadmap for researchers. We also critically analyze the limitations of **Retrieval-Augmented Generation (RAG)** systems in combating these phenomena and provide a structured overview of **detection methods** and benchmarks. This paper is an essential guide for developing more robust, trustworthy LLM systems, paving the way for reliable AI in critical applications.",
      "keywords": [
        "Large Language Models (LLMs)",
        "LLM hallucination",
        "factuality hallucination",
        "faithfulness hallucination",
        "novel hallucination taxonomy",
        "hallucination causes analysis",
        "hallucination mitigation strategies",
        "Retrieval-Augmented Generation (RAG)",
        "hallucination detection",
        "LLM hallucination benchmarks",
        "information retrieval systems",
        "training factors (SFT",
        "RLHF)",
        "LLM reliability",
        "multimodal LLMs"
      ],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/1e909e2a8cdacdcdff125ebcc566f37cb869a1c8.pdf",
      "citation_key": "huang2023akj",
      "metadata": {
        "title": "A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions",
        "authors": [
          "Lei Huang",
          "Weijiang Yu",
          "Weitao Ma",
          "Weihong Zhong",
          "Zhangyin Feng",
          "Haotian Wang",
          "Qianglong Chen",
          "Weihua Peng",
          "Xiaocheng Feng",
          "Bing Qin",
          "Ting Liu"
        ],
        "published_date": "2023",
        "abstract": "The emergence of large language models (LLMs) has marked a significant breakthrough in natural language processing (NLP), fueling a paradigm shift in information acquisition. Nevertheless, LLMs are prone to hallucination, generating plausible yet nonfactual content. This phenomenon raises significant concerns over the reliability of LLMs in real-world information retrieval (IR) systems and has attracted intensive research to detect and mitigate such hallucinations. Given the open-ended general-purpose attributes inherent to LLMs, LLM hallucinations present distinct challenges that diverge from prior task-specific models. This divergence highlights the urgency for a nuanced understanding and comprehensive overview of recent advances in LLM hallucinations. In this survey, we begin with an innovative taxonomy of hallucination in the era of LLM and then delve into the factors contributing to hallucinations. Subsequently, we present a thorough overview of hallucination detection methods and benchmarks. Our discussion then transfers to representative methodologies for mitigating LLM hallucinations. Additionally, we delve into the current limitations faced by retrieval-augmented LLMs in combating hallucinations, offering insights for developing more robust IR systems. Finally, we highlight the promising research directions on LLM hallucinations, including hallucination in large vision-language models and understanding of knowledge boundaries in LLM hallucinations.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/1e909e2a8cdacdcdff125ebcc566f37cb869a1c8.pdf",
        "venue": "ACM Trans. Inf. Syst.",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the paper for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Large Language Models (LLMs) are prone to \"hallucination,\" generating plausible but nonfactual content \\cite{huang2023akj}.\n    *   **Importance and Challenge**: This phenomenon significantly undermines the reliability of LLMs in real-world information retrieval (IR) systems (e.g., chatbots, search engines, recommender systems) \\cite{huang2023akj}. LLMs' open-ended, general-purpose nature presents distinct challenges compared to prior task-specific models, necessitating a re-evaluation of hallucination concepts. The highly convincing, human-like nature of LLM outputs makes detecting these hallucinations particularly difficult, posing risks of spreading misinformation or causing harm \\cite{huang2023akj}.\n\n*   **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: This survey differentiates itself from previous works by offering a unique and comprehensive perspective on LLM hallucinations \\cite{huang2023akj}.\n    *   **Limitations of Previous Solutions (Other Surveys)**:\n        *   Prior surveys often focused on pre-trained models for conventional NLG tasks, not specifically LLMs \\cite{huang2023akj}.\n        *   Some concentrated solely on mitigation strategies or took a broader view of LLM trustworthiness without deep-diving into specific hallucination phenomena \\cite{huang2023akj}.\n        *   While some addressed factuality, this work extends the discussion to include \"faithfulness hallucinations\" \\cite{huang2023akj}.\n        *   Compared to the most closely aligned survey, this paper proposes a *unique taxonomy*, conducts a *more comprehensive analysis of hallucination causes*, and crucially, presents *mitigation strategies directly tied to these identified causes*, offering a more targeted and coherent framework \\cite{huang2023akj}.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method**: As a survey, the core \"method\" is its structured analysis and synthesis of the field. Key innovations include:\n        *   **Novel Taxonomy**: Proposes a redefined taxonomy of LLM hallucination tailored for LLMs, categorizing it into **factuality hallucination** (discrepancy with real-world facts) and **faithfulness hallucination** (divergence from user input/context or lack of self-consistency, further subdivided into instruction, context, and logical inconsistency) \\cite{huang2023akj}.\n        *   **Comprehensive Cause Analysis**: Identifies and categorizes the multifaceted sources of LLM hallucinations into three main aspects: data-related, training-related (pre-training, supervised fine-tuning, RLHF), and inference-related factors \\cite{huang2023akj}.\n        *   **Structured Mitigation Framework**: Presents mitigation strategies systematically, directly aligning them with their corresponding causes (data-related, training-related, inference-related approaches) \\cite{huang2023akj}.\n        *   **RAG System Analysis**: Provides an in-depth analysis of the current limitations and challenges faced by Retrieval-Augmented Generation (RAG) systems in effectively combating hallucinations \\cite{huang2023akj}.\n    *   **Novelty/Difference**: The primary novelty lies in its innovative, layered taxonomy and the coherent organizational structure that directly links the identified causes of hallucination to specific mitigation strategies, providing a more actionable framework for researchers \\cite{huang2023akj}.\n\n*   **Key Technical Contributions**\n    *   An innovative and refined taxonomy of LLM hallucinations, distinguishing between \"factuality hallucination\" and \"faithfulness hallucination\" with further subcategories \\cite{huang2023akj}.\n    *   A comprehensive categorization of the underlying causes of LLM hallucinations across data, training (pre-training, SFT, RLHF), and inference stages \\cite{huang2023akj}.\n    *   A systematic overview of effective detection methods and benchmarks specifically designed for LLM hallucinations \\cite{huang2023akj}.\n    *   A structured framework of mitigation strategies, directly correlating them with their root causes, spanning data filtering, model editing, RAG, pre-training/misalignment mitigation, and factuality/faithfulness enhanced decoding \\cite{huang2023akj}.\n    *   An in-depth analysis of the inherent limitations and challenges of current Retrieval-Augmented Generation (RAG) systems in addressing LLM hallucinations \\cite{huang2023akj}.\n\n*   **Experimental Validation**\n    *   As a survey paper, this work does not conduct its own original experiments. Instead, it provides a comprehensive review and synthesis of the experimental validation landscape within the field \\cite{huang2023akj}.\n    *   It outlines various **hallucination detection methods** (e.g., for factuality and faithfulness) and provides an exhaustive overview of **benchmarks** (e.g., TruthfulQA, HalluQA, HaluEval-2.0 for evaluation; SelfCheckGPT-Wikibio, HaluEval, FELM for detection) used to assess the extent of hallucinations and the efficacy of detection/mitigation methods in the literature \\cite{huang2023akj}.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The survey's primary focus is on factual and faithfulness hallucinations in LLMs, providing a deep dive into these specific aspects rather than a broader view of all LLM trustworthiness challenges \\cite{huang2023akj}.\n    *   **Scope of Applicability**: The survey's findings and frameworks are directly applicable to understanding, detecting, and mitigating hallucinations in general-purpose LLMs. It also highlights future research directions, including hallucinations in large vision-language models and understanding knowledge boundaries \\cite{huang2023akj}.\n\n*   **Technical Significance**\n    *   **Advance State-of-the-Art**: This survey significantly advances the technical state-of-the-art by providing a much-needed, structured, and comprehensive understanding of LLM hallucinations \\cite{huang2023akj}. Its innovative taxonomy and cause-aligned mitigation framework offer a clearer conceptual and practical roadmap for researchers and practitioners.\n    *   **Potential Impact**: It is expected to guide future research by clarifying definitions, identifying root causes, and systematically organizing detection and mitigation strategies \\cite{huang2023akj}. This will be crucial for developing more robust, reliable, and trustworthy LLM-based systems, particularly in critical applications like information retrieval, and for addressing emerging challenges such as hallucinations in multimodal LLMs \\cite{huang2023akj}.",
        "keywords": [
          "Large Language Models (LLMs)",
          "LLM hallucination",
          "factuality hallucination",
          "faithfulness hallucination",
          "novel hallucination taxonomy",
          "hallucination causes analysis",
          "hallucination mitigation strategies",
          "Retrieval-Augmented Generation (RAG)",
          "hallucination detection",
          "LLM hallucination benchmarks",
          "information retrieval systems",
          "training factors (SFT",
          "RLHF)",
          "LLM reliability",
          "multimodal LLMs"
        ],
        "paper_type": "based on the provided information, especially the title:\n\n*   **title:** \"a **survey** on hallucination in large language models: principles, taxonomy, challenges, and open questions\"\n\nthe title explicitly uses the word \"survey,\" which directly matches the classification criteria for a **survey** paper. the structure indicated by \"principles, taxonomy, challenges, and open questions\" further suggests a comprehensive review and organization of existing knowledge, which is characteristic of a survey.\n\ntherefore, the paper type is: **survey**"
      },
      "file_name": "1e909e2a8cdacdcdff125ebcc566f37cb869a1c8.pdf"
    },
    {
      "success": true,
      "doc_id": "88437968547fb216549f2ff30038c1e2",
      "summary": "Here is a focused summary of the technical paper for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Large Language Models (LLMs) suffer from knowledge hallucination, generating plausible but factually incorrect information. This problem is exacerbated in long-form tasks and by Retrieval-Augmented Language Models (RALMs) when processing lengthy retrieved contexts.\n    *   **Importance and Challenge**: Hallucination undermines LLM reliability. While RALMs are promising for integrating up-to-date knowledge, they face two key challenges: (1) lack of complete contextual semantics when only retrieving short snippets, and (2) \"getting lost in long contexts\" when retrieving entire documents, as irrelevant texts distract the LLM's reasoning, even for models designed for long inputs. `\\cite{lv2024k5x}` aims to address this by preserving complete contextual semantics while being robust to long, potentially noisy contexts.\n\n*   **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: `\\cite{lv2024k5x}` builds upon and aims to improve Retrieval-Augmented Language Models (RALMs) and implicitly complements Chain-of-X approaches (e.g., Chain-of-Thought, Chain-of-Verification) by providing more focused and relevant input.\n    *   **Limitations of Previous Solutions**:\n        *   **RALMs**: Existing RALMs can exacerbate hallucination when retrieving lengthy contexts due to the presence of irrelevant information. They struggle to balance providing comprehensive context with avoiding distraction from noise.\n        *   **Chain-of-X Methods**: These methods improve reasoning logic, implicitly reducing hallucination, but do not directly address the problem of irrelevant information within retrieved external knowledge.\n        *   **Generation-time correction**: Focuses on improving token generation policies or confidence scores, rather than refining the input context itself.\n        *   `\\cite{lv2024k5x}` positions itself as a novel framework that specifically tackles the challenge of \"getting lost in long contexts\" within RALMs, while ensuring complete contextual semantics are preserved.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method**: `\\cite{lv2024k5x}` proposes COFT (COarse-to-Fine highlighTing), a novel method that focuses on different granularity-level key texts within retrieved contexts to reduce hallucination.\n    *   **Components**: COFT consists of three main components:\n        *   **Recaller**: Extracts potential key entities from the query and reference context using an external knowledge graph (Wikidata) and enriches them with one-hop neighbors. It then retains only those entities present in the reference context.\n        *   **Scorer**: Measures the importance of each candidate entity by calculating its \"contextual weight.\" This is done using a small language model (Llama 7B) to derive self-information for tokens and introducing a TF-ISF (Term Frequency-Inverse Sentence Frequency) metric to assess entity importance within sentences and the overall context. The contextual weight combines these two measures.\n        *   **Selector**: Employs a dynamic threshold algorithm that considers both the length and informativeness of reference contexts to select high contextual weight entities. It then highlights the corresponding paragraphs, sentences, or words in a coarse-to-fine manner based on these selected entities and chosen granularity.\n    *   **Novelty**: The approach is novel due to its unified framework for coarse-to-fine highlighting, which dynamically identifies and emphasizes key information at varying granularities (word, sentence, paragraph) within long contexts. The combination of knowledge graph-based entity extraction, TF-ISF, self-information for contextual weighting, and a dynamic threshold for selection represents a significant innovation in managing context for RALMs.\n\n*   **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**:\n        *   **COFT Framework**: A novel coarse-to-fine highlighting method designed to reduce knowledge hallucination by enabling LLMs to focus on key lexical units in long contexts.\n        *   **Recaller Module**: Leverages an external knowledge graph (Wikidata) for robust extraction and enrichment of candidate key entities from queries and reference contexts.\n        *   **Scorer Module**: Introduces a method to calculate entity-level contextual weights by combining TF-ISF (Term Frequency-Inverse Sentence Frequency) with self-information derived from a small language model.\n        *   **Selector Module**: Develops a dynamic threshold algorithm for intelligently filtering and selecting high-importance entities, facilitating highlighting at multiple granularities (paragraph, sentence, word).\n    *   **System Design/Architectural Innovations**: The integration of Recaller, Scorer, and Selector into a unified, plug-and-play framework that can be easily incorporated into existing RALMs.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted**: Extensive experiments were performed on a knowledge hallucination benchmark and various long-form tasks, including reading comprehension and question answering. ChatGPT was used as the backbone LLM.\n    *   **Key Performance Metrics**: F1 score (for hallucination benchmark and question answering) and Precision (for reading comprehension).\n    *   **Comparison Results**:\n        *   Achieved a superior performance with an average improvement of **over 30% in the F1 score metric** on the knowledge hallucination benchmark.\n        *   Demonstrated remarkable versatility across various long-form tasks:\n            *   Achieved an average improvement of **4.6% in the precision metric** for reading comprehension.\n            *   Showed a maximum improvement of **10.5% in the F1 score metric** for question answering.\n        *   Outperformed existing methods such as Vanilla, CoT, CoVe, and CoN on a broad range of benchmarks (e.g., FELM-WK, FELM-Sci/Tech, FELM-Wri/Rec, RACE-M, Natural Questions-0.8, Trivial-0.8, WebQ-0.8).\n\n*   **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The effectiveness of COFT relies on the quality and coverage of the external knowledge graph (Wikidata) for entity extraction. The performance of the Scorer is dependent on the small language model's ability to accurately calculate self-information and the efficacy of the TF-ISF metric. The dynamic threshold algorithm's tuning is crucial for optimal entity selection.\n    *   **Scope of Applicability**: Primarily designed to reduce knowledge hallucination in LLMs, particularly in retrieval-augmented scenarios involving long and potentially noisy contexts. Its versatility extends to other long-form tasks like reading comprehension and question answering.\n\n*   **Technical Significance**\n    *   **Advancement of State-of-the-Art**: `\\cite{lv2024k5x}` significantly advances the technical state-of-the-art by providing a robust and effective solution to a critical problem in LLMs â€“ knowledge hallucination in the presence of long, retrieved contexts. The substantial improvement in F1 score (over 30%) highlights its impact on factual accuracy.\n    *   **Potential Impact on Future Research**:\n        *   Offers a plug-and-play framework that can be widely adopted to enhance the reliability and trustworthiness of LLMs in various applications requiring factual grounding.\n        *   The coarse-to-fine highlighting mechanism opens new avenues for research in intelligent context summarization, dynamic input processing, and attention mechanisms for LLMs dealing with complex, lengthy documents.\n        *   Could inspire further work on integrating external knowledge graphs with contextual weighting for more precise information retrieval and grounding.",
      "intriguing_abstract": "Large Language Models (LLMs) are increasingly prone to knowledge hallucination, a critical reliability issue exacerbated in Retrieval-Augmented Language Models (RALMs) when confronted with lengthy, noisy retrieved contexts. This \"getting lost in long contexts\" undermines factual accuracy. We introduce COFT (COarse-to-Fine highlighTing), a novel framework designed to precisely mitigate hallucination by enabling LLMs to focus on key lexical units. COFT dynamically identifies and emphasizes crucial information at varying granularitiesâ€”from words to paragraphsâ€”within retrieved documents. Our innovative approach integrates a knowledge graph for robust entity extraction, a sophisticated Scorer module combining TF-ISF and self-information for contextual weighting, and a dynamic threshold Selector. This plug-and-play system significantly enhances factual accuracy, achieving over 30% F1 score improvement on a knowledge hallucination benchmark. Furthermore, COFT demonstrates remarkable versatility, boosting performance by up to 10.5% F1 in question answering and 4.6% precision in reading comprehension tasks, outperforming existing methods. COFT represents a significant advancement, offering a robust solution for trustworthy LLM applications and paving the way for intelligent context processing.",
      "keywords": [
        "Knowledge hallucination",
        "Retrieval-Augmented Language Models (RALMs)",
        "long contexts",
        "COFT (Coarse-to-Fine Highlighting)",
        "knowledge graph-based entity extraction",
        "TF-ISF contextual weighting",
        "dynamic threshold algorithm",
        "multi-granularity highlighting",
        "plug-and-play framework",
        "factual grounding",
        "reading comprehension",
        "question answering",
        "over 30% F1 score improvement"
      ],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/1c7ef42897ad2dced83ab1d58d8fbd4539f87ddc.pdf",
      "citation_key": "lv2024k5x",
      "metadata": {
        "title": "Coarse-to-Fine Highlighting: Reducing Knowledge Hallucination in Large Language Models",
        "authors": [
          "Qitan Lv",
          "Jie Wang",
          "Hanzhu Chen",
          "Bin Li",
          "Yongdong Zhang",
          "Feng Wu"
        ],
        "published_date": "2024",
        "abstract": "Generation of plausible but incorrect factual information, often termed hallucination, has attracted significant research interest. Retrieval-augmented language model (RALM) -- which enhances models with up-to-date knowledge -- emerges as a promising method to reduce hallucination. However, existing RALMs may instead exacerbate hallucination when retrieving lengthy contexts. To address this challenge, we propose COFT, a novel \\textbf{CO}arse-to-\\textbf{F}ine highligh\\textbf{T}ing method to focus on different granularity-level key texts, thereby avoiding getting lost in lengthy contexts. Specifically, COFT consists of three components: \\textit{recaller}, \\textit{scorer}, and \\textit{selector}. First, \\textit{recaller} applies a knowledge graph to extract potential key entities in a given context. Second, \\textit{scorer} measures the importance of each entity by calculating its contextual weight. Finally, \\textit{selector} selects high contextual weight entities with a dynamic threshold algorithm and highlights the corresponding paragraphs, sentences, or words in a coarse-to-fine manner. Extensive experiments on the knowledge hallucination benchmark demonstrate the effectiveness of COFT, leading to a superior performance over $30\\%$ in the F1 score metric. Moreover, COFT also exhibits remarkable versatility across various long-form tasks, such as reading comprehension and question answering.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/1c7ef42897ad2dced83ab1d58d8fbd4539f87ddc.pdf",
        "venue": "International Conference on Machine Learning",
        "citationCount": 0,
        "score": 0,
        "summary": "Here is a focused summary of the technical paper for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Large Language Models (LLMs) suffer from knowledge hallucination, generating plausible but factually incorrect information. This problem is exacerbated in long-form tasks and by Retrieval-Augmented Language Models (RALMs) when processing lengthy retrieved contexts.\n    *   **Importance and Challenge**: Hallucination undermines LLM reliability. While RALMs are promising for integrating up-to-date knowledge, they face two key challenges: (1) lack of complete contextual semantics when only retrieving short snippets, and (2) \"getting lost in long contexts\" when retrieving entire documents, as irrelevant texts distract the LLM's reasoning, even for models designed for long inputs. `\\cite{lv2024k5x}` aims to address this by preserving complete contextual semantics while being robust to long, potentially noisy contexts.\n\n*   **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: `\\cite{lv2024k5x}` builds upon and aims to improve Retrieval-Augmented Language Models (RALMs) and implicitly complements Chain-of-X approaches (e.g., Chain-of-Thought, Chain-of-Verification) by providing more focused and relevant input.\n    *   **Limitations of Previous Solutions**:\n        *   **RALMs**: Existing RALMs can exacerbate hallucination when retrieving lengthy contexts due to the presence of irrelevant information. They struggle to balance providing comprehensive context with avoiding distraction from noise.\n        *   **Chain-of-X Methods**: These methods improve reasoning logic, implicitly reducing hallucination, but do not directly address the problem of irrelevant information within retrieved external knowledge.\n        *   **Generation-time correction**: Focuses on improving token generation policies or confidence scores, rather than refining the input context itself.\n        *   `\\cite{lv2024k5x}` positions itself as a novel framework that specifically tackles the challenge of \"getting lost in long contexts\" within RALMs, while ensuring complete contextual semantics are preserved.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method**: `\\cite{lv2024k5x}` proposes COFT (COarse-to-Fine highlighTing), a novel method that focuses on different granularity-level key texts within retrieved contexts to reduce hallucination.\n    *   **Components**: COFT consists of three main components:\n        *   **Recaller**: Extracts potential key entities from the query and reference context using an external knowledge graph (Wikidata) and enriches them with one-hop neighbors. It then retains only those entities present in the reference context.\n        *   **Scorer**: Measures the importance of each candidate entity by calculating its \"contextual weight.\" This is done using a small language model (Llama 7B) to derive self-information for tokens and introducing a TF-ISF (Term Frequency-Inverse Sentence Frequency) metric to assess entity importance within sentences and the overall context. The contextual weight combines these two measures.\n        *   **Selector**: Employs a dynamic threshold algorithm that considers both the length and informativeness of reference contexts to select high contextual weight entities. It then highlights the corresponding paragraphs, sentences, or words in a coarse-to-fine manner based on these selected entities and chosen granularity.\n    *   **Novelty**: The approach is novel due to its unified framework for coarse-to-fine highlighting, which dynamically identifies and emphasizes key information at varying granularities (word, sentence, paragraph) within long contexts. The combination of knowledge graph-based entity extraction, TF-ISF, self-information for contextual weighting, and a dynamic threshold for selection represents a significant innovation in managing context for RALMs.\n\n*   **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**:\n        *   **COFT Framework**: A novel coarse-to-fine highlighting method designed to reduce knowledge hallucination by enabling LLMs to focus on key lexical units in long contexts.\n        *   **Recaller Module**: Leverages an external knowledge graph (Wikidata) for robust extraction and enrichment of candidate key entities from queries and reference contexts.\n        *   **Scorer Module**: Introduces a method to calculate entity-level contextual weights by combining TF-ISF (Term Frequency-Inverse Sentence Frequency) with self-information derived from a small language model.\n        *   **Selector Module**: Develops a dynamic threshold algorithm for intelligently filtering and selecting high-importance entities, facilitating highlighting at multiple granularities (paragraph, sentence, word).\n    *   **System Design/Architectural Innovations**: The integration of Recaller, Scorer, and Selector into a unified, plug-and-play framework that can be easily incorporated into existing RALMs.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted**: Extensive experiments were performed on a knowledge hallucination benchmark and various long-form tasks, including reading comprehension and question answering. ChatGPT was used as the backbone LLM.\n    *   **Key Performance Metrics**: F1 score (for hallucination benchmark and question answering) and Precision (for reading comprehension).\n    *   **Comparison Results**:\n        *   Achieved a superior performance with an average improvement of **over 30% in the F1 score metric** on the knowledge hallucination benchmark.\n        *   Demonstrated remarkable versatility across various long-form tasks:\n            *   Achieved an average improvement of **4.6% in the precision metric** for reading comprehension.\n            *   Showed a maximum improvement of **10.5% in the F1 score metric** for question answering.\n        *   Outperformed existing methods such as Vanilla, CoT, CoVe, and CoN on a broad range of benchmarks (e.g., FELM-WK, FELM-Sci/Tech, FELM-Wri/Rec, RACE-M, Natural Questions-0.8, Trivial-0.8, WebQ-0.8).\n\n*   **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The effectiveness of COFT relies on the quality and coverage of the external knowledge graph (Wikidata) for entity extraction. The performance of the Scorer is dependent on the small language model's ability to accurately calculate self-information and the efficacy of the TF-ISF metric. The dynamic threshold algorithm's tuning is crucial for optimal entity selection.\n    *   **Scope of Applicability**: Primarily designed to reduce knowledge hallucination in LLMs, particularly in retrieval-augmented scenarios involving long and potentially noisy contexts. Its versatility extends to other long-form tasks like reading comprehension and question answering.\n\n*   **Technical Significance**\n    *   **Advancement of State-of-the-Art**: `\\cite{lv2024k5x}` significantly advances the technical state-of-the-art by providing a robust and effective solution to a critical problem in LLMs â€“ knowledge hallucination in the presence of long, retrieved contexts. The substantial improvement in F1 score (over 30%) highlights its impact on factual accuracy.\n    *   **Potential Impact on Future Research**:\n        *   Offers a plug-and-play framework that can be widely adopted to enhance the reliability and trustworthiness of LLMs in various applications requiring factual grounding.\n        *   The coarse-to-fine highlighting mechanism opens new avenues for research in intelligent context summarization, dynamic input processing, and attention mechanisms for LLMs dealing with complex, lengthy documents.\n        *   Could inspire further work on integrating external knowledge graphs with contextual weighting for more precise information retrieval and grounding.",
        "keywords": [
          "Knowledge hallucination",
          "Retrieval-Augmented Language Models (RALMs)",
          "long contexts",
          "COFT (Coarse-to-Fine Highlighting)",
          "knowledge graph-based entity extraction",
          "TF-ISF contextual weighting",
          "dynamic threshold algorithm",
          "multi-granularity highlighting",
          "plug-and-play framework",
          "factual grounding",
          "reading comprehension",
          "question answering",
          "over 30% F1 score improvement"
        ],
        "paper_type": "based on the abstract and introduction:\n\n*   the abstract explicitly states: \"we **propose coft, a novel** coarse-to- fine highligh ting **method** to focus on different granularity-level key texts...\"\n*   it then details the components of this method: \"coft consists of three components: **recaller, scorer, and selector**.\"\n*   it describes the functionality of these components, including \"applies a knowledge graph,\" \"measures the importance,\" and \"selects high contextual weight entities with a **dynamic threshold algorithm**.\"\n*   the introduction sets up a technical problem (hallucination in llms, especially in long-form tasks) and discusses existing approaches before implicitly introducing their solution (coft, as shown in figure 1).\n*   while it includes \"extensive experiments\" and \"superior performance,\" these are used to demonstrate the effectiveness of the *proposed method*, making the empirical aspect a validation of the technical contribution rather than the primary focus of the paper itself.\n\nthis aligns perfectly with the criteria for a **technical** paper: \"presents new methods, algorithms, or systems\" and the abstract mentions \"propose\", \"develop\", \"present\", \"algorithm\", \"method\".\n\n**classification: technical**"
      },
      "file_name": "1c7ef42897ad2dced83ab1d58d8fbd4539f87ddc.pdf"
    },
    {
      "success": true,
      "doc_id": "8d362637d7e30fd041061563e1e97d18",
      "summary": "Here's a focused summary of the technical paper for a literature review:\n\n### ANAH-v2: Scaling Analytical Hallucination Annotation of Large Language Models \\cite{gu202414e}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Large Language Models (LLMs) frequently exhibit \"hallucinations\" (plausible but unfaithful/nonsensical information) in long-form question-answering, significantly hindering their real-world applications.\n    *   **Importance & Challenge**:\n        *   Existing hallucination detection and mitigation datasets are severely limited in domain coverage and size, making comprehensive oversight of LLM hallucinations difficult.\n        *   Scaling these datasets is prohibitively expensive and labor-intensive due to the need for fine-grained, sentence-by-sentence annotation requiring intensive human effort.\n        *   The reliability of existing automatic hallucination annotators (even powerful models like GPT-4) is insufficient, often producing inaccurate results.\n        *   The application of self-improvement techniques to *fine-grained hallucination annotation* is largely unexplored.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**:\n        *   Builds upon prior work in data augmentation and self-training strategies used in fields like image segmentation, multi-lingual translation, and math reasoning.\n        *   Extends existing hallucination annotation datasets (e.g., ANAH) by addressing their scalability and reliability issues.\n    *   **Limitations of Previous Solutions**:\n        *   Most self-improvement methods require additional resources (human labor or supplementary models), which is not sustainable for large-scale hallucination annotation.\n        *   Existing hallucination datasets, especially those with fine-grained annotations, are limited in size and scalability due to high annotation costs.\n        *   Previous fine-grained annotation methods (e.g., original ANAH) often combine multiple judgments into a single turn, diverging from human cognitive processes and leading to unsatisfactory accuracy.\n        *   Automatic annotators, including GPT-4, have not achieved human-compatible performance for this meticulous task.\n    *   **Positioning**: `\\cite{gu202414e}` introduces a novel, self-sufficient iterative framework that simultaneously scales the dataset and improves annotator accuracy, specifically tailored for fine-grained hallucination annotation without requiring external human or model supervision beyond the initial seed data.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: An iterative self-training framework grounded in the Expectation-Maximization (EM) algorithm, designed to progressively scale up the hallucination annotation dataset and improve the annotator's accuracy.\n        *   **E-Step (Data Annotation)**: The current best hallucination annotator is used to estimate ground-truth annotations for a scaled dataset. This step incorporates a **self-consistency strategy** \\cite{gu202414e} (multiple samplings, majority vote for hallucination type, cosine similarity for reference points) to provide more robust and accurate annotations.\n        *   **M-Step (Annotator Training)**: The newly annotated data, combined with existing annotations, is used to train a new, more accurate hallucination annotator.\n    *   **Novelty & Differentiation**:\n        *   **Phased Analytical Hallucination Annotation**: Instead of a single-turn judgment, `\\cite{gu202414e}` proposes a three-phase process: (1) Factual Existence Judgment, (2) Reference Information Extraction, and (3) Hallucination Type Judgment (Contradictory, Unverifiable, No Hallucination, No Fact). This aligns more closely with human cognitive processes, improving annotation reliability.\n        *   **Multi-dimensional Data Scaling**: The iterative EM process is structured into three stages to progressively scale the dataset:\n            1.  **Stage 1 (Seed Data)**: Initial training on a human-annotated dataset (ANAH).\n            2.  **Stage 2 (Response Dimension)**: Augmenting responses for *existing questions* by collecting outputs from 13 diverse open-source LLMs (with/without reference documents).\n            3.  **Stage 3 (Topic Dimension)**: Expanding the number of topics and questions across various categories (location, person, event, thing) and domains.\n        *   **Self-Sufficient Pipeline**: The framework relies solely on the annotator model and the initial seed dataset, avoiding the need for continuous human labor or supplementary models for scaling.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**: Introduction of an EM-based iterative self-training framework specifically for fine-grained hallucination annotation, incorporating a robust self-consistency mechanism for pseudo-label generation \\cite{gu202414e}.\n    *   **System Design/Architectural Innovations**: A multi-dimensional data scaling strategy that systematically expands the dataset along both response and topic dimensions, leading to a comprehensive and diverse annotation dataset (ANAH-v2) \\cite{gu202414e}.\n    *   **Theoretical Insights/Analysis**: Formulation of the problem as an EM algorithm, providing a theoretical grounding for simultaneously improving annotator accuracy and dataset scale. The phased annotation process is a practical innovation based on cognitive alignment.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**:\n        *   Evaluation of the final ANAH-v2 annotator's performance on hallucination detection.\n        *   Demonstration of ANAH-v2's utility in hallucination mitigation through a simple re-ranking strategy.\n    *   **Key Performance Metrics & Comparison Results**:\n        *   **Hallucination Detection**: The final ANAH-v2 annotator (7B parameters) achieved new state-of-the-art (SOTA) results in zero-shot inference on HaluEval (81.54% F1) and HalluQA (94.44% F1) \\cite{gu202414e}.\n        *   **Outperformance of GPT-4**: On the in-domain ANAH test set, ANAH-v2-Stage3 achieved 89.24% F1 and 89.44% Accuracy, surpassing GPT-4's 87.11% F1 and 86.97% Accuracy \\cite{gu202414e}.\n        *   **Dataset Scale**: The resulting ANAH-v2 dataset comprises over 3,000 topics, ~196,000 model responses, and ~822,000 annotated sentences, significantly larger and more diverse than previous datasets \\cite{gu202414e}.\n        *   **Hallucination Mitigation**: Using ANAH-v2 as a re-ranker, the Natural Language Inference (NLI) metric for hallucination was improved from 25% to 37% on HaluEval \\cite{gu202414e}.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The paper notes that the hallucination mitigation strategy demonstrated is a \"simple re-ranking strategy,\" implying that more advanced mitigation techniques could be explored in future work. The self-consistency mechanism's reliance on cosine similarity for reference point selection might have limitations in nuanced cases.\n    *   **Scope of Applicability**: Primarily focused on fine-grained analytical hallucination annotation in long-form question-answering tasks. While shown to be effective for evaluation and mitigation, its direct applicability to other types of LLM errors or tasks might require adaptation.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: `\\cite{gu202414e}` significantly advances the technical state-of-the-art by developing a 7B parameter hallucination annotator that outperforms much larger models like GPT-4 in zero-shot hallucination detection on established benchmarks.\n    *   **Potential Impact on Future Research**:\n        *   **Scalable Oversight**: Provides a practical and scalable solution for the oversight of LLM hallucinations, addressing a critical bottleneck in LLM development and deployment.\n        *   **Automated Evaluation**: Enables automated, large-scale evaluation of hallucination levels across numerous open-source LLMs, offering a comprehensive benchmark for the research community.\n        *   **Enhanced Mitigation**: The accurate annotator can serve as a powerful tool for hallucination mitigation, either directly (e.g., via re-ranking) or by integrating with more advanced fine-grained RLHF or other training/decoding strategies.\n        *   **Resource for Research**: The publicly released ANAH-v2 dataset, code, and model provide a valuable resource for future research into LLM hallucinations.",
      "intriguing_abstract": "Large Language Models (LLMs) are powerful, yet their pervasive **hallucinations**â€”plausible but unfaithful outputsâ€”severely limit their real-world reliability. Current **fine-grained analytical hallucination annotation** is prohibitively expensive and lacks scalability, even for advanced models like GPT-4. We introduce **ANAH-v2**, a groundbreaking, self-sufficient iterative framework that simultaneously scales annotation datasets and enhances annotator accuracy.\n\nOur novel approach leverages an **Expectation-Maximization (EM) algorithm** with a robust **self-consistency strategy**, coupled with a human-aligned, multi-phased annotation process and multi-dimensional data scaling. This framework generates ANAH-v2, the largest and most diverse dataset of its kind, comprising over 822,000 annotated sentences. Remarkably, our 7B parameter ANAH-v2 annotator achieves **state-of-the-art zero-shot performance** on HaluEval (81.54% F1) and HalluQA (94.44% F1), *outperforming GPT-4* on in-domain tasks. ANAH-v2 offers an unprecedented, scalable solution for **LLM oversight, automated evaluation, and enhanced hallucination mitigation**, providing a critical resource for advancing trustworthy AI.",
      "keywords": [
        "Large Language Models (LLMs)",
        "Hallucinations",
        "Fine-grained hallucination annotation",
        "ANAH-v2 dataset",
        "Iterative self-training framework",
        "Expectation-Maximization (EM) algorithm",
        "Self-consistency strategy",
        "Phased analytical hallucination annotation",
        "Multi-dimensional data scaling",
        "Hallucination detection",
        "Hallucination mitigation",
        "Long-form question-answering",
        "State-of-the-art performance",
        "GPT-4 outperformance",
        "Scalable oversight"
      ],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/682ff66a5ec0248f7e4a17a684b2d1e328e57f70.pdf",
      "citation_key": "gu202414e",
      "metadata": {
        "title": "ANAH-v2: Scaling Analytical Hallucination Annotation of Large Language Models",
        "authors": [
          "Yuzhe Gu",
          "Ziwei Ji",
          "Wenwei Zhang",
          "Chengqi Lyu",
          "Dahua Lin",
          "Kai Chen"
        ],
        "published_date": "2024",
        "abstract": "Large language models (LLMs) exhibit hallucinations in long-form question-answering tasks across various domains and wide applications. Current hallucination detection and mitigation datasets are limited in domains and sizes, which struggle to scale due to prohibitive labor costs and insufficient reliability of existing hallucination annotators. To facilitate the scalable oversight of LLM hallucinations, this paper introduces an iterative self-training framework that simultaneously and progressively scales up the hallucination annotation dataset and improves the accuracy of the hallucination annotator. Based on the Expectation Maximization (EM) algorithm, in each iteration, the framework first applies a hallucination annotation pipeline to annotate a scaled dataset and then trains a more accurate hallucination annotator on the dataset. This new hallucination annotator is adopted in the hallucination annotation pipeline used for the next iteration. Extensive experimental results demonstrate that the finally obtained hallucination annotator with only 7B parameters surpasses the performance of GPT-4 and obtains new state-of-the-art hallucination detection results on HaluEval and HalluQA by zero-shot inference. Such an annotator can not only evaluate the hallucination levels of various LLMs on the large-scale dataset but also help to mitigate the hallucination of LLMs generations, with the Natural Language Inference (NLI) metric increasing from 25% to 37% on HaluEval.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/682ff66a5ec0248f7e4a17a684b2d1e328e57f70.pdf",
        "venue": "Neural Information Processing Systems",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the technical paper for a literature review:\n\n### ANAH-v2: Scaling Analytical Hallucination Annotation of Large Language Models \\cite{gu202414e}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Large Language Models (LLMs) frequently exhibit \"hallucinations\" (plausible but unfaithful/nonsensical information) in long-form question-answering, significantly hindering their real-world applications.\n    *   **Importance & Challenge**:\n        *   Existing hallucination detection and mitigation datasets are severely limited in domain coverage and size, making comprehensive oversight of LLM hallucinations difficult.\n        *   Scaling these datasets is prohibitively expensive and labor-intensive due to the need for fine-grained, sentence-by-sentence annotation requiring intensive human effort.\n        *   The reliability of existing automatic hallucination annotators (even powerful models like GPT-4) is insufficient, often producing inaccurate results.\n        *   The application of self-improvement techniques to *fine-grained hallucination annotation* is largely unexplored.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**:\n        *   Builds upon prior work in data augmentation and self-training strategies used in fields like image segmentation, multi-lingual translation, and math reasoning.\n        *   Extends existing hallucination annotation datasets (e.g., ANAH) by addressing their scalability and reliability issues.\n    *   **Limitations of Previous Solutions**:\n        *   Most self-improvement methods require additional resources (human labor or supplementary models), which is not sustainable for large-scale hallucination annotation.\n        *   Existing hallucination datasets, especially those with fine-grained annotations, are limited in size and scalability due to high annotation costs.\n        *   Previous fine-grained annotation methods (e.g., original ANAH) often combine multiple judgments into a single turn, diverging from human cognitive processes and leading to unsatisfactory accuracy.\n        *   Automatic annotators, including GPT-4, have not achieved human-compatible performance for this meticulous task.\n    *   **Positioning**: `\\cite{gu202414e}` introduces a novel, self-sufficient iterative framework that simultaneously scales the dataset and improves annotator accuracy, specifically tailored for fine-grained hallucination annotation without requiring external human or model supervision beyond the initial seed data.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: An iterative self-training framework grounded in the Expectation-Maximization (EM) algorithm, designed to progressively scale up the hallucination annotation dataset and improve the annotator's accuracy.\n        *   **E-Step (Data Annotation)**: The current best hallucination annotator is used to estimate ground-truth annotations for a scaled dataset. This step incorporates a **self-consistency strategy** \\cite{gu202414e} (multiple samplings, majority vote for hallucination type, cosine similarity for reference points) to provide more robust and accurate annotations.\n        *   **M-Step (Annotator Training)**: The newly annotated data, combined with existing annotations, is used to train a new, more accurate hallucination annotator.\n    *   **Novelty & Differentiation**:\n        *   **Phased Analytical Hallucination Annotation**: Instead of a single-turn judgment, `\\cite{gu202414e}` proposes a three-phase process: (1) Factual Existence Judgment, (2) Reference Information Extraction, and (3) Hallucination Type Judgment (Contradictory, Unverifiable, No Hallucination, No Fact). This aligns more closely with human cognitive processes, improving annotation reliability.\n        *   **Multi-dimensional Data Scaling**: The iterative EM process is structured into three stages to progressively scale the dataset:\n            1.  **Stage 1 (Seed Data)**: Initial training on a human-annotated dataset (ANAH).\n            2.  **Stage 2 (Response Dimension)**: Augmenting responses for *existing questions* by collecting outputs from 13 diverse open-source LLMs (with/without reference documents).\n            3.  **Stage 3 (Topic Dimension)**: Expanding the number of topics and questions across various categories (location, person, event, thing) and domains.\n        *   **Self-Sufficient Pipeline**: The framework relies solely on the annotator model and the initial seed dataset, avoiding the need for continuous human labor or supplementary models for scaling.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**: Introduction of an EM-based iterative self-training framework specifically for fine-grained hallucination annotation, incorporating a robust self-consistency mechanism for pseudo-label generation \\cite{gu202414e}.\n    *   **System Design/Architectural Innovations**: A multi-dimensional data scaling strategy that systematically expands the dataset along both response and topic dimensions, leading to a comprehensive and diverse annotation dataset (ANAH-v2) \\cite{gu202414e}.\n    *   **Theoretical Insights/Analysis**: Formulation of the problem as an EM algorithm, providing a theoretical grounding for simultaneously improving annotator accuracy and dataset scale. The phased annotation process is a practical innovation based on cognitive alignment.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**:\n        *   Evaluation of the final ANAH-v2 annotator's performance on hallucination detection.\n        *   Demonstration of ANAH-v2's utility in hallucination mitigation through a simple re-ranking strategy.\n    *   **Key Performance Metrics & Comparison Results**:\n        *   **Hallucination Detection**: The final ANAH-v2 annotator (7B parameters) achieved new state-of-the-art (SOTA) results in zero-shot inference on HaluEval (81.54% F1) and HalluQA (94.44% F1) \\cite{gu202414e}.\n        *   **Outperformance of GPT-4**: On the in-domain ANAH test set, ANAH-v2-Stage3 achieved 89.24% F1 and 89.44% Accuracy, surpassing GPT-4's 87.11% F1 and 86.97% Accuracy \\cite{gu202414e}.\n        *   **Dataset Scale**: The resulting ANAH-v2 dataset comprises over 3,000 topics, ~196,000 model responses, and ~822,000 annotated sentences, significantly larger and more diverse than previous datasets \\cite{gu202414e}.\n        *   **Hallucination Mitigation**: Using ANAH-v2 as a re-ranker, the Natural Language Inference (NLI) metric for hallucination was improved from 25% to 37% on HaluEval \\cite{gu202414e}.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The paper notes that the hallucination mitigation strategy demonstrated is a \"simple re-ranking strategy,\" implying that more advanced mitigation techniques could be explored in future work. The self-consistency mechanism's reliance on cosine similarity for reference point selection might have limitations in nuanced cases.\n    *   **Scope of Applicability**: Primarily focused on fine-grained analytical hallucination annotation in long-form question-answering tasks. While shown to be effective for evaluation and mitigation, its direct applicability to other types of LLM errors or tasks might require adaptation.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: `\\cite{gu202414e}` significantly advances the technical state-of-the-art by developing a 7B parameter hallucination annotator that outperforms much larger models like GPT-4 in zero-shot hallucination detection on established benchmarks.\n    *   **Potential Impact on Future Research**:\n        *   **Scalable Oversight**: Provides a practical and scalable solution for the oversight of LLM hallucinations, addressing a critical bottleneck in LLM development and deployment.\n        *   **Automated Evaluation**: Enables automated, large-scale evaluation of hallucination levels across numerous open-source LLMs, offering a comprehensive benchmark for the research community.\n        *   **Enhanced Mitigation**: The accurate annotator can serve as a powerful tool for hallucination mitigation, either directly (e.g., via re-ranking) or by integrating with more advanced fine-grained RLHF or other training/decoding strategies.\n        *   **Resource for Research**: The publicly released ANAH-v2 dataset, code, and model provide a valuable resource for future research into LLM hallucinations.",
        "keywords": [
          "Large Language Models (LLMs)",
          "Hallucinations",
          "Fine-grained hallucination annotation",
          "ANAH-v2 dataset",
          "Iterative self-training framework",
          "Expectation-Maximization (EM) algorithm",
          "Self-consistency strategy",
          "Phased analytical hallucination annotation",
          "Multi-dimensional data scaling",
          "Hallucination detection",
          "Hallucination mitigation",
          "Long-form question-answering",
          "State-of-the-art performance",
          "GPT-4 outperformance",
          "Scalable oversight"
        ],
        "paper_type": "based on the abstract and introduction:\n\n*   the abstract explicitly states: \"this paper introduces an iterative self-training framework that simultaneously and progressively scales up the annotation dataset and improves the accuracy of the annotator.\" it also mentions it's \"based on the expectation maximization algorithm\" and describes an \"automatic hallucination annotation pipeline.\"\n*   the introduction highlights a technical problem: \"constructing and scaling-up hallucination annotation datasets face significant hurdles\" due to \"prohibitively high costs and labor intensity\" and \"insufficiency of accurate human annotations, the reliability of existing hallucination annotators and detectors becomes another pressing concern.\"\n*   the abstract then details the proposed solution (the framework) and presents \"extensive experimental results\" demonstrating its effectiveness, including surpassing gpt-4 and achieving \"new state-of-the-art hallucination detection results.\"\n\nthese points strongly align with the criteria for a **technical** paper, which \"presents new methods, algorithms, or systems\" and discusses \"technical problem, proposed solution.\" while it includes empirical results, these results are presented to validate the newly proposed technical framework and annotator.\n\n**classification: technical**"
      },
      "file_name": "682ff66a5ec0248f7e4a17a684b2d1e328e57f70.pdf"
    },
    {
      "success": true,
      "doc_id": "60f0fa3d891fbcddeaacf0a2c34420de",
      "summary": "Here's a focused summary of the paper for a literature review, emphasizing technical innovations and empirical validation:\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Multi-modal Large Language Models (MLLMs) frequently suffer from Visual Hallucination (VH), where they generate factually incorrect details about an image during visual question answering.\n    *   **Importance & Challenge**: VH poses a significant obstacle to developing safe and trustworthy AI. Existing methods for identifying VH instances are limited to pre-existing image datasets (e.g., COCO), which results in a lack of diversity in VH instances and potential data contamination (if MLLMs were pre-trained on these datasets). This leads to a biased and often over-optimistic understanding of MLLMs' true VH performance.\n\n*   **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: Prior works have benchmarked MLLM VHs related to object existence, OCR, counting, position, orientation, and viewpoint \\cite{huang20247wn}.\n    *   **Limitations of Previous Solutions**: These studies primarily collect VH images from existing datasets, which restricts the diversity of VH scenarios and introduces the risk of data contamination, leading to an inaccurate assessment of MLLM robustness against hallucination \\cite{huang20247wn}.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes `VHTest`, a novel tool designed for the adversarial generation of a diverse set of VH instances \\cite{huang20247wn}. The process involves three key steps:\n        1.  **Step I: Finding Initial VH Instances**: Identifies image pairs from existing datasets (e.g., COCO) that exhibit high cosine similarity in CLIP embeddings but low similarity in DINO v2 embeddings. These contradictory similarities indicate potential VH triggers. Initial questions and reference answers are manually designed for these candidates \\cite{huang20247wn}.\n        2.  **Step II: Text Description Generation**: Leverages an MLLM (e.g., GPT-4V) to generate detailed text descriptions for each VH mode. These descriptions are derived from successful (or hypothetically constructed for unsuccessful) initial VH instances, aiming to explain potential causes of hallucination and guide the generation of new images \\cite{huang20247wn}.\n        3.  **Step III: Generating More VH Instances**: Utilizes a text-to-image generative model (e.g., DALLÂ·E-3) to create novel VH images based on the text descriptions from Step II. Human workers then formulate specific questions and reference answers for these generated images using object-driven templates, ensuring non-ambiguity \\cite{huang20247wn}.\n    *   **Novelty**: `VHTest` introduces an adversarial construction paradigm for VH instances, moving beyond passive collection from existing datasets. Its innovation lies in systematically generating *new, diverse, and uncontaminated* VH images using text-to-image models, guided by MLLM-generated descriptions of hallucination modes. It also introduces `shape` and `size` as new VH modes \\cite{huang20247wn}.\n\n*   **Key Technical Contributions**\n    *   **Novel Algorithm/Method**: The `VHTest` pipeline itself, which systematically combines vision encoders, MLLMs, and text-to-image generative models for adversarial VH instance generation \\cite{huang20247wn}.\n    *   **System Design/Architectural Innovations**: The integrated architecture that uses CLIP and DINO v2 for initial instance identification, an MLLM for abstracting VH modes into text descriptions, and DALLÂ·E-3 for synthesizing diverse VH images \\cite{huang20247wn}.\n    *   **Benchmark Dataset**: Construction of a new, publicly available benchmark dataset comprising 1,200 VH instances across 8 distinct VH modes (existence, shape, color, orientation, OCR, size, position, counting). This benchmark is available in both open-ended question (OEQ) and closed-ended yes/no question (YNQ) formats, designed to be diverse and free from data contamination \\cite{huang20247wn}.\n    *   **Mitigation Strategy**: Empirical demonstration that fine-tuning an MLLM (LLaVA-1.5) on the `VHTest` benchmark dataset can effectively reduce its likelihood of hallucinating without compromising performance on other general benchmarks \\cite{huang20247wn}.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted**:\n        *   Comprehensive evaluation of state-of-the-art MLLMs (GPT-4V, LLaVA-1.5, MiniGPT-v2) on the newly constructed `VHTest` OEQ benchmark \\cite{huang20247wn}.\n        *   Fine-tuning experiments with LLaVA-1.5 using the `VHTest` dataset to assess its impact on VH mitigation \\cite{huang20247wn}.\n    *   **Key Performance Metrics**: Accuracy, defined as the fraction of VH instances for which the MLLM's text response is factually correct compared to the reference answer \\cite{huang20247wn}.\n    *   **Comparison Results**:\n        *   **Significant Hallucination**: MLLMs exhibited substantial hallucination on the `VHTest` benchmark, with overall accuracies of 0.383 for GPT-4V, 0.229 for LLaVA-1.5, and 0.075 for MiniGPT-v2 \\cite{huang20247wn}.\n        *   **Mode-Specific Vulnerabilities**: Performance varied across VH modes; for instance, GPT-4V was most prone to orientation VH (0.153 accuracy), while LLaVA-1.5 and MiniGPT-v2 were most susceptible to OCR VH (0.127 and 0.000 accuracy, respectively) \\cite{huang20247wn}.\n        *   **Effective Mitigation**: Fine-tuning LLaVA-1.5 on the `VHTest` dataset successfully reduced hallucination, improving accuracy (e.g., +0.200 in position VH mode) without sacrificing performance on other benchmarks \\cite{huang20247wn}.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The quality and diversity of generated VH instances depend on the capabilities of the description-generation MLLM and the text-to-image generative model. The benchmark construction required approximately 300 human-hours for manual question/answer design and verification \\cite{huang20247wn}.\n    *   **Scope of Applicability**: The work focuses specifically on visual hallucination in MLLMs within the context of visual question answering, covering 8 defined VH modes.\n\n*   **Technical Significance**\n    *   **Advances State-of-the-Art**: `VHTest` provides a novel, systematic, and scalable methodology for generating diverse and uncontaminated VH instances, addressing a critical gap in MLLM evaluation. It offers a more robust and unbiased benchmark for assessing MLLM vulnerabilities to hallucination \\cite{huang20247wn}.\n    *   **Potential Impact**: This work enables more rigorous testing and evaluation of MLLMs, facilitating the development of safer and more trustworthy AI systems. It also opens new avenues for research into adversarial data generation for multi-modal models and targeted fine-tuning strategies to mitigate specific failure modes \\cite{huang20247wn}.",
      "intriguing_abstract": "The promise of Multi-modal Large Language Models (MLLMs) is significantly undermined by Visual Hallucination (VH), where models generate factually incorrect visual details. This critical flaw impedes the development of trustworthy AI, exacerbated by existing benchmarks that are often contaminated and lack diversity, leading to biased performance assessments.\n\nWe introduce `VHTest`, a novel adversarial generation framework designed to systematically create diverse, uncontaminated VH instances. `VHTest` employs a unique three-step pipeline, combining vision encoders (CLIP, DINO v2), MLLMs (GPT-4V) for hallucination mode abstraction, and text-to-image generative models (DALLÂ·E-3) to synthesize novel VH images. This approach yields a robust, publicly available benchmark of 1,200 instances across 8 distinct VH modes, including newly identified 'shape' and 'size' categories. Our comprehensive evaluation exposes alarming rates of hallucination in state-of-the-art MLLMs (e.g., GPT-4V accuracy 0.383). Critically, we demonstrate that fine-tuning MLLMs on `VHTest` effectively reduces VH without compromising general performance. `VHTest` offers an indispensable tool for rigorous MLLM evaluation, accelerating the path towards safer, more reliable AI and opening new avenues for adversarial robustness research.",
      "keywords": [
        "Multi-modal Large Language Models (MLLMs)",
        "Visual Hallucination (VH)",
        "VHTest",
        "Adversarial generation of VH instances",
        "Uncontaminated benchmark dataset",
        "Visual Question Answering (VQA)",
        "CLIP and DINO v2 embeddings",
        "Text-to-image generative models",
        "Hallucination mitigation",
        "AI safety and trustworthiness",
        "Mode-specific VH vulnerabilities",
        "Fine-tuning MLLMs"
      ],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/4d7c68ec1a86ef5d187e7edb2f0ad63adddc8ea2.pdf",
      "citation_key": "huang20247wn",
      "metadata": {
        "title": "Visual Hallucinations of Multi-modal Large Language Models",
        "authors": [
          "Wen Huang",
          "Hongbin Liu",
          "Minxin Guo",
          "N. Gong"
        ],
        "published_date": "2024",
        "abstract": "Visual hallucination (VH) means that a multi-modal LLM (MLLM) imagines incorrect details about an image in visual question answering. Existing studies find VH instances only in existing image datasets, which results in biased understanding of MLLMs' performance under VH due to limited diversity of such VH instances. In this work, we propose a tool called VHTest to generate a diverse set of VH instances. Specifically, VHTest finds some initial VH instances in existing image datasets (e.g., COCO), generates a text description for each VH mode, and uses a text-to-image generative model (e.g., DALL-E-3) to generate VH images based on the text descriptions. We collect a benchmark dataset with 1,200 VH instances in 8 VH modes using VHTest. We find that existing MLLMs such as GPT-4V, LLaVA-1.5, and MiniGPT-v2 hallucinate for a large fraction of the instances in our benchmark. Moreover, we find that fine-tuning an MLLM using our benchmark dataset reduces its likelihood to hallucinate without sacrificing its performance on other benchmarks. Our benchmarks are publicly available: https://github.com/wenhuang2000/VHTest.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/4d7c68ec1a86ef5d187e7edb2f0ad63adddc8ea2.pdf",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the paper for a literature review, emphasizing technical innovations and empirical validation:\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Multi-modal Large Language Models (MLLMs) frequently suffer from Visual Hallucination (VH), where they generate factually incorrect details about an image during visual question answering.\n    *   **Importance & Challenge**: VH poses a significant obstacle to developing safe and trustworthy AI. Existing methods for identifying VH instances are limited to pre-existing image datasets (e.g., COCO), which results in a lack of diversity in VH instances and potential data contamination (if MLLMs were pre-trained on these datasets). This leads to a biased and often over-optimistic understanding of MLLMs' true VH performance.\n\n*   **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: Prior works have benchmarked MLLM VHs related to object existence, OCR, counting, position, orientation, and viewpoint \\cite{huang20247wn}.\n    *   **Limitations of Previous Solutions**: These studies primarily collect VH images from existing datasets, which restricts the diversity of VH scenarios and introduces the risk of data contamination, leading to an inaccurate assessment of MLLM robustness against hallucination \\cite{huang20247wn}.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes `VHTest`, a novel tool designed for the adversarial generation of a diverse set of VH instances \\cite{huang20247wn}. The process involves three key steps:\n        1.  **Step I: Finding Initial VH Instances**: Identifies image pairs from existing datasets (e.g., COCO) that exhibit high cosine similarity in CLIP embeddings but low similarity in DINO v2 embeddings. These contradictory similarities indicate potential VH triggers. Initial questions and reference answers are manually designed for these candidates \\cite{huang20247wn}.\n        2.  **Step II: Text Description Generation**: Leverages an MLLM (e.g., GPT-4V) to generate detailed text descriptions for each VH mode. These descriptions are derived from successful (or hypothetically constructed for unsuccessful) initial VH instances, aiming to explain potential causes of hallucination and guide the generation of new images \\cite{huang20247wn}.\n        3.  **Step III: Generating More VH Instances**: Utilizes a text-to-image generative model (e.g., DALLÂ·E-3) to create novel VH images based on the text descriptions from Step II. Human workers then formulate specific questions and reference answers for these generated images using object-driven templates, ensuring non-ambiguity \\cite{huang20247wn}.\n    *   **Novelty**: `VHTest` introduces an adversarial construction paradigm for VH instances, moving beyond passive collection from existing datasets. Its innovation lies in systematically generating *new, diverse, and uncontaminated* VH images using text-to-image models, guided by MLLM-generated descriptions of hallucination modes. It also introduces `shape` and `size` as new VH modes \\cite{huang20247wn}.\n\n*   **Key Technical Contributions**\n    *   **Novel Algorithm/Method**: The `VHTest` pipeline itself, which systematically combines vision encoders, MLLMs, and text-to-image generative models for adversarial VH instance generation \\cite{huang20247wn}.\n    *   **System Design/Architectural Innovations**: The integrated architecture that uses CLIP and DINO v2 for initial instance identification, an MLLM for abstracting VH modes into text descriptions, and DALLÂ·E-3 for synthesizing diverse VH images \\cite{huang20247wn}.\n    *   **Benchmark Dataset**: Construction of a new, publicly available benchmark dataset comprising 1,200 VH instances across 8 distinct VH modes (existence, shape, color, orientation, OCR, size, position, counting). This benchmark is available in both open-ended question (OEQ) and closed-ended yes/no question (YNQ) formats, designed to be diverse and free from data contamination \\cite{huang20247wn}.\n    *   **Mitigation Strategy**: Empirical demonstration that fine-tuning an MLLM (LLaVA-1.5) on the `VHTest` benchmark dataset can effectively reduce its likelihood of hallucinating without compromising performance on other general benchmarks \\cite{huang20247wn}.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted**:\n        *   Comprehensive evaluation of state-of-the-art MLLMs (GPT-4V, LLaVA-1.5, MiniGPT-v2) on the newly constructed `VHTest` OEQ benchmark \\cite{huang20247wn}.\n        *   Fine-tuning experiments with LLaVA-1.5 using the `VHTest` dataset to assess its impact on VH mitigation \\cite{huang20247wn}.\n    *   **Key Performance Metrics**: Accuracy, defined as the fraction of VH instances for which the MLLM's text response is factually correct compared to the reference answer \\cite{huang20247wn}.\n    *   **Comparison Results**:\n        *   **Significant Hallucination**: MLLMs exhibited substantial hallucination on the `VHTest` benchmark, with overall accuracies of 0.383 for GPT-4V, 0.229 for LLaVA-1.5, and 0.075 for MiniGPT-v2 \\cite{huang20247wn}.\n        *   **Mode-Specific Vulnerabilities**: Performance varied across VH modes; for instance, GPT-4V was most prone to orientation VH (0.153 accuracy), while LLaVA-1.5 and MiniGPT-v2 were most susceptible to OCR VH (0.127 and 0.000 accuracy, respectively) \\cite{huang20247wn}.\n        *   **Effective Mitigation**: Fine-tuning LLaVA-1.5 on the `VHTest` dataset successfully reduced hallucination, improving accuracy (e.g., +0.200 in position VH mode) without sacrificing performance on other benchmarks \\cite{huang20247wn}.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The quality and diversity of generated VH instances depend on the capabilities of the description-generation MLLM and the text-to-image generative model. The benchmark construction required approximately 300 human-hours for manual question/answer design and verification \\cite{huang20247wn}.\n    *   **Scope of Applicability**: The work focuses specifically on visual hallucination in MLLMs within the context of visual question answering, covering 8 defined VH modes.\n\n*   **Technical Significance**\n    *   **Advances State-of-the-Art**: `VHTest` provides a novel, systematic, and scalable methodology for generating diverse and uncontaminated VH instances, addressing a critical gap in MLLM evaluation. It offers a more robust and unbiased benchmark for assessing MLLM vulnerabilities to hallucination \\cite{huang20247wn}.\n    *   **Potential Impact**: This work enables more rigorous testing and evaluation of MLLMs, facilitating the development of safer and more trustworthy AI systems. It also opens new avenues for research into adversarial data generation for multi-modal models and targeted fine-tuning strategies to mitigate specific failure modes \\cite{huang20247wn}.",
        "keywords": [
          "Multi-modal Large Language Models (MLLMs)",
          "Visual Hallucination (VH)",
          "VHTest",
          "Adversarial generation of VH instances",
          "Uncontaminated benchmark dataset",
          "Visual Question Answering (VQA)",
          "CLIP and DINO v2 embeddings",
          "Text-to-image generative models",
          "Hallucination mitigation",
          "AI safety and trustworthiness",
          "Mode-specific VH vulnerabilities",
          "Fine-tuning MLLMs"
        ],
        "paper_type": "based on the abstract and introduction:\n\nthe abstract explicitly states: \"in this work, we **propose a tool called vhtest to generate a diverse set of vh instances.**\" it then describes the methodology of this tool (\"specifically, vhtest finds some initial vh instances..., generates a text description..., and uses a text-to-image generative model... to generate vh images...\"). it also mentions collecting a \"benchmark dataset\" using this tool and then presenting \"findings\" from using this benchmark and demonstrating its utility for fine-tuning.\n\nthis clearly indicates the paper is presenting a **new method/tool (vhtest)** and a **new system/resource (the benchmark dataset)** created by that method. while it includes empirical findings from using the benchmark, the primary contribution is the development and presentation of the tool and the dataset.\n\ntherefore, the most appropriate classification is **technical**."
      },
      "file_name": "4d7c68ec1a86ef5d187e7edb2f0ad63adddc8ea2.pdf"
    },
    {
      "success": true,
      "doc_id": "0105a971a0b88c678c09c335929e15c6",
      "summary": "Here's a focused summary of the paper for a literature review:\n\n### Analyzing \"MITIGATING HALLUCINATION IN LARGE MULTI-MODAL MODELS VIA ROBUST INSTRUCTION TUNING\" \\cite{liu2023882}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Large Multi-Modal Models (LMMs) frequently \"hallucinate,\" generating descriptions that are inconsistent with the associated image content and human instructions. This includes describing nonexistent objects or activities, or providing lengthy, irrelevant responses.\n    *   **Importance & Challenge**: Hallucination is a major ethical concern, inherited from Large Language Models (LLMs), and can lead to harmful consequences if users over-rely on convincing but inaccurate LMM outputs. Existing LMMs often over-rely on language priors and are trained on synthetic data that may contain inconsistencies or lack diversity, leading to this problem. Current evaluation methods for hallucination are often unstable, require specific templates, or don't adequately penalize inconsistencies.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: Previous LMMs like LLaVA, MiniGPT4, InstructBLIP, MMGPT, and mPLUG-Owl have shown progress in multi-modal tasks but are primarily instruction-tuned on datasets with limited task diversity, predefined templates, or only positive instruction samples.\n    *   **Limitations of Previous Solutions**:\n        *   Existing LMMs often use synthetic instruction data that can be long and involve nonexistent elements, contributing to hallucination.\n        *   Training datasets are typically unbalanced, containing only \"positive\" instructions, leading LMMs to default to \"Yes\" or affirmative answers even when incorrect.\n        *   Current hallucination evaluation metrics (e.g., CIDEr, SPICE) do not penalize hallucination effectively. Others like CHAIR are unstable, and some (e.g., \\cite{li2023c}) require specific, restrictive instruction templates.\n    *   **Positioning of this Work**: \\cite{liu2023882} addresses these limitations by introducing a large, diverse dataset with *negative instructions* at various semantic levels and a novel, flexible evaluation method, aiming to improve LMM robustness against hallucination.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**:\n        *   **LRV-Instruction Dataset**: A large-scale (400k instances) and diverse visual instruction tuning dataset covering 16 vision-and-language tasks with open-ended instructions and answers. It uniquely includes both positive and *negative* instruction samples.\n        *   **Negative Instruction Design**: Negative instructions are designed at three semantic levels to challenge LMMs:\n            1.  **Nonexistent Object Manipulation**: Introducing objects, activities, attributes, or interactions not present in the image.\n            2.  **Existent Object Manipulation**: Manipulating existent objects with inconsistent attributes.\n            3.  **Knowledge Manipulation**: Manipulating external knowledge (e.g., named entities, events) related to the image.\n        *   **GPT4-Assisted Visual Instruction Evaluation (GAVIE)**: A novel, stable evaluation approach that leverages GPT4 to assess LMM outputs. It measures two aspects: \"Relevancy\" (instruction-following) and \"Accuracy\" (visual hallucination).\n    *   **Novelty/Difference**:\n        *   First dataset to systematically incorporate diverse *negative instructions* across multiple semantic levels to explicitly train LMMs to identify and avoid hallucination.\n        *   Utilizes GPT4 for large-scale, open-ended instruction generation, moving beyond human-crafted templates and limited task types.\n        *   GAVIE offers a flexible, groundtruth-free, and human-aligned evaluation method for hallucination, overcoming limitations of prior metrics.\n\n4.  **Key Technical Contributions**\n    *   **Novel Dataset**: Introduction of **LRV-Instruction** \\cite{liu2023882}, a 400k-instance dataset with 16 vision-language tasks, featuring open-ended positive and, crucially, three types of semantically-rich negative instructions (Nonexistent Object, Existent Object, Knowledge Manipulation) in declarative and interrogative formats.\n    *   **Novel Evaluation Method**: Proposal of **GAVIE** \\cite{liu2023882}, a GPT4-assisted evaluation framework that assesses LMM outputs for both instruction-following relevancy and visual hallucination accuracy, without requiring human-annotated groundtruth answers or specific instruction formats.\n    *   **Empirical Insights**: Comprehensive experiments demonstrating the significant hallucination tendencies of existing LMMs, particularly with Existent Object and Knowledge Manipulation instructions, and validating the effectiveness of LRV-Instruction for robust instruction tuning.\n    *   **Architectural/Training Insight**: Discovery that a balanced ratio of positive and negative instances in the training data is critical for achieving a more robust LMM.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**:\n        *   Evaluation of five publicly available LMMs (MiniGPT4, LLaVA, InstructBLIP, MMGPT, mPLUG-Owl) on the LRV-Instruction benchmark, specifically with negative instructions.\n        *   Finetuning of MiniGPT4 and mPLUG-Owl on the LRV-Instruction dataset.\n        *   Comparison of finetuned models against original LMMs and state-of-the-art methods on both the proposed evaluation set and public benchmarks (e.g., MME, POPE, GQA).\n        *   Analysis of the impact of positive/negative instance ratios on model robustness.\n    *   **Key Performance Metrics & Comparison Results**:\n        *   Existing LMMs exhibited *significant hallucinations* when presented with negative instructions, especially Existent Object and Knowledge Manipulation types.\n        *   Finetuning MiniGPT4 and mPLUG-Owl on LRV-Instruction successfully *mitigated hallucination* and *improved performance* on several public datasets, outperforming original models and other SOTA methods.\n        *   GAVIE was shown to be stable and align well with human evaluation.\n        *   A balanced ratio of positive and negative instances in training data was empirically shown to lead to a more robust model.\n        *   Existent Object Manipulation and Knowledge Manipulation instructions were found to be more challenging for LMMs than Nonexistent Object Manipulation.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The dataset generation relies on GPT4's capabilities, and while quality control is applied, potential biases or errors from the generative model could exist. The evaluation method (GAVIE) also relies on GPT4's judgment, assuming its alignment with human experts.\n    *   **Scope of Applicability**: The work focuses on mitigating hallucination in LMMs through instruction tuning. While effective for the tested models and tasks, its direct applicability to all LMM architectures or all possible hallucination types might vary. The dataset covers 16 vision-language tasks, which is diverse but not exhaustive.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: \\cite{liu2023882} significantly advances the technical state-of-the-art by providing the first large-scale, diverse visual instruction tuning dataset that explicitly incorporates negative instructions, directly targeting the hallucination problem in LMMs. It also introduces a robust, flexible, and groundtruth-free evaluation method (GAVIE) for hallucination.\n    *   **Potential Impact on Future Research**: This work provides a crucial benchmark and training methodology for developing more robust and trustworthy LMMs. It highlights the importance of balanced training data with negative examples and offers a new paradigm for evaluating LMMs beyond traditional metrics. Future research can build upon LRV-Instruction to explore more sophisticated hallucination mitigation techniques, investigate the impact of different negative instruction types, and further refine GPT4-assisted evaluation methods.",
      "intriguing_abstract": "The pervasive challenge of hallucination in Large Multi-Modal Models (LMMs) undermines their reliability and raises significant ethical concerns. These models frequently generate visually inconsistent or factually incorrect descriptions, often due to over-reliance on language priors and imbalanced training data. We introduce a novel paradigm for robust **instruction tuning**, anchored by **LRV-Instruction**, a 400k-instance **visual instruction dataset**. Crucially, LRV-Instruction uniquely incorporates diverse **negative instructions** across three semantic levelsâ€”**Nonexistent Object**, **Existent Object**, and **Knowledge Manipulation**â€”explicitly training LMMs to discern and avoid erroneous outputs. Complementing this, our **GPT4-Assisted Visual Instruction Evaluation (GAVIE)** offers a flexible, groundtruth-free framework to accurately assess both instruction-following relevancy and visual hallucination. Experiments demonstrate that finetuning with LRV-Instruction dramatically mitigates **hallucination**, particularly against challenging existent object and knowledge manipulations, yielding significantly more **robust** and trustworthy LMMs. This work provides a critical benchmark and methodology, paving the way for safer and more reliable multi-modal AI.",
      "keywords": [
        "Large Multi-Modal Models (LMMs)",
        "Hallucination mitigation",
        "Robust instruction tuning",
        "Negative instructions",
        "LRV-Instruction dataset",
        "GPT4-Assisted Visual Instruction Evaluation (GAVIE)",
        "Visual hallucination accuracy",
        "Balanced positive/negative training data",
        "Nonexistent Object Manipulation",
        "Existent Object Manipulation",
        "Knowledge Manipulation",
        "Vision-and-language tasks",
        "Groundtruth-free evaluation"
      ],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/c7a7104df3db13737a865ede2be8146990fa4026.pdf",
      "citation_key": "liu2023882",
      "metadata": {
        "title": "Mitigating Hallucination in Large Multi-Modal Models via Robust Instruction Tuning",
        "authors": [
          "Fuxiao Liu",
          "Kevin Lin",
          "Linjie Li",
          "Jianfeng Wang",
          "Y. Yacoob",
          "Lijuan Wang"
        ],
        "published_date": "2023",
        "abstract": "Despite the promising progress in multi-modal tasks, current large multi-modal models (LMMs) are prone to hallucinating inconsistent descriptions with respect to the associated image and human instructions. This paper addresses this issue by introducing the first large and diverse visual instruction tuning dataset, named Large-scale Robust Visual (LRV)-Instruction. Our dataset comprises 400k visual instructions generated by GPT4, covering 16 vision-and-language tasks with open-ended instructions and answers. Unlike existing studies that primarily focus on positive instruction samples, we design LRV-Instruction to include both positive and negative instructions for more robust visual instruction tuning. Our negative instructions are designed at three semantic levels: (i) Nonexistent Object Manipulation, (ii) Existent Object Manipulation and (iii) Knowledge Manipulation. To efficiently measure the hallucination generated by LMMs, we propose GPT4-Assisted Visual Instruction Evaluation (GAVIE), a stable approach to evaluate visual instruction tuning like human experts. GAVIE does not require human-annotated groundtruth answers and can adapt to diverse instruction formats. We conduct comprehensive experiments to investigate the hallucination of LMMs. Our results demonstrate existing LMMs exhibit significant hallucinations when presented with our negative instructions, particularly Existent Object and Knowledge Manipulation instructions. Moreover, we successfully mitigate hallucination by finetuning MiniGPT4 and mPLUG-Owl on LRV-Instruction while improving performance on several public datasets compared to state-of-the-art methods. Additionally, we observed that a balanced ratio of positive and negative instances in the training data leads to a more robust model. Code and data are available at https://github.com/FuxiaoLiu/LRV-Instruction.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/c7a7104df3db13737a865ede2be8146990fa4026.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the paper for a literature review:\n\n### Analyzing \"MITIGATING HALLUCINATION IN LARGE MULTI-MODAL MODELS VIA ROBUST INSTRUCTION TUNING\" \\cite{liu2023882}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Large Multi-Modal Models (LMMs) frequently \"hallucinate,\" generating descriptions that are inconsistent with the associated image content and human instructions. This includes describing nonexistent objects or activities, or providing lengthy, irrelevant responses.\n    *   **Importance & Challenge**: Hallucination is a major ethical concern, inherited from Large Language Models (LLMs), and can lead to harmful consequences if users over-rely on convincing but inaccurate LMM outputs. Existing LMMs often over-rely on language priors and are trained on synthetic data that may contain inconsistencies or lack diversity, leading to this problem. Current evaluation methods for hallucination are often unstable, require specific templates, or don't adequately penalize inconsistencies.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: Previous LMMs like LLaVA, MiniGPT4, InstructBLIP, MMGPT, and mPLUG-Owl have shown progress in multi-modal tasks but are primarily instruction-tuned on datasets with limited task diversity, predefined templates, or only positive instruction samples.\n    *   **Limitations of Previous Solutions**:\n        *   Existing LMMs often use synthetic instruction data that can be long and involve nonexistent elements, contributing to hallucination.\n        *   Training datasets are typically unbalanced, containing only \"positive\" instructions, leading LMMs to default to \"Yes\" or affirmative answers even when incorrect.\n        *   Current hallucination evaluation metrics (e.g., CIDEr, SPICE) do not penalize hallucination effectively. Others like CHAIR are unstable, and some (e.g., \\cite{li2023c}) require specific, restrictive instruction templates.\n    *   **Positioning of this Work**: \\cite{liu2023882} addresses these limitations by introducing a large, diverse dataset with *negative instructions* at various semantic levels and a novel, flexible evaluation method, aiming to improve LMM robustness against hallucination.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**:\n        *   **LRV-Instruction Dataset**: A large-scale (400k instances) and diverse visual instruction tuning dataset covering 16 vision-and-language tasks with open-ended instructions and answers. It uniquely includes both positive and *negative* instruction samples.\n        *   **Negative Instruction Design**: Negative instructions are designed at three semantic levels to challenge LMMs:\n            1.  **Nonexistent Object Manipulation**: Introducing objects, activities, attributes, or interactions not present in the image.\n            2.  **Existent Object Manipulation**: Manipulating existent objects with inconsistent attributes.\n            3.  **Knowledge Manipulation**: Manipulating external knowledge (e.g., named entities, events) related to the image.\n        *   **GPT4-Assisted Visual Instruction Evaluation (GAVIE)**: A novel, stable evaluation approach that leverages GPT4 to assess LMM outputs. It measures two aspects: \"Relevancy\" (instruction-following) and \"Accuracy\" (visual hallucination).\n    *   **Novelty/Difference**:\n        *   First dataset to systematically incorporate diverse *negative instructions* across multiple semantic levels to explicitly train LMMs to identify and avoid hallucination.\n        *   Utilizes GPT4 for large-scale, open-ended instruction generation, moving beyond human-crafted templates and limited task types.\n        *   GAVIE offers a flexible, groundtruth-free, and human-aligned evaluation method for hallucination, overcoming limitations of prior metrics.\n\n4.  **Key Technical Contributions**\n    *   **Novel Dataset**: Introduction of **LRV-Instruction** \\cite{liu2023882}, a 400k-instance dataset with 16 vision-language tasks, featuring open-ended positive and, crucially, three types of semantically-rich negative instructions (Nonexistent Object, Existent Object, Knowledge Manipulation) in declarative and interrogative formats.\n    *   **Novel Evaluation Method**: Proposal of **GAVIE** \\cite{liu2023882}, a GPT4-assisted evaluation framework that assesses LMM outputs for both instruction-following relevancy and visual hallucination accuracy, without requiring human-annotated groundtruth answers or specific instruction formats.\n    *   **Empirical Insights**: Comprehensive experiments demonstrating the significant hallucination tendencies of existing LMMs, particularly with Existent Object and Knowledge Manipulation instructions, and validating the effectiveness of LRV-Instruction for robust instruction tuning.\n    *   **Architectural/Training Insight**: Discovery that a balanced ratio of positive and negative instances in the training data is critical for achieving a more robust LMM.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**:\n        *   Evaluation of five publicly available LMMs (MiniGPT4, LLaVA, InstructBLIP, MMGPT, mPLUG-Owl) on the LRV-Instruction benchmark, specifically with negative instructions.\n        *   Finetuning of MiniGPT4 and mPLUG-Owl on the LRV-Instruction dataset.\n        *   Comparison of finetuned models against original LMMs and state-of-the-art methods on both the proposed evaluation set and public benchmarks (e.g., MME, POPE, GQA).\n        *   Analysis of the impact of positive/negative instance ratios on model robustness.\n    *   **Key Performance Metrics & Comparison Results**:\n        *   Existing LMMs exhibited *significant hallucinations* when presented with negative instructions, especially Existent Object and Knowledge Manipulation types.\n        *   Finetuning MiniGPT4 and mPLUG-Owl on LRV-Instruction successfully *mitigated hallucination* and *improved performance* on several public datasets, outperforming original models and other SOTA methods.\n        *   GAVIE was shown to be stable and align well with human evaluation.\n        *   A balanced ratio of positive and negative instances in training data was empirically shown to lead to a more robust model.\n        *   Existent Object Manipulation and Knowledge Manipulation instructions were found to be more challenging for LMMs than Nonexistent Object Manipulation.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The dataset generation relies on GPT4's capabilities, and while quality control is applied, potential biases or errors from the generative model could exist. The evaluation method (GAVIE) also relies on GPT4's judgment, assuming its alignment with human experts.\n    *   **Scope of Applicability**: The work focuses on mitigating hallucination in LMMs through instruction tuning. While effective for the tested models and tasks, its direct applicability to all LMM architectures or all possible hallucination types might vary. The dataset covers 16 vision-language tasks, which is diverse but not exhaustive.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: \\cite{liu2023882} significantly advances the technical state-of-the-art by providing the first large-scale, diverse visual instruction tuning dataset that explicitly incorporates negative instructions, directly targeting the hallucination problem in LMMs. It also introduces a robust, flexible, and groundtruth-free evaluation method (GAVIE) for hallucination.\n    *   **Potential Impact on Future Research**: This work provides a crucial benchmark and training methodology for developing more robust and trustworthy LMMs. It highlights the importance of balanced training data with negative examples and offers a new paradigm for evaluating LMMs beyond traditional metrics. Future research can build upon LRV-Instruction to explore more sophisticated hallucination mitigation techniques, investigate the impact of different negative instruction types, and further refine GPT4-assisted evaluation methods.",
        "keywords": [
          "Large Multi-Modal Models (LMMs)",
          "Hallucination mitigation",
          "Robust instruction tuning",
          "Negative instructions",
          "LRV-Instruction dataset",
          "GPT4-Assisted Visual Instruction Evaluation (GAVIE)",
          "Visual hallucination accuracy",
          "Balanced positive/negative training data",
          "Nonexistent Object Manipulation",
          "Existent Object Manipulation",
          "Knowledge Manipulation",
          "Vision-and-language tasks",
          "Groundtruth-free evaluation"
        ],
        "paper_type": "based on the abstract and introduction:\n\n1.  **new methods/systems:** the paper explicitly states it \"addresses this issue by introducing the first large and diverse visual instruction tuning dataset, named large-scale robust visual (lrv)-instruction.\" it also mentions \"we propose gpt4-assisted visual instruction evaluation (gavie), a stable approach to evaluate visual instruction tuning.\" these are clear indicators of presenting new methods or systems.\n2.  **technical problem/solution:** the paper identifies a technical problem (\"lmms are prone to hallucinating inconsistent descriptions\") and proposes a technical solution (the lrv-instruction dataset and the gavie evaluation method).\n3.  **empirical validation:** while the paper does conduct \"comprehensive experiments\" and presents \"results\" demonstrating mitigation of hallucination and improved performance, these experiments serve to validate the effectiveness of the *proposed* dataset and evaluation method. the core contribution is the creation and design of these new artifacts.\n\nthe emphasis on \"introducing,\" \"design,\" and \"propose\" for a new dataset and an evaluation approach strongly aligns with the \"technical\" classification. the empirical work is a crucial part of demonstrating the utility of these technical contributions.\n\ntherefore, the paper is best classified as **technical**."
      },
      "file_name": "c7a7104df3db13737a865ede2be8146990fa4026.pdf"
    },
    {
      "success": true,
      "doc_id": "d0d8d94a6b6869b24370e8ddf6bfa0f8",
      "summary": "Here's a focused summary of the technical paper for a literature review:\n\n1.  **Research Problem & Motivation**\n    *   **Specific technical problem:** Multi-modal Large Language Models (MLLMs) occasionally generate \"hallucinations\"â€”content inconsistent with given imagesâ€”especially when inputs are perturbed \\cite{ding2024o88}.\n    *   **Why important and challenging:** Prior work primarily evaluates hallucination using standard, unperturbed benchmarks, overlooking the prevalent occurrence of perturbed inputs (e.g., cropping, blurring) in real-world scenarios. This oversight leads to an incomplete and imprecise assessment of MLLM hallucinations, which could cause serious accidents in critical applications like medical diagnosis or autonomous driving \\cite{ding2024o88}.\n\n2.  **Related Work & Positioning**\n    *   **Relation to existing approaches:** Existing research investigates MLLM hallucinations using methods like human evaluation, LLM-based evaluation (e.g., GPT-4), or detection models (e.g., POPE, M-HalDetect, HaELM, Halle-Switch, AMBER) \\cite{ding2024o88}.\n    *   **Limitations of previous solutions:** These prior works primarily focus on evaluating MLLMs using unperturbed images, often sampled from existing datasets like MSCOCO. They do not explore hallucinations in the context of perturbed inputs, which are common in real-life situations, thus providing an insufficient and incomplete evaluation of MLLM robustness \\cite{ding2024o88}.\n\n3.  **Technical Approach & Innovation**\n    *   **Core technical method:** The paper proposes **Hallu-PI**, the first benchmark specifically designed to evaluate hallucination in MLLMs within perturbed inputs \\cite{ding2024o88}.\n    *   **What makes this approach novel or different:**\n        *   **Comprehensive Perturbation Scenarios:** Hallu-PI includes seven distinct perturbed scenarios: noise, blur, weather, digital (reusing existing techniques), and three novel, meticulously proposed perturbations: image concatenation, image cropping, and prompt misleading \\cite{ding2024o88}.\n        *   **Detailed Annotations:** Each of the 1,260 perturbed images (across 11 object types) is accompanied by fine-grained annotations covering hallucination types such as Existence, Number, Color, Relation, and Hal-object \\cite{ding2024o88}.\n        *   **Dual Task Support:** The benchmark is equipped with a rich set of questions and prompt query templates suitable for evaluating MLLMs on both discriminative (Yes/No questions) and generative tasks (detailed descriptions) \\cite{ding2024o88}.\n        *   **Mitigation Baselines:** The paper introduces two baselines, Perturbed-Reminder and Perturbed-ICL, inspired by defensive strategies in text LLMs, to mitigate hallucinations in perturbed scenarios \\cite{ding2024o88}.\n\n4.  **Key Technical Contributions**\n    *   **Novel Benchmark (Hallu-PI):** Construction of the first freely available multi-modal hallucination benchmark specifically for perturbed inputs, comprising 7 scenarios, 1,260 images, and 11 object categories \\cite{ding2024o88}.\n    *   **Expanded Perturbation Taxonomy:** Introduction of image concatenation, image cropping, and prompt misleading as critical real-world perturbation types for hallucination evaluation, complementing existing noise, blur, weather, and digital categories \\cite{ding2024o88}.\n    *   **Fine-grained Hallucination Categorization:** Detailed annotations for Existence, Number, Color, Relation, and Hal-object, enabling a nuanced analysis of MLLM hallucination types \\cite{ding2024o88}.\n    *   **Evaluation Framework:** A robust evaluation pipeline supporting both generative (using metrics like CHAIR) and discriminative tasks (using ACC+ and PI-Score) for comprehensive assessment \\cite{ding2024o88}.\n    *   **Mitigation Strategies:** Design and evaluation of two initial baselines, Perturbed-Reminder and Perturbed-ICL, demonstrating potential for reducing MLLM hallucinations in perturbed contexts \\cite{ding2024o88}.\n\n5.  **Experimental Validation**\n    *   **Experiments conducted:** Extensive experiments were performed on 12 mainstream state-of-the-art MLLMs, including GPT-4V, Gemini-Pro Vision, LLaVA, CogVLM, and others, using the Hallu-PI benchmark \\cite{ding2024o88}.\n    *   **Key performance metrics:**\n        *   **Generative tasks:** CHAIR (lower values indicate less hallucination).\n        *   **Discriminative tasks:** ACC+ (accuracy incorporating Yes/No questions) and PI-Score (a combined metric for overall performance, higher is better) \\cite{ding2024o88}.\n    *   **Comparison results:**\n        *   State-of-the-art MLLMs exhibit significant hallucinations on Hallu-PI under perturbed inputs, a phenomenon not consistently observed in unperturbed scenarios (e.g., inconsistent performance trends shown in Figure 2) \\cite{ding2024o88}.\n        *   Models demonstrate a severe bias in their ability to handle different types of hallucinations and specific perturbations, particularly image concatenation, image cropping, and prompt misleading \\cite{ding2024o88}.\n        *   The proposed baselines, Perturbed-Reminder and Perturbed-ICL, effectively reduce hallucinations in GPT-4V, indicating their potential as mitigation strategies \\cite{ding2024o88}.\n\n6.  **Limitations & Scope**\n    *   **Technical limitations or assumptions:** The paper primarily focuses on evaluating MLLM limitations rather than Hallu-PI's own. The mitigation baselines (Perturbed-Reminder, Perturbed-ICL) are initial attempts and may not fully resolve the hallucination issue. The scope of perturbations is limited to the seven defined categories.\n    *   **Scope of applicability:** Hallu-PI is applicable for evaluating the robustness and hallucination tendencies of MLLMs in real-world scenarios where inputs are frequently perturbed. It is particularly relevant for developing more reliable MLLMs for safety-critical applications \\cite{ding2024o88}.\n\n7.  **Technical Significance**\n    *   **Advances the technical state-of-the-art:** Hallu-PI bridges a critical gap in MLLM evaluation by providing the first dedicated benchmark for assessing hallucination under perturbed inputs, moving beyond ideal, unperturbed scenarios \\cite{ding2024o88}.\n    *   **Potential impact on future research:**\n        *   Highlights significant vulnerabilities and biases in current MLLMs when faced with real-world input variations, urging researchers to focus on improving model robustness \\cite{ding2024o88}.\n        *   The detailed analysis of hallucination types and perturbation impacts provides a foundation for developing targeted mitigation techniques.\n        *   The publicly available code and dataset will serve as a valuable resource, spurring further investigations into robust MLLM design and hallucination reduction \\cite{ding2024o88}.",
      "intriguing_abstract": "The immense promise of Multi-modal Large Language Models (MLLMs) is shadowed by a critical vulnerability: hallucinations, particularly when confronted with real-world, imperfect visual inputs. Existing evaluations, primarily using unperturbed benchmarks, critically overlook the prevalent occurrence of perturbed inputs (e.g., cropping, blurring), leading to an incomplete assessment of MLLM robustness and posing serious risks in safety-critical applications like autonomous driving.\n\nWe introduce **Hallu-PI**, the first comprehensive benchmark specifically designed to evaluate MLLM hallucinations under perturbed conditions. Hallu-PI pioneers seven diverse perturbation scenarios, including novel types like **image concatenation**, **image cropping**, and **prompt misleading**, alongside fine-grained annotations for hallucination types (Existence, Number, Color, Relation). Supporting both **discriminative** and **generative tasks**, Hallu-PI unveils alarming biases and significant performance degradation in 12 state-of-the-art MLLMs, including GPT-4V and Gemini-Pro Vision. Our findings highlight that current MLLMs are severely susceptible to these real-world variations. Furthermore, we propose initial **mitigation baselines**, **Perturbed-Reminder** and **Perturbed-ICL**, demonstrating promising steps towards enhancing MLLM reliability. Hallu-PI serves as an imperative resource to drive research into robust and trustworthy MLLMs.",
      "keywords": [
        "Multi-modal Large Language Models (MLLMs)",
        "hallucinations",
        "perturbed inputs",
        "Hallu-PI benchmark",
        "novel perturbation scenarios",
        "fine-grained hallucination categorization",
        "MLLM robustness evaluation",
        "hallucination mitigation strategies",
        "generative and discriminative tasks",
        "safety-critical applications",
        "real-world input variations",
        "model vulnerabilities and biases",
        "comprehensive evaluation framework",
        "CHAIR",
        "ACC+"
      ],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/03e2f5cded1b1d92dc8e693e0e93ad466f6cc352.pdf",
      "citation_key": "ding2024o88",
      "metadata": {
        "title": "Hallu-PI: Evaluating Hallucination in Multi-modal Large Language Models within Perturbed Inputs",
        "authors": [
          "Peng Ding",
          "Jingyu Wu",
          "Jun Kuang",
          "Dan Ma",
          "Xuezhi Cao",
          "Xunliang Cai",
          "Shi Chen",
          "Jiajun Chen",
          "Shujian Huang"
        ],
        "published_date": "2024",
        "abstract": "Multi-modal Large Language Models (MLLMs) have demonstrated remarkable performance on various visual-language understanding and generation tasks. However, MLLMs occasionally generate content inconsistent with the given images, which is known as \"hallucination\". Prior works primarily center on evaluating hallucination using standard, unperturbed benchmarks, which overlook the prevalent occurrence of perturbed inputs in real-world scenarios-such as image cropping or blurring-that are critical for a comprehensive assessment of MLLMs' hallucination. In this paper, to bridge this gap, we propose Hallu-PI, the first benchmark designed to evaluate Hallucination in MLLMs within Perturbed Inputs. Specifically, Hallu-PI consists of seven perturbed scenarios, containing 1,260 perturbed images from 11 object types. Each image is accompanied by detailed annotations, which include fine-grained hallucination types, such as existence, attribute, and relation. We equip these annotations with a rich set of questions, making Hallu-PI suitable for both discriminative and generative tasks. Extensive experiments on 12 mainstream MLLMs, such as GPT-4V and Gemini-Pro Vision, demonstrate that these models exhibit significant hallucinations on Hallu-PI, which is not observed in unperturbed scenarios. Furthermore, our research reveals a severe bias in MLLMs' ability to handle different types of hallucinations. We also design two baselines specifically for perturbed scenarios, namely Perturbed-Reminder and Perturbed-ICL. We hope that our study will bring researchers' attention to the limitations of MLLMs when dealing with perturbed inputs, and spur further investigations to address this issue. Our code and datasets are publicly available at https://github.com/NJUNLP/Hallu-PI.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/03e2f5cded1b1d92dc8e693e0e93ad466f6cc352.pdf",
        "venue": "ACM Multimedia",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the technical paper for a literature review:\n\n1.  **Research Problem & Motivation**\n    *   **Specific technical problem:** Multi-modal Large Language Models (MLLMs) occasionally generate \"hallucinations\"â€”content inconsistent with given imagesâ€”especially when inputs are perturbed \\cite{ding2024o88}.\n    *   **Why important and challenging:** Prior work primarily evaluates hallucination using standard, unperturbed benchmarks, overlooking the prevalent occurrence of perturbed inputs (e.g., cropping, blurring) in real-world scenarios. This oversight leads to an incomplete and imprecise assessment of MLLM hallucinations, which could cause serious accidents in critical applications like medical diagnosis or autonomous driving \\cite{ding2024o88}.\n\n2.  **Related Work & Positioning**\n    *   **Relation to existing approaches:** Existing research investigates MLLM hallucinations using methods like human evaluation, LLM-based evaluation (e.g., GPT-4), or detection models (e.g., POPE, M-HalDetect, HaELM, Halle-Switch, AMBER) \\cite{ding2024o88}.\n    *   **Limitations of previous solutions:** These prior works primarily focus on evaluating MLLMs using unperturbed images, often sampled from existing datasets like MSCOCO. They do not explore hallucinations in the context of perturbed inputs, which are common in real-life situations, thus providing an insufficient and incomplete evaluation of MLLM robustness \\cite{ding2024o88}.\n\n3.  **Technical Approach & Innovation**\n    *   **Core technical method:** The paper proposes **Hallu-PI**, the first benchmark specifically designed to evaluate hallucination in MLLMs within perturbed inputs \\cite{ding2024o88}.\n    *   **What makes this approach novel or different:**\n        *   **Comprehensive Perturbation Scenarios:** Hallu-PI includes seven distinct perturbed scenarios: noise, blur, weather, digital (reusing existing techniques), and three novel, meticulously proposed perturbations: image concatenation, image cropping, and prompt misleading \\cite{ding2024o88}.\n        *   **Detailed Annotations:** Each of the 1,260 perturbed images (across 11 object types) is accompanied by fine-grained annotations covering hallucination types such as Existence, Number, Color, Relation, and Hal-object \\cite{ding2024o88}.\n        *   **Dual Task Support:** The benchmark is equipped with a rich set of questions and prompt query templates suitable for evaluating MLLMs on both discriminative (Yes/No questions) and generative tasks (detailed descriptions) \\cite{ding2024o88}.\n        *   **Mitigation Baselines:** The paper introduces two baselines, Perturbed-Reminder and Perturbed-ICL, inspired by defensive strategies in text LLMs, to mitigate hallucinations in perturbed scenarios \\cite{ding2024o88}.\n\n4.  **Key Technical Contributions**\n    *   **Novel Benchmark (Hallu-PI):** Construction of the first freely available multi-modal hallucination benchmark specifically for perturbed inputs, comprising 7 scenarios, 1,260 images, and 11 object categories \\cite{ding2024o88}.\n    *   **Expanded Perturbation Taxonomy:** Introduction of image concatenation, image cropping, and prompt misleading as critical real-world perturbation types for hallucination evaluation, complementing existing noise, blur, weather, and digital categories \\cite{ding2024o88}.\n    *   **Fine-grained Hallucination Categorization:** Detailed annotations for Existence, Number, Color, Relation, and Hal-object, enabling a nuanced analysis of MLLM hallucination types \\cite{ding2024o88}.\n    *   **Evaluation Framework:** A robust evaluation pipeline supporting both generative (using metrics like CHAIR) and discriminative tasks (using ACC+ and PI-Score) for comprehensive assessment \\cite{ding2024o88}.\n    *   **Mitigation Strategies:** Design and evaluation of two initial baselines, Perturbed-Reminder and Perturbed-ICL, demonstrating potential for reducing MLLM hallucinations in perturbed contexts \\cite{ding2024o88}.\n\n5.  **Experimental Validation**\n    *   **Experiments conducted:** Extensive experiments were performed on 12 mainstream state-of-the-art MLLMs, including GPT-4V, Gemini-Pro Vision, LLaVA, CogVLM, and others, using the Hallu-PI benchmark \\cite{ding2024o88}.\n    *   **Key performance metrics:**\n        *   **Generative tasks:** CHAIR (lower values indicate less hallucination).\n        *   **Discriminative tasks:** ACC+ (accuracy incorporating Yes/No questions) and PI-Score (a combined metric for overall performance, higher is better) \\cite{ding2024o88}.\n    *   **Comparison results:**\n        *   State-of-the-art MLLMs exhibit significant hallucinations on Hallu-PI under perturbed inputs, a phenomenon not consistently observed in unperturbed scenarios (e.g., inconsistent performance trends shown in Figure 2) \\cite{ding2024o88}.\n        *   Models demonstrate a severe bias in their ability to handle different types of hallucinations and specific perturbations, particularly image concatenation, image cropping, and prompt misleading \\cite{ding2024o88}.\n        *   The proposed baselines, Perturbed-Reminder and Perturbed-ICL, effectively reduce hallucinations in GPT-4V, indicating their potential as mitigation strategies \\cite{ding2024o88}.\n\n6.  **Limitations & Scope**\n    *   **Technical limitations or assumptions:** The paper primarily focuses on evaluating MLLM limitations rather than Hallu-PI's own. The mitigation baselines (Perturbed-Reminder, Perturbed-ICL) are initial attempts and may not fully resolve the hallucination issue. The scope of perturbations is limited to the seven defined categories.\n    *   **Scope of applicability:** Hallu-PI is applicable for evaluating the robustness and hallucination tendencies of MLLMs in real-world scenarios where inputs are frequently perturbed. It is particularly relevant for developing more reliable MLLMs for safety-critical applications \\cite{ding2024o88}.\n\n7.  **Technical Significance**\n    *   **Advances the technical state-of-the-art:** Hallu-PI bridges a critical gap in MLLM evaluation by providing the first dedicated benchmark for assessing hallucination under perturbed inputs, moving beyond ideal, unperturbed scenarios \\cite{ding2024o88}.\n    *   **Potential impact on future research:**\n        *   Highlights significant vulnerabilities and biases in current MLLMs when faced with real-world input variations, urging researchers to focus on improving model robustness \\cite{ding2024o88}.\n        *   The detailed analysis of hallucination types and perturbation impacts provides a foundation for developing targeted mitigation techniques.\n        *   The publicly available code and dataset will serve as a valuable resource, spurring further investigations into robust MLLM design and hallucination reduction \\cite{ding2024o88}.",
        "keywords": [
          "Multi-modal Large Language Models (MLLMs)",
          "hallucinations",
          "perturbed inputs",
          "Hallu-PI benchmark",
          "novel perturbation scenarios",
          "fine-grained hallucination categorization",
          "MLLM robustness evaluation",
          "hallucination mitigation strategies",
          "generative and discriminative tasks",
          "safety-critical applications",
          "real-world input variations",
          "model vulnerabilities and biases",
          "comprehensive evaluation framework",
          "CHAIR",
          "ACC+"
        ],
        "paper_type": "the paper should be classified as **technical**.\n\nhere's why:\n\n1.  **\"propose hallu-pi, the first benchmark designed to evaluate...\"**: this is a direct indicator of presenting a new system or tool (a benchmark).\n2.  **\"hallu-pi consists of seven perturbed scenarios, containing 1,260 perturbed images... accompanied by detailed annotations...\"**: this describes the *design* and *construction* of the new benchmark.\n3.  **\"we also design two baselines specifically for perturbed scenarios...\"**: this indicates the development of new methods or algorithms (the baselines).\n4.  the abstract focuses on *what* was developed (hallu-pi, baselines) and *how* it was constructed, before discussing the experimental findings. while it includes empirical results, the core contribution is the creation of the benchmark and baselines themselves.\n\nthe empirical evaluation is a crucial part of validating the proposed benchmark and baselines, but the primary nature of the paper is the presentation of these new technical artifacts."
      },
      "file_name": "03e2f5cded1b1d92dc8e693e0e93ad466f6cc352.pdf"
    },
    {
      "success": true,
      "doc_id": "8e492be311d20e757720b63b363633ce",
      "summary": "Here's a focused summary of the paper for a literature review:\n\n### Technical Paper Analysis: The Troubling Emergence of Hallucination in Large Language Models \\cite{rawte2023ao8}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: The paper addresses the significant and growing problem of hallucination in Large Language Models (LLMs), where models generate factually incorrect or unfaithful content.\n    *   **Importance and Challenge**: This problem is critical due to the widespread adoption of LLMs in real-world applications, leading to potential misuse, factual inaccuracies (e.g., Google Bard's factual error, fabricated legal precedents), and societal concerns regarding the spread of untruths. The challenge lies in the limited emphasis on nuanced categorization of hallucination and the absence of a uniform, quantifiable metric to assess and compare LLM vulnerability to hallucination.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: Previous research has loosely defined hallucination, often categorizing it into limited classes (e.g., intrinsic/extrinsic) or focusing on task-specific types (e.g., summarization, Q&A). Some work explored factual vs. non-factual prompts \\cite{rawte2023ao8}.\n    *   **Limitations of Previous Solutions**: Existing approaches lack a comprehensive, fine-grained categorization of hallucination across different dimensions (degree, orientation, specific types). Crucially, there is no uniform evaluation metric or comparative spectrum to quantify and rank LLMs based on their hallucination vulnerability, making it difficult for policymakers and researchers to assess risks and compare models effectively \\cite{rawte2023ao8}.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes a multi-faceted approach:\n        *   A novel, fine-grained framework for profiling hallucination based on its degree (mild, moderate, alarming), orientation (Factual Mirage (FM) and Silver Lining (SL), each sub-categorized into intrinsic and extrinsic), and six specific categories (Acronym Ambiguity, Numeric Nuisance, Generated Golem, Virtual Voice, Geographic Erratum, Time Wrap) \\cite{rawte2023ao8}.\n        *   The creation of `HILT` (HallucInation eLiciTation), a publicly available, human-annotated dataset comprising 75,000 text snippets generated by 15 contemporary LLMs using both factually correct and incorrect prompts \\cite{rawte2023ao8}.\n        *   The introduction of the Hallucination Vulnerability Index (`HVI`), a quantitative metric designed to evaluate and rank LLMs based on their susceptibility to producing hallucinations \\cite{rawte2023ao8}.\n    *   **Novelty/Difference**: The innovation lies in the comprehensive and systematic categorization of hallucination, moving beyond simplistic definitions. The `HILT` dataset is the first of its kind to offer such a broad collection of LLM-generated text with fine-grained human annotations across the proposed categories. The `HVI` provides a much-needed standardized, quantifiable measure for comparing LLM hallucination tendencies, which is crucial for risk assessment and policy-making \\cite{rawte2023ao8}.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**:\n        *   A comprehensive taxonomy of hallucination, including two orientations (Factual Mirage, Silver Lining), their intrinsic/extrinsic sub-categories, three degrees of severity (mild, moderate, alarming), and six distinct types (Acronym Ambiguity, Numeric Nuisance, Generated Golem, Virtual Voice, Geographic Erratum, Time Wrap) \\cite{rawte2023ao8}.\n        *   The Hallucination Vulnerability Index (`HVI`), a quantitative formula to measure and rank LLM hallucination susceptibility, incorporating factors like the ratio of hallucinated sentences and tendencies towards specific (extrinsic) hallucination types \\cite{rawte2023ao8}.\n    *   **System Design/Architectural Innovations**:\n        *   The `HILT` dataset, a large-scale (75,000 samples) and diverse resource for hallucination research, generated from 15 LLMs and meticulously human-annotated at the sentence level using the MACE tool for agreement aggregation \\cite{rawte2023ao8}.\n    *   **Theoretical Insights/Analysis**: The paper provides a deeper understanding of hallucination by dissecting its various facets, offering a framework for systematic analysis rather than isolated observations \\cite{rawte2023ao8}. It also proposes two mitigation strategies, suggesting identifying high-entropy points in text from high-HVI LLMs and replacing them with output from lower-HVI LLMs \\cite{rawte2023ao8}.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**:\n        *   **Dataset Creation**: The `HILT` dataset was curated by generating 75,000 text snippets (5,000 per LLM) from 15 contemporary LLMs (including GPT-4, LLaMA, BLOOM, etc.) using prompts from NYTimes tweets (factually correct) and Politifact (factually incorrect) \\cite{rawte2023ao8}.\n        *   **Human Annotation**: These snippets underwent sentence-level human annotation for hallucination orientations and categories by four annotators per sentence via Amazon Mechanical Turk, with inter-annotator agreement assessed and data aggregated using the MACE tool \\cite{rawte2023ao8}.\n        *   **HVI Application**: The `HVI` was calculated for all 15 LLMs, and the resulting HVIs were ranked and scaled to provide a comparative spectrum of their hallucination vulnerability \\cite{rawte2023ao8}.\n    *   **Key Performance Metrics and Comparison Results**:\n        *   The paper defines the `HVI` as the primary metric for quantifying hallucination vulnerability. While the specific comparative ranking results for the 15 LLMs are not detailed in the provided abstract/introduction, the methodology for obtaining and presenting this comparative spectrum (similar to z-score or min-max normalization) is outlined \\cite{rawte2023ao8}. The `HILT` dataset statistics (e.g., total annotated sentences, distribution across categories) are provided, demonstrating the dataset's scale and coverage \\cite{rawte2023ao8}.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The `HVI` calculation specifically excludes intrinsic hallucinations, considering them \"relatively minor\" and exhibiting \"lower vulnerability overall\" \\cite{rawte2023ao8}. The paper *proposes* mitigation strategies rather than fully implementing and evaluating them, acknowledging that \"complete mitigation can be a herculean task\" \\cite{rawte2023ao8}.\n    *   **Scope of Applicability**: The study focuses on general text generation from prompts, aiming for findings applicable across various NLP tasks rather than being confined to a specific one \\cite{rawte2023ao8}. The `HILT` dataset and `HVI` are intended to be continuously updated and accessible to the research community \\cite{rawte2023ao8}.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: This work significantly advances the technical state-of-the-art by providing the first comprehensive, fine-grained taxonomy of LLM hallucination, a large-scale human-annotated dataset (`HILT`), and a standardized quantitative metric (`HVI`) for assessing hallucination vulnerability \\cite{rawte2023ao8}.\n    *   **Potential Impact on Future Research**: The `HILT` dataset and `HVI` are positioned as valuable resources for future researchers to study LLM hallucination behaviors, design effective mitigation techniques, and assess their categorical impacts. The `HVI` is also highlighted as a crucial tool for AI-related policy-making, enabling the assessment of risks associated with different LLMs \\cite{rawte2023ao8}.",
      "intriguing_abstract": "The pervasive issue of hallucination in Large Language Models (LLMs), where models confidently generate factually incorrect or unfaithful content, poses a critical threat to their reliability and societal adoption. Current approaches lack a comprehensive understanding and a standardized metric to quantify this troubling phenomenon. This paper introduces a groundbreaking, multi-faceted framework to systematically address LLM hallucination. We propose a novel, fine-grained **taxonomy** that categorizes hallucinations by degree, orientation (e.g., **Factual Mirage**, **Silver Lining**, with **intrinsic** and **extrinsic** sub-types), and six specific categories like **Numeric Nuisance** and **Geographic Erratum**. To enable robust evaluation, we present **HILT** (HallucInation eLiciTation), the first large-scale, human-annotated dataset comprising 75,000 text snippets from 15 contemporary LLMs. Crucially, we introduce the **Hallucination Vulnerability Index (HVI)**, a novel quantitative metric designed to objectively measure and rank LLMs based on their susceptibility to generating untruths. This work provides indispensable tools for researchers to dissect, compare, and mitigate LLM hallucinations, offering a vital resource for risk assessment and informed policy-making in the era of AI.",
      "keywords": [
        "Large Language Models (LLMs)",
        "Hallucination",
        "Hallucination taxonomy",
        "Factual Mirage (FM)",
        "Silver Lining (SL)",
        "Intrinsic/extrinsic hallucination",
        "HILT dataset",
        "Hallucination Vulnerability Index (HVI)",
        "Human annotation",
        "Quantitative metric",
        "LLM risk assessment",
        "Mitigation strategies",
        "Comparative spectrum"
      ],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/99bfe503743c5ec8e16e50ab8438159cdb533a89.pdf",
      "citation_key": "rawte2023ao8",
      "metadata": {
        "title": "The Troubling Emergence of Hallucination in Large Language Models - An Extensive Definition, Quantification, and Prescriptive Remediations",
        "authors": [
          "Vipula Rawte",
          "Swagata Chakraborty",
          "Agnibh Pathak",
          "Anubhav Sarkar",
          "S.M. Towhidul Islam Tonmoy",
          "Islam Tonmoy",
          "Aman Chadha",
          "Amit P. Sheth",
          "Amitava Das",
          "Paris",
          "A. Sridhar",
          "Erik Visser",
          "Improved",
          "Jianlin Su",
          "Yu Lu",
          "Shengfeng Pan",
          "Ahmed Murtadha",
          "Bo Wen",
          "Yunfeng Liu",
          "Roformer",
          "Rohan Taori",
          "Ishaan Gulrajani",
          "Tianyi Zhang",
          "Yann Dubois",
          "Xuechen Li",
          "Carlos Guestrin",
          "Percy Liang",
          "Tatsunori Hashimoto",
          "Stanford",
          "Hugo Touvron",
          "Thibaut Lavril",
          "Gautier Izacard",
          "Xavier Martinet",
          "M. Lachaux",
          "TimothÃ©e Lacroix",
          "Baptiste RoziÃ¨re",
          "Naman Goyal",
          "Eric Hambro",
          "Faisal Azhar",
          "Aur'elien Rodriguez",
          "Armand Joulin",
          "Thomas Wolf",
          "Lysandre Debut",
          "Victor Sanh",
          "Julien Chaumond",
          "Clement Delangue",
          "Anthony Moi",
          "Pierric Cistac",
          "Tim Rault",
          "RÃ©mi Louf",
          "Morgan Funtow-icz",
          "Joe Davison",
          "Sam Shleifer",
          "Patrick von Platen",
          "Clara Ma",
          "Yacine Jernite",
          "J. Plu",
          "Canwen Xu",
          "Teven Le Scao",
          "Sylvain Gugger",
          "Mariama Drame",
          "Quentin Lhoest",
          "Susan Zhang",
          "Stephen Roller",
          "Mikel Artetxe",
          "Moya Chen",
          "Shuohui Chen",
          "Christopher De-wan",
          "Mona T. Diab",
          "Xi Xian Li",
          "Todor Victoria Lin",
          "Myle Ott",
          "Kurt Shuster",
          "Punit Daniel Simig",
          "S. Koura",
          "Anjali Sridhar",
          "Tianlu Wang",
          "Luke Zettlemoyer. 2022",
          "Daniel M. Ziegler",
          "Nisan Stiennon",
          "Jeffrey Wu",
          "Tom B. Brown",
          "Alec Radford",
          "Dario Amodei",
          "Paul F. Chris-tiano"
        ],
        "published_date": "2023",
        "abstract": "The recent advancements in Large Language Models (LLMs) have garnered widespread acclaim for their remarkable emerging capabilities. However, the issue of hallucination has parallelly emerged as a by-product, posing significant concerns. While some recent endeavors have been made to identify and mitigate different types of hallucination, there has been a limited emphasis on the nuanced categorization of hallucination and associated mitigation methods. To address this gap, we offer a fine-grained discourse on profiling hallucination based on its degree, orientation, and category, along with offering strategies for alleviation. As such, we define two overarching orientations of hallucination: (i) factual mirage (FM) and (ii) silver lining (SL). To provide a more comprehensive understanding, both orientations are further sub-categorized into intrinsic and extrinsic, with three degrees of severity - (i) mild, (ii) moderate, and (iii) alarming. We also meticulously categorize hallucination into six types: (i) acronym ambiguity, (ii) numeric nuisance, (iii) generated golem, (iv) virtual voice, (v) geographic erratum, and (vi) time wrap. Furthermore, we curate HallucInation eLiciTation (HILT), a publicly available dataset comprising of 75,000 samples generated using 15 contemporary LLMs along with human annotations for the aforementioned categories. Finally, to establish a method for quantifying and to offer a comparative spectrum that allows us to evaluate and rank LLMs based on their vulnerability to producing hallucinations, we propose Hallucination Vulnerability Index (HVI). We firmly believe that HVI holds significant value as a tool for the wider NLP community, with the potential to serve as a rubric in AI-related policy-making. In conclusion, we propose two solution strategies for mitigating hallucinations.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/99bfe503743c5ec8e16e50ab8438159cdb533a89.pdf",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the paper for a literature review:\n\n### Technical Paper Analysis: The Troubling Emergence of Hallucination in Large Language Models \\cite{rawte2023ao8}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: The paper addresses the significant and growing problem of hallucination in Large Language Models (LLMs), where models generate factually incorrect or unfaithful content.\n    *   **Importance and Challenge**: This problem is critical due to the widespread adoption of LLMs in real-world applications, leading to potential misuse, factual inaccuracies (e.g., Google Bard's factual error, fabricated legal precedents), and societal concerns regarding the spread of untruths. The challenge lies in the limited emphasis on nuanced categorization of hallucination and the absence of a uniform, quantifiable metric to assess and compare LLM vulnerability to hallucination.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: Previous research has loosely defined hallucination, often categorizing it into limited classes (e.g., intrinsic/extrinsic) or focusing on task-specific types (e.g., summarization, Q&A). Some work explored factual vs. non-factual prompts \\cite{rawte2023ao8}.\n    *   **Limitations of Previous Solutions**: Existing approaches lack a comprehensive, fine-grained categorization of hallucination across different dimensions (degree, orientation, specific types). Crucially, there is no uniform evaluation metric or comparative spectrum to quantify and rank LLMs based on their hallucination vulnerability, making it difficult for policymakers and researchers to assess risks and compare models effectively \\cite{rawte2023ao8}.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes a multi-faceted approach:\n        *   A novel, fine-grained framework for profiling hallucination based on its degree (mild, moderate, alarming), orientation (Factual Mirage (FM) and Silver Lining (SL), each sub-categorized into intrinsic and extrinsic), and six specific categories (Acronym Ambiguity, Numeric Nuisance, Generated Golem, Virtual Voice, Geographic Erratum, Time Wrap) \\cite{rawte2023ao8}.\n        *   The creation of `HILT` (HallucInation eLiciTation), a publicly available, human-annotated dataset comprising 75,000 text snippets generated by 15 contemporary LLMs using both factually correct and incorrect prompts \\cite{rawte2023ao8}.\n        *   The introduction of the Hallucination Vulnerability Index (`HVI`), a quantitative metric designed to evaluate and rank LLMs based on their susceptibility to producing hallucinations \\cite{rawte2023ao8}.\n    *   **Novelty/Difference**: The innovation lies in the comprehensive and systematic categorization of hallucination, moving beyond simplistic definitions. The `HILT` dataset is the first of its kind to offer such a broad collection of LLM-generated text with fine-grained human annotations across the proposed categories. The `HVI` provides a much-needed standardized, quantifiable measure for comparing LLM hallucination tendencies, which is crucial for risk assessment and policy-making \\cite{rawte2023ao8}.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**:\n        *   A comprehensive taxonomy of hallucination, including two orientations (Factual Mirage, Silver Lining), their intrinsic/extrinsic sub-categories, three degrees of severity (mild, moderate, alarming), and six distinct types (Acronym Ambiguity, Numeric Nuisance, Generated Golem, Virtual Voice, Geographic Erratum, Time Wrap) \\cite{rawte2023ao8}.\n        *   The Hallucination Vulnerability Index (`HVI`), a quantitative formula to measure and rank LLM hallucination susceptibility, incorporating factors like the ratio of hallucinated sentences and tendencies towards specific (extrinsic) hallucination types \\cite{rawte2023ao8}.\n    *   **System Design/Architectural Innovations**:\n        *   The `HILT` dataset, a large-scale (75,000 samples) and diverse resource for hallucination research, generated from 15 LLMs and meticulously human-annotated at the sentence level using the MACE tool for agreement aggregation \\cite{rawte2023ao8}.\n    *   **Theoretical Insights/Analysis**: The paper provides a deeper understanding of hallucination by dissecting its various facets, offering a framework for systematic analysis rather than isolated observations \\cite{rawte2023ao8}. It also proposes two mitigation strategies, suggesting identifying high-entropy points in text from high-HVI LLMs and replacing them with output from lower-HVI LLMs \\cite{rawte2023ao8}.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**:\n        *   **Dataset Creation**: The `HILT` dataset was curated by generating 75,000 text snippets (5,000 per LLM) from 15 contemporary LLMs (including GPT-4, LLaMA, BLOOM, etc.) using prompts from NYTimes tweets (factually correct) and Politifact (factually incorrect) \\cite{rawte2023ao8}.\n        *   **Human Annotation**: These snippets underwent sentence-level human annotation for hallucination orientations and categories by four annotators per sentence via Amazon Mechanical Turk, with inter-annotator agreement assessed and data aggregated using the MACE tool \\cite{rawte2023ao8}.\n        *   **HVI Application**: The `HVI` was calculated for all 15 LLMs, and the resulting HVIs were ranked and scaled to provide a comparative spectrum of their hallucination vulnerability \\cite{rawte2023ao8}.\n    *   **Key Performance Metrics and Comparison Results**:\n        *   The paper defines the `HVI` as the primary metric for quantifying hallucination vulnerability. While the specific comparative ranking results for the 15 LLMs are not detailed in the provided abstract/introduction, the methodology for obtaining and presenting this comparative spectrum (similar to z-score or min-max normalization) is outlined \\cite{rawte2023ao8}. The `HILT` dataset statistics (e.g., total annotated sentences, distribution across categories) are provided, demonstrating the dataset's scale and coverage \\cite{rawte2023ao8}.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The `HVI` calculation specifically excludes intrinsic hallucinations, considering them \"relatively minor\" and exhibiting \"lower vulnerability overall\" \\cite{rawte2023ao8}. The paper *proposes* mitigation strategies rather than fully implementing and evaluating them, acknowledging that \"complete mitigation can be a herculean task\" \\cite{rawte2023ao8}.\n    *   **Scope of Applicability**: The study focuses on general text generation from prompts, aiming for findings applicable across various NLP tasks rather than being confined to a specific one \\cite{rawte2023ao8}. The `HILT` dataset and `HVI` are intended to be continuously updated and accessible to the research community \\cite{rawte2023ao8}.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: This work significantly advances the technical state-of-the-art by providing the first comprehensive, fine-grained taxonomy of LLM hallucination, a large-scale human-annotated dataset (`HILT`), and a standardized quantitative metric (`HVI`) for assessing hallucination vulnerability \\cite{rawte2023ao8}.\n    *   **Potential Impact on Future Research**: The `HILT` dataset and `HVI` are positioned as valuable resources for future researchers to study LLM hallucination behaviors, design effective mitigation techniques, and assess their categorical impacts. The `HVI` is also highlighted as a crucial tool for AI-related policy-making, enabling the assessment of risks associated with different LLMs \\cite{rawte2023ao8}.",
        "keywords": [
          "Large Language Models (LLMs)",
          "Hallucination",
          "Hallucination taxonomy",
          "Factual Mirage (FM)",
          "Silver Lining (SL)",
          "Intrinsic/extrinsic hallucination",
          "HILT dataset",
          "Hallucination Vulnerability Index (HVI)",
          "Human annotation",
          "Quantitative metric",
          "LLM risk assessment",
          "Mitigation strategies",
          "Comparative spectrum"
        ],
        "paper_type": "based on the abstract and introduction, this paper should be classified as **technical**.\n\nhere's why:\n\n*   **proposes new methods/systems:** the abstract explicitly states, \"we offer a fine-grained discourse on profiling hallucination based on its degree, orientation, and category,\" \"we define two overarching orientations of hallucination,\" \"we meticulously categorize hallucination into six types,\" \"we curate halluc ination elicitation (hilt), a publicly available dataset,\" \"we propose hallucinationvulnerability index(hvi),\" and \"we propose two solution strategies for mitigating hallucinations.\" these are all new definitions, categorizations, a new dataset, a new metric, and new mitigation strategies.\n*   **addresses a technical problem with proposed solutions:** the introduction highlights the \"troubling emergence of hallucination\" and the \"limited emphasis on the nuanced categorization of hallucination and associated mitigation methods.\" the paper then presents its contributions as direct solutions to these technical problems.\n*   **\"our contributions\" section:** this section clearly lists the introduction of a detailed study, new categorizations, a new dataset (hilt), a new quantitative analysis index (hvi), and two mitigation strategies. these are all hallmarks of a technical paper.\n*   **empirical aspects serve the technical contribution:** while the paper involves data (hilt dataset, evaluation of llms, testing mitigation strategies), this empirical work is primarily to demonstrate and validate the *new methods, dataset, and metric* being proposed, rather than solely testing existing hypotheses or analyzing data with established tools. the core is the creation of these new technical artifacts."
      },
      "file_name": "99bfe503743c5ec8e16e50ab8438159cdb533a89.pdf"
    },
    {
      "success": true,
      "doc_id": "045a4f48975143ca1094e42adf16f0ae",
      "summary": "Here is a focused summary of the provided technical paper for literature review:\n\n*   **1. Research Problem & Motivation**\n    *   **Problem:** Large Language Models (LLMs), despite their effectiveness, are prone to critical issues such as hallucination, unfaithful reasoning, and toxicity \\cite{pan2024y3a}.\n    *   **Motivation:** These flaws hinder the reliability and trustworthiness of LLMs, necessitating methods to rectify them. Correcting LLMs with feedback, particularly automated feedback (from the LLM itself or an external system), is identified as a promising and practical approach due to its potential for minimal human intervention and enhanced deployability \\cite{pan2024y3a}.\n\n*   **2. Related Work & Positioning**\n    *   **Positioning:** This paper serves as an \"exhaustive review\" of recent advancements in correcting LLMs using automated feedback \\cite{pan2024y3a}.\n    *   **Categorization:** It structures the existing landscape by categorizing approaches into training-time, generation-time, and post-hoc methods \\cite{pan2024y3a}.\n    *   **Limitations of Previous Solutions:** The paper implicitly addresses the limitations of *uncorrected* LLMs (hallucination, unfaithful reasoning, toxicity) by reviewing methods designed to overcome these inherent flaws \\cite{pan2024y3a}.\n\n*   **3. Technical Approach & Innovation**\n    *   **Core Method:** The paper's core approach is a comprehensive literature review and synthesis of techniques that leverage automated feedback to improve LLM outputs \\cite{pan2024y3a}.\n    *   **Novelty:** Its innovation lies in providing an \"exhaustive review\" and a structured categorization of these diverse techniques, offering a unified perspective on the emerging field of automated LLM correction \\cite{pan2024y3a}. It also identifies future research directions.\n\n*   **4. Key Technical Contributions**\n    *   **Comprehensive Review:** Provides an exhaustive overview of recent advances in correcting LLMs with automated feedback \\cite{pan2024y3a}.\n    *   **Taxonomy:** Introduces a structured categorization of these techniques into training-time, generation-time, and post-hoc approaches, which helps organize and understand the field \\cite{pan2024y3a}.\n    *   **Future Directions:** Identifies potential challenges and outlines future research directions, guiding subsequent work in this domain \\cite{pan2024y3a}.\n\n*   **5. Experimental Validation**\n    *   As a review paper, this work does not present its own experimental validation of novel algorithms or methods. Instead, it synthesizes and analyzes the findings and validations presented in the primary research papers it reviews \\cite{pan2024y3a}.\n\n*   **6. Limitations & Scope**\n    *   **Scope:** The paper's scope is specifically limited to techniques that employ \"automated feedback\" for LLM correction, encompassing both self-correction and feedback from external systems \\cite{pan2024y3a}.\n    *   **Limitations:** The paper itself identifies \"potential challenges\" within the field of automated LLM correction, which it aims to highlight for future research \\cite{pan2024y3a}.\n\n*   **7. Technical Significance**\n    *   **Advancement:** By providing a structured and exhaustive overview, the paper significantly advances the understanding of the state-of-the-art in automated LLM correction \\cite{pan2024y3a}.\n    *   **Impact:** It serves as a valuable resource for researchers and practitioners, helping to navigate the complex landscape of LLM feedback mechanisms and guiding future efforts toward developing more robust, reliable, and practical LLM-based solutions \\cite{pan2024y3a}.",
      "intriguing_abstract": "Despite their transformative capabilities, Large Language Models (LLMs) grapple with pervasive issues like hallucination, unfaithful reasoning, and toxicity, severely undermining their reliability and trustworthiness. Addressing these critical flaws is paramount for their widespread, responsible deployment. This paper presents an exhaustive and timely review of cutting-edge advancements in correcting LLMs through automated feedback mechanisms, a highly promising approach minimizing human intervention.\n\nWe meticulously synthesize the diverse landscape of techniques leveraging both self-correction and external feedback systems. Our novel contribution includes a structured taxonomy, categorizing these methods into distinct training-time, generation-time, and post-hoc strategies, providing a unified framework to understand this rapidly evolving field. Beyond this comprehensive overview, we identify key challenges and chart crucial future research directions, guiding the development of more robust and dependable LLM solutions. This review serves as an indispensable resource for researchers and practitioners, illuminating pathways to enhance LLM integrity and accelerate the realization of truly trustworthy artificial intelligence.",
      "keywords": [
        "Large Language Models (LLMs)",
        "LLM correction",
        "automated feedback",
        "LLM reliability issues",
        "exhaustive literature review",
        "structured taxonomy",
        "training-time LLM correction",
        "generation-time LLM correction",
        "post-hoc LLM correction",
        "future research directions"
      ],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/c946888e2f81b1db84ba4addf2a11e87f0568fe9.pdf",
      "citation_key": "pan2024y3a",
      "metadata": {
        "title": "Automatically Correcting Large Language Models: Surveying the Landscape of Diverse Automated Correction Strategies",
        "authors": [
          "Liangming Pan",
          "Michael Stephen Saxon",
          "Wenda Xu",
          "Deepak Nathani",
          "Xinyi Wang",
          "W. Wang"
        ],
        "published_date": "2024",
        "abstract": "While large language models (LLMs) have shown remarkable effectiveness in various NLP tasks, they are still prone to issues such as hallucination, unfaithful reasoning, and toxicity. A promising approach to rectify these flaws is correcting LLMs with feedback, where the LLM itself is prompted or guided with feedback to fix problems in its own output. Techniques leveraging automated feedbackâ€”either produced by the LLM itself (self-correction) or some external systemâ€”are of particular interest as they make LLM-based solutions more practical and deployable with minimal human intervention. This paper provides an exhaustive review of the recent advances in correcting LLMs with automated feedback, categorizing them into training-time, generation-time, and post-hoc approaches. We also identify potential challenges and future directions in this emerging field.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/c946888e2f81b1db84ba4addf2a11e87f0568fe9.pdf",
        "venue": "Transactions of the Association for Computational Linguistics",
        "citationCount": 0,
        "score": 0,
        "summary": "Here is a focused summary of the provided technical paper for literature review:\n\n*   **1. Research Problem & Motivation**\n    *   **Problem:** Large Language Models (LLMs), despite their effectiveness, are prone to critical issues such as hallucination, unfaithful reasoning, and toxicity \\cite{pan2024y3a}.\n    *   **Motivation:** These flaws hinder the reliability and trustworthiness of LLMs, necessitating methods to rectify them. Correcting LLMs with feedback, particularly automated feedback (from the LLM itself or an external system), is identified as a promising and practical approach due to its potential for minimal human intervention and enhanced deployability \\cite{pan2024y3a}.\n\n*   **2. Related Work & Positioning**\n    *   **Positioning:** This paper serves as an \"exhaustive review\" of recent advancements in correcting LLMs using automated feedback \\cite{pan2024y3a}.\n    *   **Categorization:** It structures the existing landscape by categorizing approaches into training-time, generation-time, and post-hoc methods \\cite{pan2024y3a}.\n    *   **Limitations of Previous Solutions:** The paper implicitly addresses the limitations of *uncorrected* LLMs (hallucination, unfaithful reasoning, toxicity) by reviewing methods designed to overcome these inherent flaws \\cite{pan2024y3a}.\n\n*   **3. Technical Approach & Innovation**\n    *   **Core Method:** The paper's core approach is a comprehensive literature review and synthesis of techniques that leverage automated feedback to improve LLM outputs \\cite{pan2024y3a}.\n    *   **Novelty:** Its innovation lies in providing an \"exhaustive review\" and a structured categorization of these diverse techniques, offering a unified perspective on the emerging field of automated LLM correction \\cite{pan2024y3a}. It also identifies future research directions.\n\n*   **4. Key Technical Contributions**\n    *   **Comprehensive Review:** Provides an exhaustive overview of recent advances in correcting LLMs with automated feedback \\cite{pan2024y3a}.\n    *   **Taxonomy:** Introduces a structured categorization of these techniques into training-time, generation-time, and post-hoc approaches, which helps organize and understand the field \\cite{pan2024y3a}.\n    *   **Future Directions:** Identifies potential challenges and outlines future research directions, guiding subsequent work in this domain \\cite{pan2024y3a}.\n\n*   **5. Experimental Validation**\n    *   As a review paper, this work does not present its own experimental validation of novel algorithms or methods. Instead, it synthesizes and analyzes the findings and validations presented in the primary research papers it reviews \\cite{pan2024y3a}.\n\n*   **6. Limitations & Scope**\n    *   **Scope:** The paper's scope is specifically limited to techniques that employ \"automated feedback\" for LLM correction, encompassing both self-correction and feedback from external systems \\cite{pan2024y3a}.\n    *   **Limitations:** The paper itself identifies \"potential challenges\" within the field of automated LLM correction, which it aims to highlight for future research \\cite{pan2024y3a}.\n\n*   **7. Technical Significance**\n    *   **Advancement:** By providing a structured and exhaustive overview, the paper significantly advances the understanding of the state-of-the-art in automated LLM correction \\cite{pan2024y3a}.\n    *   **Impact:** It serves as a valuable resource for researchers and practitioners, helping to navigate the complex landscape of LLM feedback mechanisms and guiding future efforts toward developing more robust, reliable, and practical LLM-based solutions \\cite{pan2024y3a}.",
        "keywords": [
          "Large Language Models (LLMs)",
          "LLM correction",
          "automated feedback",
          "LLM reliability issues",
          "exhaustive literature review",
          "structured taxonomy",
          "training-time LLM correction",
          "generation-time LLM correction",
          "post-hoc LLM correction",
          "future research directions"
        ],
        "paper_type": "**survey**"
      },
      "file_name": "c946888e2f81b1db84ba4addf2a11e87f0568fe9.pdf"
    },
    {
      "success": true,
      "doc_id": "b83a1cb6bd92ca9ae404ff70e91a8b85",
      "summary": "Here's a focused summary of the paper for a literature review, adhering to the specified citation requirements:\n\n### HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models \\cite{li2023rvf}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Large Language Models (LLMs) are prone to generating \"hallucinations,\" defined as content that conflicts with source information or cannot be factually verified.\n    *   **Importance and Challenge**: Hallucinations make LLM deployment risky in real-world applications. Existing work primarily investigates causes for specific tasks and smaller language models, leaving it unclear what types of content and to what extent *general* LLMs hallucinate, and how well they can *recognize* such errors.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: Previous research largely focused on investigating the causes of hallucination for specific tasks and smaller language models \\cite{li2023rvf}.\n    *   **Limitations of Previous Solutions**: These prior efforts did not provide a comprehensive understanding of the types and extent of hallucinations in large, general-purpose LLMs like ChatGPT, nor did they offer a robust benchmark for evaluating LLMs' ability to recognize hallucinations across diverse tasks \\cite{li2023rvf}.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper introduces HaluEval, a large-scale benchmark comprising 35,000 hallucinated/normal samples. Its construction involves two main strategies:\n        *   **Automatic Generation (Sampling-then-Filtering Framework)**: For task-specific examples (Question Answering, Knowledge-Grounded Dialogue, Text Summarization), a two-stage process is proposed:\n            *   **Diverse Hallucination Sampling**: ChatGPT is prompted using carefully designed instructions (intention description, hallucination patterns, few-shot demonstrations) to generate diverse hallucinated samples. Two sampling methods are used: a one-pass instruction schema and a conversational schema, aiming for multi-facet hallucinations.\n            *   **High-quality Hallucination Filtering**: ChatGPT acts as an \"answer judge\" to select the most plausible and difficult hallucinated samples from the candidates generated in the first stage. This filtering instruction is enhanced by ground-truth examples, guiding ChatGPT to choose the hallucinated answer closest to the correct one, making it harder for other LLMs to identify.\n        *   **Human Annotation**: For general user queries, 5,000 ChatGPT responses are human-annotated. A pre-selection procedure identifies queries where LLMs are most likely to hallucinate by selecting responses with low semantic similarity among multiple ChatGPT generations. Human labelers then identify and mark hallucinated spans (unverifiable, non-factual, or irrelevant content).\n    *   **Novelty/Difference**:\n        *   **Large-scale, Multi-task Benchmark**: HaluEval is a comprehensive benchmark specifically designed for evaluating hallucination in *large* language models across general queries and three distinct NLP tasks \\cite{li2023rvf}.\n        *   **Automated Hallucination Generation**: The `sampling-then-filtering` framework leverages LLMs (ChatGPT) to automatically generate high-quality, challenging hallucinated samples, which is a novel approach to benchmark creation \\cite{li2023rvf}.\n        *   **Instruction Design for Controlled Hallucination**: The detailed instruction design, incorporating specific hallucination patterns and demonstrations, allows for controlled and diverse generation of hallucinated content tailored to different tasks \\cite{li2023rvf}.\n\n4.  **Key Technical Contributions**\n    *   **Novel Benchmark**: Introduction of HaluEval, a large-scale benchmark (35,000 samples) for evaluating LLM hallucination generation and recognition \\cite{li2023rvf}.\n    *   **Automated Hallucination Generation Framework**: A two-stage `sampling-then-filtering` framework that uses LLMs to automatically generate diverse and challenging hallucinated samples for various tasks \\cite{li2023rvf}.\n    *   **Instruction Design for Hallucination Control**: Detailed instruction templates for LLMs to generate specific types of hallucinations (e.g., factualness, specificity, inference for QA) and to filter for high-quality, plausible hallucinated examples \\cite{li2023rvf}.\n    *   **Human-Annotated Dataset**: A collection of 5,000 human-annotated ChatGPT responses to general user queries, providing insights into real-world hallucination tendencies \\cite{li2023rvf}.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**: The paper evaluated several state-of-the-art LLMs (5 closed-source including ChatGPT, Claude 2; 5 open-source including Llama 2, Alpaca) on their ability to classify whether a given text contains hallucinated content using the HaluEval benchmark \\cite{li2023rvf}.\n    *   **Key Performance Metrics and Comparison Results**:\n        *   **Hallucination Generation**: Human annotation revealed that approximately 19.5% of ChatGPT's responses to general user queries contained hallucinated content, often fabricating unverifiable information related to specific topics (e.g., language, climate, technology) \\cite{li2023rvf}.\n        *   **Hallucination Recognition**: Existing LLMs, including ChatGPT, face significant challenges in recognizing hallucinations. ChatGPT achieved an accuracy of 62.59% for question answering and 79.44% for general queries, while other models like Alpaca performed much worse (e.g., 6.68% for QA) \\cite{li2023rvf}.\n        *   **Mitigation Strategies**: Experiments showed that providing explicit external knowledge or adding intermediate reasoning steps significantly improved LLMs' performance in recognizing hallucinations. Conversely, contrasting hallucinated samples directly with ground-truth answers tended to confuse LLMs and degrade performance \\cite{li2023rvf}.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The automatic generation relies on the capabilities of a powerful LLM (ChatGPT) for both sampling and filtering, which might introduce biases inherent to that model. The definition of \"hallucination\" is based on unverifiable, non-factual, or irrelevant content, which might not cover all nuances of LLM errors \\cite{li2023rvf}.\n    *   **Scope of Applicability**: The benchmark covers general user queries and three specific NLP tasks (QA, dialogue, summarization). While adaptable, its direct applicability to other specialized tasks might require further instruction design and validation \\cite{li2023rvf}.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: HaluEval significantly advances the technical state-of-the-art by providing the first large-scale, multi-task benchmark specifically designed to systematically evaluate hallucination in *large* language models, moving beyond task-specific or small-model analyses \\cite{li2023rvf}.\n    *   **Potential Impact on Future Research**:\n        *   Enables deeper analysis of what types of content LLMs tend to hallucinate and to what extent \\cite{li2023rvf}.\n        *   Provides a robust evaluation tool for assessing and comparing LLMs' ability to recognize and mitigate hallucinations \\cite{li2023rvf}.\n        *   Facilitates the development of new methods and techniques for hallucination detection and reduction in LLMs, particularly by exploring the benefits of external knowledge and reasoning steps \\cite{li2023rvf}.",
      "intriguing_abstract": "Large Language Models (LLMs) are transforming AI, yet their pervasive tendency for \"hallucination\"â€”generating factually incorrect or unverifiable contentâ€”poses a critical challenge to their reliable deployment. Existing evaluation methods are often task-specific or limited to smaller models, leaving a significant gap in understanding general LLM hallucination and their ability to self-recognize these errors. We introduce HaluEval, a novel, large-scale benchmark comprising 35,000 samples across general queries, question answering, knowledge-grounded dialogue, and text summarization. Our innovative `sampling-then-filtering` framework leverages LLMs to automatically generate diverse, challenging hallucinated content, a significant advancement in benchmark creation. Experiments reveal that even advanced LLMs like ChatGPT hallucinate in nearly 20% of general responses and struggle profoundly with hallucination recognition (e.g., 62.59% accuracy for QA). Crucially, we demonstrate that incorporating external knowledge and intermediate reasoning steps significantly boosts recognition performance. HaluEval provides an indispensable tool for systematically evaluating and mitigating LLM hallucinations, paving the way for safer, more trustworthy AI applications.",
      "keywords": [
        "Large Language Models (LLMs)",
        "hallucination evaluation",
        "HaluEval benchmark",
        "automated hallucination generation",
        "sampling-then-filtering framework",
        "multi-task evaluation",
        "instruction design",
        "hallucination recognition challenges",
        "Question Answering",
        "Knowledge-Grounded Dialogue",
        "Text Summarization",
        "human-annotated dataset",
        "external knowledge integration",
        "reasoning steps"
      ],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/e0384ba36555232c587d4a80d527895a095a9001.pdf",
      "citation_key": "li2023rvf",
      "metadata": {
        "title": "HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models",
        "authors": [
          "Junyi Li",
          "Xiaoxue Cheng",
          "Wayne Xin Zhao",
          "J. Nie",
          "Ji-rong Wen"
        ],
        "published_date": "2023",
        "abstract": "Large language models (LLMs), such as ChatGPT, are prone to generate hallucinations, i.e., content that conflicts with the source or cannot be verified by the factual knowledge. To understand what types of content and to which extent LLMs are apt to hallucinate, we introduce the Hallucination Evaluation benchmark for Large Language Models (HaluEval), a large collection of generated and human-annotated hallucinated samples for evaluating the performance of LLMs in recognizing hallucination. To generate these samples, we propose a ChatGPT-based two-step framework, i.e., sampling-then-filtering. Besides, we also hire some human labelers to annotate the hallucinations in ChatGPT responses. The empirical results suggest that ChatGPT is likely to generate hallucinated content in specific topics by fabricating unverifiable information (i.e., about $19.5\\%$ responses). Moreover, existing LLMs face great challenges in recognizing the hallucinations in texts. However, our experiments also prove that providing external knowledge or adding reasoning steps can help LLMs recognize hallucinations. Our benchmark can be accessed at https://github.com/RUCAIBox/HaluEval.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/e0384ba36555232c587d4a80d527895a095a9001.pdf",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the paper for a literature review, adhering to the specified citation requirements:\n\n### HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models \\cite{li2023rvf}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Large Language Models (LLMs) are prone to generating \"hallucinations,\" defined as content that conflicts with source information or cannot be factually verified.\n    *   **Importance and Challenge**: Hallucinations make LLM deployment risky in real-world applications. Existing work primarily investigates causes for specific tasks and smaller language models, leaving it unclear what types of content and to what extent *general* LLMs hallucinate, and how well they can *recognize* such errors.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: Previous research largely focused on investigating the causes of hallucination for specific tasks and smaller language models \\cite{li2023rvf}.\n    *   **Limitations of Previous Solutions**: These prior efforts did not provide a comprehensive understanding of the types and extent of hallucinations in large, general-purpose LLMs like ChatGPT, nor did they offer a robust benchmark for evaluating LLMs' ability to recognize hallucinations across diverse tasks \\cite{li2023rvf}.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper introduces HaluEval, a large-scale benchmark comprising 35,000 hallucinated/normal samples. Its construction involves two main strategies:\n        *   **Automatic Generation (Sampling-then-Filtering Framework)**: For task-specific examples (Question Answering, Knowledge-Grounded Dialogue, Text Summarization), a two-stage process is proposed:\n            *   **Diverse Hallucination Sampling**: ChatGPT is prompted using carefully designed instructions (intention description, hallucination patterns, few-shot demonstrations) to generate diverse hallucinated samples. Two sampling methods are used: a one-pass instruction schema and a conversational schema, aiming for multi-facet hallucinations.\n            *   **High-quality Hallucination Filtering**: ChatGPT acts as an \"answer judge\" to select the most plausible and difficult hallucinated samples from the candidates generated in the first stage. This filtering instruction is enhanced by ground-truth examples, guiding ChatGPT to choose the hallucinated answer closest to the correct one, making it harder for other LLMs to identify.\n        *   **Human Annotation**: For general user queries, 5,000 ChatGPT responses are human-annotated. A pre-selection procedure identifies queries where LLMs are most likely to hallucinate by selecting responses with low semantic similarity among multiple ChatGPT generations. Human labelers then identify and mark hallucinated spans (unverifiable, non-factual, or irrelevant content).\n    *   **Novelty/Difference**:\n        *   **Large-scale, Multi-task Benchmark**: HaluEval is a comprehensive benchmark specifically designed for evaluating hallucination in *large* language models across general queries and three distinct NLP tasks \\cite{li2023rvf}.\n        *   **Automated Hallucination Generation**: The `sampling-then-filtering` framework leverages LLMs (ChatGPT) to automatically generate high-quality, challenging hallucinated samples, which is a novel approach to benchmark creation \\cite{li2023rvf}.\n        *   **Instruction Design for Controlled Hallucination**: The detailed instruction design, incorporating specific hallucination patterns and demonstrations, allows for controlled and diverse generation of hallucinated content tailored to different tasks \\cite{li2023rvf}.\n\n4.  **Key Technical Contributions**\n    *   **Novel Benchmark**: Introduction of HaluEval, a large-scale benchmark (35,000 samples) for evaluating LLM hallucination generation and recognition \\cite{li2023rvf}.\n    *   **Automated Hallucination Generation Framework**: A two-stage `sampling-then-filtering` framework that uses LLMs to automatically generate diverse and challenging hallucinated samples for various tasks \\cite{li2023rvf}.\n    *   **Instruction Design for Hallucination Control**: Detailed instruction templates for LLMs to generate specific types of hallucinations (e.g., factualness, specificity, inference for QA) and to filter for high-quality, plausible hallucinated examples \\cite{li2023rvf}.\n    *   **Human-Annotated Dataset**: A collection of 5,000 human-annotated ChatGPT responses to general user queries, providing insights into real-world hallucination tendencies \\cite{li2023rvf}.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**: The paper evaluated several state-of-the-art LLMs (5 closed-source including ChatGPT, Claude 2; 5 open-source including Llama 2, Alpaca) on their ability to classify whether a given text contains hallucinated content using the HaluEval benchmark \\cite{li2023rvf}.\n    *   **Key Performance Metrics and Comparison Results**:\n        *   **Hallucination Generation**: Human annotation revealed that approximately 19.5% of ChatGPT's responses to general user queries contained hallucinated content, often fabricating unverifiable information related to specific topics (e.g., language, climate, technology) \\cite{li2023rvf}.\n        *   **Hallucination Recognition**: Existing LLMs, including ChatGPT, face significant challenges in recognizing hallucinations. ChatGPT achieved an accuracy of 62.59% for question answering and 79.44% for general queries, while other models like Alpaca performed much worse (e.g., 6.68% for QA) \\cite{li2023rvf}.\n        *   **Mitigation Strategies**: Experiments showed that providing explicit external knowledge or adding intermediate reasoning steps significantly improved LLMs' performance in recognizing hallucinations. Conversely, contrasting hallucinated samples directly with ground-truth answers tended to confuse LLMs and degrade performance \\cite{li2023rvf}.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The automatic generation relies on the capabilities of a powerful LLM (ChatGPT) for both sampling and filtering, which might introduce biases inherent to that model. The definition of \"hallucination\" is based on unverifiable, non-factual, or irrelevant content, which might not cover all nuances of LLM errors \\cite{li2023rvf}.\n    *   **Scope of Applicability**: The benchmark covers general user queries and three specific NLP tasks (QA, dialogue, summarization). While adaptable, its direct applicability to other specialized tasks might require further instruction design and validation \\cite{li2023rvf}.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: HaluEval significantly advances the technical state-of-the-art by providing the first large-scale, multi-task benchmark specifically designed to systematically evaluate hallucination in *large* language models, moving beyond task-specific or small-model analyses \\cite{li2023rvf}.\n    *   **Potential Impact on Future Research**:\n        *   Enables deeper analysis of what types of content LLMs tend to hallucinate and to what extent \\cite{li2023rvf}.\n        *   Provides a robust evaluation tool for assessing and comparing LLMs' ability to recognize and mitigate hallucinations \\cite{li2023rvf}.\n        *   Facilitates the development of new methods and techniques for hallucination detection and reduction in LLMs, particularly by exploring the benefits of external knowledge and reasoning steps \\cite{li2023rvf}.",
        "keywords": [
          "Large Language Models (LLMs)",
          "hallucination evaluation",
          "HaluEval benchmark",
          "automated hallucination generation",
          "sampling-then-filtering framework",
          "multi-task evaluation",
          "instruction design",
          "hallucination recognition challenges",
          "Question Answering",
          "Knowledge-Grounded Dialogue",
          "Text Summarization",
          "human-annotated dataset",
          "external knowledge integration",
          "reasoning steps"
        ],
        "paper_type": "based on the abstract and introduction:\n\n1.  **\"we introduce the hallucination evaluation benchmark for large language models (halueval)\"**: this indicates the development and presentation of a new system or resource (a benchmark).\n2.  **\"we propose a two-stage framework, i.e., sampling-then-filtering\"**: this describes a new method or algorithm for generating samples.\n3.  **\"the empirical results suggest...\"** and **\"our experiments also prove...\"**: these phrases clearly indicate data-driven studies and findings, which are characteristic of empirical papers.\n\nthe paper presents a new benchmark and a new method for creating it, which are strong indicators of a **technical** paper. it then uses this newly developed benchmark and method to conduct an evaluation and present findings, which are **empirical**.\n\nwhen a paper introduces a novel system, method, or resource (like a benchmark) and then uses it to conduct an evaluation, it often falls primarily under the \"technical\" category, with the empirical evaluation serving to validate or demonstrate the technical contribution. the title \"halueval: a large-scale hallucination evaluation benchmark...\" further emphasizes the benchmark itself as a core contribution.\n\ntherefore, the primary classification is **technical**.\n\n**classification: technical**"
      },
      "file_name": "e0384ba36555232c587d4a80d527895a095a9001.pdf"
    },
    {
      "success": true,
      "doc_id": "901dcd114971cde109d58c11209848b9",
      "summary": "Here's a focused summary of the paper for a literature review, emphasizing technical innovations and empirical validation:\n\n**Analyzing and Mitigating Object Hallucination in Large Vision-Language Models** \\cite{zhou2023zu6}\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem:** Large Vision-Language Models (LVLMs) suffer from \"object hallucination,\" where they generate descriptions containing objects not present in the image.\n    *   **Importance & Challenge:** This problem negatively impacts the reliability and trustworthiness of LVLMs, leading to misleading outputs in critical applications like robotics, medical imaging, and human-computer interaction. Existing methods are often impractical for LVLMs or require extensive, costly fine-tuning.\n\n*   **Related Work & Positioning**\n    *   **Relation to Existing Approaches:**\n        *   Early works addressed hallucination in small-scale multimodal models via fine-grained alignment or data augmentation to reduce co-occurrence patterns.\n        *   Recent LVLM works focus on enhancing dataset quality for fine-tuning.\n    *   **Limitations of Previous Solutions:**\n        *   Direct utilization of small-scale model techniques is impractical due to LVLMs' auto-regressive architecture.\n        *   Acquiring high-quality datasets for fine-tuning LVLMs is time-consuming, labor-intensive, and requires human expertise.\n        *   This work proposes a *lightweight, post-hoc* method, LURE, to rectify hallucination without requiring extensive fine-tuning of the base LVLM.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method:** The paper proposes **LVLM Hallucination Revisor (LURE)**, a post-hoc algorithm designed to reconstruct less hallucinatory descriptions from potentially hallucinatory LVLM outputs.\n    *   **Novelty/Differentiation:**\n        *   **Statistical Analysis of Hallucination Causes:** LURE is grounded in a rigorous statistical analysis identifying three key factors contributing to object hallucination:\n            *   **Co-occurrence:** Spurious correlations between objects in training data.\n            *   **Uncertainty:** Objects generated with higher uncertainty during LVLM decoding.\n            *   **Object Position:** Hallucinations tend to appear more frequently in the later parts of generated text.\n        *   **Hallucination Revisor (RÎ¸):** A separate, fine-tuned LVLM acts as a revisor, trained to transform hallucinatory descriptions into accurate ones.\n        *   **Novel Data Generation for Revisor Training:** A synthetic hallucinatory dataset is created using GPT-3.5 by modifying original correct captions based on the identified hallucination factors:\n            *   Inserting additional objects likely to co-occur with existing objects.\n            *   Replacing uncertain objects (based on token entropy) or objects in later positions (based on a positional threshold) with a `[IDK]` placeholder tag, prompting the revisor to re-evaluate them.\n        *   **Post-hoc and Seamless Integration:** LURE operates as a plug-and-play module that can be seamlessly integrated with *any* existing LVLM without modifying its core architecture or requiring extensive re-training.\n\n*   **Key Technical Contributions**\n    *   **Novel Analysis:** Rigorous empirical and theoretical analysis identifying co-occurrence, uncertainty, and object position as primary causal factors for object hallucination in LVLMs.\n    *   **LURE Algorithm:** A novel, lightweight, and compatible post-hoc rectification algorithm for mitigating object hallucination.\n    *   **Synthetic Data Generation Strategy:** An innovative method for creating a hallucination-aware training dataset for the revisor using GPT-3.5, incorporating insights from the statistical analysis.\n    *   **Theoretical Insights:** Provides theoretical explanations (Theorems 2.1 and 2.2) supporting the empirical findings on co-occurrence and uncertainty, demonstrating how reducing these issues can lead to smaller test misclassification errors.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted:** LURE was evaluated on six open-source LVLMs.\n    *   **Key Performance Metrics:**\n        *   General object hallucination evaluation metrics (e.g., CHAIR \\cite{zhou2023zu6}).\n        *   GPT evaluation (likely using GPT-4 for qualitative assessment).\n        *   Human evaluation.\n    *   **Comparison Results:** LURE significantly reduced object hallucination, outperforming the previous best approach across all evaluated metrics.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations:** The method is post-hoc, meaning it corrects hallucinations *after* they are generated by the LVLM, rather than preventing them during the initial generation process. The reliance on GPT-3.5 for synthetic data generation might introduce its own biases or limitations. The theoretical analysis simplifies the autoregressive process.\n    *   **Scope of Applicability:** LURE is designed for object hallucination in text generation from LVLMs and is shown to be compatible with various existing LVLMs.\n\n*   **Technical Significance**\n    *   **Advancement of State-of-the-Art:** LURE provides an effective, lightweight, and generalizable solution to a critical problem in LVLMs, significantly advancing the state-of-the-art in mitigating object hallucination without requiring costly model re-training or extensive data collection.\n    *   **Potential Impact:** Its post-hoc nature and compatibility make it highly practical for deployment with existing LVLMs, improving their reliability in real-world applications. The detailed analysis of hallucination causes also provides valuable insights for future research into developing inherently less hallucinatory LVLMs.",
      "intriguing_abstract": "Large Vision-Language Models (LVLMs) are transforming AI, yet their pervasive \"object hallucination\"â€”generating descriptions of non-existent objectsâ€”severely compromises their reliability and trustworthiness in critical applications. This paper presents a rigorous statistical analysis, revealing that object hallucination primarily stems from spurious co-occurrence patterns, high generation uncertainty, and object position within the text. Building on these insights, we introduce **LVLM Hallucination Revisor (LURE)**, a novel, lightweight, and post-hoc algorithm designed to rectify hallucinatory outputs. LURE employs a fine-tuned revisor model, trained on an innovatively synthetically generated dataset that strategically incorporates these identified hallucination factors using GPT-3.5. This unique approach allows LURE to seamlessly integrate as a plug-and-play module with *any* existing LVLM, requiring no costly base model fine-tuning. Our extensive experiments across six open-source LVLMs demonstrate LURE's superior performance, significantly reducing object hallucination and outperforming state-of-the-art methods. LURE offers a practical, generalizable solution to enhance LVLM trustworthiness, paving the way for more dependable AI systems.",
      "keywords": [
        "Large Vision-Language Models (LVLMs)",
        "object hallucination",
        "LURE algorithm",
        "post-hoc rectification",
        "statistical analysis",
        "co-occurrence",
        "uncertainty",
        "object position",
        "synthetic data generation",
        "GPT-3.5",
        "theoretical insights",
        "reliability",
        "critical applications"
      ],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/93c525267e93c78309a5b28a3eb0780704125744.pdf",
      "citation_key": "zhou2023zu6",
      "metadata": {
        "title": "Analyzing and Mitigating Object Hallucination in Large Vision-Language Models",
        "authors": [
          "Yiyang Zhou",
          "Chenhang Cui",
          "Jaehong Yoon",
          "Linjun Zhang",
          "Zhun Deng",
          "Chelsea Finn",
          "Mohit Bansal",
          "Huaxiu Yao"
        ],
        "published_date": "2023",
        "abstract": "Large vision-language models (LVLMs) have shown remarkable abilities in understanding visual information with human languages. However, LVLMs still suffer from object hallucination, which is the problem of generating descriptions that include objects that do not actually exist in the images. This can negatively impact many vision-language tasks, such as visual summarization and reasoning. To address this issue, we propose a simple yet powerful algorithm, LVLM Hallucination Revisor (LURE), to post-hoc rectify object hallucination in LVLMs by reconstructing less hallucinatory descriptions. LURE is grounded in a rigorous statistical analysis of the key factors underlying object hallucination, including co-occurrence (the frequent appearance of certain objects alongside others in images), uncertainty (objects with higher uncertainty during LVLM decoding), and object position (hallucination often appears in the later part of the generated text). LURE can also be seamlessly integrated with any LVLMs. We evaluate LURE on six open-source LVLMs, achieving a 23% improvement in general object hallucination evaluation metrics over the previous best approach. In both GPT and human evaluations, LURE consistently ranks at the top. Our data and code are available at https://github.com/YiyangZhou/LURE.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/93c525267e93c78309a5b28a3eb0780704125744.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the paper for a literature review, emphasizing technical innovations and empirical validation:\n\n**Analyzing and Mitigating Object Hallucination in Large Vision-Language Models** \\cite{zhou2023zu6}\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem:** Large Vision-Language Models (LVLMs) suffer from \"object hallucination,\" where they generate descriptions containing objects not present in the image.\n    *   **Importance & Challenge:** This problem negatively impacts the reliability and trustworthiness of LVLMs, leading to misleading outputs in critical applications like robotics, medical imaging, and human-computer interaction. Existing methods are often impractical for LVLMs or require extensive, costly fine-tuning.\n\n*   **Related Work & Positioning**\n    *   **Relation to Existing Approaches:**\n        *   Early works addressed hallucination in small-scale multimodal models via fine-grained alignment or data augmentation to reduce co-occurrence patterns.\n        *   Recent LVLM works focus on enhancing dataset quality for fine-tuning.\n    *   **Limitations of Previous Solutions:**\n        *   Direct utilization of small-scale model techniques is impractical due to LVLMs' auto-regressive architecture.\n        *   Acquiring high-quality datasets for fine-tuning LVLMs is time-consuming, labor-intensive, and requires human expertise.\n        *   This work proposes a *lightweight, post-hoc* method, LURE, to rectify hallucination without requiring extensive fine-tuning of the base LVLM.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method:** The paper proposes **LVLM Hallucination Revisor (LURE)**, a post-hoc algorithm designed to reconstruct less hallucinatory descriptions from potentially hallucinatory LVLM outputs.\n    *   **Novelty/Differentiation:**\n        *   **Statistical Analysis of Hallucination Causes:** LURE is grounded in a rigorous statistical analysis identifying three key factors contributing to object hallucination:\n            *   **Co-occurrence:** Spurious correlations between objects in training data.\n            *   **Uncertainty:** Objects generated with higher uncertainty during LVLM decoding.\n            *   **Object Position:** Hallucinations tend to appear more frequently in the later parts of generated text.\n        *   **Hallucination Revisor (RÎ¸):** A separate, fine-tuned LVLM acts as a revisor, trained to transform hallucinatory descriptions into accurate ones.\n        *   **Novel Data Generation for Revisor Training:** A synthetic hallucinatory dataset is created using GPT-3.5 by modifying original correct captions based on the identified hallucination factors:\n            *   Inserting additional objects likely to co-occur with existing objects.\n            *   Replacing uncertain objects (based on token entropy) or objects in later positions (based on a positional threshold) with a `[IDK]` placeholder tag, prompting the revisor to re-evaluate them.\n        *   **Post-hoc and Seamless Integration:** LURE operates as a plug-and-play module that can be seamlessly integrated with *any* existing LVLM without modifying its core architecture or requiring extensive re-training.\n\n*   **Key Technical Contributions**\n    *   **Novel Analysis:** Rigorous empirical and theoretical analysis identifying co-occurrence, uncertainty, and object position as primary causal factors for object hallucination in LVLMs.\n    *   **LURE Algorithm:** A novel, lightweight, and compatible post-hoc rectification algorithm for mitigating object hallucination.\n    *   **Synthetic Data Generation Strategy:** An innovative method for creating a hallucination-aware training dataset for the revisor using GPT-3.5, incorporating insights from the statistical analysis.\n    *   **Theoretical Insights:** Provides theoretical explanations (Theorems 2.1 and 2.2) supporting the empirical findings on co-occurrence and uncertainty, demonstrating how reducing these issues can lead to smaller test misclassification errors.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted:** LURE was evaluated on six open-source LVLMs.\n    *   **Key Performance Metrics:**\n        *   General object hallucination evaluation metrics (e.g., CHAIR \\cite{zhou2023zu6}).\n        *   GPT evaluation (likely using GPT-4 for qualitative assessment).\n        *   Human evaluation.\n    *   **Comparison Results:** LURE significantly reduced object hallucination, outperforming the previous best approach across all evaluated metrics.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations:** The method is post-hoc, meaning it corrects hallucinations *after* they are generated by the LVLM, rather than preventing them during the initial generation process. The reliance on GPT-3.5 for synthetic data generation might introduce its own biases or limitations. The theoretical analysis simplifies the autoregressive process.\n    *   **Scope of Applicability:** LURE is designed for object hallucination in text generation from LVLMs and is shown to be compatible with various existing LVLMs.\n\n*   **Technical Significance**\n    *   **Advancement of State-of-the-Art:** LURE provides an effective, lightweight, and generalizable solution to a critical problem in LVLMs, significantly advancing the state-of-the-art in mitigating object hallucination without requiring costly model re-training or extensive data collection.\n    *   **Potential Impact:** Its post-hoc nature and compatibility make it highly practical for deployment with existing LVLMs, improving their reliability in real-world applications. The detailed analysis of hallucination causes also provides valuable insights for future research into developing inherently less hallucinatory LVLMs.",
        "keywords": [
          "Large Vision-Language Models (LVLMs)",
          "object hallucination",
          "LURE algorithm",
          "post-hoc rectification",
          "statistical analysis",
          "co-occurrence",
          "uncertainty",
          "object position",
          "synthetic data generation",
          "GPT-3.5",
          "theoretical insights",
          "reliability",
          "critical applications"
        ],
        "paper_type": "this paper should be classified as **technical**.\n\nhere's why:\n\n1.  **\"propose\", \"develop\", \"present\", \"algorithm\", \"method\"**: the abstract explicitly states, \"to address this issue, we propose a simple yet powerful algorithm, lvlm hallucination revisor (lure)...\" the introduction further emphasizes, \"instead, we aim to propose a lightweight method to post-hoc handle object hallucination by introducing lure.\" section 3 is titled \"lvlm hallucination revisor\" and details the proposed remedy.\n2.  **\"technical problem, proposed solution\"**: the paper clearly identifies a technical problem (\"lvlms still suffer from object hallucination\") and presents a novel technical solution (lure algorithm) to mitigate it.\n3.  **empirical evaluation**: while the paper includes extensive empirical evaluation (\"we evaluate lure on six open-source lvlms and found it outperforms the previous best approach...\"), this evaluation serves to validate the effectiveness of the *newly proposed algorithm*, which is characteristic of a technical paper.\n4.  **theoretical grounding**: the paper also includes \"rigorous statistical analysis\" and \"theoretical explanation\" (section 2 and 2.4), but this theoretical work is presented as grounding and informing the design of the lure algorithm, rather than being the sole or primary contribution of the paper.\n\nthe core contribution is the design and implementation of a new algorithm/system (lure) to solve a specific technical problem."
      },
      "file_name": "93c525267e93c78309a5b28a3eb0780704125744.pdf"
    },
    {
      "success": true,
      "doc_id": "47d0ac8fbc517f6f962a2af990ea39a2",
      "summary": "Here's a focused summary of the paper \\cite{han202439z} for a literature review:\n\n### **1. Research Problem & Motivation**\n\n*   **Specific Technical Problem:** The paper addresses multimodal hallucination in Large Vision-Language Models (LVLMs), where models generate descriptions of objects not present in the visual input.\n*   **Importance and Challenge:**\n    *   Despite impressive capabilities, LVLMs frequently suffer from these hallucinations, limiting their deployment in safety-critical applications (e.g., autonomous driving, medical diagnosis).\n    *   The underlying fundamental reasons for multimodal hallucinations remain poorly explored.\n\n### **2. Related Work & Positioning**\n\n*   **Relation to Existing Approaches:**\n    *   Existing hallucination mitigation methods are broadly categorized into retraining-based (e.g., redesigning vision encoders, finetuning with high-quality data, reinforcement learning) and post-hoc processing-based (e.g., decoding strategies, reviser models).\n    *   Previous studies have attributed hallucinations to imperfections in vision encoders or inherent model uncertainties.\n*   **Limitations of Previous Solutions:**\n    *   Many existing approaches lack an exploration of the fundamental factors causing LVLM hallucinations.\n    *   Retraining-based methods often incur significant additional computational overhead.\n    *   \\cite{han202439z} distinguishes itself by investigating hallucinations from the perspective of inherent biases within the models, offering a novel angle.\n\n### **3. Technical Approach & Innovation**\n\n*   **Core Technical Method/Algorithm:**\n    *   The paper proposes that an inherent \"semantic shift bias\" related to paragraph breaks ('\\n\\n') in training data is a key factor in LVLM hallucinations.\n    *   This bias causes models to infer that content following '\\n\\n' should be semantically different from preceding content, increasing the likelihood of hallucinatory descriptions.\n    *   To mitigate this, two orthogonal methods are proposed to prevent the generation of '\\n\\n':\n        *   **Mitigating Hallucinations during Input (MiHI):** Modifying the input prompt to encourage single-paragraph generation (e.g., adding \"in one paragraph\" to the instruction).\n        *   **Mitigating Hallucinations during Output (MiHO):** Adjusting the decoding strategy by reducing the logits (prediction probability) of the '\\n' token, effectively preventing its generation (by setting a penalty strength Î» to positive infinity).\n*   **Novelty/Difference:**\n    *   Identifies and systematically validates a previously unexplored \"semantic shift bias\" linked to paragraph breaks as a root cause of hallucination.\n    *   Introduces simple, efficient, and cost-effective post-hoc methods (prompt engineering and logit manipulation) that do not require model retraining or additional complex architectures.\n    *   Demonstrates that deliberately inserting '\\n\\n' can *induce* hallucinations, further supporting the hypothesis and providing a novel attack mechanism.\n\n### **4. Key Technical Contributions**\n\n*   **Novel Algorithms, Methods, or Techniques:**\n    *   Identification and empirical validation of the \"semantic shift bias\" in LVLMs, where '\\n\\n' tokens trigger increased hallucination rates.\n    *   A method to induce multimodal hallucinations by strategically inserting '\\n\\n' tokens.\n    *   **MiHI:** A prompt engineering technique to reduce hallucinations by encouraging single-paragraph outputs.\n    *   **MiHO:** A decoding strategy modification that suppresses the generation of '\\n' tokens by manipulating output logits.\n*   **Theoretical Insights or Analysis:**\n    *   Hypothesizes that the frequent semantic changes observed around '\\n\\n' in training data lead LVLMs to associate '\\n\\n' with a shift towards potentially less grounded or more divergent content, increasing hallucination probability.\n\n### **5. Experimental Validation**\n\n*   **Experiments Conducted:**\n    *   **Hypothesis Verification (Q1):** Compared hallucination severity in text generated *before* vs. *after* a '\\n' token.\n    *   **Attackability (Q2):** Tested if manually inserting '\\n\\n' at different positions in generated text induces more hallucinations.\n    *   **Effectiveness (Q3):** Evaluated MiHO, MiHI, and their combination against original LVLM outputs.\n    *   **Effectiveness (Q3, Equal Length):** Conducted comparisons at equal output lengths to control for sentence length effects.\n*   **Key Performance Metrics and Comparison Results:**\n    *   **Metrics:** Cs (ratio of hallucinated objects to all mentioned objects) and Ci (ratio of captions with hallucinated objects to all captions). Lower values indicate less hallucination.\n    *   **Models:** Evaluated on six publicly available LVLMs: BakLLaVA, LLaVA-v1.5-7B, LLaVA-v1.5-13B, InstructBLIP-7B, MiniGPT-v2, and Fuyu-8B.\n    *   **Q1 Results:** Content generated *after* '\\n' consistently exhibited significantly higher Cs and Ci across all models and decoding strategies (e.g., for BakLLaVA with greedy decoding, Cs increased from 18.14% before '\\n' to 53.42% after '\\n').\n    *   **Q2 Results:** Inserting '\\n\\n' generally increased hallucination metrics (Cs and Ci), with later insertions often leading to more severe hallucinations (e.g., BakLLaVA with greedy decoding, original Cs=48.56%, Attack-3 Cs=54.54%).\n    *   **Q3 Results (Overall):** MiHO and MiHO+MiHI significantly reduced hallucinations across almost all models. MiHI was also effective but showed less improvement for Fuyu-8B, possibly due to its instruction-tuning limitations.\n    *   **Q3 Results (Equal Length):** MiHO consistently showed significant improvements. MiHI and MiHO+MiHI sometimes exhibited performance decreases, suggesting that modified prompts might negatively impact description quality when forced to equal lengths.\n    *   **Decoding Strategy:** Sampling decoding was found to be more prone to hallucinations than greedy decoding, and the proposed methods performed better with greedy decoding.\n\n### **6. Limitations & Scope**\n\n*   **Technical Limitations/Assumptions:**\n    *   The '\\n\\n'-induced hallucination problem is not universal across all LVLMs (e.g., not observed in GPT-4).\n    *   The effectiveness of MiHI can depend on the LVLM's instruction-following capabilities (e.g., less effective for Fuyu-8B).\n    *   Forcing single-paragraph output via MiHI might, in some cases, negatively impact the overall quality or detail of descriptions when controlling for length.\n*   **Scope of Applicability:**\n    *   Primarily focuses on object hallucination in LVLMs.\n    *   The proposed methods are simple, post-hoc interventions applicable to LVLMs exhibiting the identified semantic shift bias, without requiring retraining.\n    *   The paper acknowledges that whether this bias persists or can be overcome with continued model scaling remains an open question.\n\n### **7. Technical Significance**\n\n*   **Advancement of State-of-the-Art:**\n    *   Provides a novel, fundamental insight into a previously underexplored cause of multimodal hallucinations in LVLMs, linking it to inherent biases from training data patterns.\n    *   Offers highly efficient and cost-effective mitigation strategies that can be applied without modifying model architectures or retraining.\n    *   Introduces a new \"attack\" method that can be used to probe and evaluate LVLM robustness against hallucination.\n*   **Potential Impact on Future Research:**\n    *   Encourages deeper investigation into inherent biases within foundation models as a source of undesirable behaviors.\n    *   Suggests new avenues for designing more robust LVLMs by considering subtle training data patterns and refining decoding strategies.\n    *   Highlights the critical role of prompt engineering and decoding-time interventions in controlling model outputs and improving reliability.\n    *   Poses important questions about the relationship between model scale, training data biases, and hallucination phenomena.",
      "intriguing_abstract": "Large Vision-Language Models (LVLMs) are plagued by multimodal hallucinations, severely hindering their use in critical applications. This paper uncovers a surprising, fundamental cause: an inherent \"semantic shift bias\" linked to paragraph breaks ('\\n\\n') in training data. We demonstrate that LVLMs interpret '\\n\\n' as a signal for semantic shift, significantly increasing the likelihood of generating descriptions of non-existent objects. Intriguingly, we show that strategically inserting '\\n\\n' can *induce* hallucinations, revealing a novel vulnerability. To combat this, we propose two orthogonal, cost-effective post-hoc methods: Mitigating Hallucinations during Input (MiHI) via prompt engineering, and Mitigating Hallucinations during Output (MiHO) through logit manipulation to suppress '\\n' generation. Our extensive experiments across six state-of-the-art LVLMs confirm that content generated after '\\n' is far more prone to hallucination, and our methods drastically reduce these fabrications without requiring expensive retraining. This work not only offers immediate, practical solutions but also fundamentally shifts our understanding of LVLM reliability, urging deeper investigation into subtle training data biases and paving the way for more robust, trustworthy AI.",
      "keywords": [
        "Multimodal hallucination",
        "Large Vision-Language Models (LVLMs)",
        "semantic shift bias",
        "paragraph breaks ('\\n\\n')",
        "prompt engineering (MiHI)",
        "logit manipulation (MiHO)",
        "decoding strategies",
        "post-hoc mitigation",
        "inducing hallucinations",
        "training data patterns",
        "safety-critical applications",
        "object hallucination"
      ],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/b3fd9f9245584ee41c0ba005cb262fd8f93ff3b5.pdf",
      "citation_key": "han202439z",
      "metadata": {
        "title": "Skip \\n: A Simple Method to Reduce Hallucination in Large Vision-Language Models",
        "authors": [
          "Zongbo Han",
          "Zechen Bai",
          "Haiyang Mei",
          "Qianli Xu",
          "Changqing Zhang",
          "Mike Zheng Shou"
        ],
        "published_date": "2024",
        "abstract": "Recent advancements in large vision-language models (LVLMs) have demonstrated impressive capability in visual information understanding with human language. Despite these advances, LVLMs still face challenges with multimodal hallucination, such as generating text descriptions of objects that are not present in the visual information. However, the underlying fundamental reasons of multimodal hallucinations remain poorly explored. In this paper, we propose a new perspective, suggesting that the inherent biases in LVLMs might be a key factor in hallucinations. Specifically, we systematically identify a semantic shift bias related to paragraph breaks (\\n\\n), where the content before and after '\\n\\n' in the training data frequently exhibit significant semantic changes. This pattern leads the model to infer that the contents following '\\n\\n' should be obviously different from the preceding contents with less hallucinatory descriptions, thereby increasing the probability of hallucinatory descriptions subsequent to the '\\n\\n'. We have validated this hypothesis on multiple publicly available LVLMs. Besides, we find that deliberately inserting '\\n\\n' at the generated description can induce more hallucinations. A simple method is proposed to effectively mitigate the hallucination of LVLMs by skipping the output of '\\n'.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/b3fd9f9245584ee41c0ba005cb262fd8f93ff3b5.pdf",
        "venue": "arXiv.org",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the paper \\cite{han202439z} for a literature review:\n\n### **1. Research Problem & Motivation**\n\n*   **Specific Technical Problem:** The paper addresses multimodal hallucination in Large Vision-Language Models (LVLMs), where models generate descriptions of objects not present in the visual input.\n*   **Importance and Challenge:**\n    *   Despite impressive capabilities, LVLMs frequently suffer from these hallucinations, limiting their deployment in safety-critical applications (e.g., autonomous driving, medical diagnosis).\n    *   The underlying fundamental reasons for multimodal hallucinations remain poorly explored.\n\n### **2. Related Work & Positioning**\n\n*   **Relation to Existing Approaches:**\n    *   Existing hallucination mitigation methods are broadly categorized into retraining-based (e.g., redesigning vision encoders, finetuning with high-quality data, reinforcement learning) and post-hoc processing-based (e.g., decoding strategies, reviser models).\n    *   Previous studies have attributed hallucinations to imperfections in vision encoders or inherent model uncertainties.\n*   **Limitations of Previous Solutions:**\n    *   Many existing approaches lack an exploration of the fundamental factors causing LVLM hallucinations.\n    *   Retraining-based methods often incur significant additional computational overhead.\n    *   \\cite{han202439z} distinguishes itself by investigating hallucinations from the perspective of inherent biases within the models, offering a novel angle.\n\n### **3. Technical Approach & Innovation**\n\n*   **Core Technical Method/Algorithm:**\n    *   The paper proposes that an inherent \"semantic shift bias\" related to paragraph breaks ('\\n\\n') in training data is a key factor in LVLM hallucinations.\n    *   This bias causes models to infer that content following '\\n\\n' should be semantically different from preceding content, increasing the likelihood of hallucinatory descriptions.\n    *   To mitigate this, two orthogonal methods are proposed to prevent the generation of '\\n\\n':\n        *   **Mitigating Hallucinations during Input (MiHI):** Modifying the input prompt to encourage single-paragraph generation (e.g., adding \"in one paragraph\" to the instruction).\n        *   **Mitigating Hallucinations during Output (MiHO):** Adjusting the decoding strategy by reducing the logits (prediction probability) of the '\\n' token, effectively preventing its generation (by setting a penalty strength Î» to positive infinity).\n*   **Novelty/Difference:**\n    *   Identifies and systematically validates a previously unexplored \"semantic shift bias\" linked to paragraph breaks as a root cause of hallucination.\n    *   Introduces simple, efficient, and cost-effective post-hoc methods (prompt engineering and logit manipulation) that do not require model retraining or additional complex architectures.\n    *   Demonstrates that deliberately inserting '\\n\\n' can *induce* hallucinations, further supporting the hypothesis and providing a novel attack mechanism.\n\n### **4. Key Technical Contributions**\n\n*   **Novel Algorithms, Methods, or Techniques:**\n    *   Identification and empirical validation of the \"semantic shift bias\" in LVLMs, where '\\n\\n' tokens trigger increased hallucination rates.\n    *   A method to induce multimodal hallucinations by strategically inserting '\\n\\n' tokens.\n    *   **MiHI:** A prompt engineering technique to reduce hallucinations by encouraging single-paragraph outputs.\n    *   **MiHO:** A decoding strategy modification that suppresses the generation of '\\n' tokens by manipulating output logits.\n*   **Theoretical Insights or Analysis:**\n    *   Hypothesizes that the frequent semantic changes observed around '\\n\\n' in training data lead LVLMs to associate '\\n\\n' with a shift towards potentially less grounded or more divergent content, increasing hallucination probability.\n\n### **5. Experimental Validation**\n\n*   **Experiments Conducted:**\n    *   **Hypothesis Verification (Q1):** Compared hallucination severity in text generated *before* vs. *after* a '\\n' token.\n    *   **Attackability (Q2):** Tested if manually inserting '\\n\\n' at different positions in generated text induces more hallucinations.\n    *   **Effectiveness (Q3):** Evaluated MiHO, MiHI, and their combination against original LVLM outputs.\n    *   **Effectiveness (Q3, Equal Length):** Conducted comparisons at equal output lengths to control for sentence length effects.\n*   **Key Performance Metrics and Comparison Results:**\n    *   **Metrics:** Cs (ratio of hallucinated objects to all mentioned objects) and Ci (ratio of captions with hallucinated objects to all captions). Lower values indicate less hallucination.\n    *   **Models:** Evaluated on six publicly available LVLMs: BakLLaVA, LLaVA-v1.5-7B, LLaVA-v1.5-13B, InstructBLIP-7B, MiniGPT-v2, and Fuyu-8B.\n    *   **Q1 Results:** Content generated *after* '\\n' consistently exhibited significantly higher Cs and Ci across all models and decoding strategies (e.g., for BakLLaVA with greedy decoding, Cs increased from 18.14% before '\\n' to 53.42% after '\\n').\n    *   **Q2 Results:** Inserting '\\n\\n' generally increased hallucination metrics (Cs and Ci), with later insertions often leading to more severe hallucinations (e.g., BakLLaVA with greedy decoding, original Cs=48.56%, Attack-3 Cs=54.54%).\n    *   **Q3 Results (Overall):** MiHO and MiHO+MiHI significantly reduced hallucinations across almost all models. MiHI was also effective but showed less improvement for Fuyu-8B, possibly due to its instruction-tuning limitations.\n    *   **Q3 Results (Equal Length):** MiHO consistently showed significant improvements. MiHI and MiHO+MiHI sometimes exhibited performance decreases, suggesting that modified prompts might negatively impact description quality when forced to equal lengths.\n    *   **Decoding Strategy:** Sampling decoding was found to be more prone to hallucinations than greedy decoding, and the proposed methods performed better with greedy decoding.\n\n### **6. Limitations & Scope**\n\n*   **Technical Limitations/Assumptions:**\n    *   The '\\n\\n'-induced hallucination problem is not universal across all LVLMs (e.g., not observed in GPT-4).\n    *   The effectiveness of MiHI can depend on the LVLM's instruction-following capabilities (e.g., less effective for Fuyu-8B).\n    *   Forcing single-paragraph output via MiHI might, in some cases, negatively impact the overall quality or detail of descriptions when controlling for length.\n*   **Scope of Applicability:**\n    *   Primarily focuses on object hallucination in LVLMs.\n    *   The proposed methods are simple, post-hoc interventions applicable to LVLMs exhibiting the identified semantic shift bias, without requiring retraining.\n    *   The paper acknowledges that whether this bias persists or can be overcome with continued model scaling remains an open question.\n\n### **7. Technical Significance**\n\n*   **Advancement of State-of-the-Art:**\n    *   Provides a novel, fundamental insight into a previously underexplored cause of multimodal hallucinations in LVLMs, linking it to inherent biases from training data patterns.\n    *   Offers highly efficient and cost-effective mitigation strategies that can be applied without modifying model architectures or retraining.\n    *   Introduces a new \"attack\" method that can be used to probe and evaluate LVLM robustness against hallucination.\n*   **Potential Impact on Future Research:**\n    *   Encourages deeper investigation into inherent biases within foundation models as a source of undesirable behaviors.\n    *   Suggests new avenues for designing more robust LVLMs by considering subtle training data patterns and refining decoding strategies.\n    *   Highlights the critical role of prompt engineering and decoding-time interventions in controlling model outputs and improving reliability.\n    *   Poses important questions about the relationship between model scale, training data biases, and hallucination phenomena.",
        "keywords": [
          "Multimodal hallucination",
          "Large Vision-Language Models (LVLMs)",
          "semantic shift bias",
          "paragraph breaks ('\\n\\n')",
          "prompt engineering (MiHI)",
          "logit manipulation (MiHO)",
          "decoding strategies",
          "post-hoc mitigation",
          "inducing hallucinations",
          "training data patterns",
          "safety-critical applications",
          "object hallucination"
        ],
        "paper_type": "based on the abstract and introduction:\n\n*   the abstract states: \"we **propose a new perspective**, suggesting that the inherent biases in lvlms might be a key factor in hallucinations. specifically, we **systematically identify a semantic shift bias**...\"\n*   the introduction further elaborates: \"a simple **method is proposed to effectively mitigate the hallucination** of lvlms by skipping the output of â€˜ \\nâ€™.\" it also mentions \"code is available.\"\n\nthese phrases directly align with the criteria for a **technical** paper, which \"presents new methods, algorithms, or systems.\" while there is empirical validation mentioned (\"we have validated this hypothesis on multiple publicly available lvlms\"), the core contribution is the identification of a new bias and the proposal of a specific method to address it.\n\n**classification: technical**"
      },
      "file_name": "b3fd9f9245584ee41c0ba005cb262fd8f93ff3b5.pdf"
    },
    {
      "success": true,
      "doc_id": "3623f81a6a189450a71ef2b122217143",
      "summary": "Here's a focused summary of the paper for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Problem:** Multi-modal Large Language Models (MLLMs) are prone to \"hallucination,\" generating inaccurate objects, attributes, and relationships that do not match visual content \\cite{wang2025jen}.\n    *   **Motivation:** This problem is critical because MLLMs are increasingly deployed in sensitive applications like medical reasoning, autonomous driving, and robotic manipulation. Hallucinations pose significant risks, potentially leading to misinformation or compromised decision-making \\cite{wang2025jen}. The underlying causes, particularly related to internal attention mechanisms and instruction-tuning vulnerabilities, are not fully understood.\n\n*   **Related Work & Positioning**\n    *   **Existing Approaches to Hallucination:** Previous research has attributed hallucinations to factors like the imbalance between weak vision models and powerful LLM backbones (leading to over-reliance on language priors) or statistical biases in pre-training datasets \\cite{wang2025jen}. Mitigation strategies include optimizing decoding, model retraining, and post-processing techniques \\cite{wang2025jen}.\n    *   **Limitations of Previous Solutions:** Existing studies often focus on isolated factors in a post-hoc manner, lacking a comprehensive understanding of the generation mechanisms \\cite{wang2025jen}. Prior adversarial attacks on MLLMs typically rely on predefined target responses, task-specific datasets, or fixed patterns (e.g., perturbation-based, text-based jailbreaks, structure-based attacks with typography), requiring substantial human effort and often lacking transferability \\cite{wang2025jen}.\n    *   **Positioning:** This work distinguishes itself by proposing a novel hallucination attack that directly exploits the attention sink phenomenon, aiming to induce erroneous content rather than harmful outputs, and achieving this dynamically without predefined patterns or sacrificing response quality \\cite{wang2025jen}.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method:** The paper proposes a novel hallucination attack that exploits the \"attention sink\" phenomenon in MLLMs \\cite{wang2025jen}. It identifies that attention sinks, characterized by columnar patterns of high attention scores, emerge at the turning point of image-text relevance in MLLM responses, aggregating misleading global information \\cite{wang2025jen}. The attack manipulates attention scores and hidden embeddings to induce these sink tokens, thereby triggering hallucinated content with minimal image-text relevance \\cite{wang2025jen}.\n    *   **Novelty:** This is the first hallucination attack specifically targeting MLLMs by manipulating *only* attention sinks \\cite{wang2025jen}. Unlike previous adversarial methods, it generates dynamic, effective, and highly transferable visual adversarial inputs without relying on predefined patterns or degrading the overall quality of model responses \\cite{wang2025jen}. The approach is grounded in an in-depth analysis of how instruction-tuning datasets contribute to MLLMs inheriting a \"two-segment response\" pattern, where the second segment often contains loosely related or visually uninterpretable content, exacerbated by attention sinks \\cite{wang2025jen}.\n\n*   **Key Technical Contributions**\n    *   **Theoretical Insight/Analysis:** A comprehensive analysis linking the attention sink phenomenon with MLLM hallucination, revealing how instruction-tuning processes lead to two-segment responses with declining image-text relevance and how misleading information aggregation contributes to hallucinated outputs \\cite{wang2025jen}.\n    *   **Novel Attack Method:** The first hallucination attack targeting MLLMs through the manipulation of attention sinks, crafting adversarial visual inputs that significantly exacerbate object, attribute, and relationship hallucinations without degrading response quality \\cite{wang2025jen}.\n    *   **Mechanism:** The attack achieves high transferability and adaptability by directly manipulating the attention mechanism and hidden embeddings, bypassing the need for predefined patterns or target behaviors \\cite{wang2025jen}.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted:** Extensive evaluations were performed on six prominent MLLMs, including black-box models and cutting-edge commercial APIs like GPT-4o and Gemini 1.5 \\cite{wang2025jen}. The evaluation of hallucination and response quality was assisted by GPT-4 \\cite{wang2025jen}.\n    *   **Key Performance Metrics & Results:** The attack demonstrated remarkable efficacy, successfully compromising black-box MLLMs and commercial APIs, even when extensive mitigating mechanisms (decoding, retraining, post-processing) were in place \\cite{wang2025jen}. The results showed up to a 10.90% increase in hallucinated sentences and a 12.74% increase in hallucinated words, highlighting the vulnerability of MLLMs \\cite{wang2025jen}.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations/Assumptions:** The paper focuses specifically on exploiting the attention sink phenomenon. While effective, it implies that the attack's mechanism is tied to this specific internal vulnerability. The analysis of instruction-tuning datasets highlights a widespread problem, but the attack's direct applicability might be limited to models exhibiting this attention sink behavior.\n    *   **Scope of Applicability:** The attack is demonstrated to be effective against a wide range of MLLMs, including open-source and commercial black-box models, and is designed to be highly transferable \\cite{wang2025jen}. It targets visual adversarial inputs to induce textual hallucinations.\n\n*   **Technical Significance**\n    *   **Advancement of State-of-the-Art:** This work significantly advances the understanding of MLLM hallucination by pinpointing the attention sink as a critical vulnerability stemming from instruction-tuning data patterns \\cite{wang2025jen}. It introduces a novel, dynamic, and highly transferable adversarial attack method that bypasses existing mitigation strategies \\cite{wang2025jen}.\n    *   **Potential Impact:** The findings expose a critical vulnerability in MLLMs, particularly for downstream applications where faithfulness and reliability are paramount \\cite{wang2025jen}. This research is expected to drive advancements toward more robust and high-performing multi-modal models by prompting the development of more effective hallucination detection and mitigation techniques that address the root causes identified \\cite{wang2025jen}.",
      "intriguing_abstract": "Multi-modal Large Language Models (MLLMs) are transforming AI, yet their pervasive \"hallucination\"â€”generating visually inaccurate objects, attributes, and relationshipsâ€”threatens their deployment in sensitive domains like medical reasoning and autonomous driving. This paper uncovers a critical, previously unaddressed vulnerability: the \"attention sink\" phenomenon. We reveal how instruction-tuning inadvertently fosters a \"two-segment response\" pattern, where these attention sinks emerge at the turning point of image-text relevance, aggregating misleading global information that directly triggers hallucinations.\n\nWe introduce the first adversarial attack specifically targeting MLLM hallucination by manipulating attention sinks. Our novel method dynamically crafts visual adversarial inputs that exploit attention scores and hidden embeddings to induce sink tokens, dramatically increasing hallucinated content (up to 12.74% more words) without degrading overall response quality. Tested extensively on six prominent MLLMs, including black-box commercial APIs like GPT-4o and Gemini 1.5, our attack demonstrates remarkable efficacy and transferability, bypassing existing mitigation strategies. This work offers profound theoretical insights into MLLM generation mechanisms and exposes a fundamental weakness, compelling the development of more robust and faithful multi-modal AI systems.",
      "keywords": [
        "Multi-modal Large Language Models (MLLMs)",
        "MLLM hallucination",
        "attention sink phenomenon",
        "instruction-tuning vulnerabilities",
        "novel hallucination attack",
        "visual adversarial inputs",
        "manipulating attention sinks",
        "high transferability",
        "black-box MLLMs",
        "commercial MLLM APIs",
        "two-segment response pattern",
        "critical MLLM vulnerability",
        "MLLM robustness"
      ],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/c7714dc70eb508a0b1859b7b1a5af552439b973f.pdf",
      "citation_key": "wang2025jen",
      "metadata": {
        "title": "Mirage in the Eyes: Hallucination Attack on Multi-modal Large Language Models with Only Attention Sink",
        "authors": [
          "Yining Wang",
          "Mi Zhang",
          "Junjie Sun",
          "Chenyue Wang",
          "Min Yang",
          "Hui Xue",
          "Jialing Tao",
          "Ranjie Duan",
          "Jiexi Liu"
        ],
        "published_date": "2025",
        "abstract": "Fusing visual understanding into language generation, Multi-modal Large Language Models (MLLMs) are revolutionizing visual-language applications. Yet, these models are often plagued by the hallucination problem, which involves generating inaccurate objects, attributes, and relationships that do not match the visual content. In this work, we delve into the internal attention mechanisms of MLLMs to reveal the underlying causes of hallucination, exposing the inherent vulnerabilities in the instruction-tuning process. We propose a novel hallucination attack against MLLMs that exploits attention sink behaviors to trigger hallucinated content with minimal image-text relevance, posing a significant threat to critical downstream applications. Distinguished from previous adversarial methods that rely on fixed patterns, our approach generates dynamic, effective, and highly transferable visual adversarial inputs, without sacrificing the quality of model responses. Comprehensive experiments on 6 prominent MLLMs demonstrate the efficacy of our attack in compromising black-box MLLMs even with extensive mitigating mechanisms, as well as the promising results against cutting-edge commercial APIs, such as GPT-4o and Gemini 1.5. Our code is available at https://huggingface.co/RachelHGF/Mirage-in-the-Eyes.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/c7714dc70eb508a0b1859b7b1a5af552439b973f.pdf",
        "venue": "arXiv.org",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the paper for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Problem:** Multi-modal Large Language Models (MLLMs) are prone to \"hallucination,\" generating inaccurate objects, attributes, and relationships that do not match visual content \\cite{wang2025jen}.\n    *   **Motivation:** This problem is critical because MLLMs are increasingly deployed in sensitive applications like medical reasoning, autonomous driving, and robotic manipulation. Hallucinations pose significant risks, potentially leading to misinformation or compromised decision-making \\cite{wang2025jen}. The underlying causes, particularly related to internal attention mechanisms and instruction-tuning vulnerabilities, are not fully understood.\n\n*   **Related Work & Positioning**\n    *   **Existing Approaches to Hallucination:** Previous research has attributed hallucinations to factors like the imbalance between weak vision models and powerful LLM backbones (leading to over-reliance on language priors) or statistical biases in pre-training datasets \\cite{wang2025jen}. Mitigation strategies include optimizing decoding, model retraining, and post-processing techniques \\cite{wang2025jen}.\n    *   **Limitations of Previous Solutions:** Existing studies often focus on isolated factors in a post-hoc manner, lacking a comprehensive understanding of the generation mechanisms \\cite{wang2025jen}. Prior adversarial attacks on MLLMs typically rely on predefined target responses, task-specific datasets, or fixed patterns (e.g., perturbation-based, text-based jailbreaks, structure-based attacks with typography), requiring substantial human effort and often lacking transferability \\cite{wang2025jen}.\n    *   **Positioning:** This work distinguishes itself by proposing a novel hallucination attack that directly exploits the attention sink phenomenon, aiming to induce erroneous content rather than harmful outputs, and achieving this dynamically without predefined patterns or sacrificing response quality \\cite{wang2025jen}.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method:** The paper proposes a novel hallucination attack that exploits the \"attention sink\" phenomenon in MLLMs \\cite{wang2025jen}. It identifies that attention sinks, characterized by columnar patterns of high attention scores, emerge at the turning point of image-text relevance in MLLM responses, aggregating misleading global information \\cite{wang2025jen}. The attack manipulates attention scores and hidden embeddings to induce these sink tokens, thereby triggering hallucinated content with minimal image-text relevance \\cite{wang2025jen}.\n    *   **Novelty:** This is the first hallucination attack specifically targeting MLLMs by manipulating *only* attention sinks \\cite{wang2025jen}. Unlike previous adversarial methods, it generates dynamic, effective, and highly transferable visual adversarial inputs without relying on predefined patterns or degrading the overall quality of model responses \\cite{wang2025jen}. The approach is grounded in an in-depth analysis of how instruction-tuning datasets contribute to MLLMs inheriting a \"two-segment response\" pattern, where the second segment often contains loosely related or visually uninterpretable content, exacerbated by attention sinks \\cite{wang2025jen}.\n\n*   **Key Technical Contributions**\n    *   **Theoretical Insight/Analysis:** A comprehensive analysis linking the attention sink phenomenon with MLLM hallucination, revealing how instruction-tuning processes lead to two-segment responses with declining image-text relevance and how misleading information aggregation contributes to hallucinated outputs \\cite{wang2025jen}.\n    *   **Novel Attack Method:** The first hallucination attack targeting MLLMs through the manipulation of attention sinks, crafting adversarial visual inputs that significantly exacerbate object, attribute, and relationship hallucinations without degrading response quality \\cite{wang2025jen}.\n    *   **Mechanism:** The attack achieves high transferability and adaptability by directly manipulating the attention mechanism and hidden embeddings, bypassing the need for predefined patterns or target behaviors \\cite{wang2025jen}.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted:** Extensive evaluations were performed on six prominent MLLMs, including black-box models and cutting-edge commercial APIs like GPT-4o and Gemini 1.5 \\cite{wang2025jen}. The evaluation of hallucination and response quality was assisted by GPT-4 \\cite{wang2025jen}.\n    *   **Key Performance Metrics & Results:** The attack demonstrated remarkable efficacy, successfully compromising black-box MLLMs and commercial APIs, even when extensive mitigating mechanisms (decoding, retraining, post-processing) were in place \\cite{wang2025jen}. The results showed up to a 10.90% increase in hallucinated sentences and a 12.74% increase in hallucinated words, highlighting the vulnerability of MLLMs \\cite{wang2025jen}.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations/Assumptions:** The paper focuses specifically on exploiting the attention sink phenomenon. While effective, it implies that the attack's mechanism is tied to this specific internal vulnerability. The analysis of instruction-tuning datasets highlights a widespread problem, but the attack's direct applicability might be limited to models exhibiting this attention sink behavior.\n    *   **Scope of Applicability:** The attack is demonstrated to be effective against a wide range of MLLMs, including open-source and commercial black-box models, and is designed to be highly transferable \\cite{wang2025jen}. It targets visual adversarial inputs to induce textual hallucinations.\n\n*   **Technical Significance**\n    *   **Advancement of State-of-the-Art:** This work significantly advances the understanding of MLLM hallucination by pinpointing the attention sink as a critical vulnerability stemming from instruction-tuning data patterns \\cite{wang2025jen}. It introduces a novel, dynamic, and highly transferable adversarial attack method that bypasses existing mitigation strategies \\cite{wang2025jen}.\n    *   **Potential Impact:** The findings expose a critical vulnerability in MLLMs, particularly for downstream applications where faithfulness and reliability are paramount \\cite{wang2025jen}. This research is expected to drive advancements toward more robust and high-performing multi-modal models by prompting the development of more effective hallucination detection and mitigation techniques that address the root causes identified \\cite{wang2025jen}.",
        "keywords": [
          "Multi-modal Large Language Models (MLLMs)",
          "MLLM hallucination",
          "attention sink phenomenon",
          "instruction-tuning vulnerabilities",
          "novel hallucination attack",
          "visual adversarial inputs",
          "manipulating attention sinks",
          "high transferability",
          "black-box MLLMs",
          "commercial MLLM APIs",
          "two-segment response pattern",
          "critical MLLM vulnerability",
          "MLLM robustness"
        ],
        "paper_type": "based on the provided abstract and introduction, this paper is best classified as **technical**.\n\nhere's why:\n\n*   **abstract mentions:** \"we propose a novel hallucination attack against mllms that exploits attention sink behaviors to trigger hallucinated content...\" and \"our approach generates dynamic, effective, and highly transferable visual adversarial inputs...\" these phrases directly indicate the development and presentation of a new method or system (an attack/approach).\n*   **introduction discusses:** the introduction sets up a technical problem (the hallucination problem in mllms) that the proposed solution (the attack) aims to address or investigate.\n*   while the abstract also mentions \"comprehensive experiments on 6 prominent mllms demonstrate the efficacy of our attack,\" which points to empirical work, the *primary contribution* described is the **proposal of a novel attack and approach**. the experiments serve to validate this new technical contribution. many technical papers include empirical evaluation of their proposed methods."
      },
      "file_name": "c7714dc70eb508a0b1859b7b1a5af552439b973f.pdf"
    },
    {
      "success": true,
      "doc_id": "78406c77da3c7e52d7320a624e5a8d5d",
      "summary": "Here's a focused summary of the paper \"Look, Compare, Decide: Alleviating Hallucination in Large Vision-Language Models via Multi-View Multi-Path Reasoning\" \\cite{qu2024pqc} for a literature review:\n\n---\n\n### **1. Research Problem & Motivation**\n\n*   **Specific Technical Problem**: Large Vision-Language Models (LVLMs) frequently suffer from \"hallucination,\" where they generate text that is semantically plausible but factually inconsistent or inaccurate with the visual content of an image. This includes misidentifying objects, miscounting quantities, or describing non-existent elements.\n*   **Importance and Challenge**: Hallucinations undermine the trustworthiness and reliability of LVLMs, which are increasingly deployed in critical applications like healthcare, autonomous systems, and robotics. Addressing this issue is crucial to prevent erroneous decisions and ensure the safe and effective use of these models. The challenge lies in mitigating hallucinations without incurring significant computational costs or requiring extensive retraining.\n\n### **2. Related Work & Positioning**\n\n*   **Existing Approaches**:\n    *   **Retraining LVLMs**: Most prior work focuses on supervised fine-tuning (SFT) or Reinforcement Learning from Human Feedback (RLHF) using custom hallucination-related datasets.\n    *   **Training-free Paradigms**: Some recent methods, like Woodpecker and MARINE, explore training-free approaches.\n*   **Limitations of Previous Solutions**:\n    *   **Retraining**: Inherently comes with substantial computational costs, requires a large number of high-quality training examples, and is time-consuming and labor-intensive.\n    *   **Training-free (prior art)**: Often heavily rely on external, complicated tools (e.g., Grounding DINO, BLIP-2-FlanT5X), introducing additional dependencies and complexity.\n*   **Positioning of this Work**: \\cite{qu2024pqc} proposes a novel *training-free* framework, MVP, that *maximizes the innate capabilities of existing LVLMs* without introducing additional training costs or relying on external tools. It addresses hallucination by tackling two root causes: incomplete comprehension of image content and low certainty during answer token decoding.\n\n### **3. Technical Approach & Innovation**\n\n*   **Core Technical Method**: The paper proposes MVP (Multi-View Multi-Path Reasoning), a training-free framework that enhances LVLM inference by:\n    1.  **Multi-View Information Seeking**: Devising a strategy to thoroughly perceive comprehensive image information from varying dimensions (\"top-down,\" \"regular,\" and \"bottom-up\" views). This information is generated by the LVLMs themselves using dedicated prompts, enriching the global visual context captured by the original vision encoder.\n    2.  **Multi-Path Certainty-Driven Reasoning**: During answer decoding, it observes that hallucinations correlate with low certainty of answer tokens. It quantifies the \"Certainty Score\" (difference between top-two token probabilities) for potential answers. It then generates multiple decoding paths (e.g., by considering top-K candidate first tokens) and aggregates the certainty scores for each potential answer across these paths and across different information views. The answer with the highest aggregated certainty score is selected as the final output.\n*   **Novelty/Differentiation**:\n    *   **Training-free and Tool-free**: Unlike retraining methods or other training-free approaches that use external tools, MVP leverages only the inherent capabilities of the target LVLM.\n    *   **Comprehensive Image Understanding**: Introduces a multi-view strategy to extract richer visual information directly from the LVLM, going beyond the single global representation.\n    *   **Certainty-Driven Decoding**: Explicitly quantifies and utilizes the certainty of answer tokens during decoding, guiding the model to more reliable outputs by exploring multiple reasoning paths.\n\n### **4. Key Technical Contributions**\n\n*   **Novel Algorithms/Methods**:\n    *   A multi-view information-seeking strategy that uses LVLM-generated captions from \"bottom-up,\" \"regular,\" and \"top-down\" perspectives to enrich image understanding without external tools.\n    *   A multi-path certainty-driven reasoning mechanism that quantifies answer certainty based on token probability differences and aggregates these scores across multiple decoding paths and information views.\n*   **System Design/Architectural Innovations**: MVP is a plug-and-play framework that can be integrated with existing LVLMs and potentially other decoding methods, enhancing their performance without modifying their internal architecture or requiring retraining.\n*   **Theoretical Insights/Analysis**: Identifies and leverages the strong correlation between the occurrence of hallucinations and the low certainty of answer tokens during decoding, providing a principled way to mitigate them.\n\n### **5. Experimental Validation**\n\n*   **Experiments Conducted**: Extensive experiments were conducted on four state-of-the-art LVLMs (LLaVA-1.5, Qwen-VL, InstructBLIP, and mPLUG-Owl2, all with 7B LLM backbones).\n*   **Benchmarks**: Evaluated on two widely-used hallucination benchmarks:\n    *   **POPE (Polling-based Object Probing Evaluation)**: Assesses object presence/absence across random, popular, and adversarial settings, using data from MSCOCO, A-OKVQA, and GQA.\n    *   **MME (Multi-modal Model Evaluation)**: A comprehensive benchmark covering 10 perception-related and 4 cognition-focused subtasks, with specific focus on existence, count, position, and color subsets for object/attribute-level hallucination.\n*   **Key Performance Metrics and Comparison Results**:\n    *   **POPE**: MVP significantly outperformed vanilla LVLMs and recent training-free baselines (VCD, OPERA) across all models and settings. For LLaVA-1.5, MVP achieved an average improvement of 15.9 in Accuracy and 21.84 in F1 score across random, popular, and adversarial settings on MSCOCO. Improvements were primarily driven by increased recall (LLaVA-1.5, Qwen-VL, InstructBLIP) or a balanced improvement in both precision and recall (mPLUG-Owl2).\n    *   **MME**: (Details not fully provided in the excerpt, but the abstract states \"extensive experiments verify that our proposed MVP significantly mitigates the hallucination problem across four well-known LVLMs.\")\n    *   **Visual Richness**: Statistical analysis showed that the multi-view information-seeking strategy significantly increased the number of objects recognized in captions (e.g., LLaVA-1.5 recognized 36.66 objects/image with multi-view vs. 16.43 with regular captioning).\n\n### **6. Limitations & Scope**\n\n*   **Technical Limitations/Assumptions**:\n    *   The effectiveness relies on the LVLM's ability to generate meaningful multi-view captions and its internal probability distributions for certainty scoring.\n    *   The \"training-free\" nature means it doesn't fundamentally alter the LVLM's learned representations but rather optimizes its inference process.\n    *   Hyperparameters like `K` (number of paths) and `alpha_i` (view importance) need to be tuned.\n*   **Scope of Applicability**: MVP is designed for mitigating hallucinations in existing LVLMs during inference. It is applicable to various LVLMs and can be combined with other decoding methods. Its primary focus is on factual consistency with image content, particularly for object presence, count, and attributes.\n\n### **7. Technical Significance**\n\n*   **Advancement of State-of-the-Art**: \\cite{qu2024pqc} introduces a highly effective and efficient training-free method for hallucination mitigation in LVLMs, outperforming existing training-free baselines. It demonstrates that significant improvements can be achieved by intelligently leveraging the innate capabilities of pre-trained models without costly retraining or external tools.\n*   **Potential Impact on Future Research**:\n    *   Encourages further exploration of inference-time optimization strategies for LVLMs, potentially reducing the reliance on expensive fine-tuning.\n    *   Highlights the importance of comprehensive visual grounding and robust decoding mechanisms in multi-modal models.\n    *   The plug-and-play nature of MVP makes it a valuable component for enhancing the reliability of LVLMs in real-world applications, fostering greater trust in AI-generated content.\n\n---",
      "intriguing_abstract": "Large Vision-Language Models (LVLMs) are increasingly deployed in critical applications, yet their pervasive \"hallucination\"â€”generating factually inconsistent or inaccurate contentâ€”severely undermines trustworthiness. We introduce MVP (Multi-View Multi-Path Reasoning), a novel *training-free* and *tool-free* framework designed to fundamentally alleviate this challenge by maximizing the innate capabilities of existing LVLMs. MVP tackles hallucination through two core innovations: a multi-view information-seeking strategy where the LVLM itself generates comprehensive visual contexts from \"top-down,\" \"regular,\" and \"bottom-up\" perspectives, enriching global image understanding without external dependencies. Second, we propose a multi-path certainty-driven reasoning mechanism that quantifies the \"Certainty Score\" of potential answer tokens based on probability differences. By exploring multiple decoding paths and aggregating certainty scores across diverse views, MVP guides the model towards more reliable, factually consistent outputs. Extensive experiments across LLaVA-1.5, Qwen-VL, InstructBLIP, and mPLUG-Owl2 demonstrate that MVP significantly mitigates hallucination on POPE and MME benchmarks, outperforming existing training-free baselines. Our plug-and-play framework offers an efficient inference-time optimization, paving the way for more trustworthy and robust LVLM applications.",
      "keywords": [
        "Large Vision-Language Models (LVLMs)",
        "hallucination mitigation",
        "Multi-View Multi-Path Reasoning (MVP)",
        "training-free framework",
        "Multi-View Information Seeking",
        "Multi-Path Certainty-Driven Reasoning",
        "Certainty Score",
        "inference-time optimization",
        "comprehensive image understanding",
        "factual inconsistency",
        "plug-and-play framework",
        "POPE benchmark",
        "MME benchmark",
        "low certainty of answer tokens"
      ],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/e5f7e3d55790f2031ecb0c24e6e53c21c7013bb0.pdf",
      "citation_key": "qu2024pqc",
      "metadata": {
        "title": "Look, Compare, Decide: Alleviating Hallucination in Large Vision-Language Models via Multi-View Multi-Path Reasoning",
        "authors": [
          "Xiaoye Qu",
          "Jiashuo Sun",
          "Wei Wei",
          "Yu Cheng"
        ],
        "published_date": "2024",
        "abstract": "Recently, Large Vision-Language Models (LVLMs) have demonstrated impressive capabilities in multi-modal context comprehension. However, they still suffer from hallucination problems referring to generating inconsistent outputs with the image content. To mitigate hallucinations, previous studies mainly focus on retraining LVLMs with custom datasets. Although effective, they inherently come with additional computational costs. In this paper, we propose a training-free framework, \\textbf{MVP}, that aims to reduce hallucinations by making the most of the innate capabilities of the LVLMs via \\textbf{M}ulti-\\textbf{V}iew Multi-\\textbf{P}ath Reasoning. Specifically, we first devise a multi-view information-seeking strategy to thoroughly perceive the comprehensive information in the image, which enriches the general global information captured by the original vision encoder in LVLMs. Furthermore, during the answer decoding, we observe that the occurrence of hallucinations has a strong correlation with the certainty of the answer tokens. Thus, we propose multi-path reasoning for each information view to quantify and aggregate the certainty scores for each potential answer among multiple decoding paths and finally decide the output answer. By fully grasping the information in the image and carefully considering the certainty of the potential answers when decoding, our MVP can effectively reduce hallucinations in LVLMs.The extensive experiments verify that our proposed MVP significantly mitigates the hallucination problem across four well-known LVLMs. The source code is available at: \\url{https://github.com/GasolSun36/MVP}.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/e5f7e3d55790f2031ecb0c24e6e53c21c7013bb0.pdf",
        "venue": "International Conference on Computational Linguistics",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the paper \"Look, Compare, Decide: Alleviating Hallucination in Large Vision-Language Models via Multi-View Multi-Path Reasoning\" \\cite{qu2024pqc} for a literature review:\n\n---\n\n### **1. Research Problem & Motivation**\n\n*   **Specific Technical Problem**: Large Vision-Language Models (LVLMs) frequently suffer from \"hallucination,\" where they generate text that is semantically plausible but factually inconsistent or inaccurate with the visual content of an image. This includes misidentifying objects, miscounting quantities, or describing non-existent elements.\n*   **Importance and Challenge**: Hallucinations undermine the trustworthiness and reliability of LVLMs, which are increasingly deployed in critical applications like healthcare, autonomous systems, and robotics. Addressing this issue is crucial to prevent erroneous decisions and ensure the safe and effective use of these models. The challenge lies in mitigating hallucinations without incurring significant computational costs or requiring extensive retraining.\n\n### **2. Related Work & Positioning**\n\n*   **Existing Approaches**:\n    *   **Retraining LVLMs**: Most prior work focuses on supervised fine-tuning (SFT) or Reinforcement Learning from Human Feedback (RLHF) using custom hallucination-related datasets.\n    *   **Training-free Paradigms**: Some recent methods, like Woodpecker and MARINE, explore training-free approaches.\n*   **Limitations of Previous Solutions**:\n    *   **Retraining**: Inherently comes with substantial computational costs, requires a large number of high-quality training examples, and is time-consuming and labor-intensive.\n    *   **Training-free (prior art)**: Often heavily rely on external, complicated tools (e.g., Grounding DINO, BLIP-2-FlanT5X), introducing additional dependencies and complexity.\n*   **Positioning of this Work**: \\cite{qu2024pqc} proposes a novel *training-free* framework, MVP, that *maximizes the innate capabilities of existing LVLMs* without introducing additional training costs or relying on external tools. It addresses hallucination by tackling two root causes: incomplete comprehension of image content and low certainty during answer token decoding.\n\n### **3. Technical Approach & Innovation**\n\n*   **Core Technical Method**: The paper proposes MVP (Multi-View Multi-Path Reasoning), a training-free framework that enhances LVLM inference by:\n    1.  **Multi-View Information Seeking**: Devising a strategy to thoroughly perceive comprehensive image information from varying dimensions (\"top-down,\" \"regular,\" and \"bottom-up\" views). This information is generated by the LVLMs themselves using dedicated prompts, enriching the global visual context captured by the original vision encoder.\n    2.  **Multi-Path Certainty-Driven Reasoning**: During answer decoding, it observes that hallucinations correlate with low certainty of answer tokens. It quantifies the \"Certainty Score\" (difference between top-two token probabilities) for potential answers. It then generates multiple decoding paths (e.g., by considering top-K candidate first tokens) and aggregates the certainty scores for each potential answer across these paths and across different information views. The answer with the highest aggregated certainty score is selected as the final output.\n*   **Novelty/Differentiation**:\n    *   **Training-free and Tool-free**: Unlike retraining methods or other training-free approaches that use external tools, MVP leverages only the inherent capabilities of the target LVLM.\n    *   **Comprehensive Image Understanding**: Introduces a multi-view strategy to extract richer visual information directly from the LVLM, going beyond the single global representation.\n    *   **Certainty-Driven Decoding**: Explicitly quantifies and utilizes the certainty of answer tokens during decoding, guiding the model to more reliable outputs by exploring multiple reasoning paths.\n\n### **4. Key Technical Contributions**\n\n*   **Novel Algorithms/Methods**:\n    *   A multi-view information-seeking strategy that uses LVLM-generated captions from \"bottom-up,\" \"regular,\" and \"top-down\" perspectives to enrich image understanding without external tools.\n    *   A multi-path certainty-driven reasoning mechanism that quantifies answer certainty based on token probability differences and aggregates these scores across multiple decoding paths and information views.\n*   **System Design/Architectural Innovations**: MVP is a plug-and-play framework that can be integrated with existing LVLMs and potentially other decoding methods, enhancing their performance without modifying their internal architecture or requiring retraining.\n*   **Theoretical Insights/Analysis**: Identifies and leverages the strong correlation between the occurrence of hallucinations and the low certainty of answer tokens during decoding, providing a principled way to mitigate them.\n\n### **5. Experimental Validation**\n\n*   **Experiments Conducted**: Extensive experiments were conducted on four state-of-the-art LVLMs (LLaVA-1.5, Qwen-VL, InstructBLIP, and mPLUG-Owl2, all with 7B LLM backbones).\n*   **Benchmarks**: Evaluated on two widely-used hallucination benchmarks:\n    *   **POPE (Polling-based Object Probing Evaluation)**: Assesses object presence/absence across random, popular, and adversarial settings, using data from MSCOCO, A-OKVQA, and GQA.\n    *   **MME (Multi-modal Model Evaluation)**: A comprehensive benchmark covering 10 perception-related and 4 cognition-focused subtasks, with specific focus on existence, count, position, and color subsets for object/attribute-level hallucination.\n*   **Key Performance Metrics and Comparison Results**:\n    *   **POPE**: MVP significantly outperformed vanilla LVLMs and recent training-free baselines (VCD, OPERA) across all models and settings. For LLaVA-1.5, MVP achieved an average improvement of 15.9 in Accuracy and 21.84 in F1 score across random, popular, and adversarial settings on MSCOCO. Improvements were primarily driven by increased recall (LLaVA-1.5, Qwen-VL, InstructBLIP) or a balanced improvement in both precision and recall (mPLUG-Owl2).\n    *   **MME**: (Details not fully provided in the excerpt, but the abstract states \"extensive experiments verify that our proposed MVP significantly mitigates the hallucination problem across four well-known LVLMs.\")\n    *   **Visual Richness**: Statistical analysis showed that the multi-view information-seeking strategy significantly increased the number of objects recognized in captions (e.g., LLaVA-1.5 recognized 36.66 objects/image with multi-view vs. 16.43 with regular captioning).\n\n### **6. Limitations & Scope**\n\n*   **Technical Limitations/Assumptions**:\n    *   The effectiveness relies on the LVLM's ability to generate meaningful multi-view captions and its internal probability distributions for certainty scoring.\n    *   The \"training-free\" nature means it doesn't fundamentally alter the LVLM's learned representations but rather optimizes its inference process.\n    *   Hyperparameters like `K` (number of paths) and `alpha_i` (view importance) need to be tuned.\n*   **Scope of Applicability**: MVP is designed for mitigating hallucinations in existing LVLMs during inference. It is applicable to various LVLMs and can be combined with other decoding methods. Its primary focus is on factual consistency with image content, particularly for object presence, count, and attributes.\n\n### **7. Technical Significance**\n\n*   **Advancement of State-of-the-Art**: \\cite{qu2024pqc} introduces a highly effective and efficient training-free method for hallucination mitigation in LVLMs, outperforming existing training-free baselines. It demonstrates that significant improvements can be achieved by intelligently leveraging the innate capabilities of pre-trained models without costly retraining or external tools.\n*   **Potential Impact on Future Research**:\n    *   Encourages further exploration of inference-time optimization strategies for LVLMs, potentially reducing the reliance on expensive fine-tuning.\n    *   Highlights the importance of comprehensive visual grounding and robust decoding mechanisms in multi-modal models.\n    *   The plug-and-play nature of MVP makes it a valuable component for enhancing the reliability of LVLMs in real-world applications, fostering greater trust in AI-generated content.\n\n---",
        "keywords": [
          "Large Vision-Language Models (LVLMs)",
          "hallucination mitigation",
          "Multi-View Multi-Path Reasoning (MVP)",
          "training-free framework",
          "Multi-View Information Seeking",
          "Multi-Path Certainty-Driven Reasoning",
          "Certainty Score",
          "inference-time optimization",
          "comprehensive image understanding",
          "factual inconsistency",
          "plug-and-play framework",
          "POPE benchmark",
          "MME benchmark",
          "low certainty of answer tokens"
        ],
        "paper_type": "based on the abstract and introduction:\n\n*   the abstract explicitly states: \"we **propose** a training-free framework, **mvp**,\" and details its components: \"we first **devise a multi-view information-seeking strategy**\" and \"we **propose multi-path reasoning**.\"\n*   it describes a technical problem (hallucination in lvlms) and presents a novel solution (mvp framework).\n*   the introduction further elaborates on the problem and the need for a solution.\n*   the mention of \"extensive experiments verify that our proposed mvp significantly mitigates the hallucination problem\" indicates empirical validation, but the core contribution is the *development of the new method/framework*.\n\nthis aligns perfectly with the criteria for a **technical** paper.\n\n**classification: technical**"
      },
      "file_name": "e5f7e3d55790f2031ecb0c24e6e53c21c7013bb0.pdf"
    },
    {
      "success": true,
      "doc_id": "6d01fed54b72f9cbb09e282715d21842",
      "summary": "Here's a focused summary of the paper for a literature review, adhering to your requirements:\n\n---\n\n### Analysis of \"Plausible May Not Be Faithful: Probing Object Hallucination in Vision-Language Pre-training\" \\cite{dai20229aa}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Large-scale Vision-Language Pre-trained (VLP) models frequently hallucinate non-existent visual objects when generating text based on visual information (e.g., image captioning).\n    *   **Importance and Challenge**: This \"object hallucination\" significantly limits VLP model performance and raises serious safety concerns for industrial applications (e.g., reducing diagnostic accuracy in biomedical image captioning). Despite its criticality, this problem has not been systematically studied in contemporary VLP research \\cite{dai20229aa}.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: Previous work on hallucination in deep learning includes data refinement, scene graph learning, localized region reconstruction, uncertainty-aware beam search, knowledge augmentation, and post-processing for various tasks (e.g., data-to-text, dialog systems, QA).\n    *   **Limitations of Previous Solutions**: While general hallucination has been addressed, how different VLP strategies (e.g., pre-training objectives, image encoding methods) influence the faithfulness of generated text given images has not been systematically studied \\cite{dai20229aa}. This paper is positioned as the *first* to systematically investigate object hallucination in state-of-the-art VLP models.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper systematically studies object hallucination from three key aspects:\n        1.  **Examination of SOTA VLP Models**: Quantifying hallucination rates and observing that models achieving better standard metrics (e.g., CIDEr) can be *more* unfaithful.\n        2.  **Investigation of Image Encoding**: Ablating the influence of region-based, grid-based, and patch-based image features on hallucination.\n        3.  **Decoupling VLP Objectives**: Analyzing the effects of Image-Text Contrastive (ITC), Image-Text Matching (ITM), and Image-Conditioned Language Modeling (ICLM) losses.\n    *   **Novelty/Difference**: Based on these insights, the paper proposes a novel VLP loss called **ObjMLM (Object-Masked Language Modeling)**. This objective aims to mitigate object hallucination by enhancing the alignment and restriction between text tokens and visual objects during generation, a direct approach to improving faithfulness \\cite{dai20229aa}.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**: Introduction of **ObjMLM**, a simple yet effective pre-training objective designed to reduce object hallucination by improving token-level image-text alignment and controlled generation.\n    *   **Theoretical Insights/Analysis**:\n        *   First systematic study demonstrating that state-of-the-art VLP models still frequently hallucinate, and that optimizing for standard metrics like CIDEr (e.g., via SCST) can paradoxically *increase* hallucination \\cite{dai20229aa}.\n        *   Empirical finding that patch-based image features perform best in reducing hallucination, with smaller patch resolutions yielding significant reductions.\n        *   Analysis showing that token-level image-text alignment (ICLM) is crucial for reducing hallucination, while global alignment objectives like ITC and ITM contribute less.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**:\n        *   Evaluation of object hallucination in various SOTA VLP models (OSCAR, VinVL, BLIP, OFA) on image captioning.\n        *   Ablation studies on different image encoding methods (region, grid, patch) and VLP objectives (ITC, ITM, ICLM).\n        *   Evaluation of the proposed ObjMLM loss.\n    *   **Key Performance Metrics**: CHAIR_i (instance-level hallucination) and CHAIR_s (sentence-level hallucination) were used, alongside standard captioning metrics (BLEU-4, CIDEr, METEOR, SPICE).\n    *   **Comparison Results**:\n        *   SOTA models exhibit frequent hallucination (e.g., ~10% of sentences contain hallucinated objects in-domain, much higher out-of-domain).\n        *   SCST optimization, while boosting standard metrics, increased CHAIR_s by up to 0.9 on COCO Caption and 10.9% on NoCaps for VinVL Base \\cite{dai20229aa}.\n        *   Patch-based features outperformed region- and grid-based features, with smaller patch resolutions reducing hallucination.\n        *   ObjMLM successfully reduced object hallucination by up to **17.4%** on two benchmarks (COCO Caption for in-domain and NoCaps for out-of-domain evaluation) without introducing extra training data \\cite{dai20229aa}.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The paper primarily focuses on *object* hallucination in VLP models for text generation. While ObjMLM is effective, the paper does not extensively discuss potential trade-offs with other aspects of generation (e.g., fluency, diversity) or its applicability to other types of hallucination (e.g., factual inaccuracies not related to objects). The CHAIR metric relies on pre-defined object categories, which might not capture all nuances of hallucination.\n    *   **Scope of Applicability**: The findings and proposed method are directly applicable to VLP models used for image-conditioned text generation tasks, particularly image captioning, and aim to improve their faithfulness regarding visual objects.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: This work provides the first systematic and comprehensive study of object hallucination in modern VLP models, revealing a critical and often overlooked flaw. It quantifies the problem and identifies key architectural and training objective factors influencing it.\n    *   **Potential Impact on Future Research**: The findings offer valuable guidance for designing more responsible and faithful VLP systems. The revelation that standard metric optimization can worsen faithfulness challenges current evaluation paradigms and encourages the development of new metrics and training strategies that prioritize factual grounding. The proposed ObjMLM loss provides a concrete, effective method for mitigating object hallucination, paving the way for future research into more robust and reliable VLP models \\cite{dai20229aa}.",
      "intriguing_abstract": "Despite remarkable advancements in Vision-Language Pre-trained (VLP) models, a critical, often overlooked flaw persists: **object hallucination**. These models frequently generate text describing non-existent visual objects, severely limiting their reliability in applications like image captioning and raising serious safety concerns. This paper presents the *first systematic investigation* into object hallucination in state-of-the-art VLP models, revealing a paradoxical finding: models optimized for standard metrics like CIDEr (e.g., via Self-Critical Sequence Training, SCST) can exhibit *higher* rates of unfaithfulness.\n\nOur comprehensive analysis delves into the influence of image encoding methods (region, grid, patch-based features) and pre-training objectives (Image-Text Contrastive, Image-Text Matching, Image-Conditioned Language Modeling). We demonstrate that patch-based features and strong token-level alignment are crucial for mitigating this issue. Based on these insights, we introduce **ObjMLM (Object-Masked Language Modeling)**, a novel VLP objective that significantly reduces object hallucination by up to 17.4% on benchmarks without additional data. This work not only quantifies a pervasive problem but also challenges current evaluation paradigms, offering vital guidance for developing more faithful and trustworthy VLP systems.",
      "keywords": [
        "Object hallucination",
        "Vision-Language Pre-trained (VLP) models",
        "ObjMLM (Object-Masked Language Modeling)",
        "Image captioning",
        "Systematic investigation of object hallucination",
        "Image encoding methods",
        "VLP pre-training objectives",
        "Token-level image-text alignment",
        "Faithfulness in text generation",
        "CHAIR hallucination metrics",
        "Standard metric optimization paradox",
        "Patch-based image features",
        "Factual grounding"
      ],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/7cfbd36c0043098589cbaf18dca2b41d8dc24abe.pdf",
      "citation_key": "dai20229aa",
      "metadata": {
        "title": "Plausible May Not Be Faithful: Probing Object Hallucination in Vision-Language Pre-training",
        "authors": [
          "Wenliang Dai",
          "Zihan Liu",
          "Ziwei Ji",
          "Dan Su",
          "Pascale Fung"
        ],
        "published_date": "2022",
        "abstract": "Large-scale vision-language pre-trained (VLP) models are prone to hallucinate non-existent visual objects when generating text based on visual information. In this paper, we systematically study the object hallucination problem from three aspects. First, we examine recent state-of-the-art VLP models, showing that they still hallucinate frequently and models achieving better scores on standard metrics (e.g., CIDEr) could be more unfaithful. Second, we investigate how different types of image encoding in VLP influence hallucination, including region-based, grid-based, and patch-based. Surprisingly, we find that patch-based features perform the best and smaller patch resolution yields a non-trivial reduction in object hallucination. Third, we decouple various VLP objectives and demonstrate that token-level image-text alignment and controlled generation are crucial to reducing hallucination. Based on that, we propose a simple yet effective VLP loss named ObjMLM to further mitigate object hallucination. Results show that it reduces object hallucination by up to 17.4% when tested on two benchmarks (COCO Caption for in-domain and NoCaps for out-of-domain evaluation).",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/7cfbd36c0043098589cbaf18dca2b41d8dc24abe.pdf",
        "venue": "Conference of the European Chapter of the Association for Computational Linguistics",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the paper for a literature review, adhering to your requirements:\n\n---\n\n### Analysis of \"Plausible May Not Be Faithful: Probing Object Hallucination in Vision-Language Pre-training\" \\cite{dai20229aa}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Large-scale Vision-Language Pre-trained (VLP) models frequently hallucinate non-existent visual objects when generating text based on visual information (e.g., image captioning).\n    *   **Importance and Challenge**: This \"object hallucination\" significantly limits VLP model performance and raises serious safety concerns for industrial applications (e.g., reducing diagnostic accuracy in biomedical image captioning). Despite its criticality, this problem has not been systematically studied in contemporary VLP research \\cite{dai20229aa}.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: Previous work on hallucination in deep learning includes data refinement, scene graph learning, localized region reconstruction, uncertainty-aware beam search, knowledge augmentation, and post-processing for various tasks (e.g., data-to-text, dialog systems, QA).\n    *   **Limitations of Previous Solutions**: While general hallucination has been addressed, how different VLP strategies (e.g., pre-training objectives, image encoding methods) influence the faithfulness of generated text given images has not been systematically studied \\cite{dai20229aa}. This paper is positioned as the *first* to systematically investigate object hallucination in state-of-the-art VLP models.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper systematically studies object hallucination from three key aspects:\n        1.  **Examination of SOTA VLP Models**: Quantifying hallucination rates and observing that models achieving better standard metrics (e.g., CIDEr) can be *more* unfaithful.\n        2.  **Investigation of Image Encoding**: Ablating the influence of region-based, grid-based, and patch-based image features on hallucination.\n        3.  **Decoupling VLP Objectives**: Analyzing the effects of Image-Text Contrastive (ITC), Image-Text Matching (ITM), and Image-Conditioned Language Modeling (ICLM) losses.\n    *   **Novelty/Difference**: Based on these insights, the paper proposes a novel VLP loss called **ObjMLM (Object-Masked Language Modeling)**. This objective aims to mitigate object hallucination by enhancing the alignment and restriction between text tokens and visual objects during generation, a direct approach to improving faithfulness \\cite{dai20229aa}.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**: Introduction of **ObjMLM**, a simple yet effective pre-training objective designed to reduce object hallucination by improving token-level image-text alignment and controlled generation.\n    *   **Theoretical Insights/Analysis**:\n        *   First systematic study demonstrating that state-of-the-art VLP models still frequently hallucinate, and that optimizing for standard metrics like CIDEr (e.g., via SCST) can paradoxically *increase* hallucination \\cite{dai20229aa}.\n        *   Empirical finding that patch-based image features perform best in reducing hallucination, with smaller patch resolutions yielding significant reductions.\n        *   Analysis showing that token-level image-text alignment (ICLM) is crucial for reducing hallucination, while global alignment objectives like ITC and ITM contribute less.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**:\n        *   Evaluation of object hallucination in various SOTA VLP models (OSCAR, VinVL, BLIP, OFA) on image captioning.\n        *   Ablation studies on different image encoding methods (region, grid, patch) and VLP objectives (ITC, ITM, ICLM).\n        *   Evaluation of the proposed ObjMLM loss.\n    *   **Key Performance Metrics**: CHAIR_i (instance-level hallucination) and CHAIR_s (sentence-level hallucination) were used, alongside standard captioning metrics (BLEU-4, CIDEr, METEOR, SPICE).\n    *   **Comparison Results**:\n        *   SOTA models exhibit frequent hallucination (e.g., ~10% of sentences contain hallucinated objects in-domain, much higher out-of-domain).\n        *   SCST optimization, while boosting standard metrics, increased CHAIR_s by up to 0.9 on COCO Caption and 10.9% on NoCaps for VinVL Base \\cite{dai20229aa}.\n        *   Patch-based features outperformed region- and grid-based features, with smaller patch resolutions reducing hallucination.\n        *   ObjMLM successfully reduced object hallucination by up to **17.4%** on two benchmarks (COCO Caption for in-domain and NoCaps for out-of-domain evaluation) without introducing extra training data \\cite{dai20229aa}.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The paper primarily focuses on *object* hallucination in VLP models for text generation. While ObjMLM is effective, the paper does not extensively discuss potential trade-offs with other aspects of generation (e.g., fluency, diversity) or its applicability to other types of hallucination (e.g., factual inaccuracies not related to objects). The CHAIR metric relies on pre-defined object categories, which might not capture all nuances of hallucination.\n    *   **Scope of Applicability**: The findings and proposed method are directly applicable to VLP models used for image-conditioned text generation tasks, particularly image captioning, and aim to improve their faithfulness regarding visual objects.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: This work provides the first systematic and comprehensive study of object hallucination in modern VLP models, revealing a critical and often overlooked flaw. It quantifies the problem and identifies key architectural and training objective factors influencing it.\n    *   **Potential Impact on Future Research**: The findings offer valuable guidance for designing more responsible and faithful VLP systems. The revelation that standard metric optimization can worsen faithfulness challenges current evaluation paradigms and encourages the development of new metrics and training strategies that prioritize factual grounding. The proposed ObjMLM loss provides a concrete, effective method for mitigating object hallucination, paving the way for future research into more robust and reliable VLP models \\cite{dai20229aa}.",
        "keywords": [
          "Object hallucination",
          "Vision-Language Pre-trained (VLP) models",
          "ObjMLM (Object-Masked Language Modeling)",
          "Image captioning",
          "Systematic investigation of object hallucination",
          "Image encoding methods",
          "VLP pre-training objectives",
          "Token-level image-text alignment",
          "Faithfulness in text generation",
          "CHAIR hallucination metrics",
          "Standard metric optimization paradox",
          "Patch-based image features",
          "Factual grounding"
        ],
        "paper_type": "this paper should be classified as **technical**.\n\nhere's why:\n\n1.  **new method/algorithm proposed:** the abstract explicitly states: \"based on that, we **propose a simple yet effective vlp loss named objmlm** to further mitigate object hallucination.\" this directly aligns with the \"technical\" criterion: \"presents new methods, algorithms, or systems\" and \"abstract mentions: 'propose', 'develop', 'present', 'algorithm', 'method'\".\n\n2.  **problem and solution focus:** the introduction sets up a technical problem (object hallucination in vlp models) and the abstract concludes with a proposed solution (the objmlm loss) and its evaluation.\n\nwhile the paper also involves significant empirical work (\"systematically study,\" \"examine,\" \"investigate,\" \"find,\" \"demonstrate,\" \"results show,\" \"tested on two benchmarks\"), this empirical analysis serves to understand the problem and to validate the effectiveness of the proposed technical solution (the objmlm loss). in papers that propose a new method and then evaluate it, the primary classification is typically \"technical.\""
      },
      "file_name": "7cfbd36c0043098589cbaf18dca2b41d8dc24abe.pdf"
    },
    {
      "success": true,
      "doc_id": "2c44dee6d9e66b2f80c788b3c4fd2bc7",
      "summary": "Here's a focused summary of the paper \\cite{dziri2021bw9} for a literature review:\n\n### Technical Paper Analysis: Neural Path Hunter: Reducing Hallucination in Dialogue Systems via Path Grounding \\cite{dziri2021bw9}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Addresses the problem of factual hallucination in neural dialogue systems powered by large pre-trained language models, where models generate factually incorrect statements despite fluent responses.\n    *   **Importance and Challenge**: Hallucinations impede the widespread adoption of dialogue systems, making responses uninformative and jeopardizing conversation. Existing knowledge grounding approaches often suffer from a \"source-reference divergence problem,\" where training on references alone is insufficient to guarantee faithfulness to auxiliary knowledge (e.g., Knowledge Graphs). Ensuring precise alignment between source knowledge and generated responses remains an open challenge.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: This work builds upon knowledge-grounded dialogue systems that leverage external Knowledge Graphs (KGs).\n    *   **Limitations of Previous Solutions**: Previous knowledge grounding methods often face the \"source-reference divergence problem,\" where the reference data contains additional factual information not fully captured by the source KG, making simple training insufficient to guarantee factual faithfulness \\cite{dziri2021bw9}. This leads to an inherent lack of controllability over factual correctness in large language models.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: \\cite{dziri2021bw9} proposes **NEURAL PATH HUNTER (NPH)**, a generate-then-refine strategy that amends a generated response using a Knowledge Graph (KG). NPH operates in two main stages:\n        1.  **Token-level Hallucination Critic**: Identifies and masks out plausible sources of hallucination (specifically, erroneous entity mentions) within a generated utterance. This critic is trained on a synthetic dataset of corrupted ground-truth responses (extrinsic and intrinsic negatives).\n        2.  **Entity Mention Retriever**: Takes the masked representations from the critic, uses a Masked Language Model (MLM) to obtain contextual representations, and then an autoregressive LM to craft a query signal. This query is propagated over a local k-hop subgraph of the KG (represented as \"KG-Entity Memory\" using GPT2 embeddings or CompGCN) to retrieve factually correct entities.\n    *   **Novelty/Difference**: NPH is novel in its two-stage, generate-then-refine approach that explicitly identifies and corrects hallucinated entities by querying a KG. It works without further fine-tuning the base dialogue generation model and models dialogue as a signal propagated on a local k-hop subgraph to retrieve correct entities.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**:\n        *   **NEURAL PATH HUNTER (NPH)**: A module for reducing hallucination in machine-generated dialogue responses by leveraging KG facts.\n        *   **Token-level Hallucination Critic**: A sequence labeling model trained on synthetic data to identify and mask hallucinated entity mentions.\n        *   **Entity Mention Retriever**: A module that crafts contextual queries from masked entities and retrieves correct entities by scoring against a KG-Entity Memory.\n    *   **System Design/Architectural Innovations**: A generate-then-refine architecture that decouples hallucination detection from correction, allowing for post-hoc refinement of any generated response.\n    *   **Theoretical Insights/Analysis**: Conducted a comprehensive human study revealing that the main mode of hallucination in KG-grounded dialogue systems is the injection of erroneous entities (extrinsic hallucinations), and that increased response diversity often correlates with increased hallucination.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**:\n        *   A systematic human study on hallucination modes in GPT-2 generated responses on OpenDialKG, using various decoding strategies.\n        *   Empirical validation of NPH's ability to reduce hallucinations in KG-grounded dialogue systems.\n    *   **Key Performance Metrics & Comparison Results**:\n        *   Evaluated on the **OpenDialKG dataset** \\cite{dziri2021bw9}.\n        *   Achieved a **relative improvement of 20.35% in faithfulness** based on **FeQA** (a QA-based faithfulness metric) \\cite{dziri2021bw9}.\n        *   Demonstrated an **improvement of 39.98% in human evaluation** for faithfulness \\cite{dziri2021bw9}.\n        *   Human study showed that extrinsic hallucinations are the most common, and greedy decoding results in the least hallucination, while top-k sampling leads to the highest.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**:\n        *   Assumes the availability of a Knowledge Graph (KG) for refinement.\n        *   Relies on a local k-hop subgraph being either provided or extractable based on dialogue history.\n        *   The synthetic data generation for the critic might not perfectly capture all real-world hallucination patterns.\n    *   **Scope of Applicability**: Applicable to any machine-generated response as long as an auxiliary KG is provided, and it works without requiring further fine-tuning of the base generation model. The primary focus is on correcting entity misattributions.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: \\cite{dziri2021bw9} significantly advances the state-of-the-art in reducing factual hallucination in neural dialogue systems by providing a robust, post-hoc refinement mechanism. It offers a practical solution to a critical problem that hinders the deployment of advanced dialogue agents.\n    *   **Potential Impact on Future Research**: The generate-then-refine paradigm, the token-level hallucination critic, and the entity mention retriever could inspire future work on controllable generation, factual consistency, and knowledge grounding in various NLP tasks. The insights from the human study on hallucination modes also provide a valuable foundation for understanding and mitigating these issues.",
      "intriguing_abstract": "Factual hallucination remains a pervasive challenge in neural dialogue systems, where fluent responses from large pre-trained language models often contain factually incorrect statements, hindering their widespread adoption. This paper introduces **NEURAL PATH HUNTER (NPH)**, a pioneering generate-then-refine framework designed to drastically reduce factual inconsistencies. NPH operates in two novel stages: a **Token-level Hallucination Critic** identifies and masks erroneous entity mentions, followed by an **Entity Mention Retriever** that queries a local **Knowledge Graph (KG)** subgraph to retrieve and inject factually correct entities. This post-hoc refinement mechanism works without fine-tuning the base generation model, offering unprecedented controllability over factual faithfulness. Our comprehensive human study reveals that erroneous entity injection is the primary mode of hallucination. Evaluated on **OpenDialKG**, NPH achieves a remarkable 20.35% relative improvement in faithfulness (FeQA) and a 39.98% gain in human evaluation. NPH provides a robust solution to a critical bottleneck, paving the way for more reliable and trustworthy knowledge-grounded dialogue systems and inspiring future research in controllable generation and factual consistency.",
      "keywords": [
        "Factual hallucination",
        "Neural dialogue systems",
        "Knowledge Graphs (KGs)",
        "NEURAL PATH HUNTER (NPH)",
        "Generate-then-refine strategy",
        "Token-level Hallucination Critic",
        "Entity Mention Retriever",
        "Source-reference divergence problem",
        "Extrinsic hallucinations",
        "Post-hoc refinement",
        "Knowledge grounding",
        "Faithfulness improvement",
        "OpenDialKG dataset",
        "Pre-trained language models"
      ],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/889feabe31ba0d24c093ac94d54a06eecb87e3f4.pdf",
      "citation_key": "dziri2021bw9",
      "metadata": {
        "title": "Neural Path Hunter: Reducing Hallucination in Dialogue Systems via Path Grounding",
        "authors": [
          "Nouha Dziri",
          "Andrea Madotto",
          "Osmar Zaiane",
          "A. Bose"
        ],
        "published_date": "2021",
        "abstract": "Dialogue systems powered by large pre-trained language models exhibit an innate ability to deliver fluent and natural-sounding responses. Despite their impressive performance, these models are fitful and can often generate factually incorrect statements impeding their widespread adoption. In this paper, we focus on the task of improving faithfulness and reducing hallucination of neural dialogue systems to known facts supplied by a Knowledge Graph (KG). We propose Neural Path Hunter which follows a generate-then-refine strategy whereby a generated response is amended using the KG. Neural Path Hunter leverages a separate token-level fact critic to identify plausible sources of hallucination followed by a refinement stage that retrieves correct entities by crafting a query signal that is propagated over a k-hop subgraph. We empirically validate our proposed approach on the OpenDialKG dataset (Moon et al., 2019) against a suite of metrics and report a relative improvement of faithfulness over dialogue responses by 20.35% based on FeQA (Durmus et al., 2020). The code is available at https://github.com/nouhadziri/Neural-Path-Hunter.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/889feabe31ba0d24c093ac94d54a06eecb87e3f4.pdf",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the paper \\cite{dziri2021bw9} for a literature review:\n\n### Technical Paper Analysis: Neural Path Hunter: Reducing Hallucination in Dialogue Systems via Path Grounding \\cite{dziri2021bw9}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Addresses the problem of factual hallucination in neural dialogue systems powered by large pre-trained language models, where models generate factually incorrect statements despite fluent responses.\n    *   **Importance and Challenge**: Hallucinations impede the widespread adoption of dialogue systems, making responses uninformative and jeopardizing conversation. Existing knowledge grounding approaches often suffer from a \"source-reference divergence problem,\" where training on references alone is insufficient to guarantee faithfulness to auxiliary knowledge (e.g., Knowledge Graphs). Ensuring precise alignment between source knowledge and generated responses remains an open challenge.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: This work builds upon knowledge-grounded dialogue systems that leverage external Knowledge Graphs (KGs).\n    *   **Limitations of Previous Solutions**: Previous knowledge grounding methods often face the \"source-reference divergence problem,\" where the reference data contains additional factual information not fully captured by the source KG, making simple training insufficient to guarantee factual faithfulness \\cite{dziri2021bw9}. This leads to an inherent lack of controllability over factual correctness in large language models.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: \\cite{dziri2021bw9} proposes **NEURAL PATH HUNTER (NPH)**, a generate-then-refine strategy that amends a generated response using a Knowledge Graph (KG). NPH operates in two main stages:\n        1.  **Token-level Hallucination Critic**: Identifies and masks out plausible sources of hallucination (specifically, erroneous entity mentions) within a generated utterance. This critic is trained on a synthetic dataset of corrupted ground-truth responses (extrinsic and intrinsic negatives).\n        2.  **Entity Mention Retriever**: Takes the masked representations from the critic, uses a Masked Language Model (MLM) to obtain contextual representations, and then an autoregressive LM to craft a query signal. This query is propagated over a local k-hop subgraph of the KG (represented as \"KG-Entity Memory\" using GPT2 embeddings or CompGCN) to retrieve factually correct entities.\n    *   **Novelty/Difference**: NPH is novel in its two-stage, generate-then-refine approach that explicitly identifies and corrects hallucinated entities by querying a KG. It works without further fine-tuning the base dialogue generation model and models dialogue as a signal propagated on a local k-hop subgraph to retrieve correct entities.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**:\n        *   **NEURAL PATH HUNTER (NPH)**: A module for reducing hallucination in machine-generated dialogue responses by leveraging KG facts.\n        *   **Token-level Hallucination Critic**: A sequence labeling model trained on synthetic data to identify and mask hallucinated entity mentions.\n        *   **Entity Mention Retriever**: A module that crafts contextual queries from masked entities and retrieves correct entities by scoring against a KG-Entity Memory.\n    *   **System Design/Architectural Innovations**: A generate-then-refine architecture that decouples hallucination detection from correction, allowing for post-hoc refinement of any generated response.\n    *   **Theoretical Insights/Analysis**: Conducted a comprehensive human study revealing that the main mode of hallucination in KG-grounded dialogue systems is the injection of erroneous entities (extrinsic hallucinations), and that increased response diversity often correlates with increased hallucination.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**:\n        *   A systematic human study on hallucination modes in GPT-2 generated responses on OpenDialKG, using various decoding strategies.\n        *   Empirical validation of NPH's ability to reduce hallucinations in KG-grounded dialogue systems.\n    *   **Key Performance Metrics & Comparison Results**:\n        *   Evaluated on the **OpenDialKG dataset** \\cite{dziri2021bw9}.\n        *   Achieved a **relative improvement of 20.35% in faithfulness** based on **FeQA** (a QA-based faithfulness metric) \\cite{dziri2021bw9}.\n        *   Demonstrated an **improvement of 39.98% in human evaluation** for faithfulness \\cite{dziri2021bw9}.\n        *   Human study showed that extrinsic hallucinations are the most common, and greedy decoding results in the least hallucination, while top-k sampling leads to the highest.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**:\n        *   Assumes the availability of a Knowledge Graph (KG) for refinement.\n        *   Relies on a local k-hop subgraph being either provided or extractable based on dialogue history.\n        *   The synthetic data generation for the critic might not perfectly capture all real-world hallucination patterns.\n    *   **Scope of Applicability**: Applicable to any machine-generated response as long as an auxiliary KG is provided, and it works without requiring further fine-tuning of the base generation model. The primary focus is on correcting entity misattributions.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: \\cite{dziri2021bw9} significantly advances the state-of-the-art in reducing factual hallucination in neural dialogue systems by providing a robust, post-hoc refinement mechanism. It offers a practical solution to a critical problem that hinders the deployment of advanced dialogue agents.\n    *   **Potential Impact on Future Research**: The generate-then-refine paradigm, the token-level hallucination critic, and the entity mention retriever could inspire future work on controllable generation, factual consistency, and knowledge grounding in various NLP tasks. The insights from the human study on hallucination modes also provide a valuable foundation for understanding and mitigating these issues.",
        "keywords": [
          "Factual hallucination",
          "Neural dialogue systems",
          "Knowledge Graphs (KGs)",
          "NEURAL PATH HUNTER (NPH)",
          "Generate-then-refine strategy",
          "Token-level Hallucination Critic",
          "Entity Mention Retriever",
          "Source-reference divergence problem",
          "Extrinsic hallucinations",
          "Post-hoc refinement",
          "Knowledge grounding",
          "Faithfulness improvement",
          "OpenDialKG dataset",
          "Pre-trained language models"
        ],
        "paper_type": "based on the abstract and introduction:\n\n*   the abstract explicitly states: \"we **propose neural path hunter** which follows a generate-then-refine strategy...\" and describes its components (\"token-level fact critic,\" \"refinement stage\").\n*   it then mentions: \"we **empirically validate our proposed approach** on the opendialkg dataset... and report a relative improvement...\"\n*   the introduction discusses a \"technical problem\" (hallucination in dialogue systems) and points towards a \"remedy by leveraging external oracle knowledge,\" which is then addressed by the proposed system.\n\nwhile the paper includes empirical validation, its core contribution is the **development and presentation of a new method/system** (neural path hunter) to solve a specific technical problem. the empirical validation serves to demonstrate the effectiveness of this new technical solution.\n\ntherefore, the paper type is **technical**."
      },
      "file_name": "889feabe31ba0d24c093ac94d54a06eecb87e3f4.pdf"
    },
    {
      "success": true,
      "doc_id": "fcb255d80385b51e87cfeb718f2e8bc8",
      "summary": "Here's a focused summary of the technical paper for a literature review, emphasizing technical innovations and empirical validation:\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Audio-visual Large Language Models (AV-LLMs) exhibit \"cross-modal driven hallucinations,\" where they misinterpret or generate non-existent information due to subtle relationships or over-reliance on one modality when processing combined audio and visual signals \\cite{sungbin2024r2g}. This includes perceiving imaginary sounds from visual cues (video-driven audio hallucination) or fake visual events from audio cues (audio-driven video hallucination).\n    *   **Importance and Challenge**: Human perception is inherently multimodal, making robust AV-LLMs crucial for comprehensive scene understanding. However, the inherent heterogeneity and varying information content across modalities make joint processing challenging. A significant gap exists in dedicated benchmarks to systematically evaluate these cross-modal hallucinations, hindering the development of reliable and robust AV-LLMs \\cite{sungbin2024r2g}.\n\n*   **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: Previous hallucination benchmarks for Multimodal LLMs (MLLMs) primarily focus on single modalities, such as visual LLMs (e.g., POPE, CIEM, NOPE for image-based object presence/absence; AMBER, GAVIE for generative captions) or audio-only models \\cite{sungbin2024r2g}.\n    *   **Limitations of Previous Solutions**: These existing benchmarks are insufficient for evaluating *cross-modal driven hallucinations* because they assess within a single modality. They fail to capture the complex interactions and potential misinterpretations that arise when AV-LLMs process simultaneous audio-visual inputs and their intricate relationships \\cite{sungbin2024r2g}.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper introduces **AVHBench**, the first comprehensive benchmark specifically designed to evaluate cross-modal hallucinations in audio-visual LLMs \\cite{sungbin2024r2g}. It comprises four distinct tasks:\n        *   **Judgment Tasks (Yes/No QnA)**:\n            *   **Audio-driven Video Hallucination**: Assesses if an audio signal causes the model to hallucinate a visible object/event.\n            *   **Video-driven Audio Hallucination**: Evaluates if a visual signal causes the model to hallucinate an audible sound.\n            *   **Audio-visual Matching**: Determines the model's ability to recognize correspondence between audio and visual contexts.\n        *   **Description Task**:\n            *   **Audio-visual Captioning**: Prompts the model to generate a caption describing both visual and auditory signals, assessing for hallucinated descriptions.\n    *   **Novelty/Difference**:\n        *   **Cross-modal Focus**: AVHBench uniquely targets hallucinations arising from the *interaction* between audio and visual modalities, a critical aspect for true multimodal understanding \\cite{sungbin2024r2g}.\n        *   **Semi-automatic Annotation Pipeline**: A novel two-stage pipeline minimizes human labor while ensuring high-quality annotations:\n            1.  **Disentangling Audio-visual Information**: Leverages existing dataset captions (VALOR, AudioCaps), visual tags from RAM++, and ChatGPT (GPT4) with few-shot examples to categorize fine-grained audio-visual cues (e.g., in-view sound source, out-of-view silent object).\n            2.  **QnA Generation**: Employs a rule-based algorithm and ChatGPT to generate QnA pairs and captions for the four tasks, followed by human verification.\n        *   **Synthetic Videos**: Creation of challenging synthetic videos by swapping audio from different sources to introduce natural mismatches, specifically designed to test cross-modal discernment \\cite{sungbin2024r2g}.\n\n*   **Key Technical Contributions**\n    *   **Novel Benchmark**: Introduction of **AVHBench**, the first benchmark specifically designed to assess cross-modal hallucinations in audio-visual LLMs \\cite{sungbin2024r2g}.\n    *   **Efficient Data Generation**: Development of a semi-automatic annotation pipeline that significantly reduces manual labeling costs while ensuring high-quality, fine-grained audio-visual annotations.\n    *   **Comprehensive Analysis**: Provides a systematic framework and dataset (5,302 QnA pairs, 1,106 captions across 2,136 videos) for analyzing the presence and causes of cross-modal hallucinations in AV-LLMs.\n    *   **Insights for Robustness**: Demonstrates that simple training methods, such as Low-Rank Adaptation (LoRA) fine-tuning with enhanced feature alignment, can improve AV-LLM robustness against cross-modal hallucinations \\cite{sungbin2024r2g}.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted**: The paper evaluates six recent audio-visual LLMs on the proposed AVHBench dataset \\cite{sungbin2024r2g}. It also conducts ablation studies using a simple training method (LoRA fine-tuning) with an annotation-enriched training dataset generated by their pipeline.\n    *   **Key Performance Metrics**: Performance is assessed based on accuracy for the judgment tasks (Yes/No QnA) and the ability to generate accurate, non-hallucinated captions for the description task.\n    *   **Comparison Results**:\n        *   Current audio-visual LLMs are generally prone to both audio-driven and video-driven hallucinations \\cite{sungbin2024r2g}.\n        *   These models tend to perform better with unimodal signals or even text-only inputs, indicating a limited capacity to handle complex multimodal signals and their relationships.\n        *   LoRA fine-tuning, combined with enhanced feature alignment, significantly improves the performance and robustness of AV-LLMs against cross-modal hallucinations \\cite{sungbin2024r2g}.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations**: The pipeline's reliance on existing datasets and large language models (ChatGPT/GPT4) for initial disentanglement and QnA generation implies potential dependencies on their inherent biases or limitations. The \"simple training method\" for improvement suggests an initial exploration rather than a complete solution to all forms of hallucination \\cite{sungbin2024r2g}.\n    *   **Scope of Applicability**: AVHBench is specifically tailored to cross-modal hallucinations in *audio-visual* LLMs. While insights on multimodal signal handling may generalize, the benchmark's direct applicability is within this specific modality combination.\n\n*   **Technical Significance**\n    *   **Advancement of State-of-the-Art**: AVHBench fills a critical gap by providing the first dedicated benchmark for evaluating cross-modal driven hallucinations in audio-visual LLMs, moving beyond single-modality assessments \\cite{sungbin2024r2g}. This enables a more nuanced and comprehensive understanding of AV-LLM capabilities and failure modes.\n    *   **Potential Impact on Future Research**:\n        *   **Robust AV-LLM Development**: The benchmark provides a crucial tool for identifying weaknesses in current AV-LLMs, guiding the development of more robust models that can accurately perceive and reason about complex audio-visual relationships.\n        *   **Feature Alignment and Attention Mechanisms**: The finding that improved feature alignment and refining attention mechanisms (e.g., via LoRA fine-tuning) can enhance robustness against hallucinations offers a clear direction for future architectural and training research in multimodal LLMs \\cite{sungbin2024r2g}.\n        *   **Dataset Generation Methodologies**: The semi-automatic annotation pipeline offers a scalable and cost-effective methodology for creating complex multimodal benchmarks, potentially inspiring similar approaches for other challenging multimodal tasks.",
      "intriguing_abstract": "Audio-visual Large Language Models (AV-LLMs) promise comprehensive scene understanding, yet they are plagued by \"cross-modal driven hallucinations\"â€”perceiving imaginary sounds from visuals or fake visual events from audio. This critical flaw, stemming from complex inter-modal relationships, lacks systematic evaluation. We introduce **AVHBench**, the first dedicated benchmark to rigorously assess these challenging cross-modal hallucinations. AVHBench features four novel tasks, including audio-driven video and video-driven audio hallucination judgment, and leverages a unique semi-automatic annotation pipeline to generate high-quality QnA pairs and captions, even creating synthetic videos to expose subtle misinterpretations. Our comprehensive evaluation of six state-of-the-art AV-LLMs reveals their significant susceptibility to these hallucinations. Crucially, we demonstrate that simple Low-Rank Adaptation (LoRA) fine-tuning with enhanced feature alignment substantially improves robustness. AVHBench provides an indispensable tool for developing truly robust AV-LLMs, paving the way for advanced multimodal understanding and mitigating critical failure modes in real-world applications.",
      "keywords": [
        "AVHBench",
        "cross-modal hallucinations",
        "Audio-visual Large Language Models (AV-LLMs)",
        "semi-automatic annotation pipeline",
        "feature alignment",
        "LoRA fine-tuning",
        "video-driven audio hallucination",
        "audio-driven video hallucination",
        "synthetic videos",
        "robustness evaluation",
        "multimodal perception",
        "benchmark dataset"
      ],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/9b05e1dfd158c307b74298df3d4608b93d2060a7.pdf",
      "citation_key": "sungbin2024r2g",
      "metadata": {
        "title": "AVHBench: A Cross-Modal Hallucination Benchmark for Audio-Visual Large Language Models",
        "authors": [
          "Kim Sung-Bin",
          "Oh Hyun-Bin",
          "JungMok Lee",
          "Arda Senocak",
          "Joon Son Chung",
          "Tae-Hyun Oh"
        ],
        "published_date": "2024",
        "abstract": "Following the success of Large Language Models (LLMs), expanding their boundaries to new modalities represents a significant paradigm shift in multimodal understanding. Human perception is inherently multimodal, relying not only on text but also on auditory and visual cues for a complete understanding of the world. In recognition of this fact, audio-visual LLMs have recently emerged. Despite promising developments, the lack of dedicated benchmarks poses challenges for understanding and evaluating models. In this work, we show that audio-visual LLMs struggle to discern subtle relationships between audio and visual signals, leading to hallucinations and highlighting the need for reliable benchmarks. To address this, we introduce AVHBench, the first comprehensive benchmark specifically designed to evaluate the perception and comprehension capabilities of audio-visual LLMs. Our benchmark includes tests for assessing hallucinations, as well as the cross-modal matching and reasoning abilities of these models. Our results reveal that most existing audio-visual LLMs struggle with hallucinations caused by cross-interactions between modalities, due to their limited capacity to perceive complex multimodal signals and their relationships. Additionally, we demonstrate that simple training with our AVHBench improves robustness of audio-visual LLMs against hallucinations. Dataset: https://github.com/kaist-ami/AVHBench",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/9b05e1dfd158c307b74298df3d4608b93d2060a7.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the technical paper for a literature review, emphasizing technical innovations and empirical validation:\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Audio-visual Large Language Models (AV-LLMs) exhibit \"cross-modal driven hallucinations,\" where they misinterpret or generate non-existent information due to subtle relationships or over-reliance on one modality when processing combined audio and visual signals \\cite{sungbin2024r2g}. This includes perceiving imaginary sounds from visual cues (video-driven audio hallucination) or fake visual events from audio cues (audio-driven video hallucination).\n    *   **Importance and Challenge**: Human perception is inherently multimodal, making robust AV-LLMs crucial for comprehensive scene understanding. However, the inherent heterogeneity and varying information content across modalities make joint processing challenging. A significant gap exists in dedicated benchmarks to systematically evaluate these cross-modal hallucinations, hindering the development of reliable and robust AV-LLMs \\cite{sungbin2024r2g}.\n\n*   **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: Previous hallucination benchmarks for Multimodal LLMs (MLLMs) primarily focus on single modalities, such as visual LLMs (e.g., POPE, CIEM, NOPE for image-based object presence/absence; AMBER, GAVIE for generative captions) or audio-only models \\cite{sungbin2024r2g}.\n    *   **Limitations of Previous Solutions**: These existing benchmarks are insufficient for evaluating *cross-modal driven hallucinations* because they assess within a single modality. They fail to capture the complex interactions and potential misinterpretations that arise when AV-LLMs process simultaneous audio-visual inputs and their intricate relationships \\cite{sungbin2024r2g}.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper introduces **AVHBench**, the first comprehensive benchmark specifically designed to evaluate cross-modal hallucinations in audio-visual LLMs \\cite{sungbin2024r2g}. It comprises four distinct tasks:\n        *   **Judgment Tasks (Yes/No QnA)**:\n            *   **Audio-driven Video Hallucination**: Assesses if an audio signal causes the model to hallucinate a visible object/event.\n            *   **Video-driven Audio Hallucination**: Evaluates if a visual signal causes the model to hallucinate an audible sound.\n            *   **Audio-visual Matching**: Determines the model's ability to recognize correspondence between audio and visual contexts.\n        *   **Description Task**:\n            *   **Audio-visual Captioning**: Prompts the model to generate a caption describing both visual and auditory signals, assessing for hallucinated descriptions.\n    *   **Novelty/Difference**:\n        *   **Cross-modal Focus**: AVHBench uniquely targets hallucinations arising from the *interaction* between audio and visual modalities, a critical aspect for true multimodal understanding \\cite{sungbin2024r2g}.\n        *   **Semi-automatic Annotation Pipeline**: A novel two-stage pipeline minimizes human labor while ensuring high-quality annotations:\n            1.  **Disentangling Audio-visual Information**: Leverages existing dataset captions (VALOR, AudioCaps), visual tags from RAM++, and ChatGPT (GPT4) with few-shot examples to categorize fine-grained audio-visual cues (e.g., in-view sound source, out-of-view silent object).\n            2.  **QnA Generation**: Employs a rule-based algorithm and ChatGPT to generate QnA pairs and captions for the four tasks, followed by human verification.\n        *   **Synthetic Videos**: Creation of challenging synthetic videos by swapping audio from different sources to introduce natural mismatches, specifically designed to test cross-modal discernment \\cite{sungbin2024r2g}.\n\n*   **Key Technical Contributions**\n    *   **Novel Benchmark**: Introduction of **AVHBench**, the first benchmark specifically designed to assess cross-modal hallucinations in audio-visual LLMs \\cite{sungbin2024r2g}.\n    *   **Efficient Data Generation**: Development of a semi-automatic annotation pipeline that significantly reduces manual labeling costs while ensuring high-quality, fine-grained audio-visual annotations.\n    *   **Comprehensive Analysis**: Provides a systematic framework and dataset (5,302 QnA pairs, 1,106 captions across 2,136 videos) for analyzing the presence and causes of cross-modal hallucinations in AV-LLMs.\n    *   **Insights for Robustness**: Demonstrates that simple training methods, such as Low-Rank Adaptation (LoRA) fine-tuning with enhanced feature alignment, can improve AV-LLM robustness against cross-modal hallucinations \\cite{sungbin2024r2g}.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted**: The paper evaluates six recent audio-visual LLMs on the proposed AVHBench dataset \\cite{sungbin2024r2g}. It also conducts ablation studies using a simple training method (LoRA fine-tuning) with an annotation-enriched training dataset generated by their pipeline.\n    *   **Key Performance Metrics**: Performance is assessed based on accuracy for the judgment tasks (Yes/No QnA) and the ability to generate accurate, non-hallucinated captions for the description task.\n    *   **Comparison Results**:\n        *   Current audio-visual LLMs are generally prone to both audio-driven and video-driven hallucinations \\cite{sungbin2024r2g}.\n        *   These models tend to perform better with unimodal signals or even text-only inputs, indicating a limited capacity to handle complex multimodal signals and their relationships.\n        *   LoRA fine-tuning, combined with enhanced feature alignment, significantly improves the performance and robustness of AV-LLMs against cross-modal hallucinations \\cite{sungbin2024r2g}.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations**: The pipeline's reliance on existing datasets and large language models (ChatGPT/GPT4) for initial disentanglement and QnA generation implies potential dependencies on their inherent biases or limitations. The \"simple training method\" for improvement suggests an initial exploration rather than a complete solution to all forms of hallucination \\cite{sungbin2024r2g}.\n    *   **Scope of Applicability**: AVHBench is specifically tailored to cross-modal hallucinations in *audio-visual* LLMs. While insights on multimodal signal handling may generalize, the benchmark's direct applicability is within this specific modality combination.\n\n*   **Technical Significance**\n    *   **Advancement of State-of-the-Art**: AVHBench fills a critical gap by providing the first dedicated benchmark for evaluating cross-modal driven hallucinations in audio-visual LLMs, moving beyond single-modality assessments \\cite{sungbin2024r2g}. This enables a more nuanced and comprehensive understanding of AV-LLM capabilities and failure modes.\n    *   **Potential Impact on Future Research**:\n        *   **Robust AV-LLM Development**: The benchmark provides a crucial tool for identifying weaknesses in current AV-LLMs, guiding the development of more robust models that can accurately perceive and reason about complex audio-visual relationships.\n        *   **Feature Alignment and Attention Mechanisms**: The finding that improved feature alignment and refining attention mechanisms (e.g., via LoRA fine-tuning) can enhance robustness against hallucinations offers a clear direction for future architectural and training research in multimodal LLMs \\cite{sungbin2024r2g}.\n        *   **Dataset Generation Methodologies**: The semi-automatic annotation pipeline offers a scalable and cost-effective methodology for creating complex multimodal benchmarks, potentially inspiring similar approaches for other challenging multimodal tasks.",
        "keywords": [
          "AVHBench",
          "cross-modal hallucinations",
          "Audio-visual Large Language Models (AV-LLMs)",
          "semi-automatic annotation pipeline",
          "feature alignment",
          "LoRA fine-tuning",
          "video-driven audio hallucination",
          "audio-driven video hallucination",
          "synthetic videos",
          "robustness evaluation",
          "multimodal perception",
          "benchmark dataset"
        ],
        "paper_type": "the paper type is **technical**.\n\n**reasoning:**\n\n*   the abstract explicitly states: \"to address this, we introduce avhbench, the first comprehensive benchmark specifically designed to evaluate the perception and comprehension capabilities of audio-visual llms.\" this indicates the development and presentation of a new system or tool (a benchmark).\n*   the introduction reiterates this: \"we introduce avhbench, the first comprehensive benchmark specifically designed to evaluate...\"\n*   while it mentions \"our results reveal...\" and \"we demonstrate that simple training with our avhbench improves robustness...\", these are empirical findings *using* the newly developed benchmark. the core contribution is the *creation and presentation* of the benchmark itself, which serves as a new system for evaluation.\n*   the criteria for \"technical\" include \"presents new methods, algorithms, or systems.\" a benchmark, especially a \"comprehensive benchmark specifically designed to evaluate,\" fits the description of a new system or tool."
      },
      "file_name": "9b05e1dfd158c307b74298df3d4608b93d2060a7.pdf"
    },
    {
      "success": true,
      "doc_id": "95819a10b3770bfde3056c4664061c4e",
      "summary": "Here is a focused summary of the technical paper for a literature review, adhering to your requirements:\n\n*   **CITATION**: \\cite{hakim2024d4u}\n\n---\n\n### Technical Paper Analysis of \\cite{hakim2024d4u}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: The paper addresses the critical issue of Large Language Model (LLM) \"hallucinations\" and inaccuracies when deployed in high-risk, safety-critical domains, specifically pharmacovigilance (PV). This includes generating fabricated information, omitting key details, or failing to communicate uncertainty.\n    *   **Importance and Challenge**: In pharmacovigilance, inaccuracies in processing Individual Case Safety Reports (ICSRs) can lead to patient harm, misallocation of resources, and failure to identify genuine safety signals. The challenge lies in ensuring LLM outputs adhere to stringent regulatory and quality standards, preventing \"never event\" errors (e.g., incorrect drug identification) that are wholly preventable and unacceptable in medical contexts.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: The work positions itself within the broader context of LLM application in biology and medicine, acknowledging their potential but highlighting their inherent limitations, particularly hallucinations and uncertainty communication.\n    *   **Limitations of Previous Solutions**: While \"structural guardrails\" (ensuring consistent output format) exist, the paper focuses on the need for \"semantic guardrails\" to verify the accuracy and content of LLM output. Previous LLM deployments in safety-critical areas often lack robust mechanisms to prevent critical errors or effectively convey uncertainty, making them unsuitable for domains like pharmacovigilance where \"never event\" errors must be absolutely prevented.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes and demonstrates a suite of \"semantic guardrails\" integrated with an LLM fine-tuned for a text-to-text task. This task involves converting structured and unstructured data from adverse event reports (specifically Japanese ICSRs) into English narrative text.\n    *   **Novelty/Difference**: The core innovation is the development of both \"hard\" and \"soft\" semantic guardrails specifically engineered to mitigate defined \"never event\" errors and communicate uncertainty in a safety-critical medical context. This framework ensures adherence to regulatory standards by actively detecting and preventing key inaccuracies in LLM-generated content.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**:\n        *   **Document-wise Uncertainty Quantification (DL-UQ)**: A soft semantic guardrail that uses k-nearest neighbors on document embeddings (derived from source language encoder LLM) to identify anomalous input documents unlikely to be ICSRs, flagging them for human review or discarding.\n        *   **MISMATCH Guardrail**: A hard semantic guardrail that enforces a \"never event\" by identifying drug names or adverse event terms present in either the source or target text but not both. It utilizes custom drug dictionaries and MedDRA (Medical Dictionary for Regulatory Activities) with regular expressions to ensure consistency and prevent hallucination or omission of critical terms.\n        *   **Uncertainty Communication**: The framework includes soft guardrails designed to communicate the model's uncertainty regarding the quality and accuracy of both the input text and its final translation, thereby flagging instances requiring further human review.\n    *   **System Design/Architectural Innovations**: Integration of these guardrails into a sequential processing pipeline for LLM-based ICSR translation, ensuring that potential errors are caught at different stages (input validation, content accuracy).\n    *   **Theoretical Insights/Analysis**: Introduction and operationalization of the concept of \"never event\" errors for LLM outputs in safety-critical domains, emphasizing the need for absolute prevention through robust guardrail mechanisms.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**:\n        *   Fine-tuning and evaluation of three LLMs (mt5-xl, mpt-7b-instruct, stablelm-japanese) on a multilingual corpus of ICSRs and OPUS-100 data for text-to-text translation.\n        *   Expert human evaluation of 210 LLM-generated Japanese-to-English ICSR translations against a \"ground truth\" baseline established by independent bilingual PV experts.\n        *   Application and demonstration of the developed hard and soft semantic guardrails within the ICSR processing pipeline.\n    *   **Key Performance Metrics & Comparison Results**:\n        *   LLM translation quality was assessed using per-token perplexity, BLEU score, SACRE-BLEU score, and word error rate.\n        *   Human expert evaluation used a detailed five-category assessment system, binary evaluation criteria (presence of any error), and a five-point Likert scale for clinical acceptability.\n        *   The optimal LLM generation hyperparameters were selected based on BLEU score performance (contrastive search with Î±=0.2 and top-k=16). The guardrails were demonstrated to mitigate identified failure modes and prevent \"never event\" errors, ensuring adherence to safety standards.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The paper focuses primarily on Japanese ICSRs, although the framework is posited to be broadly applicable. The tuning of distance thresholds for the DL-UQ guardrail involves a trade-off between sensitivity and specificity, requiring careful calibration for specific applications.\n    *   **Scope of Applicability**: The framework is developed and demonstrated for pharmacovigilance, specifically for the text-to-text conversion and translation of ICSRs. However, the authors suggest its broad applicability to other medical safety-critical domains where LLM accuracy and reliability are paramount.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: This work significantly advances the safe deployment of LLMs in high-stakes medical environments by providing a concrete, implementable framework of semantic guardrails. It moves beyond general LLM safety discussions to specific, actionable mechanisms for preventing critical errors.\n    *   **Potential Impact on Future Research**: The guardrail framework establishes a precedent for integrating robust safety mechanisms into LLM pipelines for regulated industries. It highlights the importance of designing LLM applications with explicit \"never event\" prevention and uncertainty communication, paving the way for more trustworthy and clinically acceptable AI tools in healthcare and other safety-critical sectors.",
      "intriguing_abstract": "Large Language Models (LLMs) promise transformative potential for pharmacovigilance, yet their inherent propensity for \"hallucinations\" poses an unacceptable risk, particularly in generating \"never event\" errors within Individual Case Safety Reports (ICSRs). This paper introduces a pioneering framework of **semantic guardrails** engineered to ensure the absolute reliability and regulatory compliance of LLM outputs in such safety-critical medical contexts. Moving beyond mere structural validation, our novel approach integrates both \"hard\" and \"soft\" guardrails directly into the LLM pipeline for text-to-text translation of Japanese ICSRs. Key innovations include **Document-wise Uncertainty Quantification (DL-UQ)**, which flags anomalous inputs, and the **MISMATCH Guardrail**, which rigorously prevents critical drug or adverse event term inconsistencies using **MedDRA** and custom dictionaries. We also incorporate explicit **uncertainty communication** mechanisms. Validated through expert human evaluation, these guardrails effectively mitigate critical inaccuracies, preventing patient harm and establishing a crucial precedent for deploying trustworthy LLMs in healthcare and other high-stakes domains.",
      "keywords": [
        "LLM hallucinations",
        "Pharmacovigilance",
        "Safety-critical domains",
        "Semantic guardrails",
        "Never event errors",
        "Uncertainty communication",
        "Document-wise Uncertainty Quantification (DL-UQ)",
        "MISMATCH Guardrail",
        "Individual Case Safety Reports (ICSRs)",
        "LLM fine-tuning",
        "Human expert evaluation",
        "Trustworthy AI",
        "Regulatory compliance"
      ],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/b169426b9181adee0e7d6616fc12fc12611d9901.pdf",
      "citation_key": "hakim2024d4u",
      "metadata": {
        "title": "The Need for Guardrails with Large Language Models in Medical Safety-Critical Settings: An Artificial Intelligence Application in the Pharmacovigilance Ecosystem",
        "authors": [
          "Joe B Hakim",
          "Jeffery L. Painter",
          "D. Ramcharran",
          "V. Kara",
          "Greg Powell",
          "Paulina Sobczak",
          "Chiho Sato",
          "Andrew Bate",
          "Andrew Beam"
        ],
        "published_date": "2024",
        "abstract": "Large language models (LLMs) are useful tools with the capacity for performing specific types of knowledge work at an effective scale. However, LLM deployments in high-risk and safety-critical domains pose unique challenges, notably the issue of ``hallucination,'' where LLMs can generate fabricated information. This is particularly concerning in settings such as drug safety, where inaccuracies could lead to patient harm. To mitigate these risks, we have developed and demonstrated a proof of concept suite of guardrails specifically designed to mitigate certain types of hallucinations and errors for drug safety, and potentially applicable to other medical safety-critical contexts. These guardrails include mechanisms to detect anomalous documents to prevent the ingestion of inappropriate data, identify incorrect drug names or adverse event terms, and convey uncertainty in generated content. We integrated these guardrails with an LLM fine-tuned for a text-to-text task, which involves converting both structured and unstructured data within adverse event reports into natural language. This method was applied to translate individual case safety reports, demonstrating effective application in a pharmacovigilance processing task. Our guardrail framework offers a set of tools with broad applicability across various domains, ensuring LLMs can be safely used in high-risk situations by eliminating the occurrence of key errors, including the generation of incorrect pharmacovigilance-related terms, thus adhering to stringent regulatory and quality standards in medical safety-critical environments.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/b169426b9181adee0e7d6616fc12fc12611d9901.pdf",
        "venue": "arXiv.org",
        "citationCount": 0,
        "score": 0,
        "summary": "Here is a focused summary of the technical paper for a literature review, adhering to your requirements:\n\n*   **CITATION**: \\cite{hakim2024d4u}\n\n---\n\n### Technical Paper Analysis of \\cite{hakim2024d4u}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: The paper addresses the critical issue of Large Language Model (LLM) \"hallucinations\" and inaccuracies when deployed in high-risk, safety-critical domains, specifically pharmacovigilance (PV). This includes generating fabricated information, omitting key details, or failing to communicate uncertainty.\n    *   **Importance and Challenge**: In pharmacovigilance, inaccuracies in processing Individual Case Safety Reports (ICSRs) can lead to patient harm, misallocation of resources, and failure to identify genuine safety signals. The challenge lies in ensuring LLM outputs adhere to stringent regulatory and quality standards, preventing \"never event\" errors (e.g., incorrect drug identification) that are wholly preventable and unacceptable in medical contexts.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: The work positions itself within the broader context of LLM application in biology and medicine, acknowledging their potential but highlighting their inherent limitations, particularly hallucinations and uncertainty communication.\n    *   **Limitations of Previous Solutions**: While \"structural guardrails\" (ensuring consistent output format) exist, the paper focuses on the need for \"semantic guardrails\" to verify the accuracy and content of LLM output. Previous LLM deployments in safety-critical areas often lack robust mechanisms to prevent critical errors or effectively convey uncertainty, making them unsuitable for domains like pharmacovigilance where \"never event\" errors must be absolutely prevented.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes and demonstrates a suite of \"semantic guardrails\" integrated with an LLM fine-tuned for a text-to-text task. This task involves converting structured and unstructured data from adverse event reports (specifically Japanese ICSRs) into English narrative text.\n    *   **Novelty/Difference**: The core innovation is the development of both \"hard\" and \"soft\" semantic guardrails specifically engineered to mitigate defined \"never event\" errors and communicate uncertainty in a safety-critical medical context. This framework ensures adherence to regulatory standards by actively detecting and preventing key inaccuracies in LLM-generated content.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**:\n        *   **Document-wise Uncertainty Quantification (DL-UQ)**: A soft semantic guardrail that uses k-nearest neighbors on document embeddings (derived from source language encoder LLM) to identify anomalous input documents unlikely to be ICSRs, flagging them for human review or discarding.\n        *   **MISMATCH Guardrail**: A hard semantic guardrail that enforces a \"never event\" by identifying drug names or adverse event terms present in either the source or target text but not both. It utilizes custom drug dictionaries and MedDRA (Medical Dictionary for Regulatory Activities) with regular expressions to ensure consistency and prevent hallucination or omission of critical terms.\n        *   **Uncertainty Communication**: The framework includes soft guardrails designed to communicate the model's uncertainty regarding the quality and accuracy of both the input text and its final translation, thereby flagging instances requiring further human review.\n    *   **System Design/Architectural Innovations**: Integration of these guardrails into a sequential processing pipeline for LLM-based ICSR translation, ensuring that potential errors are caught at different stages (input validation, content accuracy).\n    *   **Theoretical Insights/Analysis**: Introduction and operationalization of the concept of \"never event\" errors for LLM outputs in safety-critical domains, emphasizing the need for absolute prevention through robust guardrail mechanisms.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**:\n        *   Fine-tuning and evaluation of three LLMs (mt5-xl, mpt-7b-instruct, stablelm-japanese) on a multilingual corpus of ICSRs and OPUS-100 data for text-to-text translation.\n        *   Expert human evaluation of 210 LLM-generated Japanese-to-English ICSR translations against a \"ground truth\" baseline established by independent bilingual PV experts.\n        *   Application and demonstration of the developed hard and soft semantic guardrails within the ICSR processing pipeline.\n    *   **Key Performance Metrics & Comparison Results**:\n        *   LLM translation quality was assessed using per-token perplexity, BLEU score, SACRE-BLEU score, and word error rate.\n        *   Human expert evaluation used a detailed five-category assessment system, binary evaluation criteria (presence of any error), and a five-point Likert scale for clinical acceptability.\n        *   The optimal LLM generation hyperparameters were selected based on BLEU score performance (contrastive search with Î±=0.2 and top-k=16). The guardrails were demonstrated to mitigate identified failure modes and prevent \"never event\" errors, ensuring adherence to safety standards.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The paper focuses primarily on Japanese ICSRs, although the framework is posited to be broadly applicable. The tuning of distance thresholds for the DL-UQ guardrail involves a trade-off between sensitivity and specificity, requiring careful calibration for specific applications.\n    *   **Scope of Applicability**: The framework is developed and demonstrated for pharmacovigilance, specifically for the text-to-text conversion and translation of ICSRs. However, the authors suggest its broad applicability to other medical safety-critical domains where LLM accuracy and reliability are paramount.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: This work significantly advances the safe deployment of LLMs in high-stakes medical environments by providing a concrete, implementable framework of semantic guardrails. It moves beyond general LLM safety discussions to specific, actionable mechanisms for preventing critical errors.\n    *   **Potential Impact on Future Research**: The guardrail framework establishes a precedent for integrating robust safety mechanisms into LLM pipelines for regulated industries. It highlights the importance of designing LLM applications with explicit \"never event\" prevention and uncertainty communication, paving the way for more trustworthy and clinically acceptable AI tools in healthcare and other safety-critical sectors.",
        "keywords": [
          "LLM hallucinations",
          "Pharmacovigilance",
          "Safety-critical domains",
          "Semantic guardrails",
          "Never event errors",
          "Uncertainty communication",
          "Document-wise Uncertainty Quantification (DL-UQ)",
          "MISMATCH Guardrail",
          "Individual Case Safety Reports (ICSRs)",
          "LLM fine-tuning",
          "Human expert evaluation",
          "Trustworthy AI",
          "Regulatory compliance"
        ],
        "paper_type": "based on the abstract and introduction:\n\nthe paper clearly states: \"we have developed and demonstrated a proof of concept suite of guardrails specifically designed to mitigate certain types of hallucinations and errors for drug safety...\" it describes the mechanisms of these guardrails, their integration with an llm, and their application to a specific task. the introduction highlights a technical problem (llm hallucinations in safety-critical settings) that the paper then addresses with its proposed solution.\n\nthis aligns perfectly with the criteria for a **technical** paper:\n*   abstract mentions: \"developed and demonstrated\", \"mechanisms to detect\", \"integrated\", \"method was applied\", \"our guardrail framework offers a set of tools\".\n*   introduction discusses: \"limitations that may impede their applicability\", \"phenomenon of 'hallucinations' â€“ instances of generating baseless information â€“ stands as a pivotal concern\", setting up the technical problem for which the guardrails are the proposed solution.\n\ntherefore, the paper is a **technical** paper."
      },
      "file_name": "b169426b9181adee0e7d6616fc12fc12611d9901.pdf"
    },
    {
      "success": true,
      "doc_id": "f549c82dbaf10576678370ec09bb49f3",
      "summary": "This paper, \"Lokiâ€™s Dance of Illusions: A Comprehensive Survey of Hallucination in Large Language Models\" by Li et al. \\cite{li2025qzg}, provides an in-depth analysis of hallucination phenomena in Large Language Models (LLMs), offering a novel theoretical framework and a systematic review of causes, detection, evaluation, and mitigation strategies.\n\n*   **1. Research Problem & Motivation**\n    *   **Specific Technical Problem**: Large Language Models (LLMs) frequently generate \"hallucinations\"â€”information that appears plausible but is factually inaccurate or contextually disconnected \\cite{li2025qzg}.\n    *   **Importance and Challenge**: The prevalence of hallucinations misleads users and poses significant risks in high-stakes domains like finance, law, and healthcare, potentially causing substantial economic losses, legal disputes, and health risks. A precise, unified definition and a comprehensive understanding of its underlying mechanisms are challenging but critical for developing reliable LLMs \\cite{li2025qzg}.\n\n*   **2. Related Work & Positioning**\n    *   **Relation to Existing Approaches**: This work builds upon existing surveys that have documented LLM hallucination phenomena \\cite{li2025qzg}.\n    *   **Limitations of Previous Solutions**: Previous surveys primarily focused on empirical perspectives, lacked rigorous mathematical analysis of hallucination mechanisms, employed generic taxonomies that failed to capture domain-specific manifestations, and often overlooked the inherent limitations and constraints of existing mitigation techniques \\cite{li2025qzg}.\n\n*   **3. Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes a novel, unified theoretical framework for understanding LLM hallucinations, moving beyond purely empirical observations to include mathematical origins and inevitabilities \\cite{li2025qzg}. It systematically categorizes hallucinations into factual, faithfulness, and logical inconsistencies, and analyzes their causes (mathematical, empirical), detection methods (white-box, black-box), evaluation metrics (metric-based, benchmark-based), and mitigation strategies (shifting demand, task simplification, capability enhancement) \\cite{li2025qzg}.\n    *   **Novelty/Difference**: The key innovation lies in establishing the *first unified theoretical framework* that formally addresses research fragmentation, revealing the *unavoidable nature* of some hallucinations, and developing a *task-aware evaluation taxonomy* linking semantic divergence to model architecture properties for precision diagnostics \\cite{li2025qzg}. It also provides a formal mathematical definition of hallucination based on canonical responses \\cite{li2025qzg}.\n\n*   **4. Key Technical Contributions**\n    *   **Novel Algorithms, Methods, or Techniques**:\n        *   A formal mathematical definition of hallucination, distinguishing between partial correctness and complete divergence \\cite{li2025qzg}.\n        *   A comprehensive taxonomy of hallucination types: factual, faithfulness, and logical inconsistencies \\cite{li2025qzg}.\n        *   A structured analysis of hallucination origins, including \"Mathematical Origins\" (e.g., Undecidability Principles like GÃ¶del's Incompleteness Theorems and Turing's Halting Problem, and Mathematical Constraints in LLMs) and \"Empirical Causes\" (data-related, model architecture, cognitive processing barriers) \\cite{li2025qzg}.\n        *   A detailed categorization of detection methods (white-box: embedding, logit, activation-based; black-box: consistency, confidence, auxiliary models) and mitigation strategies (refusal, calibration, RAG, knowledge graphs, prompt engineering, reflection, fine-tuning, structural optimization, decoding strategies) \\cite{li2025qzg}.\n    *   **Theoretical Insights or Analysis**: The paper provides mechanistic insights into various hallucination types by examining internal architecture and generation processes, laying a theoretical foundation for understanding the phenomenon \\cite{li2025qzg}. It also explores the \"Mathematical Inevitabilities\" of hallucinations, suggesting fundamental constraints \\cite{li2025qzg}.\n\n*   **5. Experimental Validation**\n    *   **Experiments Conducted**: As a survey paper, it does not conduct new experiments. Instead, it systematically reviews and categorizes existing experimental validation approaches and benchmarks used in the field \\cite{li2025qzg}.\n    *   **Key Performance Metrics and Comparison Results**: The paper details various evaluation methodologies, including metric-based evaluations (e.g., Accuracy, F1 Score for classification; BLEU, ROUGE, BERTScore for generation; ECE, MACROCE for confidence) and benchmark-based evaluations (e.g., HaluEval, DEFAN for general domains; task-specific benchmarks for Math, Long QA; vertical domain benchmarks for Health, Legal, Science; and method evaluation benchmarks) \\cite{li2025qzg}. This comprehensive review implicitly compares the efficacy and applicability of these metrics and benchmarks across different hallucination contexts.\n\n*   **6. Limitations & Scope**\n    *   **Technical Limitations or Assumptions**: The paper highlights that current mitigation approaches are fundamentally constrained by the \"unavoidable nature\" of hallucinations, implying inherent limitations in completely eradicating them \\cite{li2025qzg}. It also notes that existing solutions often fail to address structural weaknesses, rely heavily on task-specific assumptions, and have limited adaptability to dynamic data environments \\cite{li2025qzg}.\n    *   **Scope of Applicability**: The survey's scope covers a broad range of LLM applications, from casual use to high-stakes professional environments (finance, law, healthcare, education), emphasizing the widespread impact of hallucinations \\cite{li2025qzg}.\n\n*   **7. Technical Significance**\n    *   **Advancement of State-of-the-Art**: This survey significantly advances the technical state-of-the-art by providing the first unified theoretical framework for LLM hallucinations, integrating mathematical and empirical perspectives \\cite{li2025qzg}. It moves beyond descriptive analyses to offer mechanistic insights and a critical examination of the limitations of current solutions \\cite{li2025qzg}.\n    *   **Potential Impact on Future Research**: The paper identifies crucial gaps and suggests future research directions, such as investigating the relationship between hallucinations and the subspace of true information, and the role of confidence calibration \\cite{li2025qzg}. This comprehensive roadmap is expected to inspire and guide the development of next-generation mitigation strategies and more robust, reliable LLMs \\cite{li2025qzg}.",
      "intriguing_abstract": "The deceptive allure of Large Language Models (LLMs) often masks a critical flaw: pervasive \"hallucinations\" that generate plausible but factually inaccurate or contextually disconnected information. These fabrications pose significant risks across high-stakes domains, from finance to healthcare, demanding a profound understanding beyond empirical observation. This paper, \"Lokiâ€™s Dance of Illusions,\" introduces the *first unified theoretical framework* for LLM hallucination, fundamentally redefining the problem.\n\nWe move beyond descriptive analyses, formally defining hallucination and unveiling its deep-seated *mathematical origins* and *inherent inevitabilities*. Our comprehensive survey systematically categorizes hallucinations into *factual, faithfulness, and logical inconsistencies*, dissecting their causes, *detection methods*, *evaluation metrics*, and *mitigation strategies*. By providing mechanistic insights and a critical examination of current limitations, this work offers an indispensable roadmap for developing robust and reliable LLMs, guiding future research toward truly trustworthy artificial intelligence.",
      "keywords": [
        "Large Language Models (LLMs)",
        "Hallucination (LLMs)",
        "Unified theoretical framework",
        "Factual",
        "faithfulness",
        "logical inconsistencies",
        "Mathematical origins",
        "empirical causes",
        "Hallucination detection",
        "Hallucination mitigation strategies",
        "Unavoidable nature of hallucinations",
        "Formal mathematical definition",
        "Task-aware evaluation taxonomy",
        "High-stakes application domains",
        "Mechanistic insights"
      ],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/4d608203639087e0fe3c5d2b7a374941dd182cb7.pdf",
      "citation_key": "li2025qzg",
      "metadata": {
        "title": "Loki's Dance of Illusions: A Comprehensive Survey of Hallucination in Large Language Models",
        "authors": [
          "Chaozhuo Li",
          "Pengbo Wang",
          "Chenxu Wang",
          "Litian Zhang",
          "Zheng Liu",
          "Qiwei Ye",
          "Yuanbo Xu",
          "Feiran Huang",
          "Xi Zhang",
          "Philip S. Yu"
        ],
        "published_date": "2025",
        "abstract": "Edgar Allan Poe noted,\"Truth often lurks in the shadow of error,\"highlighting the deep complexity intrinsic to the interplay between truth and falsehood, notably under conditions of cognitive and informational asymmetry. This dynamic is strikingly evident in large language models (LLMs). Despite their impressive linguistic generation capabilities, LLMs sometimes produce information that appears factually accurate but is, in reality, fabricated, an issue often referred to as'hallucinations'. The prevalence of these hallucinations can mislead users, affecting their judgments and decisions. In sectors such as finance, law, and healthcare, such misinformation risks causing substantial economic losses, legal disputes, and health risks, with wide-ranging consequences.In our research, we have methodically categorized, analyzed the causes, detection methods, and solutions related to LLM hallucinations. Our efforts have particularly focused on understanding the roots of hallucinations and evaluating the efficacy of current strategies in revealing the underlying logic, thereby paving the way for the development of innovative and potent approaches. By examining why certain measures are effective against hallucinations, our study aims to foster a comprehensive approach to tackling this issue within the domain of LLMs.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/4d608203639087e0fe3c5d2b7a374941dd182cb7.pdf",
        "venue": "arXiv.org",
        "citationCount": 0,
        "score": 0,
        "summary": "This paper, \"Lokiâ€™s Dance of Illusions: A Comprehensive Survey of Hallucination in Large Language Models\" by Li et al. \\cite{li2025qzg}, provides an in-depth analysis of hallucination phenomena in Large Language Models (LLMs), offering a novel theoretical framework and a systematic review of causes, detection, evaluation, and mitigation strategies.\n\n*   **1. Research Problem & Motivation**\n    *   **Specific Technical Problem**: Large Language Models (LLMs) frequently generate \"hallucinations\"â€”information that appears plausible but is factually inaccurate or contextually disconnected \\cite{li2025qzg}.\n    *   **Importance and Challenge**: The prevalence of hallucinations misleads users and poses significant risks in high-stakes domains like finance, law, and healthcare, potentially causing substantial economic losses, legal disputes, and health risks. A precise, unified definition and a comprehensive understanding of its underlying mechanisms are challenging but critical for developing reliable LLMs \\cite{li2025qzg}.\n\n*   **2. Related Work & Positioning**\n    *   **Relation to Existing Approaches**: This work builds upon existing surveys that have documented LLM hallucination phenomena \\cite{li2025qzg}.\n    *   **Limitations of Previous Solutions**: Previous surveys primarily focused on empirical perspectives, lacked rigorous mathematical analysis of hallucination mechanisms, employed generic taxonomies that failed to capture domain-specific manifestations, and often overlooked the inherent limitations and constraints of existing mitigation techniques \\cite{li2025qzg}.\n\n*   **3. Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes a novel, unified theoretical framework for understanding LLM hallucinations, moving beyond purely empirical observations to include mathematical origins and inevitabilities \\cite{li2025qzg}. It systematically categorizes hallucinations into factual, faithfulness, and logical inconsistencies, and analyzes their causes (mathematical, empirical), detection methods (white-box, black-box), evaluation metrics (metric-based, benchmark-based), and mitigation strategies (shifting demand, task simplification, capability enhancement) \\cite{li2025qzg}.\n    *   **Novelty/Difference**: The key innovation lies in establishing the *first unified theoretical framework* that formally addresses research fragmentation, revealing the *unavoidable nature* of some hallucinations, and developing a *task-aware evaluation taxonomy* linking semantic divergence to model architecture properties for precision diagnostics \\cite{li2025qzg}. It also provides a formal mathematical definition of hallucination based on canonical responses \\cite{li2025qzg}.\n\n*   **4. Key Technical Contributions**\n    *   **Novel Algorithms, Methods, or Techniques**:\n        *   A formal mathematical definition of hallucination, distinguishing between partial correctness and complete divergence \\cite{li2025qzg}.\n        *   A comprehensive taxonomy of hallucination types: factual, faithfulness, and logical inconsistencies \\cite{li2025qzg}.\n        *   A structured analysis of hallucination origins, including \"Mathematical Origins\" (e.g., Undecidability Principles like GÃ¶del's Incompleteness Theorems and Turing's Halting Problem, and Mathematical Constraints in LLMs) and \"Empirical Causes\" (data-related, model architecture, cognitive processing barriers) \\cite{li2025qzg}.\n        *   A detailed categorization of detection methods (white-box: embedding, logit, activation-based; black-box: consistency, confidence, auxiliary models) and mitigation strategies (refusal, calibration, RAG, knowledge graphs, prompt engineering, reflection, fine-tuning, structural optimization, decoding strategies) \\cite{li2025qzg}.\n    *   **Theoretical Insights or Analysis**: The paper provides mechanistic insights into various hallucination types by examining internal architecture and generation processes, laying a theoretical foundation for understanding the phenomenon \\cite{li2025qzg}. It also explores the \"Mathematical Inevitabilities\" of hallucinations, suggesting fundamental constraints \\cite{li2025qzg}.\n\n*   **5. Experimental Validation**\n    *   **Experiments Conducted**: As a survey paper, it does not conduct new experiments. Instead, it systematically reviews and categorizes existing experimental validation approaches and benchmarks used in the field \\cite{li2025qzg}.\n    *   **Key Performance Metrics and Comparison Results**: The paper details various evaluation methodologies, including metric-based evaluations (e.g., Accuracy, F1 Score for classification; BLEU, ROUGE, BERTScore for generation; ECE, MACROCE for confidence) and benchmark-based evaluations (e.g., HaluEval, DEFAN for general domains; task-specific benchmarks for Math, Long QA; vertical domain benchmarks for Health, Legal, Science; and method evaluation benchmarks) \\cite{li2025qzg}. This comprehensive review implicitly compares the efficacy and applicability of these metrics and benchmarks across different hallucination contexts.\n\n*   **6. Limitations & Scope**\n    *   **Technical Limitations or Assumptions**: The paper highlights that current mitigation approaches are fundamentally constrained by the \"unavoidable nature\" of hallucinations, implying inherent limitations in completely eradicating them \\cite{li2025qzg}. It also notes that existing solutions often fail to address structural weaknesses, rely heavily on task-specific assumptions, and have limited adaptability to dynamic data environments \\cite{li2025qzg}.\n    *   **Scope of Applicability**: The survey's scope covers a broad range of LLM applications, from casual use to high-stakes professional environments (finance, law, healthcare, education), emphasizing the widespread impact of hallucinations \\cite{li2025qzg}.\n\n*   **7. Technical Significance**\n    *   **Advancement of State-of-the-Art**: This survey significantly advances the technical state-of-the-art by providing the first unified theoretical framework for LLM hallucinations, integrating mathematical and empirical perspectives \\cite{li2025qzg}. It moves beyond descriptive analyses to offer mechanistic insights and a critical examination of the limitations of current solutions \\cite{li2025qzg}.\n    *   **Potential Impact on Future Research**: The paper identifies crucial gaps and suggests future research directions, such as investigating the relationship between hallucinations and the subspace of true information, and the role of confidence calibration \\cite{li2025qzg}. This comprehensive roadmap is expected to inspire and guide the development of next-generation mitigation strategies and more robust, reliable LLMs \\cite{li2025qzg}.",
        "keywords": [
          "Large Language Models (LLMs)",
          "Hallucination (LLMs)",
          "Unified theoretical framework",
          "Factual",
          "faithfulness",
          "logical inconsistencies",
          "Mathematical origins",
          "empirical causes",
          "Hallucination detection",
          "Hallucination mitigation strategies",
          "Unavoidable nature of hallucinations",
          "Formal mathematical definition",
          "Task-aware evaluation taxonomy",
          "High-stakes application domains",
          "Mechanistic insights"
        ],
        "paper_type": "the paper explicitly and repeatedly identifies itself as a \"survey\" in its title, abstract, and introduction.\n\nhere's why:\n\n*   **title:** \"loki's dance of illusions: a comprehensive **survey** of hallucination in large language models\"\n*   **abstract:** mentions \"methodically categorized, analyzed the causes, detection methods, and solutions related to llm hallucinations,\" and \"index terms â€”llm hallucination **survey**.\" it aims to \"foster a **comprehensive approach**.\"\n*   **introduction:**\n    *   states \"the widespread prevalence of hallucinatory phenomena in llms highlights the critical need for a **comprehensive survey** to thoroughly examine current research advancements and mitigation strategies.\"\n    *   compares itself to \"**existing surveys**\" and identifies \"critical limitations persist in **existing surveys**.\"\n    *   declares, \"in **this survey**, our work addresses these gaps through three fundamental contributions.\"\n    *   outlines the paper's aim: \"this **survey** aims to conduct an in-depth analysis... the objective is to develop a **comprehensive framework**...\"\n    *   the organization (\"first, we present the mathematical origins... second, we explore the empirical causes... third, we provide an evaluation... fourth, we discuss detection techniques... and finally, we propose mitigating methods\") is typical of a literature review or survey.\n    *   the contributions (mechanistic insights, comprehensive methods for evaluation/detection/mitigation, exploration of future research directions) are all characteristic of a survey that synthesizes existing knowledge and points to future work.\n\nwhile it includes formal definitions and proposes a new taxonomy, these are presented as part of a comprehensive review and synthesis of the field, which is characteristic of an advanced survey.\n\n**classification:** survey"
      },
      "file_name": "4d608203639087e0fe3c5d2b7a374941dd182cb7.pdf"
    },
    {
      "success": true,
      "doc_id": "90d8ca352c2c44c547ca3e04ea2a9bb9",
      "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Large Vision-Language Models (LVLMs) suffer from \"fine-grained object hallucination,\" where they generate inaccurate object attributes and behaviors that are non-existent or incorrect in the input image \\cite{wang2023ubf}.\n    *   **Importance & Challenge**: Existing evaluation methods for LVLMs primarily focus on \"coarse-grained\" object hallucination (i.e., the presence or absence of entire objects). They fail to measure or mitigate these finer details, which compromises model performance and user experience in real-world applications \\cite{wang2023ubf}.\n\n*   **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: The paper builds upon instruction-tuned LVLMs (e.g., LLaVA, MiniGPT-4) and existing hallucination evaluation methods like POPE \\cite{li2023pope}.\n    *   **Limitations of Previous Solutions**: Previous work, including POPE, predominantly addresses coarse-grained object hallucination (whether an object exists). They do not account for or evaluate the accuracy of object attributes (e.g., color, size) or behaviors (e.g., actions) \\cite{wang2023ubf}.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes `ReCaption`, a framework consisting of two main components:\n        1.  **Caption Rewriting**: Utilizes a two-stage prompting strategy with ChatGPT to generate multiple diverse, high-quality rewritten captions for training images.\n            *   **Stage 1 (Keyword Extraction)**: Extracts verbs, nouns, and adjectives from original captions to preserve essential information related to objects, attributes, and behaviors.\n            *   **Stage 2 (Caption Generation)**: Uses these extracted keywords to prompt ChatGPT to generate new, varied captions, ensuring semantic consistency while introducing diversity \\cite{wang2023ubf}.\n        2.  **Additional Tuning**: Fine-tunes instruction-tuned LVLMs on these newly generated rewritten image-caption pairs using cross-entropy loss.\n    *   **Novelty**: `ReCaption` is novel in its approach to enriching training data specifically for fine-grained visual-text alignment by leveraging a powerful LLM (ChatGPT) for controlled caption rewriting. This targeted fine-tuning aims to reduce hallucination at a granular level. The introduction of `Fine-Grained Object Hallucination Evaluation (FGHE)` is also a key innovation for measuring this specific type of hallucination \\cite{wang2023ubf}.\n\n*   **Key Technical Contributions**\n    *   **Novel Framework (`ReCaption`)**: A two-component framework for mitigating fine-grained object hallucination in LVLMs through ChatGPT-driven caption rewriting and subsequent fine-tuning \\cite{wang2023ubf}.\n    *   **Two-Stage Prompting Strategy**: A specific method for generating high-quality, diverse, and semantically consistent rewritten captions by extracting keywords and then using them for guided generation \\cite{wang2023ubf}.\n    *   **Novel Evaluation Method (`FGHE`)**: Introduces `Fine-Grained Object Hallucination Evaluation (FGHE)`, a probing-based method that extends existing techniques (like POPE) to specifically assess hallucinated object attributes, behaviors, and multi-object relationships using binary Yes-or-No questions \\cite{wang2023ubf}.\n    *   **FGHE Dataset**: A manually annotated dataset of 50 images and 200 binary questions categorized into multiple-object, attribute, and behavior questions for fine-grained evaluation \\cite{wang2023ubf}.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted**: The paper evaluates the effectiveness of `ReCaption` in reducing fine-grained object hallucination across different LVLM options. It also assesses the improvement in text generation quality \\cite{wang2023ubf}.\n    *   **Key Performance Metrics**: Evaluation is performed using both the existing POPE metric (for coarse-grained hallucination) and the newly proposed `FGHE` metric (for fine-grained hallucination). Metrics include Accuracy, Precision, Recall, and F1 scores, where higher scores indicate less hallucination \\cite{wang2023ubf}.\n    *   **Comparison Results**: The results demonstrate that LVLMs adopting the `ReCaption` framework effectively reduce fine-grained object hallucination and improve text generation quality \\cite{wang2023ubf}.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations**: The `ReCaption` framework relies on an external, potentially closed-source LLM (ChatGPT) for caption rewriting, which could introduce dependencies and potential biases from the LLM itself. The `FGHE` dataset, while novel, is relatively small (50 images, 200 questions) compared to large-scale benchmarks.\n    *   **Scope of Applicability**: `ReCaption` is designed to be LVLM-agnostic, meaning it can be applied to various instruction-tuned LVLMs (e.g., MiniGPT-4, LLaVA, mPLUG-Owl) with minimal modifications \\cite{wang2023ubf}.\n\n*   **Technical Significance**\n    *   **Advancement of State-of-the-Art**: This work significantly advances the technical state-of-the-art by identifying and providing a targeted solution for fine-grained hallucination, a critical and previously underexplored aspect of LVLM performance \\cite{wang2023ubf}.\n    *   **Potential Impact**: It offers a practical and computationally efficient method to improve the factual consistency of LVLM outputs, making them more reliable for real-world applications. The introduction of `FGHE` provides a crucial tool for future research to rigorously evaluate and compare methods for mitigating fine-grained hallucination, fostering further development in robust multimodal AI \\cite{wang2023ubf}.",
      "intriguing_abstract": "Large Vision-Language Models (LVLMs) are transforming AI, yet their real-world reliability is critically undermined by persistent \"fine-grained object hallucination\"â€”the generation of non-existent or incorrect object attributes and behaviors. Existing evaluation methods predominantly address only coarse-grained object presence, failing to capture these crucial granular inaccuracies.\n\nWe introduce `ReCaption`, a novel framework designed to directly combat this challenge. `ReCaption` employs a sophisticated two-stage prompting strategy with ChatGPT to generate diverse, high-quality rewritten captions, enriching training data for instruction-tuned LVLMs. By fine-tuning models with these semantically consistent image-caption pairs using cross-entropy loss, we significantly enhance fine-grained visual-text alignment. To rigorously assess this improvement, we also present `Fine-Grained Object Hallucination Evaluation (FGHE)`, a novel probing-based method and dataset that extends existing techniques to specifically measure hallucinated object attributes, behaviors, and multi-object relationships. Our experiments demonstrate that `ReCaption` effectively reduces fine-grained hallucination and improves text generation quality across various LVLMs. This work offers a crucial step towards building more factually consistent and reliable multimodal AI systems, providing both a practical solution and a vital evaluation tool for future research.",
      "keywords": [
        "Large Vision-Language Models (LVLMs)",
        "fine-grained object hallucination",
        "ReCaption framework",
        "caption rewriting",
        "two-stage prompting strategy",
        "ChatGPT",
        "Fine-Grained Object Hallucination Evaluation (FGHE)",
        "object attributes and behaviors",
        "instruction-tuned LVLMs",
        "FGHE dataset",
        "factual consistency",
        "multimodal AI"
      ],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/06b2ac5153e3d8d05c13c82f93d7f4e13eee6d0f.pdf",
      "citation_key": "wang2023ubf",
      "metadata": {
        "title": "Mitigating Fine-Grained Hallucination by Fine-Tuning Large Vision-Language Models with Caption Rewrites",
        "authors": [
          "Lei Wang",
          "Jiabang He",
          "Shenshen Li",
          "Ning Liu",
          "Ee-Peng Lim"
        ],
        "published_date": "2023",
        "abstract": "Large language models (LLMs) have shown remarkable performance in natural language processing (NLP) tasks. To comprehend and execute diverse human instructions over image data, instruction-tuned large vision-language models (LVLMs) have been introduced. However, LVLMs may suffer from different types of object hallucinations. Nevertheless, LVLMs are evaluated for coarse-grained object hallucinations only (i.e., generated objects non-existent in the input image). The fine-grained object attributes and behaviors non-existent in the image may still be generated but not measured by the current evaluation methods. In this paper, we thus focus on reducing fine-grained hallucinations of LVLMs. We propose \\textit{ReCaption}, a framework that consists of two components: rewriting captions using ChatGPT and fine-tuning the instruction-tuned LVLMs on the rewritten captions. We also propose a fine-grained probing-based evaluation method named \\textit{Fine-Grained Object Hallucination Evaluation} (\\textit{FGHE}). Our experiment results demonstrate that ReCaption effectively reduces fine-grained object hallucination for different LVLM options and improves their text generation quality. The code can be found at https://github.com/Anonymousanoy/FOHE.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/06b2ac5153e3d8d05c13c82f93d7f4e13eee6d0f.pdf",
        "venue": "Conference on Multimedia Modeling",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Large Vision-Language Models (LVLMs) suffer from \"fine-grained object hallucination,\" where they generate inaccurate object attributes and behaviors that are non-existent or incorrect in the input image \\cite{wang2023ubf}.\n    *   **Importance & Challenge**: Existing evaluation methods for LVLMs primarily focus on \"coarse-grained\" object hallucination (i.e., the presence or absence of entire objects). They fail to measure or mitigate these finer details, which compromises model performance and user experience in real-world applications \\cite{wang2023ubf}.\n\n*   **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: The paper builds upon instruction-tuned LVLMs (e.g., LLaVA, MiniGPT-4) and existing hallucination evaluation methods like POPE \\cite{li2023pope}.\n    *   **Limitations of Previous Solutions**: Previous work, including POPE, predominantly addresses coarse-grained object hallucination (whether an object exists). They do not account for or evaluate the accuracy of object attributes (e.g., color, size) or behaviors (e.g., actions) \\cite{wang2023ubf}.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes `ReCaption`, a framework consisting of two main components:\n        1.  **Caption Rewriting**: Utilizes a two-stage prompting strategy with ChatGPT to generate multiple diverse, high-quality rewritten captions for training images.\n            *   **Stage 1 (Keyword Extraction)**: Extracts verbs, nouns, and adjectives from original captions to preserve essential information related to objects, attributes, and behaviors.\n            *   **Stage 2 (Caption Generation)**: Uses these extracted keywords to prompt ChatGPT to generate new, varied captions, ensuring semantic consistency while introducing diversity \\cite{wang2023ubf}.\n        2.  **Additional Tuning**: Fine-tunes instruction-tuned LVLMs on these newly generated rewritten image-caption pairs using cross-entropy loss.\n    *   **Novelty**: `ReCaption` is novel in its approach to enriching training data specifically for fine-grained visual-text alignment by leveraging a powerful LLM (ChatGPT) for controlled caption rewriting. This targeted fine-tuning aims to reduce hallucination at a granular level. The introduction of `Fine-Grained Object Hallucination Evaluation (FGHE)` is also a key innovation for measuring this specific type of hallucination \\cite{wang2023ubf}.\n\n*   **Key Technical Contributions**\n    *   **Novel Framework (`ReCaption`)**: A two-component framework for mitigating fine-grained object hallucination in LVLMs through ChatGPT-driven caption rewriting and subsequent fine-tuning \\cite{wang2023ubf}.\n    *   **Two-Stage Prompting Strategy**: A specific method for generating high-quality, diverse, and semantically consistent rewritten captions by extracting keywords and then using them for guided generation \\cite{wang2023ubf}.\n    *   **Novel Evaluation Method (`FGHE`)**: Introduces `Fine-Grained Object Hallucination Evaluation (FGHE)`, a probing-based method that extends existing techniques (like POPE) to specifically assess hallucinated object attributes, behaviors, and multi-object relationships using binary Yes-or-No questions \\cite{wang2023ubf}.\n    *   **FGHE Dataset**: A manually annotated dataset of 50 images and 200 binary questions categorized into multiple-object, attribute, and behavior questions for fine-grained evaluation \\cite{wang2023ubf}.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted**: The paper evaluates the effectiveness of `ReCaption` in reducing fine-grained object hallucination across different LVLM options. It also assesses the improvement in text generation quality \\cite{wang2023ubf}.\n    *   **Key Performance Metrics**: Evaluation is performed using both the existing POPE metric (for coarse-grained hallucination) and the newly proposed `FGHE` metric (for fine-grained hallucination). Metrics include Accuracy, Precision, Recall, and F1 scores, where higher scores indicate less hallucination \\cite{wang2023ubf}.\n    *   **Comparison Results**: The results demonstrate that LVLMs adopting the `ReCaption` framework effectively reduce fine-grained object hallucination and improve text generation quality \\cite{wang2023ubf}.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations**: The `ReCaption` framework relies on an external, potentially closed-source LLM (ChatGPT) for caption rewriting, which could introduce dependencies and potential biases from the LLM itself. The `FGHE` dataset, while novel, is relatively small (50 images, 200 questions) compared to large-scale benchmarks.\n    *   **Scope of Applicability**: `ReCaption` is designed to be LVLM-agnostic, meaning it can be applied to various instruction-tuned LVLMs (e.g., MiniGPT-4, LLaVA, mPLUG-Owl) with minimal modifications \\cite{wang2023ubf}.\n\n*   **Technical Significance**\n    *   **Advancement of State-of-the-Art**: This work significantly advances the technical state-of-the-art by identifying and providing a targeted solution for fine-grained hallucination, a critical and previously underexplored aspect of LVLM performance \\cite{wang2023ubf}.\n    *   **Potential Impact**: It offers a practical and computationally efficient method to improve the factual consistency of LVLM outputs, making them more reliable for real-world applications. The introduction of `FGHE` provides a crucial tool for future research to rigorously evaluate and compare methods for mitigating fine-grained hallucination, fostering further development in robust multimodal AI \\cite{wang2023ubf}.",
        "keywords": [
          "Large Vision-Language Models (LVLMs)",
          "fine-grained object hallucination",
          "ReCaption framework",
          "caption rewriting",
          "two-stage prompting strategy",
          "ChatGPT",
          "Fine-Grained Object Hallucination Evaluation (FGHE)",
          "object attributes and behaviors",
          "instruction-tuned LVLMs",
          "FGHE dataset",
          "factual consistency",
          "multimodal AI"
        ],
        "paper_type": "based on the abstract and introduction:\n\n1.  **abstract analysis:**\n    *   identifies a problem: \"lvlms may suffer from different types of object hallucinations,\" specifically \"fine-grained object attributes and behaviors non-existent in the image may still be generated but not measured.\"\n    *   states the paper's focus: \"reducing fine-grained hallucinations of lvlms.\"\n    *   **proposes a new framework:** \"we **propose recaption**, a framework that consists of two components...\"\n    *   **proposes a new evaluation method:** \"we also **propose a fine-grained probing-based evaluation method** named fine-grained object hallucination evaluation (fghe).\"\n    *   presents results: \"our **experiment results demonstrate that recaption effectively reduces** fine-grained object hallucination...\"\n    *   mentions code availability, indicating a practical implementation.\n\n2.  **introduction analysis (first part):**\n    *   provides background on llms and lvlms.\n    *   discusses existing approaches and their limitations (e.g., \"requires good alignment across modalities\").\n    *   sets the stage for the paper's proposed solution to address these limitations.\n\n**classification matching:**\n\n*   the abstract explicitly uses \"propose\" twice for a new framework and a new method. this is a strong indicator for **technical**.\n*   it describes the components of the proposed framework (\"rewriting captions using chatgpt and fine-tuning...\").\n*   it mentions \"experiment results demonstrate\" the effectiveness of the proposed solution, which is an empirical validation of the technical contribution.\n\nwhile the paper includes empirical results, its core contribution is the **development and presentation of new methods and a framework (recaption and fghe)** to address a specific technical problem (fine-grained hallucination in lvlms). this aligns perfectly with the definition of a **technical** paper.\n\nthe other categories do not fit as well:\n*   it's not a **survey** as its primary goal isn't to review existing literature comprehensively.\n*   it's not purely **theoretical** as it focuses on practical methods and empirical validation, not mathematical proofs or formal models.\n*   while it has **empirical** results, the main contribution is the *creation* of the system/method, not just a data-driven study of an existing phenomenon or comparison of existing methods.\n*   it's not a **case_study** as it addresses a general problem in lvlms, not a specific application.\n*   it's not a **position** paper as it offers concrete solutions and evaluations, not just arguments or future visions.\n*   there are no indicators that it is a **short** paper.\n\n**conclusion:** the paper presents new methods and a system to solve a technical problem, followed by experimental validation.\n\n**classification:** technical"
      },
      "file_name": "06b2ac5153e3d8d05c13c82f93d7f4e13eee6d0f.pdf"
    },
    {
      "success": true,
      "doc_id": "85eb679858cc5b9d98fe9498ab31180b",
      "summary": "Here's a focused summary of the paper for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem**: The paper addresses the persistent issue of hallucination in Large Language Models (LLMs), specifically highlighting gaps in existing evaluation benchmarks.\n    *   **Importance and Challenge**:\n        *   Current benchmarks often feature hallucinated content that is *intentionally induced* rather than *naturally generated* by LLMs in typical usage \\cite{chen2024c4k}.\n        *   Most benchmarks *only focus on factuality hallucination*, neglecting *faithfulness hallucination* (divergence from instructions, self-consistency) \\cite{chen2024c4k}.\n        *   Existing evaluations are primarily at the *sentence-level or passage-level*, despite the increasing prevalence and complexity of *multi-turn dialogue patterns* in LLM applications, which can exhibit unique and challenging types of hallucination \\cite{chen2024c4k}.\n\n*   **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: `DiaHalu` is positioned as a novel benchmark that overcomes several limitations of prior work.\n    *   **Limitations of Previous Solutions**:\n        *   **Generation Method**: Benchmarks like FactCollect, HADES, and BEGIN are either manually generated or use conventional LMs (BERT, T5), not naturally generated by LLMs, making them less representative of real-world LLM outputs \\cite{chen2024c4k}.\n        *   **Hallucination Types**: Many benchmarks (e.g., those for QA tasks, FactCHD) predominantly focus on *factuality hallucination*, overlooking the critical aspect of *faithfulness hallucination* (coherence, relevance, consistency) \\cite{chen2024c4k}.\n        *   **Evaluation Level**: Prior work primarily offers sentence-level or passage-level hallucination detection, failing to address the complexities and unique challenges of hallucination within *multi-turn dialogue contexts* \\cite{chen2024c4k}. `DiaHalu` explicitly addresses this dialogue-level gap.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method**: `DiaHalu` is constructed through a three-step process:\n        1.  **LLM Self-Dialogue Generation**: Topics are integrated into system prompts, and two LLMs (ChatGPT3.5/GPT4) engage in multi-turn dialogues \\cite{chen2024c4k}.\n        2.  **Human-Aligned Content Modification**: For knowledge-grounded and task-oriented dialogues (simulating human-machine interaction), responses from one LLM (acting as the user) that do not conform to human language conventions are *manually modified*, and the LLMs are prompted to re-generate, ensuring realistic user input \\cite{chen2024c4k}.\n        3.  **Expert Annotation**: Professional scholars meticulously annotate all samples, identifying hallucination instances, their subtypes, and locations, along with explanations \\cite{chen2024c4k}.\n    *   **Novelty/Differentiation**:\n        *   **Dialogue-Level Focus**: It is the first dedicated benchmark for dialogue-level hallucination evaluation \\cite{chen2024c4k}.\n        *   **Comprehensive Hallucination Taxonomy**: Extends beyond simple factuality to include five subtypes: Non-factual, Incoherence (input-conflicting, context-conflicting, self-conflicting), Irrelevance, Overreliance, and Reasoning Error \\cite{chen2024c4k}.\n        *   **Multi-Domain Coverage**: Encompasses four diverse multi-turn dialogue domains: knowledge-grounded, task-oriented, chit-chat, and reasoning \\cite{chen2024c4k}.\n        *   **Natural Generation Simulation**: The manual modification and re-generation step for specific domains simulates authentic human-machine interaction, leading to more naturally generated hallucination scenarios.\n\n*   **Key Technical Contributions**\n    *   **Novel Benchmark**: Introduction of `DiaHalu`, the first dedicated dialogue-level hallucination detection benchmark for LLMs \\cite{chen2024c4k}.\n    *   **Extended Hallucination Taxonomy**: Proposes a more granular classification of hallucination, including five subtypes that cover both factuality and faithfulness, making it more applicable to real-world LLM interactions \\cite{chen2024c4k}.\n    *   **Multi-Domain Dialogue Data**: Provides a diverse dataset covering four distinct multi-turn dialogue domains, enriching the scope of hallucination evaluation \\cite{chen2024c4k}.\n    *   **High-Quality Annotation**: Achieves almost perfect inter-annotator agreement (Fleissâ€™s Kappa of 0.8842) through a rigorous annotation process involving seasoned researchers and experts \\cite{chen2024c4k}.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted**: The paper states that experiments were conducted using well-known LLMs and existing hallucination detection methods on the `DiaHalu` benchmark \\cite{chen2024c4k}.\n    *   **Key Performance Metrics and Results**: The experimental results consistently indicate that `DiaHalu` is a *highly challenging benchmark* for both current LLMs and existing detection methods \\cite{chen2024c4k}. This suggests that current models and techniques struggle with dialogue-level and nuanced hallucination types.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations**: The manual modification step for certain dialogue domains, while enhancing realism, introduces human intervention into the generation process. The benchmark's generation relies on specific LLMs (ChatGPT3.5 and GPT4).\n    *   **Scope of Applicability**: While comprehensive, the benchmark focuses on four specific dialogue domains and five defined hallucination subtypes. Other, less common, hallucination types or dialogue scenarios might not be fully covered.\n\n*   **Technical Significance**\n    *   **Advancement of State-of-the-Art**: `DiaHalu` significantly advances the technical state-of-the-art by providing the first benchmark specifically designed for dialogue-level hallucination, a critical and under-explored area in LLM evaluation \\cite{chen2024c4k}.\n    *   **Potential Impact on Future Research**: Its challenging nature and comprehensive taxonomy are expected to stimulate future research into more robust hallucination detection, mitigation, and generation techniques for conversational AI, leading to more reliable and trustworthy LLMs in real-world applications \\cite{chen2024c4k}.",
      "intriguing_abstract": "The pervasive issue of hallucination in Large Language Models (LLMs) severely limits their reliability, yet current evaluation benchmarks fall short, often focusing on induced, sentence-level errors and neglecting critical faithfulness aspects in complex multi-turn dialogues. We introduce `DiaHalu`, the first dedicated benchmark designed to rigorously evaluate **dialogue-level hallucination** in LLMs. Unlike prior work, `DiaHalu` features naturally generated hallucination scenarios across four diverse multi-turn dialogue domains: knowledge-grounded, task-oriented, chit-chat, and reasoning. Our novel, comprehensive **hallucination taxonomy** extends beyond mere factuality to encompass five critical subtypes, including **incoherence, irrelevance, overreliance, and reasoning errors**, capturing the nuanced failures of conversational AI. Constructed through LLM self-dialogue and human-aligned modification, then meticulously annotated by experts (Fleissâ€™s Kappa of 0.8842), `DiaHalu` proves to be a highly challenging benchmark for existing LLMs and detection methods. This pioneering work not only exposes significant gaps in current capabilities but also provides an essential tool to drive the development of more robust, trustworthy, and truly intelligent conversational agents.",
      "keywords": [
        "Large Language Models (LLMs)",
        "hallucination",
        "DiaHalu benchmark",
        "dialogue-level hallucination",
        "multi-turn dialogue",
        "factuality and faithfulness hallucination",
        "extended hallucination taxonomy",
        "LLM self-dialogue generation",
        "human-aligned content modification",
        "expert annotation",
        "multi-domain dialogue data",
        "challenging benchmark",
        "conversational AI"
      ],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/83d81e31f5c32f6989d98be1133adfc08db094ce.pdf",
      "citation_key": "chen2024c4k",
      "metadata": {
        "title": "DiaHalu: A Dialogue-level Hallucination Evaluation Benchmark for Large Language Models",
        "authors": [
          "Kedi Chen",
          "Qin Chen",
          "Jie Zhou",
          "Yishen He",
          "Liang He"
        ],
        "published_date": "2024",
        "abstract": "Since large language models (LLMs) achieve significant success in recent years, the hallucination issue remains a challenge, numerous benchmarks are proposed to detect the hallucination. Nevertheless, some of these benchmarks are not naturally generated by LLMs but are intentionally induced. Also, many merely focus on the factuality hallucination while ignoring the faithfulness hallucination. Additionally, although dialogue pattern is more widely utilized in the era of LLMs, current benchmarks only concentrate on sentence-level and passage-level hallucination. In this study, we propose DiaHalu, the first dialogue-level hallucination evaluation benchmark to our knowledge. Initially, we integrate the collected topics into system prompts and facilitate a dialogue between two ChatGPT3.5. Subsequently, we manually modify the contents that do not adhere to human language conventions and then have LLMs re-generate, simulating authentic human-machine interaction scenarios. Finally, professional scholars annotate all the samples in the dataset. DiaHalu covers four common multi-turn dialogue domains and five hallucination subtypes, extended from factuality and faithfulness hallucination. Experiments through some well-known LLMs and detection methods on the dataset show that DiaHalu is a challenging benchmark, holding significant value for further research.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/83d81e31f5c32f6989d98be1133adfc08db094ce.pdf",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the paper for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem**: The paper addresses the persistent issue of hallucination in Large Language Models (LLMs), specifically highlighting gaps in existing evaluation benchmarks.\n    *   **Importance and Challenge**:\n        *   Current benchmarks often feature hallucinated content that is *intentionally induced* rather than *naturally generated* by LLMs in typical usage \\cite{chen2024c4k}.\n        *   Most benchmarks *only focus on factuality hallucination*, neglecting *faithfulness hallucination* (divergence from instructions, self-consistency) \\cite{chen2024c4k}.\n        *   Existing evaluations are primarily at the *sentence-level or passage-level*, despite the increasing prevalence and complexity of *multi-turn dialogue patterns* in LLM applications, which can exhibit unique and challenging types of hallucination \\cite{chen2024c4k}.\n\n*   **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: `DiaHalu` is positioned as a novel benchmark that overcomes several limitations of prior work.\n    *   **Limitations of Previous Solutions**:\n        *   **Generation Method**: Benchmarks like FactCollect, HADES, and BEGIN are either manually generated or use conventional LMs (BERT, T5), not naturally generated by LLMs, making them less representative of real-world LLM outputs \\cite{chen2024c4k}.\n        *   **Hallucination Types**: Many benchmarks (e.g., those for QA tasks, FactCHD) predominantly focus on *factuality hallucination*, overlooking the critical aspect of *faithfulness hallucination* (coherence, relevance, consistency) \\cite{chen2024c4k}.\n        *   **Evaluation Level**: Prior work primarily offers sentence-level or passage-level hallucination detection, failing to address the complexities and unique challenges of hallucination within *multi-turn dialogue contexts* \\cite{chen2024c4k}. `DiaHalu` explicitly addresses this dialogue-level gap.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method**: `DiaHalu` is constructed through a three-step process:\n        1.  **LLM Self-Dialogue Generation**: Topics are integrated into system prompts, and two LLMs (ChatGPT3.5/GPT4) engage in multi-turn dialogues \\cite{chen2024c4k}.\n        2.  **Human-Aligned Content Modification**: For knowledge-grounded and task-oriented dialogues (simulating human-machine interaction), responses from one LLM (acting as the user) that do not conform to human language conventions are *manually modified*, and the LLMs are prompted to re-generate, ensuring realistic user input \\cite{chen2024c4k}.\n        3.  **Expert Annotation**: Professional scholars meticulously annotate all samples, identifying hallucination instances, their subtypes, and locations, along with explanations \\cite{chen2024c4k}.\n    *   **Novelty/Differentiation**:\n        *   **Dialogue-Level Focus**: It is the first dedicated benchmark for dialogue-level hallucination evaluation \\cite{chen2024c4k}.\n        *   **Comprehensive Hallucination Taxonomy**: Extends beyond simple factuality to include five subtypes: Non-factual, Incoherence (input-conflicting, context-conflicting, self-conflicting), Irrelevance, Overreliance, and Reasoning Error \\cite{chen2024c4k}.\n        *   **Multi-Domain Coverage**: Encompasses four diverse multi-turn dialogue domains: knowledge-grounded, task-oriented, chit-chat, and reasoning \\cite{chen2024c4k}.\n        *   **Natural Generation Simulation**: The manual modification and re-generation step for specific domains simulates authentic human-machine interaction, leading to more naturally generated hallucination scenarios.\n\n*   **Key Technical Contributions**\n    *   **Novel Benchmark**: Introduction of `DiaHalu`, the first dedicated dialogue-level hallucination detection benchmark for LLMs \\cite{chen2024c4k}.\n    *   **Extended Hallucination Taxonomy**: Proposes a more granular classification of hallucination, including five subtypes that cover both factuality and faithfulness, making it more applicable to real-world LLM interactions \\cite{chen2024c4k}.\n    *   **Multi-Domain Dialogue Data**: Provides a diverse dataset covering four distinct multi-turn dialogue domains, enriching the scope of hallucination evaluation \\cite{chen2024c4k}.\n    *   **High-Quality Annotation**: Achieves almost perfect inter-annotator agreement (Fleissâ€™s Kappa of 0.8842) through a rigorous annotation process involving seasoned researchers and experts \\cite{chen2024c4k}.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted**: The paper states that experiments were conducted using well-known LLMs and existing hallucination detection methods on the `DiaHalu` benchmark \\cite{chen2024c4k}.\n    *   **Key Performance Metrics and Results**: The experimental results consistently indicate that `DiaHalu` is a *highly challenging benchmark* for both current LLMs and existing detection methods \\cite{chen2024c4k}. This suggests that current models and techniques struggle with dialogue-level and nuanced hallucination types.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations**: The manual modification step for certain dialogue domains, while enhancing realism, introduces human intervention into the generation process. The benchmark's generation relies on specific LLMs (ChatGPT3.5 and GPT4).\n    *   **Scope of Applicability**: While comprehensive, the benchmark focuses on four specific dialogue domains and five defined hallucination subtypes. Other, less common, hallucination types or dialogue scenarios might not be fully covered.\n\n*   **Technical Significance**\n    *   **Advancement of State-of-the-Art**: `DiaHalu` significantly advances the technical state-of-the-art by providing the first benchmark specifically designed for dialogue-level hallucination, a critical and under-explored area in LLM evaluation \\cite{chen2024c4k}.\n    *   **Potential Impact on Future Research**: Its challenging nature and comprehensive taxonomy are expected to stimulate future research into more robust hallucination detection, mitigation, and generation techniques for conversational AI, leading to more reliable and trustworthy LLMs in real-world applications \\cite{chen2024c4k}.",
        "keywords": [
          "Large Language Models (LLMs)",
          "hallucination",
          "DiaHalu benchmark",
          "dialogue-level hallucination",
          "multi-turn dialogue",
          "factuality and faithfulness hallucination",
          "extended hallucination taxonomy",
          "LLM self-dialogue generation",
          "human-aligned content modification",
          "expert annotation",
          "multi-domain dialogue data",
          "challenging benchmark",
          "conversational AI"
        ],
        "paper_type": "the paper should be classified as **technical**.\n\nhere's why:\n\n1.  **\"propose\", \"develop\", \"present\"**: the abstract explicitly states, \"in this study, we propose diahalu, the first dedicated dialogue-level hallucination evaluation benchmark for llms to our knowledge.\" this directly aligns with the \"technical\" criterion of presenting new methods, algorithms, or systems (a benchmark is a type of system/resource).\n2.  **\"method\"**: the abstract details the methodology for creating diahalu: \"initially, we integrate the collected topics... facilitate a dialogue... manually modify... have llms re-generate... professional scholars annotate all the samples...\" this describes the technical process of developing the benchmark.\n3.  **focus on the artifact**: the title \"diahalu: a dialogue-level hallucination evaluation benchmark...\" emphasizes the creation and presentation of the benchmark itself as the core contribution.\n\nwhile the paper does include an empirical component (\"experiments with the well-known llms and detection methods show that diahalu is a challenging benchmark\"), this empirical evaluation serves to validate the utility and challenge of the *newly proposed benchmark*. the primary contribution is the development and presentation of diahalu, making it fundamentally a technical paper."
      },
      "file_name": "83d81e31f5c32f6989d98be1133adfc08db094ce.pdf"
    },
    {
      "success": true,
      "doc_id": "c6b9a6ae329a2b69a72cf9af2ce80ed9",
      "summary": "Here's a focused summary of the technical paper for a literature review, emphasizing technical innovations and empirical validation:\n\n### Focused Summary for Literature Review\n\n**1. Research Problem & Motivation**\n*   **Specific Technical Problem:** Large Language Models (LLMs) frequently generate factually incorrect or nonsensical outputs, known as hallucinations. These can be categorized into:\n    *   **Internal Hallucinations:** Arise from LLMs' limited parametric knowledge, especially for recent or domain-specific information \\cite{ding20244yr}.\n    *   **External Hallucinations:** Occur in Retrieval-Augmented Generation (RAG) systems when irrelevant or erroneous external information is incorporated, leading to compromised output accuracy \\cite{ding20244yr}.\n*   **Importance & Challenge:** Hallucinations pose significant risks in practical LLM deployments, undermining trust and reliability. The challenge lies in effectively balancing the utilization of LLMs' internal parametric knowledge with external information to mitigate both types of hallucinations without introducing new errors or unnecessary computational overhead \\cite{ding20244yr}.\n\n**2. Related Work & Positioning**\n*   **Existing Approaches:**\n    *   **Self-improvement methods (e.g., self-reflection, multi-model debate):** Leverage LLMs' parametric knowledge and reasoning capabilities to enhance logical consistency \\cite{ding20244yr}.\n    *   **Retrieval-Augmented Generation (RAG):** Integrates external documents to overcome knowledge limitations \\cite{ding20244yr}.\n    *   **Adaptive Retrieval Methods:** Assess question difficulty or response confidence to decide when to retrieve documents \\cite{ding20244yr}.\n*   **Limitations of Previous Solutions:**\n    *   Self-improvement methods are often constrained by LLMs' inherent knowledge boundaries and reasoning chain complexity, leading to internal hallucinations \\cite{ding20244yr}.\n    *   RAG methods are susceptible to external hallucinations due to the risk of incorporating irrelevant evidence, leading to error accumulation \\cite{ding20244yr}.\n    *   Existing consistency-based methods may fail when LLMs provide consistent but incorrect answers, often due to focusing solely on semantic coherence within a single language or model \\cite{ding20244yr}.\n\n**3. Technical Approach & Innovation**\n*   **Core Technical Method:** The paper introduces **Rowen** (Retrieve only when it needs), an adaptive retrieval augmentation framework that enhances LLMs by dynamically deciding when to retrieve external information based on a novel consistency-based hallucination detection module \\cite{ding20244yr}.\n*   **Novelty/Differentiation:**\n    *   **Adaptive Retrieval Trigger:** Unlike always-on RAG or simpler confidence-based adaptive methods, Rowen activates retrieval *only* when high uncertainty (inconsistency) is detected in LLM responses, balancing internal reasoning and external evidence \\cite{ding20244yr}.\n    *   **Consistency-based Hallucination Detection:** Rowen's detection module goes beyond single-language/model consistency by evaluating semantic inconsistencies across:\n        *   **Cross-Language (Rowen-CL):** Responses to the same query translated into different languages \\cite{ding20244yr}.\n        *   **Cross-Model (Rowen-CM):** Responses to the same query generated by different LLMs (e.g., a primary model and a verifier model) \\cite{ding20244yr}.\n    *   **Three-Stage Framework:**\n        1.  **Stage 1: Generating Initial Answer:** Uses Chain-of-Thought (CoT) reasoning to produce a preliminary response \\cite{ding20244yr}.\n        2.  **Stage 2: Deciding Whether to Retrieve:** Employs the consistency-based detection module (cross-language, cross-model, or hybrid) to assess the reliability of the initial response. If the consistency score falls below a threshold, retrieval is triggered \\cite{ding20244yr}.\n        3.  **Stage 3: Retrieval Augmented Generation:** If retrieval is triggered, relevant external knowledge is searched (e.g., via web search API) and used to repair and refine the initial hallucinated content \\cite{ding20244yr}.\n\n**4. Key Technical Contributions**\n*   **Novel Algorithms/Methods:**\n    *   A novel **consistency-based hallucination detection module** that leverages semantic inconsistencies across diverse language verbalizations and different LLMs (cross-language and cross-model consistency checks) to estimate model uncertainty \\cite{ding20244yr}.\n    *   An **adaptive retrieval strategy** that intelligently decides *when* to engage external knowledge retrieval, thereby mitigating both internal and external hallucinations and optimizing retrieval efficiency \\cite{ding20244yr}.\n*   **System Design/Architectural Innovations:** The Rowen framework integrates CoT reasoning, multi-faceted consistency checking, and targeted retrieval-augmented generation into a cohesive pipeline for robust hallucination mitigation \\cite{ding20244yr}.\n\n**5. Experimental Validation**\n*   **Experiments Conducted:**\n    *   Evaluated on the **TruthfulQA dataset** (generation task) to assess LLMs' ability to produce truthful responses \\cite{ding20244yr}.\n    *   Evaluated on the **StrategyQA dataset** (yes/no questions requiring multi-step reasoning) to assess accuracy \\cite{ding20244yr}.\n    *   Ablation studies and hyperparameter analysis were conducted (details in Appendix D) \\cite{ding20244yr}.\n*   **Key Performance Metrics & Comparison Results:**\n    *   **TruthfulQA:** Rowen achieved a GPT-Judge score of 59.34%, a substantial improvement of +16.74% over the state-of-the-art (SOTA) baseline. It also reported BLEU and Rouge-L scores \\cite{ding20244yr}.\n    *   **StrategyQA:** Rowen achieved an accuracy of 75.60%, surpassing existing self-improvement and RAG-based baselines \\cite{ding20244yr}.\n    *   **Efficiency:** The adaptive retrieval strategy significantly reduced unnecessary retrievals, enhancing the efficiency of RAG systems \\cite{ding20244yr}.\n    *   **Baselines:** Compared against Vanilla LLMs (ChatGPT), Self-improvement methods (CoVe, Multi-agent Debate, Self-Reflection), Retrieval-augmented methods (Factool, Detect-and-Mitigate), and Adaptive retrieval methods (FLARE, Adaptive-Retrieval, Self-RAG, Adaptive-RAG, LUQ) \\cite{ding20244yr}.\n    *   **Implementation Details:** Used ChatGPT (gpt-3.5-turbo) as the primary LLM, Qwen-Max-0428 as the verifier LM for cross-model checks, and Google Search API for retrieval \\cite{ding20244yr}.\n\n**6. Limitations & Scope**\n*   **Technical Limitations/Assumptions:** The paper does not explicitly state limitations of the Rowen framework itself within the provided abstract and introduction. However, potential implicit limitations could include:\n    *   Reliance on the quality of semantic perturbations and translations for consistency checks.\n    *   The effectiveness of the consistency check depends on the chosen verifier model (for Rowen-CM) and the target language (for Rowen-CL).\n    *   The performance is sensitive to hyperparameters like the consistency score threshold and the weight factor `Î±` for hybrid detection \\cite{ding20244yr}.\n*   **Scope of Applicability:** Rowen is designed for factual question-answering and generation tasks where hallucination mitigation is critical. Its adaptive nature makes it suitable for scenarios where balancing internal knowledge and external retrieval is beneficial for accuracy and efficiency \\cite{ding20244yr}.\n\n**7. Technical Significance**\n*   **Advancement of State-of-the-Art:** Rowen significantly advances the technical state-of-the-art in both detecting and mitigating hallucinations in LLM outputs, as evidenced by its superior performance on TruthfulQA and StrategyQA datasets \\cite{ding20244yr}.\n*   **Potential Impact on Future Research:**\n    *   Introduces a novel paradigm for adaptive retrieval, moving beyond simple confidence scores to multi-faceted consistency checks for uncertainty estimation.\n    *   Highlights the utility of cross-language and cross-model inconsistencies as robust indicators for hallucination detection.\n    *   Provides a framework for more efficient RAG systems by reducing unnecessary retrievals, which can lead to cost savings and faster response times in real-world applications \\cite{ding20244yr}.\n    *   Encourages further research into dynamic, context-aware strategies for integrating parametric and external knowledge in LLMs.",
      "intriguing_abstract": "The pervasive problem of factual inaccuracies, or \"hallucinations,\" in Large Language Models (LLMs) critically undermines their reliability and trustworthiness. Existing mitigation strategies, whether relying on parametric knowledge or Retrieval-Augmented Generation (RAG), often introduce new errors or inefficiencies. We introduce **Rowen** (Retrieve only when it needs), a novel adaptive retrieval augmentation framework designed to dynamically combat both internal and external hallucinations with unparalleled precision.\n\nRowen's core innovation is a sophisticated, consistency-based hallucination detection module. Unlike prior methods, Rowen intelligently triggers external knowledge retrieval *only* when significant uncertainty is detected via semantic inconsistencies. This is uniquely achieved by evaluating responses across diverse linguistic verbalizations (Cross-Language consistency) or from multiple LLMs (Cross-Model consistency). This dynamic approach ensures efficient resource utilization and robustly identifies potential factual errors.\n\nEmpirical validation on challenging benchmarks demonstrates Rowen's superior performance. It achieves a remarkable +16.74% improvement over state-of-the-art baselines on TruthfulQA and significantly boosts accuracy on StrategyQA, drastically reducing unnecessary retrievals. Rowen represents a critical advancement towards building more reliable and efficient LLM applications, offering a powerful paradigm for integrating parametric and external knowledge.",
      "keywords": [
        "Rowen framework",
        "LLM hallucinations",
        "Retrieval-Augmented Generation (RAG)",
        "adaptive retrieval strategy",
        "consistency-based hallucination detection",
        "cross-language consistency",
        "cross-model consistency",
        "internal hallucinations",
        "external hallucinations",
        "hallucination mitigation",
        "retrieval efficiency",
        "TruthfulQA dataset",
        "StrategyQA dataset",
        "state-of-the-art advancement"
      ],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/25243632a6159c19db280e2f0064aa59562a518a.pdf",
      "citation_key": "ding20244yr",
      "metadata": {
        "title": "Retrieve Only When It Needs: Adaptive Retrieval Augmentation for Hallucination Mitigation in Large Language Models",
        "authors": [
          "Hanxing Ding",
          "Liang Pang",
          "Zihao Wei",
          "Huawei Shen",
          "Xueqi Cheng"
        ],
        "published_date": "2024",
        "abstract": "Hallucinations pose a significant challenge for the practical implementation of large language models (LLMs). The utilization of parametric knowledge in generating factual content is constrained by the limited knowledge of LLMs, potentially resulting in internal hallucinations. While incorporating external information can help fill knowledge gaps, it also introduces the risk of irrelevant information, thereby increasing the likelihood of external hallucinations. A careful and balanced integration of the parametric knowledge within LLMs with external information is crucial to alleviate hallucinations. In this study, we present Rowen, a novel approach that enhances LLMs with a selective retrieval augmentation process tailored to address hallucinated outputs. This process is governed by a multilingual semantic-aware detection module, which evaluates the consistency of the perturbed responses across various languages for the same queries. Upon detecting inconsistencies indicative of hallucinations, Rowen activates the retrieval of external information to rectify the model outputs. Rowen adeptly harmonizes the intrinsic parameters in LLMs with external knowledge sources, effectively mitigating hallucinations by ensuring a balanced integration of internal reasoning and external evidence. Through a comprehensive empirical analysis, we demonstrate that Rowen surpasses the current state-of-the-art in both detecting and mitigating hallucinated content within the outputs of LLMs.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/25243632a6159c19db280e2f0064aa59562a518a.pdf",
        "venue": "arXiv.org",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the technical paper for a literature review, emphasizing technical innovations and empirical validation:\n\n### Focused Summary for Literature Review\n\n**1. Research Problem & Motivation**\n*   **Specific Technical Problem:** Large Language Models (LLMs) frequently generate factually incorrect or nonsensical outputs, known as hallucinations. These can be categorized into:\n    *   **Internal Hallucinations:** Arise from LLMs' limited parametric knowledge, especially for recent or domain-specific information \\cite{ding20244yr}.\n    *   **External Hallucinations:** Occur in Retrieval-Augmented Generation (RAG) systems when irrelevant or erroneous external information is incorporated, leading to compromised output accuracy \\cite{ding20244yr}.\n*   **Importance & Challenge:** Hallucinations pose significant risks in practical LLM deployments, undermining trust and reliability. The challenge lies in effectively balancing the utilization of LLMs' internal parametric knowledge with external information to mitigate both types of hallucinations without introducing new errors or unnecessary computational overhead \\cite{ding20244yr}.\n\n**2. Related Work & Positioning**\n*   **Existing Approaches:**\n    *   **Self-improvement methods (e.g., self-reflection, multi-model debate):** Leverage LLMs' parametric knowledge and reasoning capabilities to enhance logical consistency \\cite{ding20244yr}.\n    *   **Retrieval-Augmented Generation (RAG):** Integrates external documents to overcome knowledge limitations \\cite{ding20244yr}.\n    *   **Adaptive Retrieval Methods:** Assess question difficulty or response confidence to decide when to retrieve documents \\cite{ding20244yr}.\n*   **Limitations of Previous Solutions:**\n    *   Self-improvement methods are often constrained by LLMs' inherent knowledge boundaries and reasoning chain complexity, leading to internal hallucinations \\cite{ding20244yr}.\n    *   RAG methods are susceptible to external hallucinations due to the risk of incorporating irrelevant evidence, leading to error accumulation \\cite{ding20244yr}.\n    *   Existing consistency-based methods may fail when LLMs provide consistent but incorrect answers, often due to focusing solely on semantic coherence within a single language or model \\cite{ding20244yr}.\n\n**3. Technical Approach & Innovation**\n*   **Core Technical Method:** The paper introduces **Rowen** (Retrieve only when it needs), an adaptive retrieval augmentation framework that enhances LLMs by dynamically deciding when to retrieve external information based on a novel consistency-based hallucination detection module \\cite{ding20244yr}.\n*   **Novelty/Differentiation:**\n    *   **Adaptive Retrieval Trigger:** Unlike always-on RAG or simpler confidence-based adaptive methods, Rowen activates retrieval *only* when high uncertainty (inconsistency) is detected in LLM responses, balancing internal reasoning and external evidence \\cite{ding20244yr}.\n    *   **Consistency-based Hallucination Detection:** Rowen's detection module goes beyond single-language/model consistency by evaluating semantic inconsistencies across:\n        *   **Cross-Language (Rowen-CL):** Responses to the same query translated into different languages \\cite{ding20244yr}.\n        *   **Cross-Model (Rowen-CM):** Responses to the same query generated by different LLMs (e.g., a primary model and a verifier model) \\cite{ding20244yr}.\n    *   **Three-Stage Framework:**\n        1.  **Stage 1: Generating Initial Answer:** Uses Chain-of-Thought (CoT) reasoning to produce a preliminary response \\cite{ding20244yr}.\n        2.  **Stage 2: Deciding Whether to Retrieve:** Employs the consistency-based detection module (cross-language, cross-model, or hybrid) to assess the reliability of the initial response. If the consistency score falls below a threshold, retrieval is triggered \\cite{ding20244yr}.\n        3.  **Stage 3: Retrieval Augmented Generation:** If retrieval is triggered, relevant external knowledge is searched (e.g., via web search API) and used to repair and refine the initial hallucinated content \\cite{ding20244yr}.\n\n**4. Key Technical Contributions**\n*   **Novel Algorithms/Methods:**\n    *   A novel **consistency-based hallucination detection module** that leverages semantic inconsistencies across diverse language verbalizations and different LLMs (cross-language and cross-model consistency checks) to estimate model uncertainty \\cite{ding20244yr}.\n    *   An **adaptive retrieval strategy** that intelligently decides *when* to engage external knowledge retrieval, thereby mitigating both internal and external hallucinations and optimizing retrieval efficiency \\cite{ding20244yr}.\n*   **System Design/Architectural Innovations:** The Rowen framework integrates CoT reasoning, multi-faceted consistency checking, and targeted retrieval-augmented generation into a cohesive pipeline for robust hallucination mitigation \\cite{ding20244yr}.\n\n**5. Experimental Validation**\n*   **Experiments Conducted:**\n    *   Evaluated on the **TruthfulQA dataset** (generation task) to assess LLMs' ability to produce truthful responses \\cite{ding20244yr}.\n    *   Evaluated on the **StrategyQA dataset** (yes/no questions requiring multi-step reasoning) to assess accuracy \\cite{ding20244yr}.\n    *   Ablation studies and hyperparameter analysis were conducted (details in Appendix D) \\cite{ding20244yr}.\n*   **Key Performance Metrics & Comparison Results:**\n    *   **TruthfulQA:** Rowen achieved a GPT-Judge score of 59.34%, a substantial improvement of +16.74% over the state-of-the-art (SOTA) baseline. It also reported BLEU and Rouge-L scores \\cite{ding20244yr}.\n    *   **StrategyQA:** Rowen achieved an accuracy of 75.60%, surpassing existing self-improvement and RAG-based baselines \\cite{ding20244yr}.\n    *   **Efficiency:** The adaptive retrieval strategy significantly reduced unnecessary retrievals, enhancing the efficiency of RAG systems \\cite{ding20244yr}.\n    *   **Baselines:** Compared against Vanilla LLMs (ChatGPT), Self-improvement methods (CoVe, Multi-agent Debate, Self-Reflection), Retrieval-augmented methods (Factool, Detect-and-Mitigate), and Adaptive retrieval methods (FLARE, Adaptive-Retrieval, Self-RAG, Adaptive-RAG, LUQ) \\cite{ding20244yr}.\n    *   **Implementation Details:** Used ChatGPT (gpt-3.5-turbo) as the primary LLM, Qwen-Max-0428 as the verifier LM for cross-model checks, and Google Search API for retrieval \\cite{ding20244yr}.\n\n**6. Limitations & Scope**\n*   **Technical Limitations/Assumptions:** The paper does not explicitly state limitations of the Rowen framework itself within the provided abstract and introduction. However, potential implicit limitations could include:\n    *   Reliance on the quality of semantic perturbations and translations for consistency checks.\n    *   The effectiveness of the consistency check depends on the chosen verifier model (for Rowen-CM) and the target language (for Rowen-CL).\n    *   The performance is sensitive to hyperparameters like the consistency score threshold and the weight factor `Î±` for hybrid detection \\cite{ding20244yr}.\n*   **Scope of Applicability:** Rowen is designed for factual question-answering and generation tasks where hallucination mitigation is critical. Its adaptive nature makes it suitable for scenarios where balancing internal knowledge and external retrieval is beneficial for accuracy and efficiency \\cite{ding20244yr}.\n\n**7. Technical Significance**\n*   **Advancement of State-of-the-Art:** Rowen significantly advances the technical state-of-the-art in both detecting and mitigating hallucinations in LLM outputs, as evidenced by its superior performance on TruthfulQA and StrategyQA datasets \\cite{ding20244yr}.\n*   **Potential Impact on Future Research:**\n    *   Introduces a novel paradigm for adaptive retrieval, moving beyond simple confidence scores to multi-faceted consistency checks for uncertainty estimation.\n    *   Highlights the utility of cross-language and cross-model inconsistencies as robust indicators for hallucination detection.\n    *   Provides a framework for more efficient RAG systems by reducing unnecessary retrievals, which can lead to cost savings and faster response times in real-world applications \\cite{ding20244yr}.\n    *   Encourages further research into dynamic, context-aware strategies for integrating parametric and external knowledge in LLMs.",
        "keywords": [
          "Rowen framework",
          "LLM hallucinations",
          "Retrieval-Augmented Generation (RAG)",
          "adaptive retrieval strategy",
          "consistency-based hallucination detection",
          "cross-language consistency",
          "cross-model consistency",
          "internal hallucinations",
          "external hallucinations",
          "hallucination mitigation",
          "retrieval efficiency",
          "TruthfulQA dataset",
          "StrategyQA dataset",
          "state-of-the-art advancement"
        ],
        "paper_type": "based on the abstract and introduction:\n\n*   the abstract explicitly states: \"in this study, we present **rowen , a novel framework** that enhances llms with an adaptive retrieval augmentation process...\" and \"rowen **introduces a consistency-based hallucination detection module**...\". these phrases clearly indicate the development and presentation of a new system or method.\n*   the introduction sets the stage by discussing the problem (hallucinations in llms) that the proposed framework (rowen) aims to solve.\n*   while \"comprehensive empirical experiments\" are mentioned, they serve to demonstrate the effectiveness of the *new framework* rather than being the sole focus of the paper as a data-driven study without a novel method. the primary contribution is the creation of rowen.\n\nthis aligns best with the criteria for a **technical** paper.\n\n**classification: technical**"
      },
      "file_name": "25243632a6159c19db280e2f0064aa59562a518a.pdf"
    },
    {
      "success": true,
      "doc_id": "f3ee5c77b1c5752d3ad5759983077019",
      "summary": "Here's a focused summary of the paper for a literature review, emphasizing technical innovations and empirical validation:\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Enhancing the reliability and robustness of Multimodal Large Language Models (MLLMs) through self-improvement, specifically addressing hallucination control.\n    *   **Importance and Challenge**: Current MLLM self-improvement methods heavily rely on MLLMs themselves as judges for feedback and verification \\cite{deng202405j}. This leads to:\n        *   High computational costs due to running large models in the verification loop.\n        *   Potential pitfalls like reward hacking and model collapse, stemming from the inherent biases and limitations of MLLMs acting as their own evaluators.\n        *   Resource inefficiency, as many samples are generated but only a small subset is used.\n\n*   **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: Previous self-improvement methods for MLLMs (e.g., RLAIF-V \\cite{deng202405j}, M3ID \\cite{deng202405j}, POVID \\cite{deng202405j}, STIC \\cite{deng202405j}) typically involve sampling responses and then using another MLLM as a reward model or verifier to create preference learning pairs. Some generate negative samples through techniques like bad prompts or image corruption.\n    *   **Limitations of Previous Solutions**: These methods are limited by:\n        *   Heavy reliance on the quality and unbiased nature of the MLLM verifier.\n        *   Significant computational expense, especially when a large model is required for detailed evaluation or reasoning.\n        *   Inefficiency in data utilization, as a large volume of data is generated but only a fraction is deemed useful.\n    *   **Positioning**: This work proposes a \"model-level judge-free\" framework that eliminates the need for MLLMs in the verification loop, offering a more efficient and robust alternative to existing MLLM-as-judge paradigms \\cite{deng202405j}. It extends the idea of external verifiers (like CLIP-DPO \\cite{deng202405j}) to handle long captions.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method**: The framework comprises three main stages:\n        1.  **Controllable Dataset Generation**: The seed MLLM generates both positive and negative response candidates for a given image and instruction. Negative samples are generated by introducing a \"hallucination ratio\" (`hratio`) during the decoding process. This `hratio` blends token distributions from a conditional path (with image) and an unconditional path (without image), allowing precise control over the level of hallucination injected \\cite{deng202405j}.\n        2.  **Lightweight Preference Data Inversion**: A lightweight, contrastive language-image encoder (specifically, a CLIP model) is used to evaluate the quality of the generated positive and negative captions. It computes average sentence-level CLIP scores for each caption. If the initially designated \"positive\" caption scores lower than the \"negative\" caption, their labels are automatically swapped to ensure accurate preference learning \\cite{deng202405j}.\n        3.  **Preference Learning Finetuning**: The refined preference pairs are then used to finetune the seed MLLM using Direct Preference Optimization (DPO) \\cite{deng202405j}, a simpler and more stable alternative to RLHF/RLAIF.\n    *   **Novelty/Difference**: The key innovation is the \"judge-free\" nature of the verification process. Instead of relying on another MLLM, the framework uses a predefined, controllable mechanism for negative sample generation combined with a lightweight, objective verifier (CLIP-score) for data quality control and label inversion. This significantly reduces computational costs and mitigates the biases associated with MLLM-as-judge approaches \\cite{deng202405j}.\n\n*   **Key Technical Contributions**\n    *   **Novel Framework**: An efficient, model-level judge-free self-improvement framework for MLLMs that integrates a controllable hallucination mechanism for negative sample generation and a lightweight verifier for robust data quality control \\cite{deng202405j}.\n    *   **Controllable Negative Sample Generation**: A method to generate preference learning pairs by dynamically controlling the hallucination level in responses using a `hratio` to blend conditional and unconditional decoding paths \\cite{deng202405j}.\n    *   **Lightweight Verification and Data Inversion**: Utilization of a lightweight CLIP model to objectively evaluate and automatically reverse preference labels based on average sentence-level CLIP scores, ensuring high-quality training data without MLLM-level computational overhead \\cite{deng202405j}.\n    *   **New Evaluation Dataset**: Introduction of the \"IC dataset,\" specifically designed to comprehensively challenge MLLMs' hallucination control by evaluating both precision and recall of generated captions, with GPT-4o-assisted evaluation \\cite{deng202405j}.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted**: The proposed framework was evaluated by training a self-improved model (`m1`) from a seed model (`m0`) using the generated and verified preference data. The improved model's performance was then assessed on two benchmarks.\n    *   **Key Performance Metrics and Comparison Results**:\n        *   **Seed Model**: LLaVA-1.5-13B \\cite{deng202405j}.\n        *   **Datasets**: The newly collected IC dataset (150 challenging images, evaluated by GPT-4o for precision, recall, and F1 score) and the public Object HalBench dataset \\cite{deng202405j}.\n        *   **Data Generation**: 100k image-instruction pairs from the LLaVA instruction tuning dataset were used to generate preference pairs with varying `hratio` \\cite{deng202405j}.\n        *   **Verification**: A CLIP model was used to compute average sentence-level CLIP scores, and pairs were swapped if the initial negative caption scored higher than the positive \\cite{deng202405j}.\n        *   **Training**: DPO was applied to a subset of the preference dataset, with training conducted on an 8xA100 node \\cite{deng202405j}.\n        *   **Results**: The self-improved model (`m1`) demonstrated \"significantly better performance\" over the original seed model (`m0`) on both the IC and Object HalBench datasets \\cite{deng202405j}. The method achieved \"superior precision and recall with significantly lower computational demands\" compared to conventional techniques \\cite{deng202405j}.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: While not explicitly detailed in the provided text, the \"lightweight\" nature of the verifier (CLIP model) implies a reliance on its inherent capabilities for semantic similarity. The effectiveness of the `hratio` for controlled hallucination generation is also a key assumption. The paper focuses on improving MLLM reliability and robustness, particularly concerning hallucination.\n    *   **Scope of Applicability**: The method is primarily applicable to MLLMs for self-improvement tasks, particularly those involving caption generation and hallucination control. It offers a general framework for generating and filtering preference data efficiently.\n\n*   **Technical Significance**\n    *   **Advancement of State-of-the-Art**: This work advances the technical state-of-the-art by providing an efficient and scalable pathway for MLLM self-improvement that bypasses the computational and reliability issues of MLLM-as-judge approaches \\cite{deng202405j}. It demonstrates that high-quality preference data can be generated and verified without relying on expensive and potentially biased large models.\n    *   **Potential Impact on Future Research**: The judge-free framework could inspire future research into more resource-efficient and robust self-improvement mechanisms for large models, reducing the barriers to entry for training and finetuning. It opens avenues for exploring alternative lightweight verifiers and more sophisticated controllable data generation strategies, ultimately leading to more reliable and trustworthy MLLMs \\cite{deng202405j}.",
      "intriguing_abstract": "Multimodal Large Language Models (MLLMs) often suffer from debilitating hallucinations, yet current self-improvement methods, which rely on MLLMs themselves as judges for feedback, are computationally prohibitive and prone to inherent biases like reward hacking. We introduce a novel **\"model-level judge-free\" framework** that revolutionizes MLLM self-improvement by eliminating the need for expensive and fallible MLLM verifiers.\n\nOur approach features a **controllable dataset generation** mechanism, where a unique `hratio` dynamically blends token distributions from conditional and unconditional decoding paths to precisely inject and control hallucination levels in negative samples. This is coupled with a **lightweight preference data inversion** stage utilizing a CLIP model for objective, sentence-level evaluation and automatic label correction, ensuring high-quality preference pairs. These refined pairs then finetune the MLLM via **Direct Preference Optimization (DPO)**. Experiments on our new IC dataset and the Object HalBench demonstrate significantly improved hallucination control, achieving superior precision and recall with dramatically lower computational demands. This framework offers an efficient, robust, and scalable pathway towards more reliable MLLMs, bypassing the limitations of MLLM-as-judge paradigms and setting a new standard for self-improvement.",
      "keywords": [
        "Multimodal Large Language Models (MLLMs)",
        "MLLM self-improvement",
        "hallucination control",
        "model-level judge-free framework",
        "controllable hallucination ratio (hratio)",
        "lightweight CLIP-based verification",
        "Direct Preference Optimization (DPO)",
        "controllable negative sample generation",
        "lightweight preference data inversion",
        "IC dataset",
        "improved MLLM reliability and robustness",
        "reduced computational costs",
        "superior precision and recall"
      ],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/a2f44fc0f0c24fd4ab848f01a770a68dfa114f62.pdf",
      "citation_key": "deng202405j",
      "metadata": {
        "title": "Efficient Self-Improvement in Multimodal Large Language Models: A Model-Level Judge-Free Approach",
        "authors": [
          "Shijian Deng",
          "Wentian Zhao",
          "Yu-Jhe Li",
          "Kun Wan",
          "Daniel Miranda",
          "Ajinkya Kale",
          "Yapeng Tian"
        ],
        "published_date": "2024",
        "abstract": "Self-improvement in multimodal large language models (MLLMs) is crucial for enhancing their reliability and robustness. However, current methods often rely heavily on MLLMs themselves as judges, leading to high computational costs and potential pitfalls like reward hacking and model collapse. This paper introduces a novel, model-level judge-free self-improvement framework. Our approach employs a controlled feedback mechanism while eliminating the need for MLLMs in the verification loop. We generate preference learning pairs using a controllable hallucination mechanism and optimize data quality by leveraging lightweight, contrastive language-image encoders to evaluate and reverse pairs when necessary. Evaluations across public benchmarks and our newly introduced IC dataset designed to challenge hallucination control demonstrate that our model outperforms conventional techniques. We achieve superior precision and recall with significantly lower computational demands. This method offers an efficient pathway to scalable self-improvement in MLLMs, balancing performance gains with reduced resource requirements.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/a2f44fc0f0c24fd4ab848f01a770a68dfa114f62.pdf",
        "venue": "arXiv.org",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the paper for a literature review, emphasizing technical innovations and empirical validation:\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Enhancing the reliability and robustness of Multimodal Large Language Models (MLLMs) through self-improvement, specifically addressing hallucination control.\n    *   **Importance and Challenge**: Current MLLM self-improvement methods heavily rely on MLLMs themselves as judges for feedback and verification \\cite{deng202405j}. This leads to:\n        *   High computational costs due to running large models in the verification loop.\n        *   Potential pitfalls like reward hacking and model collapse, stemming from the inherent biases and limitations of MLLMs acting as their own evaluators.\n        *   Resource inefficiency, as many samples are generated but only a small subset is used.\n\n*   **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: Previous self-improvement methods for MLLMs (e.g., RLAIF-V \\cite{deng202405j}, M3ID \\cite{deng202405j}, POVID \\cite{deng202405j}, STIC \\cite{deng202405j}) typically involve sampling responses and then using another MLLM as a reward model or verifier to create preference learning pairs. Some generate negative samples through techniques like bad prompts or image corruption.\n    *   **Limitations of Previous Solutions**: These methods are limited by:\n        *   Heavy reliance on the quality and unbiased nature of the MLLM verifier.\n        *   Significant computational expense, especially when a large model is required for detailed evaluation or reasoning.\n        *   Inefficiency in data utilization, as a large volume of data is generated but only a fraction is deemed useful.\n    *   **Positioning**: This work proposes a \"model-level judge-free\" framework that eliminates the need for MLLMs in the verification loop, offering a more efficient and robust alternative to existing MLLM-as-judge paradigms \\cite{deng202405j}. It extends the idea of external verifiers (like CLIP-DPO \\cite{deng202405j}) to handle long captions.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method**: The framework comprises three main stages:\n        1.  **Controllable Dataset Generation**: The seed MLLM generates both positive and negative response candidates for a given image and instruction. Negative samples are generated by introducing a \"hallucination ratio\" (`hratio`) during the decoding process. This `hratio` blends token distributions from a conditional path (with image) and an unconditional path (without image), allowing precise control over the level of hallucination injected \\cite{deng202405j}.\n        2.  **Lightweight Preference Data Inversion**: A lightweight, contrastive language-image encoder (specifically, a CLIP model) is used to evaluate the quality of the generated positive and negative captions. It computes average sentence-level CLIP scores for each caption. If the initially designated \"positive\" caption scores lower than the \"negative\" caption, their labels are automatically swapped to ensure accurate preference learning \\cite{deng202405j}.\n        3.  **Preference Learning Finetuning**: The refined preference pairs are then used to finetune the seed MLLM using Direct Preference Optimization (DPO) \\cite{deng202405j}, a simpler and more stable alternative to RLHF/RLAIF.\n    *   **Novelty/Difference**: The key innovation is the \"judge-free\" nature of the verification process. Instead of relying on another MLLM, the framework uses a predefined, controllable mechanism for negative sample generation combined with a lightweight, objective verifier (CLIP-score) for data quality control and label inversion. This significantly reduces computational costs and mitigates the biases associated with MLLM-as-judge approaches \\cite{deng202405j}.\n\n*   **Key Technical Contributions**\n    *   **Novel Framework**: An efficient, model-level judge-free self-improvement framework for MLLMs that integrates a controllable hallucination mechanism for negative sample generation and a lightweight verifier for robust data quality control \\cite{deng202405j}.\n    *   **Controllable Negative Sample Generation**: A method to generate preference learning pairs by dynamically controlling the hallucination level in responses using a `hratio` to blend conditional and unconditional decoding paths \\cite{deng202405j}.\n    *   **Lightweight Verification and Data Inversion**: Utilization of a lightweight CLIP model to objectively evaluate and automatically reverse preference labels based on average sentence-level CLIP scores, ensuring high-quality training data without MLLM-level computational overhead \\cite{deng202405j}.\n    *   **New Evaluation Dataset**: Introduction of the \"IC dataset,\" specifically designed to comprehensively challenge MLLMs' hallucination control by evaluating both precision and recall of generated captions, with GPT-4o-assisted evaluation \\cite{deng202405j}.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted**: The proposed framework was evaluated by training a self-improved model (`m1`) from a seed model (`m0`) using the generated and verified preference data. The improved model's performance was then assessed on two benchmarks.\n    *   **Key Performance Metrics and Comparison Results**:\n        *   **Seed Model**: LLaVA-1.5-13B \\cite{deng202405j}.\n        *   **Datasets**: The newly collected IC dataset (150 challenging images, evaluated by GPT-4o for precision, recall, and F1 score) and the public Object HalBench dataset \\cite{deng202405j}.\n        *   **Data Generation**: 100k image-instruction pairs from the LLaVA instruction tuning dataset were used to generate preference pairs with varying `hratio` \\cite{deng202405j}.\n        *   **Verification**: A CLIP model was used to compute average sentence-level CLIP scores, and pairs were swapped if the initial negative caption scored higher than the positive \\cite{deng202405j}.\n        *   **Training**: DPO was applied to a subset of the preference dataset, with training conducted on an 8xA100 node \\cite{deng202405j}.\n        *   **Results**: The self-improved model (`m1`) demonstrated \"significantly better performance\" over the original seed model (`m0`) on both the IC and Object HalBench datasets \\cite{deng202405j}. The method achieved \"superior precision and recall with significantly lower computational demands\" compared to conventional techniques \\cite{deng202405j}.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: While not explicitly detailed in the provided text, the \"lightweight\" nature of the verifier (CLIP model) implies a reliance on its inherent capabilities for semantic similarity. The effectiveness of the `hratio` for controlled hallucination generation is also a key assumption. The paper focuses on improving MLLM reliability and robustness, particularly concerning hallucination.\n    *   **Scope of Applicability**: The method is primarily applicable to MLLMs for self-improvement tasks, particularly those involving caption generation and hallucination control. It offers a general framework for generating and filtering preference data efficiently.\n\n*   **Technical Significance**\n    *   **Advancement of State-of-the-Art**: This work advances the technical state-of-the-art by providing an efficient and scalable pathway for MLLM self-improvement that bypasses the computational and reliability issues of MLLM-as-judge approaches \\cite{deng202405j}. It demonstrates that high-quality preference data can be generated and verified without relying on expensive and potentially biased large models.\n    *   **Potential Impact on Future Research**: The judge-free framework could inspire future research into more resource-efficient and robust self-improvement mechanisms for large models, reducing the barriers to entry for training and finetuning. It opens avenues for exploring alternative lightweight verifiers and more sophisticated controllable data generation strategies, ultimately leading to more reliable and trustworthy MLLMs \\cite{deng202405j}.",
        "keywords": [
          "Multimodal Large Language Models (MLLMs)",
          "MLLM self-improvement",
          "hallucination control",
          "model-level judge-free framework",
          "controllable hallucination ratio (hratio)",
          "lightweight CLIP-based verification",
          "Direct Preference Optimization (DPO)",
          "controllable negative sample generation",
          "lightweight preference data inversion",
          "IC dataset",
          "improved MLLM reliability and robustness",
          "reduced computational costs",
          "superior precision and recall"
        ],
        "paper_type": "the paper should be classified as **technical**.\n\nhere's why:\n\n*   **abstract:**\n    *   \"this paper introduces a novel, model-level judge-free self-improvement framework.\" - directly indicates a new system/method.\n    *   \"our approach employs a controlled feedback mechanism...\" - describes the proposed method.\n    *   \"we generate preference learning pairs using a controllable hallucination mechanism and optimize data quality by leveraging lightweight, contrastive language-image encoders...\" - details specific technical components and steps of their new approach.\n    *   while it mentions \"evaluations across public benchmarks\" and \"demonstrate that our model outperforms conventional techniques,\" these are presented as validation for the *new method* rather than the primary focus of an empirical study on an existing phenomenon.\n*   **introduction:**\n    *   it sets the context by discussing existing approaches to self-improvement in mllms, but immediately pivots to introducing their own.\n    *   figure 1 explicitly compares \"conventional improvement,\" \"self-improvement,\" and \"our efficient self-improvement paradigm,\" clearly positioning their work as a new paradigm/method.\n    *   the text explicitly states \"(c) our efficient self-improvement paradigm improves the model without human feedback or model-level self-feedback by using a predefined data generation strategy...\" - this is a direct statement of a proposed solution/method.\n\nthe core contribution is the development and presentation of a new framework and approach for self-improvement in mllms, which aligns perfectly with the definition of a **technical** paper. the empirical evaluation serves to validate this new technical contribution."
      },
      "file_name": "a2f44fc0f0c24fd4ab848f01a770a68dfa114f62.pdf"
    },
    {
      "success": true,
      "doc_id": "f41b80a99d2425ef3cb16374721c7985",
      "summary": "Here's a focused summary of the technical paper for a literature review:\n\n### Analysis of \"ICT: Image-Object Cross-Level Trusted Intervention for Mitigating Object Hallucination in Large Vision-Language Models\" \\cite{chen2024j0g}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Large Vision-Language Models (LVLMs) frequently suffer from \"object hallucination,\" where they generate text inconsistent with visual input, such as incorrectly identifying object presence or inaccurately describing attributes \\cite{chen2024j0g}.\n    *   **Importance and Challenge**: This hallucination tendency severely limits the practical application of LVLMs in real-world scenarios demanding high precision (e.g., autonomous driving, medical surgery) \\cite{chen2024j0g}. The problem is challenging due to:\n        *   **Overly Strong Language Priors**: Powerful Large Language Models (LLMs) within LVLMs often dominate weaker visual encoders, leading to an excessive reliance on language cues over visual input \\cite{chen2024j0g}.\n        *   **Lack of Fine-Grained Visual Semantics**: Current visual decoders struggle to capture detailed visual features, resulting in errors regarding object attributes like color and quantity \\cite{chen2024j0g}.\n\n2.  **Related Work & Positioning**\n    *   **Existing Approaches**: Previous efforts to mitigate hallucinations generally fall into three categories:\n        *   **Fine-tuning with additional data**: Involves introducing high-quality annotated data to align model behavior with human interpretations \\cite{chen2024j0g}.\n            *   *Limitations*: Requires costly manual annotation, substantial computational resources for model parameter updates, and limits scalability \\cite{chen2024j0g}.\n        *   **Perceptual enhancement**: Incorporates auxiliary information (e.g., depth maps, segmentation maps) to assist visual encoders \\cite{chen2024j0g}.\n            *   *Limitations*: Often requires manual selection of auxiliary features, limiting generalizability across tasks \\cite{chen2024j0g}.\n        *   **Contrastive decoding (CD)**: Induces hallucinations (e.g., via blurring) and penalizes associated tokens during the decoding stage to mitigate language priors \\cite{chen2024j0g}.\n            *   *Limitations*: Indiscriminately eliminates *all* language priors, including potentially beneficial ones, which can inadvertently lead to new hallucinations \\cite{chen2024j0g}. Also introduces inference time overhead \\cite{chen2024j0g}.\n    *   **Positioning of this Work**: `\\cite{chen2024j0g}` proposes ICT, a novel, lightweight, training-free, and plug-and-play method that operates during the *forward pass*, unlike contrastive decoding methods. It enhances the model's focus on visual details without eliminating useful language priors and introduces no additional inference latency. It specifically targets the \"severely underexploited\" activation space during inference \\cite{chen2024j0g}.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method (ICT)**: Image-Object Cross-Level Trusted Intervention \\cite{chen2024j0g} is a training-free, plug-and-play method that calculates an \"intervention direction\" to shift the model's focus towards different levels of visual information during the forward pass.\n    *   **Mechanism**: `\\cite{chen2024j0g}` identifies activation value deviations in attention heads that can shift the model from \"untrustworthy\" to \"trustworthy\" responses. It then applies targeted interventions to these heads.\n    *   **Two Levels of Intervention**:\n        *   **Image-Level Intervention**: Aims to enhance the model's focus on overall visual information to reduce over-reliance on language priors. It constructs trusted (original image) and untrusted (globally blurred image) data pairs, calculates an activation shift vector based on their differences, and trains binary classifiers (SVMs) for each attention head to identify those encoding overall image information. The shift is then applied to these selected heads \\cite{chen2024j0g}.\n        *   **Object-Level Intervention**: Focuses on increasing attention to fine-grained object details to mitigate hallucinations caused by the omission of critical objects. It uses Grounding DINO to identify object regions, creates trusted (original image) and untrusted (locally blurred object region) data pairs, calculates an object-level activation shift vector, and trains binary classifiers to identify heads distinguishing these object-focused pairs. The shift is applied to these selected heads \\cite{chen2024j0g}.\n    *   **Unified Approach**: Both Image-Level and Object-Level interventions are integrated by summing their respective shifts on selected attention heads, providing a balanced attention mechanism \\cite{chen2024j0g}.\n    *   **Novelty/Difference**:\n        *   **Forward Pass Intervention**: Operates during the forward pass, making it orthogonal and complementary to existing decoding strategies and introducing no additional latency \\cite{chen2024j0g}.\n        *   **Targeted Head-Level Intervention**: Identifies and intervenes on specific attention heads responsible for encoding different levels of visual information (overall image vs. fine-grained objects) using binary classifiers \\cite{chen2024j0g}.\n        *   **Preservation of Language Priors**: Enhances visual focus without indiscriminately eliminating beneficial language priors, a key limitation of contrastive decoding \\cite{chen2024j0g}.\n        *   **Training-Free**: Does not require additional training or fine-tuning of the LVLM parameters \\cite{chen2024j0g}.\n\n4.  **Key Technical Contributions**\n    *   Proposes ICT \\cite{chen2024j0g}, a novel, training-free, plug-and-play method that effectively reduces hallucinations in LVLMs by enhancing focus on both overall visual information and fine-grained object details during the forward pass.\n    *   Introduces an intervention mechanism that operates during the forward pass, offering an alternative to decoding-stage methods and avoiding additional inference latency \\cite{chen2024j0g}.\n    *   Develops a method to identify and target specific attention heads responsible for encoding different levels of visual information using binary classifiers and activation shift vectors derived from trusted/untrusted data pairs \\cite{chen2024j0g}.\n    *   Demonstrates a balanced attention mechanism that mitigates excessive reliance on language priors while preserving useful ones \\cite{chen2024j0g}.\n\n5.  **Experimental Validation**\n    *   **Models Evaluated**: LLaVA-v1.5 \\cite{chen2024j0g} and Qwen-VL \\cite{chen2024j0g} were used as baseline LVLMs.\n    *   **Datasets**:\n        *   **POPE \\cite{chen2024j0g} (Polling-based Object Probing Evaluation)**: Used to assess object-level hallucinations with Yes/No questions. Metrics include Accuracy, Precision, Recall, and F1 score. 1,500 QA pairs from the COCO Random subset were used to train intervention shift vectors \\cite{chen2024j0g}.\n        *   **MME \\cite{chen2024j0g} (Multimodal Large Language Model Evaluation)**: A comprehensive benchmark evaluating LVLMs across perception and cognition tasks, including object existence, count, position, and color. The evaluation metric is Accuracy \\cite{chen2024j0g}.\n    *   **Baselines for Comparison**: VCD \\cite{chen2024j0g} and Opera \\cite{chen2024j0g}, both decoding-stage methods, were compared.\n    *   **Key Performance Metrics and Results**:\n        *   ICT \\cite{chen2024j0g} achieved an average improvement of 6.27% on the POPE benchmark and 67.37 points on the MME benchmark for LLaVA-v1.5 \\cite{chen2024j0g} and Qwen-VL \\cite{chen2024j0g}.\n        *   The method demonstrated strong performance with a small amount of data used for intervention vector calculation \\cite{chen2024j0g}.\n        *   ICT \\cite{chen2024j0g} exhibited cross-dataset and model-agnostic generalizability \\cite{chen2024j0g}.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The paper implicitly relies on the ability of binary classifiers (SVMs) to accurately identify attention heads encoding specific levels of visual information. The effectiveness of Grounding DINO for object localization is also a prerequisite for the Object-Level intervention \\cite{chen2024j0g}. The pre-computation of intervention vectors requires a small, representative dataset.\n    *   **Scope of Applicability**: ICT \\cite{chen2024j0g} is applicable to mitigating object and attribute hallucinations in LVLMs. It has been validated on LLaVA-v1.5 \\cite{chen2024j0g} and Qwen-VL \\cite{chen2024j0g} and shows generalizability across different datasets and models.\n\n7.  **Technical Significance**\n    *   **Advances State-of-the-Art**: `\\cite{chen2024j0g}` significantly advances the technical state-of-the-art by introducing a novel, efficient, and effective training-free method for hallucination mitigation that operates directly in the forward pass. This addresses key limitations of prior approaches, such as computational cost, data dependency, and the indiscriminate removal of beneficial language priors \\cite{chen2024j0g}.\n    *   **Potential Impact on Future Research**:\n        *   Opens new avenues for research into understanding and manipulating the internal activation space of LVLMs during inference to improve trustworthiness and control model behavior.\n        *   Provides a plug-and-play solution that can be easily integrated into existing LVLM pipelines without extensive retraining, making it highly practical for real-world deployment.\n        *   Encourages further exploration of targeted, fine-grained interventions within transformer architectures for various multimodal challenges.",
      "intriguing_abstract": "Object hallucination plagues Large Vision-Language Models (LVLMs), undermining their trustworthiness and hindering deployment in critical applications. This pervasive issue stems from an over-reliance on language priors and insufficient capture of fine-grained visual semantics. We introduce ICT (Image-Object Cross-Level Trusted Intervention), a novel, training-free, and plug-and-play method that fundamentally redefines hallucination mitigation. Unlike prior decoding-stage approaches, ICT intervenes directly within the forward pass, introducing zero inference latency.\n\nICT strategically shifts internal model activations by identifying and targeting specific attention heads responsible for encoding visual information. Through image-level and object-level interventions, derived from trusted/untrusted visual pairs, ICT enhances focus on both global scene understanding and intricate object details. Crucially, it achieves this without indiscriminately eliminating beneficial language priors, a common pitfall of existing methods. Evaluated on LLaVA-v1.5 and Qwen-VL across POPE and MME benchmarks, ICT demonstrates substantial performance gains, improving accuracy by up to 6.27% and 67.37 points respectively. This generalizable approach offers a powerful, efficient paradigm for building more reliable LVLMs, unlocking their potential for high-stakes real-world applications and paving new research directions in trustworthy AI.",
      "keywords": [
        "Large Vision-Language Models (LVLMs)",
        "object hallucination mitigation",
        "ICT (Image-Object Cross-Level Trusted Intervention)",
        "forward pass intervention",
        "training-free",
        "plug-and-play method",
        "targeted attention head intervention",
        "activation space manipulation",
        "image-level and object-level intervention",
        "language priors preservation",
        "binary classifiers",
        "cross-dataset generalizability"
      ],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/c680e5d34b713f8b63ad68149973d5b2b485dd07.pdf",
      "citation_key": "chen2024j0g",
      "metadata": {
        "title": "ICT: Image-Object Cross-Level Trusted Intervention for Mitigating Object Hallucination in Large Vision-Language Models",
        "authors": [
          "Junzhe Chen",
          "Tianshu Zhang",
          "Shiyu Huang",
          "Yuwei Niu",
          "Linfeng Zhang",
          "Lijie Wen",
          "Xuming Hu"
        ],
        "published_date": "2024",
        "abstract": "Despite the recent breakthroughs achieved by Large Vision Language Models (LVLMs) in understanding and responding to complex visual-textual contexts, their inherent hallucination tendencies limit their practical application in real-world scenarios that demand high levels of precision. Existing methods typically either fine-tune the LVLMs using additional data, which incurs extra costs in manual annotation and computational resources or perform comparisons at the decoding stage, which may eliminate useful language priors for reasoning while introducing inference time overhead. Therefore, we propose ICT, a lightweight, training-free method that calculates an intervention direction to shift the modelâ€™s focus towards different levels of visual information, enhancing its attention to high-level and fine-grained visual details. During the forward pass stage, the intervention is applied to the attention heads that encode the overall image information and the fine-grained object details, effectively mitigating the phenomenon of overly language priors, and thereby alleviating hallucinations. Extensive experiments demonstrate that ICT achieves strong performance with a small amount of data and generalizes well across different datasets and models. Our codes are publicly available at:https://github.com/THU-BPM/ICT/.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/c680e5d34b713f8b63ad68149973d5b2b485dd07.pdf",
        "venue": "Computer Vision and Pattern Recognition",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the technical paper for a literature review:\n\n### Analysis of \"ICT: Image-Object Cross-Level Trusted Intervention for Mitigating Object Hallucination in Large Vision-Language Models\" \\cite{chen2024j0g}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Large Vision-Language Models (LVLMs) frequently suffer from \"object hallucination,\" where they generate text inconsistent with visual input, such as incorrectly identifying object presence or inaccurately describing attributes \\cite{chen2024j0g}.\n    *   **Importance and Challenge**: This hallucination tendency severely limits the practical application of LVLMs in real-world scenarios demanding high precision (e.g., autonomous driving, medical surgery) \\cite{chen2024j0g}. The problem is challenging due to:\n        *   **Overly Strong Language Priors**: Powerful Large Language Models (LLMs) within LVLMs often dominate weaker visual encoders, leading to an excessive reliance on language cues over visual input \\cite{chen2024j0g}.\n        *   **Lack of Fine-Grained Visual Semantics**: Current visual decoders struggle to capture detailed visual features, resulting in errors regarding object attributes like color and quantity \\cite{chen2024j0g}.\n\n2.  **Related Work & Positioning**\n    *   **Existing Approaches**: Previous efforts to mitigate hallucinations generally fall into three categories:\n        *   **Fine-tuning with additional data**: Involves introducing high-quality annotated data to align model behavior with human interpretations \\cite{chen2024j0g}.\n            *   *Limitations*: Requires costly manual annotation, substantial computational resources for model parameter updates, and limits scalability \\cite{chen2024j0g}.\n        *   **Perceptual enhancement**: Incorporates auxiliary information (e.g., depth maps, segmentation maps) to assist visual encoders \\cite{chen2024j0g}.\n            *   *Limitations*: Often requires manual selection of auxiliary features, limiting generalizability across tasks \\cite{chen2024j0g}.\n        *   **Contrastive decoding (CD)**: Induces hallucinations (e.g., via blurring) and penalizes associated tokens during the decoding stage to mitigate language priors \\cite{chen2024j0g}.\n            *   *Limitations*: Indiscriminately eliminates *all* language priors, including potentially beneficial ones, which can inadvertently lead to new hallucinations \\cite{chen2024j0g}. Also introduces inference time overhead \\cite{chen2024j0g}.\n    *   **Positioning of this Work**: `\\cite{chen2024j0g}` proposes ICT, a novel, lightweight, training-free, and plug-and-play method that operates during the *forward pass*, unlike contrastive decoding methods. It enhances the model's focus on visual details without eliminating useful language priors and introduces no additional inference latency. It specifically targets the \"severely underexploited\" activation space during inference \\cite{chen2024j0g}.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method (ICT)**: Image-Object Cross-Level Trusted Intervention \\cite{chen2024j0g} is a training-free, plug-and-play method that calculates an \"intervention direction\" to shift the model's focus towards different levels of visual information during the forward pass.\n    *   **Mechanism**: `\\cite{chen2024j0g}` identifies activation value deviations in attention heads that can shift the model from \"untrustworthy\" to \"trustworthy\" responses. It then applies targeted interventions to these heads.\n    *   **Two Levels of Intervention**:\n        *   **Image-Level Intervention**: Aims to enhance the model's focus on overall visual information to reduce over-reliance on language priors. It constructs trusted (original image) and untrusted (globally blurred image) data pairs, calculates an activation shift vector based on their differences, and trains binary classifiers (SVMs) for each attention head to identify those encoding overall image information. The shift is then applied to these selected heads \\cite{chen2024j0g}.\n        *   **Object-Level Intervention**: Focuses on increasing attention to fine-grained object details to mitigate hallucinations caused by the omission of critical objects. It uses Grounding DINO to identify object regions, creates trusted (original image) and untrusted (locally blurred object region) data pairs, calculates an object-level activation shift vector, and trains binary classifiers to identify heads distinguishing these object-focused pairs. The shift is applied to these selected heads \\cite{chen2024j0g}.\n    *   **Unified Approach**: Both Image-Level and Object-Level interventions are integrated by summing their respective shifts on selected attention heads, providing a balanced attention mechanism \\cite{chen2024j0g}.\n    *   **Novelty/Difference**:\n        *   **Forward Pass Intervention**: Operates during the forward pass, making it orthogonal and complementary to existing decoding strategies and introducing no additional latency \\cite{chen2024j0g}.\n        *   **Targeted Head-Level Intervention**: Identifies and intervenes on specific attention heads responsible for encoding different levels of visual information (overall image vs. fine-grained objects) using binary classifiers \\cite{chen2024j0g}.\n        *   **Preservation of Language Priors**: Enhances visual focus without indiscriminately eliminating beneficial language priors, a key limitation of contrastive decoding \\cite{chen2024j0g}.\n        *   **Training-Free**: Does not require additional training or fine-tuning of the LVLM parameters \\cite{chen2024j0g}.\n\n4.  **Key Technical Contributions**\n    *   Proposes ICT \\cite{chen2024j0g}, a novel, training-free, plug-and-play method that effectively reduces hallucinations in LVLMs by enhancing focus on both overall visual information and fine-grained object details during the forward pass.\n    *   Introduces an intervention mechanism that operates during the forward pass, offering an alternative to decoding-stage methods and avoiding additional inference latency \\cite{chen2024j0g}.\n    *   Develops a method to identify and target specific attention heads responsible for encoding different levels of visual information using binary classifiers and activation shift vectors derived from trusted/untrusted data pairs \\cite{chen2024j0g}.\n    *   Demonstrates a balanced attention mechanism that mitigates excessive reliance on language priors while preserving useful ones \\cite{chen2024j0g}.\n\n5.  **Experimental Validation**\n    *   **Models Evaluated**: LLaVA-v1.5 \\cite{chen2024j0g} and Qwen-VL \\cite{chen2024j0g} were used as baseline LVLMs.\n    *   **Datasets**:\n        *   **POPE \\cite{chen2024j0g} (Polling-based Object Probing Evaluation)**: Used to assess object-level hallucinations with Yes/No questions. Metrics include Accuracy, Precision, Recall, and F1 score. 1,500 QA pairs from the COCO Random subset were used to train intervention shift vectors \\cite{chen2024j0g}.\n        *   **MME \\cite{chen2024j0g} (Multimodal Large Language Model Evaluation)**: A comprehensive benchmark evaluating LVLMs across perception and cognition tasks, including object existence, count, position, and color. The evaluation metric is Accuracy \\cite{chen2024j0g}.\n    *   **Baselines for Comparison**: VCD \\cite{chen2024j0g} and Opera \\cite{chen2024j0g}, both decoding-stage methods, were compared.\n    *   **Key Performance Metrics and Results**:\n        *   ICT \\cite{chen2024j0g} achieved an average improvement of 6.27% on the POPE benchmark and 67.37 points on the MME benchmark for LLaVA-v1.5 \\cite{chen2024j0g} and Qwen-VL \\cite{chen2024j0g}.\n        *   The method demonstrated strong performance with a small amount of data used for intervention vector calculation \\cite{chen2024j0g}.\n        *   ICT \\cite{chen2024j0g} exhibited cross-dataset and model-agnostic generalizability \\cite{chen2024j0g}.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The paper implicitly relies on the ability of binary classifiers (SVMs) to accurately identify attention heads encoding specific levels of visual information. The effectiveness of Grounding DINO for object localization is also a prerequisite for the Object-Level intervention \\cite{chen2024j0g}. The pre-computation of intervention vectors requires a small, representative dataset.\n    *   **Scope of Applicability**: ICT \\cite{chen2024j0g} is applicable to mitigating object and attribute hallucinations in LVLMs. It has been validated on LLaVA-v1.5 \\cite{chen2024j0g} and Qwen-VL \\cite{chen2024j0g} and shows generalizability across different datasets and models.\n\n7.  **Technical Significance**\n    *   **Advances State-of-the-Art**: `\\cite{chen2024j0g}` significantly advances the technical state-of-the-art by introducing a novel, efficient, and effective training-free method for hallucination mitigation that operates directly in the forward pass. This addresses key limitations of prior approaches, such as computational cost, data dependency, and the indiscriminate removal of beneficial language priors \\cite{chen2024j0g}.\n    *   **Potential Impact on Future Research**:\n        *   Opens new avenues for research into understanding and manipulating the internal activation space of LVLMs during inference to improve trustworthiness and control model behavior.\n        *   Provides a plug-and-play solution that can be easily integrated into existing LVLM pipelines without extensive retraining, making it highly practical for real-world deployment.\n        *   Encourages further exploration of targeted, fine-grained interventions within transformer architectures for various multimodal challenges.",
        "keywords": [
          "Large Vision-Language Models (LVLMs)",
          "object hallucination mitigation",
          "ICT (Image-Object Cross-Level Trusted Intervention)",
          "forward pass intervention",
          "training-free",
          "plug-and-play method",
          "targeted attention head intervention",
          "activation space manipulation",
          "image-level and object-level intervention",
          "language priors preservation",
          "binary classifiers",
          "cross-dataset generalizability"
        ],
        "paper_type": "based on the abstract and introduction:\n\n*   the abstract explicitly states: \"therefore, we **propose ict, a lightweight, training-free method** that calculates an intervention direction...\" and then describes *how* this method works (\"during the forward pass stage, the intervention is applied...\"). this directly aligns with the \"technical\" criteria of presenting new methods or algorithms.\n*   the introduction further elaborates on a \"technical problem\" (hallucination in lvlms) and immediately introduces \"our proposed ict\" in comparison to existing methods, detailing its approach.\n*   while the abstract also mentions \"extensive **experiments demonstrate** that ict achieves strong performance,\" indicating an empirical component, the primary focus described is the *development and presentation of a new method* to solve a technical problem. the experiments serve to validate this proposed method.\n\ntherefore, the most fitting classification is **technical**."
      },
      "file_name": "c680e5d34b713f8b63ad68149973d5b2b485dd07.pdf"
    },
    {
      "success": true,
      "doc_id": "2954bac54398c08012161f2875d6ecbe",
      "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **Research Problem & Motivation**\n    *   Large Vision-Language Models (LVLMs) invariably suffer from \"hallucinations,\" where generated text disconnects from the corresponding images, leading to imagined objects, fabricated scenes, incorrect spatial relationships, and misidentified categories \\cite{chen20247jb}.\n    *   This problem is critical as it undermines the reliability and trustworthiness of LVLMs, despite their exceptional multimodal comprehension and reasoning abilities \\cite{chen20247jb}.\n    *   Existing visual contrastive decoding methods, which attempt to mitigate hallucinations by introducing global visual uncertainty, are limited by their inability to precisely induce specific hallucinatory tokens, often leading to unsatisfactory or even undesired hallucinatory outputs \\cite{chen20247jb}.\n\n*   **Related Work & Positioning**\n    *   Previous efforts include post-hoc correction, self-correcting frameworks, and various decoding strategies that integrate visual uncertainty or textual/visual priors \\cite{chen20247jb}.\n    *   Methods like introducing Gaussian noise or substantial image noise into visual inputs (e.g., Leng et al. [2023], Zhang et al. [2024], Favero et al. [2024]) aim to amplify hallucinatory effects \\cite{chen20247jb}.\n    *   **Limitations of previous solutions:** The \"uncontrollable nature of global visual uncertainty\" prevents precise induction of hallucinatory tokens, severely limiting effectiveness and potentially generating new, undesired hallucinations \\cite{chen20247jb}. Traditional Direct Preference Optimization (DPO) also doesn't reliably induce hallucinations in the specific manner required for effective contrastive decoding \\cite{chen20247jb}.\n\n*   **Technical Approach & Innovation**\n    *   **Core Method:** Hallucination-Induced Optimization (HIO) \\cite{chen20247jb}. HIO operates in two phases: a training stage to create an \"Evil LVLM\" by inducing hallucinations, and an inference stage where logits from this \"Evil LVLM\" are contrasted with the original LVLM to reduce hallucinations \\cite{chen20247jb}.\n    *   **Contrary Bradley-Terry Model (CBTM):** Instead of training a model to prefer non-hallucinatory outputs, HIO utilizes a *reversed* Bradley-Terry model. This trains the \"Evil LVLM\" to *prioritize* hallucinatory content by amplifying the logits of hallucinatory tokens and reducing those of correct tokens, which is then used for contrastive decoding \\cite{chen20247jb}.\n    *   **Amplification of Multiple Targeted Hallucination (AMTH):** Addresses the limitation of single-hallucination focus by simultaneously fitting multiple pairs of preference data. This ensures that the \"Evil LVLM\" amplifies a diverse set of potential hallucinations relative to the target tokens, as required by their theoretical analysis (Eqn. 17) \\cite{chen20247jb}.\n    *   **Acquisition of Multiple Candidate Hallucinations:** Proposes a novel data acquisition method where the model directly outputs high-confidence tokens as negative (hallucinatory) samples, supplemented by true value-labeled data for correction \\cite{chen20247jb}.\n\n*   **Key Technical Contributions**\n    *   **Theoretical Analysis:** A comprehensive theoretical analysis exploring mechanisms to enhance the effectiveness of contrast decoding, leading to insights like the necessity of consistent logit differences between potential hallucinated and correct tokens (Eqn. 17) \\cite{chen20247jb}.\n    *   **Hallucination-Induced Optimization (HIO):** A novel optimization strategy that leverages a fine-tuned theoretical preference model (CBTM) to intensify the contrast between hallucinatory and target tokens, thereby strengthening contrast decoding \\cite{chen20247jb}.\n    *   **Contrary Bradley-Terry Model (CBTM):** An innovative application of the Bradley-Terry model in a \"reversed\" manner to specifically induce and amplify hallucinations in a controlled way for the purpose of contrastive decoding \\cite{chen20247jb}.\n    *   **Amplification of Multiple Targeted Hallucination (AMTH):** A method for simultaneously training against multiple potential hallucination tokens, moving beyond single-hallucination focus to create a more robust \"Evil LVLM\" \\cite{chen20247jb}.\n    *   **Novel Data Acquisition:** A practical approach for generating multiple candidate hallucinations by allowing the model to output high-confidence tokens as negative samples \\cite{chen20247jb}.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted:** Extensive experimental research was performed to evaluate the HIO strategy \\cite{chen20247jb}.\n    *   **Key Performance Metrics:** The paper mentions using CHAIR metrics (CHAIRs and CHAIRi) to evaluate hallucination reduction \\cite{chen20247jb}.\n    *   **Comparison Results:** HIO demonstrates superior performance, generating descriptions with fewer hallucination tokens and achieving lower scores on CHAIRs and CHAIRi metrics compared to other visual contrastive decoding methods \\cite{chen20247jb}. It outperforms state-of-the-art methods across various benchmarks \\cite{chen20247jb}.\n\n*   **Limitations & Scope**\n    *   The reliance on a \"fine-tuned theoretical preference model\" implies the need for preference data, which can be costly or complex to acquire \\cite{chen20247jb}.\n    *   The proposed method for \"Acquisition of Multiple Candidate Hallucinations\" acknowledges that it \"may incorrectly classify some correct tokens as hallucinations,\" requiring \"true value-labeled data for correction and supplementation\" \\cite{chen20247jb}.\n    *   The scope is focused on mitigating hallucinations in Large Vision-Language Models (LVLMs) through a specific contrastive decoding enhancement \\cite{chen20247jb}.\n\n*   **Technical Significance**\n    *   **Advances State-of-the-Art:** HIO significantly advances the technical state-of-the-art in hallucination mitigation for LVLMs by providing a more precise and effective contrastive decoding mechanism than previous methods \\cite{chen20247jb}.\n    *   **Novel Paradigm:** Introduces a novel paradigm of \"hallucination-induced optimization\" where a model is intentionally trained to generate hallucinations (the \"Evil LVLM\") to then be used as a contrastive baseline, offering a fresh perspective on tackling the problem \\cite{chen20247jb}.\n    *   **Theoretical Foundation:** Provides a strong theoretical foundation (Eqn. 17) for understanding and improving contrast decoding, which can guide future research \\cite{chen20247jb}.\n    *   **Potential Impact:** Could lead to more reliable and trustworthy LVLM applications by reducing factual inconsistencies, and inspire new research directions in controlled adversarial training or preference-based optimization for mitigating other undesirable model behaviors \\cite{chen20247jb}.",
      "intriguing_abstract": "Large Vision-Language Models (LVLMs) are plagued by pervasive \"hallucinations,\" generating text that fabricates objects, misidentifies scenes, and distorts spatial relationships, severely undermining their trustworthiness. Current visual contrastive decoding methods struggle with this, limited by their inability to precisely induce *specific* hallucinatory tokens for effective mitigation. We introduce **Hallucination-Induced Optimization (HIO)**, a novel paradigm that paradoxically trains an \"Evil LVLM\" to *intentionally generate* hallucinations. Our approach leverages a **Contrary Bradley-Terry Model (CBTM)**, a reversed preference optimization strategy that amplifies hallucinatory logits, and **Amplification of Multiple Targeted Hallucination (AMTH)** to simultaneously target diverse hallucination types. Grounded in a comprehensive theoretical analysis, HIO precisely strengthens the contrast between correct and hallucinatory tokens during inference. Extensive experiments demonstrate HIO's superior performance, significantly reducing hallucination rates and outperforming state-of-the-art methods on CHAIR metrics. This work offers a transformative solution for building more reliable and trustworthy LVLMs.",
      "keywords": [
        "Large Vision-Language Models (LVLMs)",
        "hallucinations",
        "Hallucination-Induced Optimization (HIO)",
        "contrastive decoding",
        "\"Evil LVLM\"",
        "Contrary Bradley-Terry Model (CBTM)",
        "Amplification of Multiple Targeted Hallucination (AMTH)",
        "theoretical analysis",
        "novel data acquisition",
        "hallucination mitigation",
        "reliability and trustworthiness",
        "CHAIR metrics"
      ],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/db646f0eb37bb97fda3a89f94c81e507f9421ba9.pdf",
      "citation_key": "chen20247jb",
      "metadata": {
        "title": "Alleviating Hallucinations in Large Vision-Language Models through Hallucination-Induced Optimization",
        "authors": [
          "Beitao Chen",
          "Xinyu Lyu",
          "Lianli Gao",
          "Jingkuan Song",
          "Hengtao Shen"
        ],
        "published_date": "2024",
        "abstract": "Although Large Visual Language Models (LVLMs) have demonstrated exceptional abilities in understanding multimodal data, they invariably suffer from hallucinations, leading to a disconnect between the generated text and the corresponding images. Almost all current visual contrastive decoding methods attempt to mitigate these hallucinations by introducing visual uncertainty information that appropriately widens the contrastive logits gap between hallucinatory and targeted ones. However, due to uncontrollable nature of the global visual uncertainty, they struggle to precisely induce the hallucinatory tokens, which severely limits their effectiveness in mitigating hallucinations and may even lead to the generation of undesired hallucinations. To tackle this issue, we conducted the theoretical analysis to promote the effectiveness of contrast decoding. Building on this insight, we introduce a novel optimization strategy named Hallucination-Induced Optimization (HIO). This strategy seeks to amplify the contrast between hallucinatory and targeted tokens relying on a fine-tuned theoretical preference model (i.e., Contrary Bradley-Terry Model), thereby facilitating efficient contrast decoding to alleviate hallucinations in LVLMs. Extensive experimental research demonstrates that our HIO strategy can effectively reduce hallucinations in LVLMs, outperforming state-of-the-art methods across various benchmarks.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/db646f0eb37bb97fda3a89f94c81e507f9421ba9.pdf",
        "venue": "Neural Information Processing Systems",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **Research Problem & Motivation**\n    *   Large Vision-Language Models (LVLMs) invariably suffer from \"hallucinations,\" where generated text disconnects from the corresponding images, leading to imagined objects, fabricated scenes, incorrect spatial relationships, and misidentified categories \\cite{chen20247jb}.\n    *   This problem is critical as it undermines the reliability and trustworthiness of LVLMs, despite their exceptional multimodal comprehension and reasoning abilities \\cite{chen20247jb}.\n    *   Existing visual contrastive decoding methods, which attempt to mitigate hallucinations by introducing global visual uncertainty, are limited by their inability to precisely induce specific hallucinatory tokens, often leading to unsatisfactory or even undesired hallucinatory outputs \\cite{chen20247jb}.\n\n*   **Related Work & Positioning**\n    *   Previous efforts include post-hoc correction, self-correcting frameworks, and various decoding strategies that integrate visual uncertainty or textual/visual priors \\cite{chen20247jb}.\n    *   Methods like introducing Gaussian noise or substantial image noise into visual inputs (e.g., Leng et al. [2023], Zhang et al. [2024], Favero et al. [2024]) aim to amplify hallucinatory effects \\cite{chen20247jb}.\n    *   **Limitations of previous solutions:** The \"uncontrollable nature of global visual uncertainty\" prevents precise induction of hallucinatory tokens, severely limiting effectiveness and potentially generating new, undesired hallucinations \\cite{chen20247jb}. Traditional Direct Preference Optimization (DPO) also doesn't reliably induce hallucinations in the specific manner required for effective contrastive decoding \\cite{chen20247jb}.\n\n*   **Technical Approach & Innovation**\n    *   **Core Method:** Hallucination-Induced Optimization (HIO) \\cite{chen20247jb}. HIO operates in two phases: a training stage to create an \"Evil LVLM\" by inducing hallucinations, and an inference stage where logits from this \"Evil LVLM\" are contrasted with the original LVLM to reduce hallucinations \\cite{chen20247jb}.\n    *   **Contrary Bradley-Terry Model (CBTM):** Instead of training a model to prefer non-hallucinatory outputs, HIO utilizes a *reversed* Bradley-Terry model. This trains the \"Evil LVLM\" to *prioritize* hallucinatory content by amplifying the logits of hallucinatory tokens and reducing those of correct tokens, which is then used for contrastive decoding \\cite{chen20247jb}.\n    *   **Amplification of Multiple Targeted Hallucination (AMTH):** Addresses the limitation of single-hallucination focus by simultaneously fitting multiple pairs of preference data. This ensures that the \"Evil LVLM\" amplifies a diverse set of potential hallucinations relative to the target tokens, as required by their theoretical analysis (Eqn. 17) \\cite{chen20247jb}.\n    *   **Acquisition of Multiple Candidate Hallucinations:** Proposes a novel data acquisition method where the model directly outputs high-confidence tokens as negative (hallucinatory) samples, supplemented by true value-labeled data for correction \\cite{chen20247jb}.\n\n*   **Key Technical Contributions**\n    *   **Theoretical Analysis:** A comprehensive theoretical analysis exploring mechanisms to enhance the effectiveness of contrast decoding, leading to insights like the necessity of consistent logit differences between potential hallucinated and correct tokens (Eqn. 17) \\cite{chen20247jb}.\n    *   **Hallucination-Induced Optimization (HIO):** A novel optimization strategy that leverages a fine-tuned theoretical preference model (CBTM) to intensify the contrast between hallucinatory and target tokens, thereby strengthening contrast decoding \\cite{chen20247jb}.\n    *   **Contrary Bradley-Terry Model (CBTM):** An innovative application of the Bradley-Terry model in a \"reversed\" manner to specifically induce and amplify hallucinations in a controlled way for the purpose of contrastive decoding \\cite{chen20247jb}.\n    *   **Amplification of Multiple Targeted Hallucination (AMTH):** A method for simultaneously training against multiple potential hallucination tokens, moving beyond single-hallucination focus to create a more robust \"Evil LVLM\" \\cite{chen20247jb}.\n    *   **Novel Data Acquisition:** A practical approach for generating multiple candidate hallucinations by allowing the model to output high-confidence tokens as negative samples \\cite{chen20247jb}.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted:** Extensive experimental research was performed to evaluate the HIO strategy \\cite{chen20247jb}.\n    *   **Key Performance Metrics:** The paper mentions using CHAIR metrics (CHAIRs and CHAIRi) to evaluate hallucination reduction \\cite{chen20247jb}.\n    *   **Comparison Results:** HIO demonstrates superior performance, generating descriptions with fewer hallucination tokens and achieving lower scores on CHAIRs and CHAIRi metrics compared to other visual contrastive decoding methods \\cite{chen20247jb}. It outperforms state-of-the-art methods across various benchmarks \\cite{chen20247jb}.\n\n*   **Limitations & Scope**\n    *   The reliance on a \"fine-tuned theoretical preference model\" implies the need for preference data, which can be costly or complex to acquire \\cite{chen20247jb}.\n    *   The proposed method for \"Acquisition of Multiple Candidate Hallucinations\" acknowledges that it \"may incorrectly classify some correct tokens as hallucinations,\" requiring \"true value-labeled data for correction and supplementation\" \\cite{chen20247jb}.\n    *   The scope is focused on mitigating hallucinations in Large Vision-Language Models (LVLMs) through a specific contrastive decoding enhancement \\cite{chen20247jb}.\n\n*   **Technical Significance**\n    *   **Advances State-of-the-Art:** HIO significantly advances the technical state-of-the-art in hallucination mitigation for LVLMs by providing a more precise and effective contrastive decoding mechanism than previous methods \\cite{chen20247jb}.\n    *   **Novel Paradigm:** Introduces a novel paradigm of \"hallucination-induced optimization\" where a model is intentionally trained to generate hallucinations (the \"Evil LVLM\") to then be used as a contrastive baseline, offering a fresh perspective on tackling the problem \\cite{chen20247jb}.\n    *   **Theoretical Foundation:** Provides a strong theoretical foundation (Eqn. 17) for understanding and improving contrast decoding, which can guide future research \\cite{chen20247jb}.\n    *   **Potential Impact:** Could lead to more reliable and trustworthy LVLM applications by reducing factual inconsistencies, and inspire new research directions in controlled adversarial training or preference-based optimization for mitigating other undesirable model behaviors \\cite{chen20247jb}.",
        "keywords": [
          "Large Vision-Language Models (LVLMs)",
          "hallucinations",
          "Hallucination-Induced Optimization (HIO)",
          "contrastive decoding",
          "\"Evil LVLM\"",
          "Contrary Bradley-Terry Model (CBTM)",
          "Amplification of Multiple Targeted Hallucination (AMTH)",
          "theoretical analysis",
          "novel data acquisition",
          "hallucination mitigation",
          "reliability and trustworthiness",
          "CHAIR metrics"
        ],
        "paper_type": "based on the abstract and introduction, this paper is best classified as **technical**.\n\nhere's why:\n\n*   **abstract mentions:** \"we introduce a novel optimization strategy named hallucination-induced optimization (hio).\" this directly aligns with the \"propose,\" \"develop,\" \"present,\" \"algorithm,\" \"method\" criteria for technical papers.\n*   **introduction discusses:** the problem of hallucinations in lvlms and then visually presents the \"hallucination induction optimization\" as a solution (figure 1), indicating a proposed solution to a technical problem.\n*   while it mentions \"theoretical analysis,\" this analysis is conducted \"to promote the effectiveness of contrast decoding\" and serves as insight *for* the novel optimization strategy, rather than being the sole or primary contribution.\n*   it also mentions \"extensive experimental research demonstrates that our hio strategy can effectively reduce hallucinations,\" indicating an empirical evaluation, but this is the validation of the *new method*, making the paper primarily technical with strong empirical support."
      },
      "file_name": "db646f0eb37bb97fda3a89f94c81e507f9421ba9.pdf"
    },
    {
      "success": true,
      "doc_id": "683d01644f06f5cb8d497dd58ee12859",
      "summary": "Here's a focused summary of the paper for a literature review, emphasizing technical innovations and empirical validation:\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Neural abstractive summarization models are highly prone to \"hallucinate\" content that is unfaithful (not supported by the source document) or unfactual. This issue stems from standard likelihood training and approximate decoding objectives.\n    *   **Importance & Challenge**: While current models achieve high fluency and coherence, faithfulness and factuality are critical for summarization. Hallucinations undermine the reliability and trustworthiness of generated summaries, making the problem challenging to address given the inherent abstractive nature of the task.\n\n*   **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: The paper acknowledges the advancements in sequence-to-sequence, Transformer, and pretrained language models that have improved fluency. It contrasts summarization with open-ended generation tasks (like story generation) where some \"hallucination\" (novel content) might be desirable.\n    *   **Limitations of Previous Solutions**: Existing summarization models, despite their architectural sophistication (RNN, CNN, Transformer), frequently produce unfaithful content. Standard automatic metrics like ROUGE and BERTScore, while useful for fluency and content overlap, correlate poorly with human judgments of faithfulness and factuality.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper's core approach is a large-scale human evaluation to systematically analyze and characterize the types and prevalence of hallucinations in abstractive summaries generated by various state-of-the-art models.\n    *   **Novelty**:\n        *   Introduces a novel categorization of hallucinations:\n            *   **Intrinsic hallucinations**: Misrepresenting information *present* in the input document.\n            *   **Extrinsic hallucinations**: Adding information *not directly inferable* from the input document.\n            *   **Factual hallucinations**: A subset of extrinsic hallucinations that, despite not being in the document, are factually correct (e.g., adding a correct year). The paper argues for tolerance of factual hallucinations if they improve summary quality.\n        *   Explores the use of textual entailment measures as a more semantically-aware automatic evaluation metric for faithfulness, demonstrating its superior correlation with human judgments compared to traditional metrics.\n\n*   **Key Technical Contributions**\n    *   **Novel Algorithms, Methods, or Techniques**:\n        *   A robust human annotation scheme and methodology for identifying and classifying intrinsic, extrinsic, and factual hallucinations in abstractive summaries \\cite{maynez2020h3q}.\n        *   Empirical demonstration that textual entailment measures offer a more reliable automatic proxy for human judgments of faithfulness than ROUGE or BERTScore \\cite{maynez2020h3q}.\n    *   **Theoretical Insights or Analysis**:\n        *   Quantifies the high prevalence of hallucinations (over 70% of summaries) and their breakdown (majority extrinsic, and over 90% of extrinsic being erroneous) across diverse models \\cite{maynez2020h3q}.\n        *   Provides evidence that pretrained models, while still hallucinating, produce more faithful and factual summaries, and are better at integrating background knowledge to generate *factual* extrinsic hallucinations \\cite{maynez2020h3q}.\n        *   Challenges the sole reliance on surface-level metrics for evaluating summarization quality, advocating for deeper semantic understanding.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted**:\n        *   A large-scale human evaluation was performed on 500 randomly sampled articles from the XSUM dataset. Human annotators were trained to identify and categorize intrinsic, extrinsic, and factual hallucinations in summaries.\n        *   Summaries from five state-of-the-art abstractive models were evaluated: PTGEN (RNN-based), TCONVS2S (CNN-based), TRANS2S (Transformer), GPT-TUNED (pretrained GPT), and BERTS2S (pretrained BERT-based seq2seq), alongside human-written reference summaries \\cite{maynez2020h3q}.\n        *   The human judgments were correlated with various automatic metrics, including ROUGE-1, ROUGE-2, ROUGE-L, BERTScore, and textual entailment measures.\n    *   **Key Performance Metrics and Comparison Results**:\n        *   Human evaluation revealed that over 70% of single-sentence summaries contained hallucinations. The majority were extrinsic, and over 90% of these extrinsic hallucinations were erroneous (not factual) \\cite{maynez2020h3q}.\n        *   Pretrained models (BERTS2S and GPT-TUNED) consistently performed best in terms of human judgments for faithfulness and factuality, and also showed the highest percentage of *factual* extrinsic hallucinations \\cite{maynez2020h3q}.\n        *   BERTS2S achieved the highest ROUGE and BERTScore scores among the models.\n        *   Crucially, textual entailment measures demonstrated a stronger correlation with human judgments of faithfulness and factuality compared to ROUGE and BERTScore \\cite{maynez2020h3q}.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The study primarily focuses on extreme summarization (single-sentence summaries) using the XSUM dataset. While the authors argue the problem is likely worse for longer summaries, direct generalization requires further validation. The human evaluation, while extensive, is limited by its sample size (500 articles) and cost. The paper identifies and characterizes the problem but does not propose a new model or training objective to *solve* hallucination.\n    *   **Scope of Applicability**: The findings are directly applicable to abstractive summarization, particularly in settings where faithfulness to the source document is paramount. The insights into evaluation metrics could extend to other conditional text generation tasks requiring factual consistency.\n\n*   **Technical Significance**\n    *   **Advance State-of-the-Art**: This paper provides the first large-scale, systematic human evaluation and categorization of hallucinations in abstractive summarization, establishing a critical baseline for understanding this pervasive problem \\cite{maynez2020h3q}. It empirically demonstrates the superior performance of pretrained models in generating more faithful and factual summaries.\n    *   **Potential Impact on Future Research**: The work highlights the inadequacy of traditional automatic metrics for evaluating faithfulness and strongly advocates for the adoption of semantically-aware metrics like textual entailment. This will likely drive future research into developing novel training objectives, decoding strategies, and evaluation protocols that explicitly address and mitigate hallucinations in abstractive summarization and other conditional text generation tasks.",
      "intriguing_abstract": "Despite remarkable advancements in **neural abstractive summarization**, a pervasive and critical flaw persists: the generation of 'hallucinated' content that is unfaithful or unfactual to the source document. This paper presents the first large-scale **human evaluation** systematically characterizing these hallucinations, introducing a novel taxonomy distinguishing **intrinsic, extrinsic, and factual hallucinations**. Our analysis of **state-of-the-art models** reveals that over 70% of summaries contain hallucinations, predominantly erroneous extrinsic ones. Crucially, we demonstrate that **textual entailment measures** offer a significantly more reliable automatic proxy for **human judgments** of **faithfulness** and **factuality** than traditional metrics like ROUGE or BERTScore. These findings not only quantify the severity of the hallucination problem but also recalibrate our understanding of summarization quality. This work is pivotal for guiding the development of robust, trustworthy **abstractive summarization** systems and advancing **conditional text generation** by advocating for semantically-aware evaluation and fostering future research into mitigating unfaithful content.",
      "keywords": [
        "Neural abstractive summarization",
        "Hallucinations",
        "Faithfulness and factuality",
        "Intrinsic hallucinations",
        "Extrinsic hallucinations",
        "Factual hallucinations",
        "Large-scale human evaluation",
        "Human annotation scheme",
        "Textual entailment measures",
        "Pretrained language models",
        "Automatic evaluation metrics inadequacy",
        "Problem characterization",
        "Conditional text generation"
      ],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/dbeeca8466e0c177ec67c60d529899232415ca87.pdf",
      "citation_key": "maynez2020h3q",
      "metadata": {
        "title": "On Faithfulness and Factuality in Abstractive Summarization",
        "authors": [
          "Joshua Maynez",
          "Shashi Narayan",
          "Bernd Bohnet",
          "Ryan T. McDonald"
        ],
        "published_date": "2020",
        "abstract": "It is well known that the standard likelihood training and approximate decoding objectives in neural text generation models lead to less human-like responses for open-ended tasks such as language modeling and story generation. In this paper we have analyzed limitations of these models for abstractive document summarization and found that these models are highly prone to hallucinate content that is unfaithful to the input document. We conducted a large scale human evaluation of several neural abstractive summarization systems to better understand the types of hallucinations they produce. Our human annotators found substantial amounts of hallucinated content in all model generated summaries. However, our analysis does show that pretrained models are better summarizers not only in terms of raw metrics, i.e., ROUGE, but also in generating faithful and factual summaries as evaluated by humans. Furthermore, we show that textual entailment measures better correlate with faithfulness than standard metrics, potentially leading the way to automatic evaluation metrics as well as training and decoding criteria.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/dbeeca8466e0c177ec67c60d529899232415ca87.pdf",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the paper for a literature review, emphasizing technical innovations and empirical validation:\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Neural abstractive summarization models are highly prone to \"hallucinate\" content that is unfaithful (not supported by the source document) or unfactual. This issue stems from standard likelihood training and approximate decoding objectives.\n    *   **Importance & Challenge**: While current models achieve high fluency and coherence, faithfulness and factuality are critical for summarization. Hallucinations undermine the reliability and trustworthiness of generated summaries, making the problem challenging to address given the inherent abstractive nature of the task.\n\n*   **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: The paper acknowledges the advancements in sequence-to-sequence, Transformer, and pretrained language models that have improved fluency. It contrasts summarization with open-ended generation tasks (like story generation) where some \"hallucination\" (novel content) might be desirable.\n    *   **Limitations of Previous Solutions**: Existing summarization models, despite their architectural sophistication (RNN, CNN, Transformer), frequently produce unfaithful content. Standard automatic metrics like ROUGE and BERTScore, while useful for fluency and content overlap, correlate poorly with human judgments of faithfulness and factuality.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper's core approach is a large-scale human evaluation to systematically analyze and characterize the types and prevalence of hallucinations in abstractive summaries generated by various state-of-the-art models.\n    *   **Novelty**:\n        *   Introduces a novel categorization of hallucinations:\n            *   **Intrinsic hallucinations**: Misrepresenting information *present* in the input document.\n            *   **Extrinsic hallucinations**: Adding information *not directly inferable* from the input document.\n            *   **Factual hallucinations**: A subset of extrinsic hallucinations that, despite not being in the document, are factually correct (e.g., adding a correct year). The paper argues for tolerance of factual hallucinations if they improve summary quality.\n        *   Explores the use of textual entailment measures as a more semantically-aware automatic evaluation metric for faithfulness, demonstrating its superior correlation with human judgments compared to traditional metrics.\n\n*   **Key Technical Contributions**\n    *   **Novel Algorithms, Methods, or Techniques**:\n        *   A robust human annotation scheme and methodology for identifying and classifying intrinsic, extrinsic, and factual hallucinations in abstractive summaries \\cite{maynez2020h3q}.\n        *   Empirical demonstration that textual entailment measures offer a more reliable automatic proxy for human judgments of faithfulness than ROUGE or BERTScore \\cite{maynez2020h3q}.\n    *   **Theoretical Insights or Analysis**:\n        *   Quantifies the high prevalence of hallucinations (over 70% of summaries) and their breakdown (majority extrinsic, and over 90% of extrinsic being erroneous) across diverse models \\cite{maynez2020h3q}.\n        *   Provides evidence that pretrained models, while still hallucinating, produce more faithful and factual summaries, and are better at integrating background knowledge to generate *factual* extrinsic hallucinations \\cite{maynez2020h3q}.\n        *   Challenges the sole reliance on surface-level metrics for evaluating summarization quality, advocating for deeper semantic understanding.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted**:\n        *   A large-scale human evaluation was performed on 500 randomly sampled articles from the XSUM dataset. Human annotators were trained to identify and categorize intrinsic, extrinsic, and factual hallucinations in summaries.\n        *   Summaries from five state-of-the-art abstractive models were evaluated: PTGEN (RNN-based), TCONVS2S (CNN-based), TRANS2S (Transformer), GPT-TUNED (pretrained GPT), and BERTS2S (pretrained BERT-based seq2seq), alongside human-written reference summaries \\cite{maynez2020h3q}.\n        *   The human judgments were correlated with various automatic metrics, including ROUGE-1, ROUGE-2, ROUGE-L, BERTScore, and textual entailment measures.\n    *   **Key Performance Metrics and Comparison Results**:\n        *   Human evaluation revealed that over 70% of single-sentence summaries contained hallucinations. The majority were extrinsic, and over 90% of these extrinsic hallucinations were erroneous (not factual) \\cite{maynez2020h3q}.\n        *   Pretrained models (BERTS2S and GPT-TUNED) consistently performed best in terms of human judgments for faithfulness and factuality, and also showed the highest percentage of *factual* extrinsic hallucinations \\cite{maynez2020h3q}.\n        *   BERTS2S achieved the highest ROUGE and BERTScore scores among the models.\n        *   Crucially, textual entailment measures demonstrated a stronger correlation with human judgments of faithfulness and factuality compared to ROUGE and BERTScore \\cite{maynez2020h3q}.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The study primarily focuses on extreme summarization (single-sentence summaries) using the XSUM dataset. While the authors argue the problem is likely worse for longer summaries, direct generalization requires further validation. The human evaluation, while extensive, is limited by its sample size (500 articles) and cost. The paper identifies and characterizes the problem but does not propose a new model or training objective to *solve* hallucination.\n    *   **Scope of Applicability**: The findings are directly applicable to abstractive summarization, particularly in settings where faithfulness to the source document is paramount. The insights into evaluation metrics could extend to other conditional text generation tasks requiring factual consistency.\n\n*   **Technical Significance**\n    *   **Advance State-of-the-Art**: This paper provides the first large-scale, systematic human evaluation and categorization of hallucinations in abstractive summarization, establishing a critical baseline for understanding this pervasive problem \\cite{maynez2020h3q}. It empirically demonstrates the superior performance of pretrained models in generating more faithful and factual summaries.\n    *   **Potential Impact on Future Research**: The work highlights the inadequacy of traditional automatic metrics for evaluating faithfulness and strongly advocates for the adoption of semantically-aware metrics like textual entailment. This will likely drive future research into developing novel training objectives, decoding strategies, and evaluation protocols that explicitly address and mitigate hallucinations in abstractive summarization and other conditional text generation tasks.",
        "keywords": [
          "Neural abstractive summarization",
          "Hallucinations",
          "Faithfulness and factuality",
          "Intrinsic hallucinations",
          "Extrinsic hallucinations",
          "Factual hallucinations",
          "Large-scale human evaluation",
          "Human annotation scheme",
          "Textual entailment measures",
          "Pretrained language models",
          "Automatic evaluation metrics inadequacy",
          "Problem characterization",
          "Conditional text generation"
        ],
        "paper_type": "based on the abstract and introduction:\n\n*   the abstract explicitly states: \"we conducted a large scale human evaluation of several neural abstractive summarization systems to better understand the types of hallucinations they produce. our human annotators found substantial amounts of hallucinated content... our analysis does show that pretrained models are better summarizers... furthermore, we show that textual entailment measures better correlate with faithfulness than standard metrics...\"\n*   the introduction mentions: \"in this paper we investigate how these models are prone to generate hallucinated text...\", and \"our human annotated summaries for faithfulness and factuality will be released...\"\n\nthese phrases strongly indicate a data-driven study involving experiments (human evaluation), data collection (annotations), and statistical analysis (correlations, rouge metrics, findings).\n\ntherefore, the paper type is **empirical**."
      },
      "file_name": "dbeeca8466e0c177ec67c60d529899232415ca87.pdf"
    },
    {
      "success": true,
      "doc_id": "df31123d9ff1b06d908972391aae4101",
      "summary": "Here's a focused summary of the paper for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem:** Large Vision-Language Models (LVLMs) frequently suffer from \"hallucination,\" generating plausible but factually incorrect responses that misalign with image content \\cite{qu20246yn}.\n    *   **Importance & Challenge:** This problem severely damages the practical employment of LVLMs, especially in critical fields like medical and robotics, where accurate content generation is paramount \\cite{qu20246yn}. While retrieval augmentation has shown promise in Large Language Models (LLMs) for mitigating hallucinations, its application in LVLMs significantly lags and can even exacerbate hallucination when naively transferred due to the multimodal nature of LVLMs \\cite{qu20246yn}.\n\n*   **Related Work & Positioning**\n    *   **Relation to Existing Approaches:**\n        *   **Model Refinement:** Previous attempts include retraining LVLMs with hallucination-related datasets via supervised fine-tuning (SFT) or Reinforcement Learning from Human Feedback (RLHF) \\cite{qu20246yn}.\n        *   **Robust Decoding Strategies:** Other solutions focus on designing training-free decoding strategies, such as visual contrastive decoding (VCD) or integrating penalty terms (OPERA) \\cite{qu20246yn}.\n        *   **Retrieval-Augmented Generation (RAG) in LLMs:** RAG has been widely used in LLMs to retrieve external knowledge, but its direct applicability to LVLMs is limited \\cite{qu20246yn}.\n        *   **RAG in Multimodal Tasks:** Existing multimodal RAG research is largely confined to image captioning or generation, overlooking broader implications for hallucination reduction in LVLMs \\cite{qu20246yn}.\n    *   **Limitations of Previous Solutions:**\n        *   Model refinement methods introduce significant additional training costs and can be prone to overfitting \\cite{qu20246yn}.\n        *   Training-free decoding strategies still suffer from the limitations of LVLMsâ€™ static parametric capacity \\cite{qu20246yn}.\n        *   Findings from RAG in LLMs cannot be indiscriminately extrapolated to LVLMs due to their multimodal nature \\cite{qu20246yn}.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method:** The paper introduces the **Active Retrieval-Augmented large vision-language model (ARA)**, a novel framework specifically designed to address hallucinations in LVLMs \\cite{qu20246yn}.\n    *   **Novelty/Difference:** ARA's innovation lies in incorporating three critical dimensions for retrieval augmentation in LVLMs:\n        1.  **Dissecting Retrieval Targets:** Decomposing targets based on the inherent hierarchical structures of images (coarse-to-fine retrieval) \\cite{qu20246yn}.\n        2.  **Pinpointing Effective Retrieval Methods:** Identifying optimal retrieval techniques and filtering reliable results from diverse multimodal inputs \\cite{qu20246yn}.\n        3.  **Timing Retrieval Process:** Activating retrieval only during periods of low model certainty and knowledge deficiency, avoiding unnecessary retrieval during high certainty \\cite{qu20246yn}.\n    *   **Pipeline:**\n        *   **Active Triggering Retrieval:** Determines whether retrieval is necessary using difficulty metrics based on model confidence, query-image mutual information, or original-noisy image mutual information \\cite{qu20246yn}.\n        *   **Coarse-to-Fine Hierarchical Retrieval:** If retrieval is triggered, it performs both coarse-grained (full-image) and fine-grained (target object/region-specific) retrieval from external knowledge bases \\cite{qu20246yn}.\n        *   **Reranking Strategy:** Retrieved text and image pairs are further reranked (e.g., based on image captioning) to eliminate unreliable outcomes \\cite{qu20246yn}.\n        *   **Joint Decoding:** The LVLM then leverages this refined, externally sourced knowledge to generate the final response \\cite{qu20246yn}.\n\n*   **Key Technical Contributions**\n    *   **Novel Algorithms/Methods:**\n        *   Introduction of active retrieval triggering mechanisms for LVLMs based on confidence and mutual information metrics \\cite{qu20246yn}.\n        *   Development of a coarse-to-fine hierarchical retrieval paradigm tailored for images, combining global and local visual information \\cite{qu20246yn}.\n        *   A reranking strategy for multimodal retrieval results to enhance the reliability of augmented information \\cite{qu20246yn}.\n    *   **System Design/Architectural Innovations:** The ARA framework provides a comprehensive architecture that integrates active decision-making, hierarchical multimodal retrieval, and result refinement for hallucination mitigation \\cite{qu20246yn}.\n    *   **Theoretical Insights/Analysis:** The study provides deeper insights into how to adapt retrieval augmentation to LVLMs effectively, focusing on targeted retrieval and minimal occurrences \\cite{qu20246yn}.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted:** Empirical evaluations were performed using three prevalent LVLMs (LLaVA-1.5, Qwen-VL, and mPLUG-Owl2) across four benchmarks specifically designed to assess hallucination challenges \\cite{qu20246yn}.\n    *   **Key Performance Metrics & Comparison Results:** The results demonstrate that the ARA framework effectively mitigates hallucinations, suggesting that judiciously timed and fitting retrieval mechanisms significantly improve performance \\cite{qu20246yn}.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations/Assumptions:** The approach aims for \"minimal retrieval occurrences\" and \"moderate frequency of retrieval,\" implying a balance between performance gains and computational overhead \\cite{qu20246yn}. The effectiveness is also dependent on the quality and relevance of the external knowledge base and the accuracy of object grounding.\n    *   **Scope of Applicability:** The framework is primarily applicable to LVLMs for tasks requiring factual accuracy in visual comprehension and generation, such as visual question answering \\cite{qu20246yn}.\n\n*   **Technical Significance**\n    *   **Advances State-of-the-Art:** This work significantly advances the technical state-of-the-art by providing a principled and effective method for adapting retrieval augmentation to the unique challenges of LVLMs, moving beyond direct transfer from LLMs \\cite{qu20246yn}.\n    *   **Potential Impact on Future Research:** ARA offers a flexible and potentially cost-effective solution to enhance the factual accuracy and credibility of LVLMs, opening avenues for future research in more robust and reliable multimodal AI systems, especially in high-stakes applications \\cite{qu20246yn}.",
      "intriguing_abstract": "The pervasive issue of hallucination in Large Vision-Language Models (LVLMs) severely impedes their deployment in critical applications, from medical diagnostics to robotics. While Retrieval-Augmented Generation (RAG) has transformed Large Language Models (LLMs), its naive application to LVLMs often exacerbates factual inaccuracies due to their complex multimodal nature. We introduce **ARA (Active Retrieval-Augmented LVLM)**, a novel framework that fundamentally rethinks retrieval augmentation for LVLMs.\n\nARA innovates across three crucial dimensions: actively triggering retrieval only when model certainty is low or knowledge is deficient, dissecting retrieval targets through a novel **coarse-to-fine hierarchical retrieval** paradigm, and pinpointing reliable multimodal knowledge via a robust reranking strategy. This comprehensive architecture integrates active decision-making with targeted, context-aware retrieval, moving beyond static parametric capacities. Extensive experiments across LLaVA-1.5, Qwen-VL, and mPLUG-Owl2 on four hallucination benchmarks demonstrate ARA's superior ability to mitigate hallucinations. ARA significantly advances the state-of-the-art, offering a flexible and effective solution to enhance the factual accuracy and trustworthiness of LVLMs, paving the way for more reliable multimodal AI systems.",
      "keywords": [
        "Large Vision-Language Models (LVLMs)",
        "hallucination mitigation",
        "retrieval augmentation",
        "Active Retrieval-Augmented LVLM (ARA)",
        "multimodal nature",
        "coarse-to-fine hierarchical retrieval",
        "active retrieval triggering",
        "multimodal reranking strategy",
        "model confidence metrics",
        "mutual information metrics",
        "joint decoding",
        "factual accuracy"
      ],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/171807aeeb88f0c7983bc6cc960b5605441d7121.pdf",
      "citation_key": "qu20246yn",
      "metadata": {
        "title": "Alleviating Hallucination in Large Vision-Language Models with Active Retrieval Augmentation",
        "authors": [
          "Xiaoye Qu",
          "Qiyuan Chen",
          "Wei Wei",
          "Jiashuo Sun",
          "Jianfeng Dong"
        ],
        "published_date": "2024",
        "abstract": "Despite the remarkable ability of large vision-language models (LVLMs) in image comprehension, these models frequently generate plausible yet factually incorrect responses, a phenomenon known as hallucination. Recently, in large language models (LLMs), augmenting LLMs by retrieving information from external knowledge resources has been proven as a promising solution to mitigate hallucinations. However, the retrieval augmentation in LVLM significantly lags behind the widespread applications of LVLM. Moreover, when transferred to augmenting LVLMs, sometimes the hallucination degree of the model is even exacerbated. Motivated by the research gap and counter-intuitive phenomenon, we introduce a novel framework, the Active Retrieval-Augmented large vision-language model (ARA), specifically designed to address hallucinations by incorporating three critical dimensions: (i) dissecting the retrieval targets based on the inherent hierarchical structures of images. (ii) pinpointing the most effective retrieval methods and filtering out the reliable retrieval results. (iii) timing the retrieval process to coincide with episodes of low certainty, while circumventing unnecessary retrieval during periods of high certainty. To assess the capability of our proposed ARA model in reducing hallucination, we employ three widely used LVLM models (LLaVA-1.5, Qwen-VL, and mPLUG-Owl2) across four benchmarks. Our empirical observations suggest that by utilizing fitting retrieval mechanisms and timing the retrieval judiciously, we can effectively mitigate the hallucination problem. We hope that this study can provide deeper insights into how to adapt the retrieval augmentation to LVLMs for reducing hallucinations with more effective retrieval and minimal retrieval occurrences.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/171807aeeb88f0c7983bc6cc960b5605441d7121.pdf",
        "venue": "ACM Transactions on Multimedia Computing, Communications, and Applications (TOMCCAP)",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the paper for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem:** Large Vision-Language Models (LVLMs) frequently suffer from \"hallucination,\" generating plausible but factually incorrect responses that misalign with image content \\cite{qu20246yn}.\n    *   **Importance & Challenge:** This problem severely damages the practical employment of LVLMs, especially in critical fields like medical and robotics, where accurate content generation is paramount \\cite{qu20246yn}. While retrieval augmentation has shown promise in Large Language Models (LLMs) for mitigating hallucinations, its application in LVLMs significantly lags and can even exacerbate hallucination when naively transferred due to the multimodal nature of LVLMs \\cite{qu20246yn}.\n\n*   **Related Work & Positioning**\n    *   **Relation to Existing Approaches:**\n        *   **Model Refinement:** Previous attempts include retraining LVLMs with hallucination-related datasets via supervised fine-tuning (SFT) or Reinforcement Learning from Human Feedback (RLHF) \\cite{qu20246yn}.\n        *   **Robust Decoding Strategies:** Other solutions focus on designing training-free decoding strategies, such as visual contrastive decoding (VCD) or integrating penalty terms (OPERA) \\cite{qu20246yn}.\n        *   **Retrieval-Augmented Generation (RAG) in LLMs:** RAG has been widely used in LLMs to retrieve external knowledge, but its direct applicability to LVLMs is limited \\cite{qu20246yn}.\n        *   **RAG in Multimodal Tasks:** Existing multimodal RAG research is largely confined to image captioning or generation, overlooking broader implications for hallucination reduction in LVLMs \\cite{qu20246yn}.\n    *   **Limitations of Previous Solutions:**\n        *   Model refinement methods introduce significant additional training costs and can be prone to overfitting \\cite{qu20246yn}.\n        *   Training-free decoding strategies still suffer from the limitations of LVLMsâ€™ static parametric capacity \\cite{qu20246yn}.\n        *   Findings from RAG in LLMs cannot be indiscriminately extrapolated to LVLMs due to their multimodal nature \\cite{qu20246yn}.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method:** The paper introduces the **Active Retrieval-Augmented large vision-language model (ARA)**, a novel framework specifically designed to address hallucinations in LVLMs \\cite{qu20246yn}.\n    *   **Novelty/Difference:** ARA's innovation lies in incorporating three critical dimensions for retrieval augmentation in LVLMs:\n        1.  **Dissecting Retrieval Targets:** Decomposing targets based on the inherent hierarchical structures of images (coarse-to-fine retrieval) \\cite{qu20246yn}.\n        2.  **Pinpointing Effective Retrieval Methods:** Identifying optimal retrieval techniques and filtering reliable results from diverse multimodal inputs \\cite{qu20246yn}.\n        3.  **Timing Retrieval Process:** Activating retrieval only during periods of low model certainty and knowledge deficiency, avoiding unnecessary retrieval during high certainty \\cite{qu20246yn}.\n    *   **Pipeline:**\n        *   **Active Triggering Retrieval:** Determines whether retrieval is necessary using difficulty metrics based on model confidence, query-image mutual information, or original-noisy image mutual information \\cite{qu20246yn}.\n        *   **Coarse-to-Fine Hierarchical Retrieval:** If retrieval is triggered, it performs both coarse-grained (full-image) and fine-grained (target object/region-specific) retrieval from external knowledge bases \\cite{qu20246yn}.\n        *   **Reranking Strategy:** Retrieved text and image pairs are further reranked (e.g., based on image captioning) to eliminate unreliable outcomes \\cite{qu20246yn}.\n        *   **Joint Decoding:** The LVLM then leverages this refined, externally sourced knowledge to generate the final response \\cite{qu20246yn}.\n\n*   **Key Technical Contributions**\n    *   **Novel Algorithms/Methods:**\n        *   Introduction of active retrieval triggering mechanisms for LVLMs based on confidence and mutual information metrics \\cite{qu20246yn}.\n        *   Development of a coarse-to-fine hierarchical retrieval paradigm tailored for images, combining global and local visual information \\cite{qu20246yn}.\n        *   A reranking strategy for multimodal retrieval results to enhance the reliability of augmented information \\cite{qu20246yn}.\n    *   **System Design/Architectural Innovations:** The ARA framework provides a comprehensive architecture that integrates active decision-making, hierarchical multimodal retrieval, and result refinement for hallucination mitigation \\cite{qu20246yn}.\n    *   **Theoretical Insights/Analysis:** The study provides deeper insights into how to adapt retrieval augmentation to LVLMs effectively, focusing on targeted retrieval and minimal occurrences \\cite{qu20246yn}.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted:** Empirical evaluations were performed using three prevalent LVLMs (LLaVA-1.5, Qwen-VL, and mPLUG-Owl2) across four benchmarks specifically designed to assess hallucination challenges \\cite{qu20246yn}.\n    *   **Key Performance Metrics & Comparison Results:** The results demonstrate that the ARA framework effectively mitigates hallucinations, suggesting that judiciously timed and fitting retrieval mechanisms significantly improve performance \\cite{qu20246yn}.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations/Assumptions:** The approach aims for \"minimal retrieval occurrences\" and \"moderate frequency of retrieval,\" implying a balance between performance gains and computational overhead \\cite{qu20246yn}. The effectiveness is also dependent on the quality and relevance of the external knowledge base and the accuracy of object grounding.\n    *   **Scope of Applicability:** The framework is primarily applicable to LVLMs for tasks requiring factual accuracy in visual comprehension and generation, such as visual question answering \\cite{qu20246yn}.\n\n*   **Technical Significance**\n    *   **Advances State-of-the-Art:** This work significantly advances the technical state-of-the-art by providing a principled and effective method for adapting retrieval augmentation to the unique challenges of LVLMs, moving beyond direct transfer from LLMs \\cite{qu20246yn}.\n    *   **Potential Impact on Future Research:** ARA offers a flexible and potentially cost-effective solution to enhance the factual accuracy and credibility of LVLMs, opening avenues for future research in more robust and reliable multimodal AI systems, especially in high-stakes applications \\cite{qu20246yn}.",
        "keywords": [
          "Large Vision-Language Models (LVLMs)",
          "hallucination mitigation",
          "retrieval augmentation",
          "Active Retrieval-Augmented LVLM (ARA)",
          "multimodal nature",
          "coarse-to-fine hierarchical retrieval",
          "active retrieval triggering",
          "multimodal reranking strategy",
          "model confidence metrics",
          "mutual information metrics",
          "joint decoding",
          "factual accuracy"
        ],
        "paper_type": "based on the abstract and introduction:\n\n*   the **abstract** explicitly mentions \"by employing the retrieval augmentation strategy proposed in this paper\". this directly indicates the presentation of a new method or system.\n*   the **introduction** identifies a \"significant challenge termed 'hallucination'\" in lvlms and states that \"addressing the hallucination problem is paramount\". it then discusses existing \"attempts\" to mitigate this problem, setting the stage for the paper to introduce its own solution. the title \"alleviating hallucination in large vision-language models with active retrieval augmentation\" also points to a proposed solution.\n\nthese elements strongly align with the criteria for a **technical** paper.\n\n**classification:** technical"
      },
      "file_name": "171807aeeb88f0c7983bc6cc960b5605441d7121.pdf"
    },
    {
      "success": true,
      "doc_id": "312fc5186b27496c83f045ae360cd51a",
      "summary": "Here's a focused summary of the paper for a literature review, emphasizing technical innovations and empirical validation:\n\n*   **1. Research Problem & Motivation**\n    *   **Specific Technical Problem**: While Retrieval-Augmented Generation (RAG) is a promising approach to mitigate Large Language Model (LLM) hallucinations, knowledge outdating, and lack of domain-specific expertise, there is a significant lack of rigorous, systematic evaluation of RAG's impact on *different* LLMs. This makes it challenging to identify the specific bottlenecks in LLMs' capabilities when utilizing RAG \\cite{chen2023h04}.\n    *   **Importance and Challenge**: RAG introduces new complexities. External retrieved information often contains noise or even fake news, which can mislead LLMs. Furthermore, LLMs themselves can generate unreliable content or be unduly influenced by incorrect context. A comprehensive understanding of how these factors influence RAG performance and how different LLMs cope with these drawbacks is critically missing.\n\n*   **2. Related Work & Positioning**\n    *   **Relation to Existing Approaches**: The work builds upon existing research in retrieval-augmented models (e.g., Guu et al. 2020; Lewis et al. 2020) that integrate external knowledge to improve LLM responses. It also relates to the broader field of LLM evaluation benchmarks (e.g., MMLU, AGIEval, C-Eval) that assess general model capabilities.\n    *   **Limitations of Previous Solutions**: Existing RAG evaluations often use general QA datasets (e.g., Adlakha et al. 2023) and do not systematically dissect the *specific fundamental abilities* required for robust RAG. General LLM benchmarks fail to capture the nuances of how LLMs interact with and process retrieved external information, especially in the presence of noise or conflicting facts \\cite{chen2023h04}.\n\n*   **3. Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper systematically investigates RAG's impact on LLMs by analyzing their performance across four fundamental abilities crucial for RAG: Noise Robustness, Negative Rejection, Information Integration, and Counterfactual Robustness \\cite{chen2023h04}.\n    *   **Novelty/Differentiation**:\n        *   **Retrieval-Augmented Generation Benchmark (RGB)**: The core innovation is the establishment of RGB, a novel, multi-lingual (English and Chinese) corpus specifically designed for RAG evaluation.\n        *   **Four Testbeds for Fundamental Abilities**: RGB is uniquely structured into four separate testbeds, each tailored to evaluate one of the identified fundamental RAG abilities. This allows for a granular diagnosis of LLM shortcomings.\n        *   **Bias Mitigation in Data Construction**: To prevent LLMs' internal knowledge from biasing evaluation, RGB instances are constructed using *latest news information*. QA pairs are generated by ChatGPT, relevant documents are fetched via Search API, and then re-ranked using dense retrieval models.\n        *   **Specific Testbed Design**: For instance, Noise Robustness testbeds include varying ratios of noisy documents; Negative Rejection testbeds contain *only* noisy documents; Information Integration testbeds require combining facts from multiple documents; and Counterfactual Robustness testbeds include factual errors in retrieved documents, with LLMs explicitly warned about potential risks \\cite{chen2023h04}.\n\n*   **4. Key Technical Contributions**\n    *   **Novel Benchmark**: Introduction of RGB, the first benchmark specifically designed to assess the four critical RAG capabilities (Noise Robustness, Negative Rejection, Information Integration, Counterfactual Robustness) for LLMs in both English and Chinese \\cite{chen2023h04}.\n    *   **Systematic Evaluation Framework**: A novel framework that breaks down RAG performance into distinct, measurable abilities, providing a more comprehensive and diagnostic evaluation than previous approaches.\n    *   **Robust Data Generation Methodology**: A pipeline for creating RAG evaluation data that leverages recent news, LLMs for QA generation, and search engines with dense retrieval for document collection, ensuring relevance and minimizing internal knowledge bias \\cite{chen2023h04}.\n\n*   **5. Experimental Validation**\n    *   **Experiments Conducted**: The authors evaluated 6 state-of-the-art LLMs (ChatGPT, ChatGLM-6B, ChatGLM2-6B, Vicuna-7b, Qwen-7B-Chat, BELLE-7B) on the RGB benchmark \\cite{chen2023h04}.\n    *   **Key Performance Metrics and Comparison Results**:\n        *   **Noise Robustness**: LLMs showed *some* robustness but often confused similar information (e.g., 2021 vs. 2022 Nobel Prize), leading to inaccurate answers when relevant information was present alongside noise.\n        *   **Negative Rejection**: LLMs frequently failed to reject answering when no relevant information was available, instead generating incorrect responses.\n        *   **Information Integration**: LLMs demonstrated a significant lack of ability to synthesize information from multiple documents to answer complex questions.\n        *   **Counterfactual Robustness**: Even when LLMs possessed the correct internal knowledge and were explicitly warned about potential risks in retrieved information, they tended to trust and prioritize the factually incorrect retrieved information \\cite{chen2023h04}.\n        *   **Overall**: While RAG can improve LLM accuracy, current models still struggle significantly with these fundamental challenges, indicating substantial room for improvement.\n\n*   **6. Limitations & Scope**\n    *   **Technical Limitations**: The evaluation of Counterfactual Robustness specifically focuses on scenarios where LLMs are *given warnings* about potential risks in retrieved information, which might not cover all real-world counterfactual scenarios. The definition of \"noisy documents\" is specific to documents relevant to the question but lacking the answer.\n    *   **Scope of Applicability**: The benchmark and findings are primarily applicable to RAG systems where external documents are retrieved from web sources (simulated via Search API) and LLMs are used for generation. The evaluation focuses on specific factual QA tasks derived from news.\n\n*   **7. Technical Significance**\n    *   **Advancement of State-of-the-Art**: This paper significantly advances the technical state-of-the-art by providing the first systematic and multi-faceted benchmark (RGB) for diagnosing the specific RAG capabilities of LLMs \\cite{chen2023h04}. It moves beyond general performance metrics to pinpoint *where* LLMs fail in RAG scenarios.\n    *   **Potential Impact on Future Research**: The findings highlight critical areas for future research and development in RAG, particularly in improving LLMs' abilities in negative rejection, multi-document information integration, and robust handling of conflicting or noisy external knowledge. The RGB benchmark itself serves as a valuable tool for guiding the development of more reliable and robust RAG systems and LLMs \\cite{chen2023h04}.",
      "intriguing_abstract": "While Retrieval-Augmented Generation (RAG) holds immense promise for mitigating Large Language Model (LLM) hallucinations and knowledge gaps, a systematic understanding of its true impact on LLM capabilities remains elusive. We introduce the **Retrieval-Augmented Generation Benchmark (RGB)**, a novel, multi-lingual corpus designed for granular diagnosis of LLM performance in RAG scenarios. RGB uniquely evaluates LLMs across four fundamental abilities critical for robust RAG: **Noise Robustness**, **Negative Rejection**, **Information Integration**, and **Counterfactual Robustness**.\n\nOur meticulously constructed benchmark leverages recent news and dense retrieval to prevent internal knowledge bias. Evaluating six state-of-the-art LLMs, we uncover significant vulnerabilities: models frequently fail to reject irrelevant information, struggle to synthesize facts from multiple documents, and alarmingly, prioritize factually incorrect retrieved information even when explicitly warned. These findings reveal critical bottlenecks in current RAG systems, moving beyond general performance metrics to pinpoint specific areas for improvement. RGB serves as an indispensable diagnostic tool, charting a clear path for developing more reliable, trustworthy, and truly robust RAG-powered LLMs.",
      "keywords": [
        "Retrieval-Augmented Generation (RAG)",
        "LLM hallucinations",
        "systematic RAG evaluation",
        "Retrieval-Augmented Generation Benchmark (RGB)",
        "Noise Robustness",
        "Negative Rejection",
        "Information Integration",
        "Counterfactual Robustness",
        "multi-lingual RAG evaluation",
        "robust data generation methodology",
        "dense retrieval models",
        "LLM performance bottlenecks",
        "conflicting external knowledge",
        "diagnostic evaluation framework"
      ],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/28e2ecb4183ebc0eec504b12dddc677f8aef8745.pdf",
      "citation_key": "chen2023h04",
      "metadata": {
        "title": "Benchmarking Large Language Models in Retrieval-Augmented Generation",
        "authors": [
          "Jiawei Chen",
          "Hongyu Lin",
          "Xianpei Han",
          "Le Sun"
        ],
        "published_date": "2023",
        "abstract": "Retrieval-Augmented Generation (RAG) is a promising approach for mitigating the hallucination of large language models (LLMs). However, existing research lacks rigorous evaluation of the impact of retrieval-augmented generation on different large language models, which make it challenging to identify the potential bottlenecks in the capabilities of RAG for different LLMs. In this paper, we systematically investigate the impact of Retrieval-Augmented Generation on large language models. We analyze the performance of different large language models in 4 fundamental abilities required for RAG, including noise robustness, negative rejection, information integration, and counterfactual robustness. To this end, we establish Retrieval-Augmented Generation Benchmark (RGB), a new corpus for RAG evaluation in both English and Chinese. RGB divides the instances within the benchmark into 4 separate testbeds based on the aforementioned fundamental abilities required to resolve the case. Then we evaluate 6 representative LLMs on RGB to diagnose the challenges of current LLMs when applying RAG. Evaluation reveals that while LLMs exhibit a certain degree of noise robustness, they still struggle significantly in terms of negative rejection, information integration, and dealing with false information. The aforementioned assessment outcomes indicate that there is still a considerable journey ahead to effectively apply RAG to LLMs.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/28e2ecb4183ebc0eec504b12dddc677f8aef8745.pdf",
        "venue": "AAAI Conference on Artificial Intelligence",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the paper for a literature review, emphasizing technical innovations and empirical validation:\n\n*   **1. Research Problem & Motivation**\n    *   **Specific Technical Problem**: While Retrieval-Augmented Generation (RAG) is a promising approach to mitigate Large Language Model (LLM) hallucinations, knowledge outdating, and lack of domain-specific expertise, there is a significant lack of rigorous, systematic evaluation of RAG's impact on *different* LLMs. This makes it challenging to identify the specific bottlenecks in LLMs' capabilities when utilizing RAG \\cite{chen2023h04}.\n    *   **Importance and Challenge**: RAG introduces new complexities. External retrieved information often contains noise or even fake news, which can mislead LLMs. Furthermore, LLMs themselves can generate unreliable content or be unduly influenced by incorrect context. A comprehensive understanding of how these factors influence RAG performance and how different LLMs cope with these drawbacks is critically missing.\n\n*   **2. Related Work & Positioning**\n    *   **Relation to Existing Approaches**: The work builds upon existing research in retrieval-augmented models (e.g., Guu et al. 2020; Lewis et al. 2020) that integrate external knowledge to improve LLM responses. It also relates to the broader field of LLM evaluation benchmarks (e.g., MMLU, AGIEval, C-Eval) that assess general model capabilities.\n    *   **Limitations of Previous Solutions**: Existing RAG evaluations often use general QA datasets (e.g., Adlakha et al. 2023) and do not systematically dissect the *specific fundamental abilities* required for robust RAG. General LLM benchmarks fail to capture the nuances of how LLMs interact with and process retrieved external information, especially in the presence of noise or conflicting facts \\cite{chen2023h04}.\n\n*   **3. Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper systematically investigates RAG's impact on LLMs by analyzing their performance across four fundamental abilities crucial for RAG: Noise Robustness, Negative Rejection, Information Integration, and Counterfactual Robustness \\cite{chen2023h04}.\n    *   **Novelty/Differentiation**:\n        *   **Retrieval-Augmented Generation Benchmark (RGB)**: The core innovation is the establishment of RGB, a novel, multi-lingual (English and Chinese) corpus specifically designed for RAG evaluation.\n        *   **Four Testbeds for Fundamental Abilities**: RGB is uniquely structured into four separate testbeds, each tailored to evaluate one of the identified fundamental RAG abilities. This allows for a granular diagnosis of LLM shortcomings.\n        *   **Bias Mitigation in Data Construction**: To prevent LLMs' internal knowledge from biasing evaluation, RGB instances are constructed using *latest news information*. QA pairs are generated by ChatGPT, relevant documents are fetched via Search API, and then re-ranked using dense retrieval models.\n        *   **Specific Testbed Design**: For instance, Noise Robustness testbeds include varying ratios of noisy documents; Negative Rejection testbeds contain *only* noisy documents; Information Integration testbeds require combining facts from multiple documents; and Counterfactual Robustness testbeds include factual errors in retrieved documents, with LLMs explicitly warned about potential risks \\cite{chen2023h04}.\n\n*   **4. Key Technical Contributions**\n    *   **Novel Benchmark**: Introduction of RGB, the first benchmark specifically designed to assess the four critical RAG capabilities (Noise Robustness, Negative Rejection, Information Integration, Counterfactual Robustness) for LLMs in both English and Chinese \\cite{chen2023h04}.\n    *   **Systematic Evaluation Framework**: A novel framework that breaks down RAG performance into distinct, measurable abilities, providing a more comprehensive and diagnostic evaluation than previous approaches.\n    *   **Robust Data Generation Methodology**: A pipeline for creating RAG evaluation data that leverages recent news, LLMs for QA generation, and search engines with dense retrieval for document collection, ensuring relevance and minimizing internal knowledge bias \\cite{chen2023h04}.\n\n*   **5. Experimental Validation**\n    *   **Experiments Conducted**: The authors evaluated 6 state-of-the-art LLMs (ChatGPT, ChatGLM-6B, ChatGLM2-6B, Vicuna-7b, Qwen-7B-Chat, BELLE-7B) on the RGB benchmark \\cite{chen2023h04}.\n    *   **Key Performance Metrics and Comparison Results**:\n        *   **Noise Robustness**: LLMs showed *some* robustness but often confused similar information (e.g., 2021 vs. 2022 Nobel Prize), leading to inaccurate answers when relevant information was present alongside noise.\n        *   **Negative Rejection**: LLMs frequently failed to reject answering when no relevant information was available, instead generating incorrect responses.\n        *   **Information Integration**: LLMs demonstrated a significant lack of ability to synthesize information from multiple documents to answer complex questions.\n        *   **Counterfactual Robustness**: Even when LLMs possessed the correct internal knowledge and were explicitly warned about potential risks in retrieved information, they tended to trust and prioritize the factually incorrect retrieved information \\cite{chen2023h04}.\n        *   **Overall**: While RAG can improve LLM accuracy, current models still struggle significantly with these fundamental challenges, indicating substantial room for improvement.\n\n*   **6. Limitations & Scope**\n    *   **Technical Limitations**: The evaluation of Counterfactual Robustness specifically focuses on scenarios where LLMs are *given warnings* about potential risks in retrieved information, which might not cover all real-world counterfactual scenarios. The definition of \"noisy documents\" is specific to documents relevant to the question but lacking the answer.\n    *   **Scope of Applicability**: The benchmark and findings are primarily applicable to RAG systems where external documents are retrieved from web sources (simulated via Search API) and LLMs are used for generation. The evaluation focuses on specific factual QA tasks derived from news.\n\n*   **7. Technical Significance**\n    *   **Advancement of State-of-the-Art**: This paper significantly advances the technical state-of-the-art by providing the first systematic and multi-faceted benchmark (RGB) for diagnosing the specific RAG capabilities of LLMs \\cite{chen2023h04}. It moves beyond general performance metrics to pinpoint *where* LLMs fail in RAG scenarios.\n    *   **Potential Impact on Future Research**: The findings highlight critical areas for future research and development in RAG, particularly in improving LLMs' abilities in negative rejection, multi-document information integration, and robust handling of conflicting or noisy external knowledge. The RGB benchmark itself serves as a valuable tool for guiding the development of more reliable and robust RAG systems and LLMs \\cite{chen2023h04}.",
        "keywords": [
          "Retrieval-Augmented Generation (RAG)",
          "LLM hallucinations",
          "systematic RAG evaluation",
          "Retrieval-Augmented Generation Benchmark (RGB)",
          "Noise Robustness",
          "Negative Rejection",
          "Information Integration",
          "Counterfactual Robustness",
          "multi-lingual RAG evaluation",
          "robust data generation methodology",
          "dense retrieval models",
          "LLM performance bottlenecks",
          "conflicting external knowledge",
          "diagnostic evaluation framework"
        ],
        "paper_type": "the paper type is **empirical**.\n\n**reasoning:**\n\n*   the abstract explicitly states: \"we systematically investigate the impact of retrieval-augmented generation on large language models.\"\n*   it details the methodology: \"we analyze the performance of different large language models in 4 fundamental abilities...\"\n*   it describes the creation of a data-driven artifact for evaluation: \"to this end, we establish retrieval-augmented generation benchmark (rgb), a new corpus for rag evaluation...\"\n*   it outlines the experimental setup: \"then we evaluate 6 representative llms on rgb to diagnose the challenges...\"\n*   it presents clear findings: \"evaluation reveals that while llms exhibit a certain degree of noise robustness, they still struggle significantly in terms of negative rejection, information integration, and dealing with false information.\"\n\nthese elements strongly align with the criteria for an **empirical** paper, which focuses on data-driven studies, experiments, and statistical analysis to derive findings."
      },
      "file_name": "28e2ecb4183ebc0eec504b12dddc677f8aef8745.pdf"
    },
    {
      "success": true,
      "doc_id": "2e0d2e66743548a494f33e3d7ed51498",
      "summary": "Here's a focused summary of the paper \"Aligning Modalities in Vision Large Language Models via Preference Fine-tuning\" \\cite{zhou2024wbi} for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Vision Large Language Models (VLLMs) frequently suffer from \"hallucinations,\" where they generate content (e.g., imagined objects, incorrect relationships, wrong categories) that is not grounded in the input image. This occurs even when the underlying LLM is factual and the vision backbone provides complete representations.\n    *   **Importance and Challenge**: Hallucinations pose significant risks in high-stakes applications like medical diagnostics or autonomous driving. The problem is challenging because it stems from a misalignment between separately trained vision and language components, leading VLLMs to prioritize common sense or stereotypes from language data over actual visual input. Existing preference tuning methods often struggle with VLLMs because both preferred and dispreferred responses might be incorrect, making accurate image-text alignment difficult.\n\n*   **Related Work & Positioning**\n    *   **Existing Approaches**: Prior work includes reinforcement learning from human feedback (RLHF) and Direct Preference Optimization (DPO) frameworks.\n    *   **Limitations of Previous Solutions**:\n        *   Concurrent DPO works (e.g., Li et al., 2023d; Zhao et al., 2023) typically generate both preferred and dispreferred responses, where *both* might be incorrect for the given image-centric task, hindering effective alignment.\n        *   Approaches relying on human feedback (e.g., Yu et al., 2023a) are effective but incur significant costs and are not easily scalable.\n        *   These methods often fail to explicitly address the unique challenge of generating preference data for VLLMs, where responses are inherently linked to image data.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes **POVID (Preference Optimization in VLLM with AI-Generated Dispreferences)**, a novel framework that tackles VLLM hallucination by framing it as a modality alignment issue and addressing it with preference tuning. POVID exclusively generates *dispreferred* feedback data using AI models, while treating ground-truth instructions as preferred responses.\n    *   **Novelty/Difference**: POVID's key innovation lies in its two-stage, automated approach to generating dispreferred responses, eliminating the need for human labeling:\n        1.  **Hallucinating Textual Responses**: Utilizes GPT-4V to inject plausible hallucinations into the *correct* ground-truth answer. This is tailored for different tasks:\n            *   For image captioning, it introduces errors related to object co-occurrence, logical relationships between entities, and incorrect entity attributes.\n            *   For reasoning tasks (VQA), it modifies the reasoning process itself, introducing logical errors while attempting to keep changes subtle.\n        2.  **Mitigating Inherent Hallucination Patterns**: Introduces controlled diffusion noise into the input image during training. This noise disrupts the VLLM's visual comprehension, provoking its *inherent* hallucination patterns (e.g., prioritizing textual context or learned associations over visual cues). The responses generated under these noisy conditions are treated as dispreferred.\n    *   **Integration**: Both types of AI-generated dispreferences are integrated into a Direct Preference Optimization (DPO) framework via a reformulated loss function (L_POVID), which balances preferred and dispreferred terms. The noise-triggered dispreferred responses are conditioned on prior tokens from the preferred response to control reliability.\n\n*   **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**: The POVID framework for VLLM preference optimization.\n    *   **Data Generation Technique**: A scalable, automated, two-stage AI-generated dispreference data creation process, leveraging GPT-4V for textual hallucination injection and diffusion noise for triggering inherent VLLM hallucinations.\n    *   **Optimization Formulation**: A reformulated DPO loss (L_POVID) that effectively combines dispreferences from both textual manipulation and image distortion, specifically designed for VLLM modality alignment.\n    *   **Mechanism for Inherent Hallucination**: A method to provoke and correct inherent hallucination patterns in the target VLLM by introducing controlled noise to the input image during real-time training.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted**: The authors conducted experiments across a broad range of VLLM benchmarks to evaluate POVID's effectiveness in reducing hallucinations and improving overall performance.\n    *   **Key Performance Metrics and Comparison Results**:\n        *   **Benchmarks**: SciQA-IMG, MMBench, MM-Vet, LLaVA-Bench, 100-CHAIRS, 100-CHAIRi, and POPEMMHal.\n        *   **Comparison**: POVID was compared against LLaVA-1.5 (baseline) and other preference learning approaches for VLLMs, including Vlfeedback, Human-Preference, and RLHF-V.\n        *   **Results**: POVID significantly outperformed prior approaches, achieving an average performance improvement of 12.4% across the evaluated benchmarks. The results demonstrate that POVID not only reduces hallucinations but also enhances overall VLLM performance and redirects the model's attention towards the image modality, leading to better alignment.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The approach relies on the quality and controllability of GPT-4V for generating plausible textual hallucinations and the effectiveness of diffusion noise in reliably triggering *inherent* VLLM hallucinations within a \"reasonable range\" where the image remains recognizable. The specific hyperparameters for noise steps (`k`) and loss coefficients (`Î±, Î²1, Î²2`) are crucial for performance.\n    *   **Scope of Applicability**: POVID is primarily applicable to VLLMs that require improved modality alignment and hallucination reduction, particularly in tasks like image captioning and visual question answering. Its automated nature makes it suitable for large-scale deployment.\n\n*   **Technical Significance**\n    *   **Advancement of State-of-the-Art**: POVID significantly advances the technical state-of-the-art in VLLM alignment and hallucination mitigation by introducing a novel, automated, and scalable preference tuning framework. It moves beyond costly human feedback and addresses the unique challenges of preference data generation for image-centric models.\n    *   **Potential Impact**: This work has the potential to enable the development of more reliable, factual, and trustworthy VLLMs, making them safer for deployment in critical applications. It opens new avenues for research into automated, AI-driven feedback generation for multimodal models and more sophisticated techniques for aligning disparate modalities.",
      "intriguing_abstract": "Vision Large Language Models (VLLMs) are plagued by persistent \"hallucinations,\" generating content ungrounded in visual input and posing severe risks in critical applications. Current preference tuning methods struggle with VLLM-specific challenges, often relying on costly human feedback or generating ineffective dispreferences. We introduce **POVID (Preference Optimization in VLLM with AI-Generated Dispreferences)**, a novel framework that tackles VLLM hallucination as a fundamental modality alignment problem through an innovative, automated preference fine-tuning approach.\n\nPOVID's core innovation lies in its two-stage, AI-driven dispreference generation: first, leveraging GPT-4V to inject plausible textual hallucinations into ground-truth answers; second, strategically introducing diffusion noise to input images to provoke and capture the VLLM's inherent hallucination patterns. These AI-generated dispreferences are seamlessly integrated into a reformulated Direct Preference Optimization (DPO) loss. Our extensive experiments across diverse VLLM benchmarks demonstrate POVID's superior efficacy, achieving an average 12.4% performance improvement and significantly reducing hallucinations. This work offers a scalable solution for building more reliable, factual, and trustworthy VLLMs, paving the way for safer multimodal AI deployments.",
      "keywords": [
        "Vision Large Language Models (VLLMs)",
        "hallucinations",
        "modality alignment",
        "preference fine-tuning",
        "POVID framework",
        "AI-generated dispreferences",
        "automated data generation",
        "Direct Preference Optimization (DPO)",
        "GPT-4V hallucination injection",
        "diffusion noise",
        "reformulated DPO loss",
        "hallucination reduction",
        "VLLM performance improvement",
        "image captioning",
        "visual question answering"
      ],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/f8a642fbb51e0b0ae4774781309545d15d6d9b11.pdf",
      "citation_key": "zhou2024wbi",
      "metadata": {
        "title": "Aligning Modalities in Vision Large Language Models via Preference Fine-tuning",
        "authors": [
          "Yiyang Zhou",
          "Chenhang Cui",
          "Rafael Rafailov",
          "Chelsea Finn",
          "Huaxiu Yao"
        ],
        "published_date": "2024",
        "abstract": "Instruction-following Vision Large Language Models (VLLMs) have achieved significant progress recently on a variety of tasks. These approaches merge strong pre-trained vision models and large language models (LLMs). Since these components are trained separately, the learned representations need to be aligned with joint training on additional image-language pairs. This procedure is not perfect and can cause the model to hallucinate - provide answers that do not accurately reflect the image, even when the core LLM is highly factual and the vision backbone has sufficiently complete representations. In this work, we frame the hallucination problem as an alignment issue, tackle it with preference tuning. Specifically, we propose POVID to generate feedback data with AI models. We use ground-truth instructions as the preferred response and a two-stage approach to generate dispreferred data. First, we prompt GPT-4V to inject plausible hallucinations into the correct answer. Second, we distort the image to trigger the inherent hallucination behavior of the VLLM. This is an automated approach, which does not rely on human data generation or require a perfect expert, which makes it easily scalable. Finally, both of these generation strategies are integrated into an RLHF pipeline via Direct Preference Optimization. In experiments across broad benchmarks, we show that we can not only reduce hallucinations, but improve model performance across standard benchmarks, outperforming prior approaches. Our data and code are available at https://github.com/YiyangZhou/POVID.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/f8a642fbb51e0b0ae4774781309545d15d6d9b11.pdf",
        "venue": "arXiv.org",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the paper \"Aligning Modalities in Vision Large Language Models via Preference Fine-tuning\" \\cite{zhou2024wbi} for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Vision Large Language Models (VLLMs) frequently suffer from \"hallucinations,\" where they generate content (e.g., imagined objects, incorrect relationships, wrong categories) that is not grounded in the input image. This occurs even when the underlying LLM is factual and the vision backbone provides complete representations.\n    *   **Importance and Challenge**: Hallucinations pose significant risks in high-stakes applications like medical diagnostics or autonomous driving. The problem is challenging because it stems from a misalignment between separately trained vision and language components, leading VLLMs to prioritize common sense or stereotypes from language data over actual visual input. Existing preference tuning methods often struggle with VLLMs because both preferred and dispreferred responses might be incorrect, making accurate image-text alignment difficult.\n\n*   **Related Work & Positioning**\n    *   **Existing Approaches**: Prior work includes reinforcement learning from human feedback (RLHF) and Direct Preference Optimization (DPO) frameworks.\n    *   **Limitations of Previous Solutions**:\n        *   Concurrent DPO works (e.g., Li et al., 2023d; Zhao et al., 2023) typically generate both preferred and dispreferred responses, where *both* might be incorrect for the given image-centric task, hindering effective alignment.\n        *   Approaches relying on human feedback (e.g., Yu et al., 2023a) are effective but incur significant costs and are not easily scalable.\n        *   These methods often fail to explicitly address the unique challenge of generating preference data for VLLMs, where responses are inherently linked to image data.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes **POVID (Preference Optimization in VLLM with AI-Generated Dispreferences)**, a novel framework that tackles VLLM hallucination by framing it as a modality alignment issue and addressing it with preference tuning. POVID exclusively generates *dispreferred* feedback data using AI models, while treating ground-truth instructions as preferred responses.\n    *   **Novelty/Difference**: POVID's key innovation lies in its two-stage, automated approach to generating dispreferred responses, eliminating the need for human labeling:\n        1.  **Hallucinating Textual Responses**: Utilizes GPT-4V to inject plausible hallucinations into the *correct* ground-truth answer. This is tailored for different tasks:\n            *   For image captioning, it introduces errors related to object co-occurrence, logical relationships between entities, and incorrect entity attributes.\n            *   For reasoning tasks (VQA), it modifies the reasoning process itself, introducing logical errors while attempting to keep changes subtle.\n        2.  **Mitigating Inherent Hallucination Patterns**: Introduces controlled diffusion noise into the input image during training. This noise disrupts the VLLM's visual comprehension, provoking its *inherent* hallucination patterns (e.g., prioritizing textual context or learned associations over visual cues). The responses generated under these noisy conditions are treated as dispreferred.\n    *   **Integration**: Both types of AI-generated dispreferences are integrated into a Direct Preference Optimization (DPO) framework via a reformulated loss function (L_POVID), which balances preferred and dispreferred terms. The noise-triggered dispreferred responses are conditioned on prior tokens from the preferred response to control reliability.\n\n*   **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**: The POVID framework for VLLM preference optimization.\n    *   **Data Generation Technique**: A scalable, automated, two-stage AI-generated dispreference data creation process, leveraging GPT-4V for textual hallucination injection and diffusion noise for triggering inherent VLLM hallucinations.\n    *   **Optimization Formulation**: A reformulated DPO loss (L_POVID) that effectively combines dispreferences from both textual manipulation and image distortion, specifically designed for VLLM modality alignment.\n    *   **Mechanism for Inherent Hallucination**: A method to provoke and correct inherent hallucination patterns in the target VLLM by introducing controlled noise to the input image during real-time training.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted**: The authors conducted experiments across a broad range of VLLM benchmarks to evaluate POVID's effectiveness in reducing hallucinations and improving overall performance.\n    *   **Key Performance Metrics and Comparison Results**:\n        *   **Benchmarks**: SciQA-IMG, MMBench, MM-Vet, LLaVA-Bench, 100-CHAIRS, 100-CHAIRi, and POPEMMHal.\n        *   **Comparison**: POVID was compared against LLaVA-1.5 (baseline) and other preference learning approaches for VLLMs, including Vlfeedback, Human-Preference, and RLHF-V.\n        *   **Results**: POVID significantly outperformed prior approaches, achieving an average performance improvement of 12.4% across the evaluated benchmarks. The results demonstrate that POVID not only reduces hallucinations but also enhances overall VLLM performance and redirects the model's attention towards the image modality, leading to better alignment.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The approach relies on the quality and controllability of GPT-4V for generating plausible textual hallucinations and the effectiveness of diffusion noise in reliably triggering *inherent* VLLM hallucinations within a \"reasonable range\" where the image remains recognizable. The specific hyperparameters for noise steps (`k`) and loss coefficients (`Î±, Î²1, Î²2`) are crucial for performance.\n    *   **Scope of Applicability**: POVID is primarily applicable to VLLMs that require improved modality alignment and hallucination reduction, particularly in tasks like image captioning and visual question answering. Its automated nature makes it suitable for large-scale deployment.\n\n*   **Technical Significance**\n    *   **Advancement of State-of-the-Art**: POVID significantly advances the technical state-of-the-art in VLLM alignment and hallucination mitigation by introducing a novel, automated, and scalable preference tuning framework. It moves beyond costly human feedback and addresses the unique challenges of preference data generation for image-centric models.\n    *   **Potential Impact**: This work has the potential to enable the development of more reliable, factual, and trustworthy VLLMs, making them safer for deployment in critical applications. It opens new avenues for research into automated, AI-driven feedback generation for multimodal models and more sophisticated techniques for aligning disparate modalities.",
        "keywords": [
          "Vision Large Language Models (VLLMs)",
          "hallucinations",
          "modality alignment",
          "preference fine-tuning",
          "POVID framework",
          "AI-generated dispreferences",
          "automated data generation",
          "Direct Preference Optimization (DPO)",
          "GPT-4V hallucination injection",
          "diffusion noise",
          "reformulated DPO loss",
          "hallucination reduction",
          "VLLM performance improvement",
          "image captioning",
          "visual question answering"
        ],
        "paper_type": "based on the provided abstract and introduction:\n\n*   the abstract explicitly states: \"we propose povid to generate feedback data with ai models.\" it then details a \"two-stage approach\" and an \"automated approach\" integrated into an \"rlhf pipeline via direct preference optimization.\" these are all descriptions of a new method or system.\n*   the introduction discusses a technical problem (\"hallucinations\" in vllms) and immediately presents \"povid (ours)\" as a solution, showing a performance comparison against other approaches (figure 1).\n*   while the paper includes \"experiments across broad benchmarks\" and \"improve model performance,\" these are the empirical evaluations of the *new method* being proposed. the core contribution is the development and presentation of povid.\n\nthis aligns perfectly with the criteria for a **technical** paper: \"presents new methods, algorithms, or systems\" and \"abstract mentions: 'propose', 'develop', 'present', 'algorithm', 'method'\".\n\n**classification: technical**"
      },
      "file_name": "f8a642fbb51e0b0ae4774781309545d15d6d9b11.pdf"
    },
    {
      "success": true,
      "doc_id": "a89f657e78005a3c15e3a652225ef314",
      "summary": "Here's a focused summary of the paper `\\cite{jiang2024792}` for a literature review:\n\n### Hal-Eval: a Universal and Fine-grained Hallucination Evaluation Framework for Large Vision Language Models \\cite{jiang2024792}\n\n1.  **Research Problem & Motivation**\n    *   **Problem:** Large Vision-Language Models (LVLMs) frequently suffer from \"hallucinations,\" where generated text descriptions are inconsistent with image content. Existing hallucination evaluation methods for LVLMs are incomplete, primarily focusing on object, attribute, and relation hallucinations.\n    *   **Motivation:** These prior approaches overlook more complex \"event hallucinations\" that involve inventing fictional entities and weaving entire narratives around them (e.g., actions, attributes, relationships of non-existent elements). Such complex hallucinations significantly escalate with increased output length, posing a critical challenge to LVLM reliability and robustness, and necessitating a more comprehensive evaluation framework.\n\n2.  **Related Work & Positioning**\n    *   **Existing Approaches:** Previous hallucination benchmarks are categorized into discriminative (e.g., POPE, NOPE, CIEM) and generative methods. Discriminative benchmarks primarily focus on object-level hallucinations. Generative benchmarks broaden the scope to include attribute and relation hallucinations. AMBER is noted as a holistic benchmark combining both.\n    *   **Limitations of Previous Solutions:**\n        *   Existing taxonomies (object, attribute, relation) are insufficient to cover the full spectrum of LVLM hallucinations, particularly neglecting complex \"event hallucinations.\"\n        *   Many benchmarks use in-domain data (e.g., COCO) for evaluation, which can overlap with LVLM fine-tuning data, hindering a true assessment of zero-shot hallucination capabilities.\n        *   There is a lack of fine-grained hallucination evaluation benchmarks that comprehensively address various hallucination types (object, attribute, relation, *and* event) while unifying both discriminative and generative evaluation methodologies.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method:** `\\cite{jiang2024792}` introduces Hal-Eval, a universal and fine-grained hallucination evaluation framework for LVLMs that integrates both discriminative and generative evaluation methods.\n    *   **Novel Taxonomy:** Proposes a refined taxonomy of hallucinations, introducing a new category: **Event Hallucination**. This type involves describing a non-existent target and constructing a complete narrative around it, including its attributes, relations, and actions.\n    *   **Automatic Annotation Pipeline:** Develops an automatic fine-grained hallucination annotation pipeline leveraging advanced LLMs (GPT-4) to generate and filter hallucinatory data. This pipeline injects specific hallucinatory elements (object, attribute, relation, event) into original image captions and annotates their positions. A subsequent GPT-4-based filtering step ensures high data quality (>97% adherence).\n    *   **Unified Evaluation:**\n        *   **Discriminative Evaluation:** Constructs a dataset with image captions containing various hallucination types (in-domain and out-of-domain). LVLMs are prompted with a natural question (\"Does the description in the caption accurately reflect the content of the image?\") to determine the presence of specific hallucinations.\n        *   **Generative Evaluation:** The pipeline creates a large-scale hallucinatory dataset (Hal-Data) used to fine-tune a specialized LVLM, the **Hal-Evaluator**. This evaluator assesses LVLM-generated descriptions and associated images to identify various hallucination types *without needing reference captions*.\n\n4.  **Key Technical Contributions**\n    *   **Novel Hallucination Category:** Introduction and formal definition of \"Event Hallucination,\" addressing a previously overlooked complex type of hallucination.\n    *   **Universal and Fine-grained Evaluation Framework (Hal-Eval):** A comprehensive framework that evaluates a broad spectrum of hallucination types (object, attribute, relation, event) and unifies discriminative and generative evaluation methodologies.\n    *   **Automatic Annotation Pipeline:** A robust, LLM-powered pipeline for generating high-quality, fine-grained hallucinatory datasets, crucial for training evaluators and benchmarking.\n    *   **Hal-Evaluator:** A specialized LVLM trained to automatically detect and categorize hallucinations in generated text, enabling efficient generative evaluation without human intervention or reference captions.\n    *   **Out-of-Domain Evaluation Data:** Construction of evaluation datasets including out-of-domain samples to provide a more accurate assessment of LVLMs' zero-shot hallucination capabilities.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted:** Thorough experiments were performed with six leading LVLMs under both discriminative and generative paradigms using the Hal-Eval framework.\n    *   **Key Findings & Results:**\n        *   **Insufficiency of Existing Categories:** Empirical evidence (Figure 2) demonstrates that event hallucinations significantly increase with output length, confirming that the existing three categories are insufficient.\n        *   **Impact of Chain-of-Thought (COT):** Utilizing COT significantly helps models minimize hallucinations during discriminative evaluations, particularly for relationship and event types.\n        *   **Output Length Correlation:** The incidence of hallucinations, especially event hallucinations, increases with the length of the LVLM's output. Length control is a crucial aspect of generative evaluations, affecting comparative performance trends.\n        *   **Evaluation Methodology Suitability:** The suitability of discriminative vs. generative evaluation varies by hallucination type, suggesting that combining both provides a more comprehensive view of LVLM hallucination tendencies.\n        *   **Dual Use of Hal-Data:** The hallucinatory samples used to train the Hal-Evaluator also prove effective as supervised fine-tuning (SFT) data for LVLMs, contributing to reducing hallucinations and enhancing benchmark performance.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations:** The annotation pipeline relies heavily on the capabilities of advanced LLMs like GPT-4 for generation and filtering, which might introduce biases or limitations inherent to these models.\n    *   **Scope of Applicability:** The framework is designed for Large Vision-Language Models and focuses specifically on image-to-text generation hallucinations. While comprehensive for this domain, its direct applicability to other modalities or types of model failures is not explicitly discussed.\n\n7.  **Technical Significance**\n    *   **Advances State-of-the-Art:** `\\cite{jiang2024792}` significantly advances the technical state-of-the-art in LVLM hallucination evaluation by introducing a novel, more comprehensive taxonomy and a unified, fine-grained evaluation framework.\n    *   **Improved Reliability:** Provides a more reliable and comprehensive tool for gauging LVLMs' efficacy in handling a broader spectrum of hallucinations, which is crucial for their practical deployment.\n    *   **Impact on Future Research:** Offers groundbreaking insights into the nature of LVLM hallucinations (e.g., the prevalence of event hallucinations with output length, the benefits of COT, the dual role of hallucinatory data), guiding future research towards more effective hallucination mitigation strategies and robust LVLM development.",
      "intriguing_abstract": "Large Vision-Language Models (LVLMs) are plagued by pervasive hallucinations, where generated descriptions diverge from image content. Existing evaluation frameworks, however, critically overlook complex \"Event Hallucinations\"â€”the invention of entire narratives around non-existent entities, a problem escalating significantly with output length. We introduce **Hal-Eval**, a universal and fine-grained framework that revolutionizes LVLM hallucination assessment. Our novel taxonomy formally defines **Event Hallucination**, alongside object, attribute, and relation types, providing an unprecedented comprehensive scope.\n\nHal-Eval unifies both discriminative and generative evaluation methodologies. We develop an innovative, GPT-4-powered automatic annotation pipeline to create high-quality, fine-grained hallucinatory datasets, including out-of-domain samples for robust zero-shot evaluation. Crucially, this data trains our **Hal-Evaluator**, a specialized LVLM capable of automatically detecting and categorizing hallucinations in generated text without reference captions. Experiments across leading LVLMs confirm that Event Hallucinations are a major concern, and reveal that Chain-of-Thought prompting significantly mitigates these complex errors. Hal-Eval offers a critical tool for advancing LVLM reliability, guiding future research towards truly robust and trustworthy models.",
      "keywords": [
        "Large Vision-Language Models (LVLMs)",
        "hallucinations",
        "Hal-Eval framework",
        "Event Hallucination",
        "fine-grained hallucination evaluation",
        "unified discriminative and generative evaluation",
        "automatic annotation pipeline",
        "Hal-Evaluator",
        "out-of-domain evaluation",
        "Chain-of-Thought (COT)",
        "output length correlation with hallucinations",
        "LVLM reliability"
      ],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/05839a68bd05880beef2f171cee7aab960bb6d2f.pdf",
      "citation_key": "jiang2024792",
      "metadata": {
        "title": "Hal-Eval: A Universal and Fine-grained Hallucination Evaluation Framework for Large Vision Language Models",
        "authors": [
          "Chaoya Jiang",
          "Wei Ye",
          "Mengfan Dong",
          "Hongrui Jia",
          "Haiyang Xu",
          "Mingshi Yan",
          "Ji Zhang",
          "Shikun Zhang"
        ],
        "published_date": "2024",
        "abstract": "Large Vision-Language Models (LVLMs) exhibit remarkable capabilities but struggle with ''hallucinations''-inconsistencies between images and their descriptions. Previous hallucination evaluation studies on LVLMs have identified hallucinations in terms of objects, attributes, and relations but overlooked complex hallucinations that create an entire narrative around a fictional entity. In this paper, we introduce a refined taxonomy of hallucinations, featuring a new category: Event Hallucination. We then utilize advanced LLMs to generate and filter fine-grained hallucinatory data consisting of various types of hallucinations, with a particular focus on event hallucinations, laying the groundwork for integrating discriminative and generative evaluation methods within our universal evaluation framework. The proposed benchmark distinctively assesses LVLMs' ability to tackle a broad spectrum of hallucinations, making it a reliable and comprehensive tool for gauging LVLMs' efficacy in handling hallucinations. We will release our code and data.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/05839a68bd05880beef2f171cee7aab960bb6d2f.pdf",
        "venue": "ACM Multimedia",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the paper `\\cite{jiang2024792}` for a literature review:\n\n### Hal-Eval: a Universal and Fine-grained Hallucination Evaluation Framework for Large Vision Language Models \\cite{jiang2024792}\n\n1.  **Research Problem & Motivation**\n    *   **Problem:** Large Vision-Language Models (LVLMs) frequently suffer from \"hallucinations,\" where generated text descriptions are inconsistent with image content. Existing hallucination evaluation methods for LVLMs are incomplete, primarily focusing on object, attribute, and relation hallucinations.\n    *   **Motivation:** These prior approaches overlook more complex \"event hallucinations\" that involve inventing fictional entities and weaving entire narratives around them (e.g., actions, attributes, relationships of non-existent elements). Such complex hallucinations significantly escalate with increased output length, posing a critical challenge to LVLM reliability and robustness, and necessitating a more comprehensive evaluation framework.\n\n2.  **Related Work & Positioning**\n    *   **Existing Approaches:** Previous hallucination benchmarks are categorized into discriminative (e.g., POPE, NOPE, CIEM) and generative methods. Discriminative benchmarks primarily focus on object-level hallucinations. Generative benchmarks broaden the scope to include attribute and relation hallucinations. AMBER is noted as a holistic benchmark combining both.\n    *   **Limitations of Previous Solutions:**\n        *   Existing taxonomies (object, attribute, relation) are insufficient to cover the full spectrum of LVLM hallucinations, particularly neglecting complex \"event hallucinations.\"\n        *   Many benchmarks use in-domain data (e.g., COCO) for evaluation, which can overlap with LVLM fine-tuning data, hindering a true assessment of zero-shot hallucination capabilities.\n        *   There is a lack of fine-grained hallucination evaluation benchmarks that comprehensively address various hallucination types (object, attribute, relation, *and* event) while unifying both discriminative and generative evaluation methodologies.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method:** `\\cite{jiang2024792}` introduces Hal-Eval, a universal and fine-grained hallucination evaluation framework for LVLMs that integrates both discriminative and generative evaluation methods.\n    *   **Novel Taxonomy:** Proposes a refined taxonomy of hallucinations, introducing a new category: **Event Hallucination**. This type involves describing a non-existent target and constructing a complete narrative around it, including its attributes, relations, and actions.\n    *   **Automatic Annotation Pipeline:** Develops an automatic fine-grained hallucination annotation pipeline leveraging advanced LLMs (GPT-4) to generate and filter hallucinatory data. This pipeline injects specific hallucinatory elements (object, attribute, relation, event) into original image captions and annotates their positions. A subsequent GPT-4-based filtering step ensures high data quality (>97% adherence).\n    *   **Unified Evaluation:**\n        *   **Discriminative Evaluation:** Constructs a dataset with image captions containing various hallucination types (in-domain and out-of-domain). LVLMs are prompted with a natural question (\"Does the description in the caption accurately reflect the content of the image?\") to determine the presence of specific hallucinations.\n        *   **Generative Evaluation:** The pipeline creates a large-scale hallucinatory dataset (Hal-Data) used to fine-tune a specialized LVLM, the **Hal-Evaluator**. This evaluator assesses LVLM-generated descriptions and associated images to identify various hallucination types *without needing reference captions*.\n\n4.  **Key Technical Contributions**\n    *   **Novel Hallucination Category:** Introduction and formal definition of \"Event Hallucination,\" addressing a previously overlooked complex type of hallucination.\n    *   **Universal and Fine-grained Evaluation Framework (Hal-Eval):** A comprehensive framework that evaluates a broad spectrum of hallucination types (object, attribute, relation, event) and unifies discriminative and generative evaluation methodologies.\n    *   **Automatic Annotation Pipeline:** A robust, LLM-powered pipeline for generating high-quality, fine-grained hallucinatory datasets, crucial for training evaluators and benchmarking.\n    *   **Hal-Evaluator:** A specialized LVLM trained to automatically detect and categorize hallucinations in generated text, enabling efficient generative evaluation without human intervention or reference captions.\n    *   **Out-of-Domain Evaluation Data:** Construction of evaluation datasets including out-of-domain samples to provide a more accurate assessment of LVLMs' zero-shot hallucination capabilities.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted:** Thorough experiments were performed with six leading LVLMs under both discriminative and generative paradigms using the Hal-Eval framework.\n    *   **Key Findings & Results:**\n        *   **Insufficiency of Existing Categories:** Empirical evidence (Figure 2) demonstrates that event hallucinations significantly increase with output length, confirming that the existing three categories are insufficient.\n        *   **Impact of Chain-of-Thought (COT):** Utilizing COT significantly helps models minimize hallucinations during discriminative evaluations, particularly for relationship and event types.\n        *   **Output Length Correlation:** The incidence of hallucinations, especially event hallucinations, increases with the length of the LVLM's output. Length control is a crucial aspect of generative evaluations, affecting comparative performance trends.\n        *   **Evaluation Methodology Suitability:** The suitability of discriminative vs. generative evaluation varies by hallucination type, suggesting that combining both provides a more comprehensive view of LVLM hallucination tendencies.\n        *   **Dual Use of Hal-Data:** The hallucinatory samples used to train the Hal-Evaluator also prove effective as supervised fine-tuning (SFT) data for LVLMs, contributing to reducing hallucinations and enhancing benchmark performance.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations:** The annotation pipeline relies heavily on the capabilities of advanced LLMs like GPT-4 for generation and filtering, which might introduce biases or limitations inherent to these models.\n    *   **Scope of Applicability:** The framework is designed for Large Vision-Language Models and focuses specifically on image-to-text generation hallucinations. While comprehensive for this domain, its direct applicability to other modalities or types of model failures is not explicitly discussed.\n\n7.  **Technical Significance**\n    *   **Advances State-of-the-Art:** `\\cite{jiang2024792}` significantly advances the technical state-of-the-art in LVLM hallucination evaluation by introducing a novel, more comprehensive taxonomy and a unified, fine-grained evaluation framework.\n    *   **Improved Reliability:** Provides a more reliable and comprehensive tool for gauging LVLMs' efficacy in handling a broader spectrum of hallucinations, which is crucial for their practical deployment.\n    *   **Impact on Future Research:** Offers groundbreaking insights into the nature of LVLM hallucinations (e.g., the prevalence of event hallucinations with output length, the benefits of COT, the dual role of hallucinatory data), guiding future research towards more effective hallucination mitigation strategies and robust LVLM development.",
        "keywords": [
          "Large Vision-Language Models (LVLMs)",
          "hallucinations",
          "Hal-Eval framework",
          "Event Hallucination",
          "fine-grained hallucination evaluation",
          "unified discriminative and generative evaluation",
          "automatic annotation pipeline",
          "Hal-Evaluator",
          "out-of-domain evaluation",
          "Chain-of-Thought (COT)",
          "output length correlation with hallucinations",
          "LVLM reliability"
        ],
        "paper_type": "based on the abstract and introduction:\n\n*   the abstract explicitly states: \"we introduce a refined taxonomy of hallucinations\", \"we then utilize advanced llms to generate and filter fine-grained hallucinatory data\", and \"the proposed benchmark distinctively assesses lvlmsâ€™ ability...\".\n*   these phrases align perfectly with the criteria for a **technical** paper: \"abstract mentions: 'propose', 'develop', 'present', 'algorithm', 'method'\" and \"introduction discusses: technical problem, proposed solution\".\n*   the paper proposes a new taxonomy, a method for generating evaluation data, and a new evaluation framework/benchmark (hal-eval) to address the technical problem of hallucination evaluation in lvlms.\n\ntherefore, the paper type is **technical**."
      },
      "file_name": "05839a68bd05880beef2f171cee7aab960bb6d2f.pdf"
    },
    {
      "success": true,
      "doc_id": "17beadf1a34cd9cc02e95d9a2d94fbd8",
      "summary": "Here's a focused summary of the paper for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem:** Large Language Models (LLMs) are prone to hallucination (generating plausible but inaccurate text), which poses significant risks in critical applications (e.g., medicine, law). While fine-tuning LLMs to abstain from answering questions beyond their knowledge is a promising mitigation strategy, existing methods suffer from two key limitations:\n        1.  They often rely on the availability of ground-truth labels, which are difficult or costly to obtain and can be noisy.\n        2.  They are typically limited to short-form responses, failing to account for the lexical and syntactical variations inherent in longer generations.\n    *   **Importance and Challenge:** Mitigating hallucinations is crucial for ensuring the safety, trustworthiness, and overall reliability of LLMs, especially as they are increasingly deployed in high-stakes real-world scenarios. Developing a label-free and versatile abstention mechanism applicable to diverse generation lengths is a significant challenge.\n\n*   **Related Work & Positioning**\n    *   **Relation to Existing Approaches:** This work builds upon existing hallucination mitigation strategies, particularly those involving uncertainty estimation and abstention fine-tuning. It specifically improves upon uncertainty-based fine-tuning methods like R-Tuning-U \\cite{zhang2024r} by addressing its limitations.\n    *   **Limitations of Previous Solutions:**\n        *   Many abstention fine-tuning methods (e.g., R-Tuning \\cite{zhang2024r}, \\cite{yang2023teaching}, \\cite{cheng2024uncertainty}, \\cite{wolfe2024abstain}) require ground-truth labels, making them resource-intensive and potentially susceptible to label noise.\n        *   Uncertainty-based methods like R-Tuning-U \\cite{zhang2024r} use classical entropy, which is sensitive to minor lexical and syntactical variations, thereby limiting its applicability to only short-form responses and hindering its usefulness for more complex, free-form generations.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method:** The paper proposes a fine-tuning strategy that leverages *semantic entropy* to enable LLMs to appropriately abstain from answering questions when uncertain \\cite{tjandra2024umq}. This approach is label-free and robust to variations in generation length.\n    *   **Novelty/Difference:**\n        *   **Semantic Entropy for Abstention:** Unlike R-Tuning-U which uses classical entropy over raw token sequences, \\cite{tjandra2024umq} computes entropy over the *semantic space* of model generations. This makes the uncertainty measure robust to lexical and syntactical variations, allowing it to accurately indicate hallucinations in both short-form and long-form generations. Semantic equivalence is operationalized using question-dependent bi-directional entailment to cluster responses.\n        *   **Label-Free Fine-Tuning:** The method constructs a training dataset by partitioning questions based on their computed semantic entropy. High-entropy questions are assigned an \"abstain\" label (\"I don't know the answer.\"), while low-entropy questions retain the model's standard response. The model is then fine-tuned using supervised learning with cross-entropy loss.\n        *   **Accuracy-Engagement Distance (AED):** A novel evaluation metric is introduced \\cite{tjandra2024umq} that holistically quantifies model hallucination by considering both accuracy and *engagement* (the number of questions a model willingly answers). AED is a normalized Euclidean distance from an ideal model (max accuracy, max engagement), penalizing models that abstain too frequently or inaccurately.\n\n*   **Key Technical Contributions**\n    *   **Novel Algorithm/Method:** A novel fine-tuning approach for LLMs that enables abstention based on *semantic entropy*, providing a label-free and robust mechanism for hallucination mitigation applicable to both short-form and long-form generations \\cite{tjandra2024umq}.\n    *   **Novel Evaluation Metric:** The introduction of the Accuracy-Engagement Distance (AED) \\cite{tjandra2024umq}, a more comprehensive metric for evaluating abstention-capable models by balancing accuracy with the willingness to answer.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted:** The proposed method was evaluated on LLAMA-3-8B-INSTRUCT across four datasets (TriviaQA, BioASQ, NQ, SQuAD) in a closed-book setting. Experiments covered two answering settings: Long-QA (free-form) and Short-QA (short answers). The method was compared against R-Tuning (label-dependent) and R-Tuning-U (label-independent, classical entropy) \\cite{zhang2024r}. Semantic entropy was computed using two different entailment models (DeBERTa and Llama-3-70B-Instruct).\n    *   **Key Performance Metrics and Comparison Results:**\n        *   The method using semantic entropy (SE) matched or outperformed models fine-tuned with R-Tuning and R-Tuning-U \\cite{tjandra2024umq}.\n        *   It achieved a significant reduction in hallucination rates: up to **30.1% for long-form generations** and up to **8.7% for short-form generations** compared to R-Tuning and R-Tuning-U.\n        *   The AED metric effectively demonstrated the improved balance between accuracy and engagement of the proposed method, penalizing models with low engagement that would otherwise appear equivalent based solely on accuracy.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations/Assumptions:** The effectiveness of semantic entropy relies on the quality and contextual understanding of the underlying entailment model used for semantic clustering. The uncertainty threshold (Ï„) for partitioning the dataset is a user-defined hyperparameter. Due to resource constraints, LoRA was used for fine-tuning, which might not capture the full potential of full fine-tuning.\n    *   **Scope of Applicability:** The method is demonstrated for factual QA tasks in a closed-book setting. Its label-free nature makes it highly scalable and applicable to a wide range of tasks requiring uncertainty-aware abstention, for both short and long-form text generation.\n\n*   **Technical Significance**\n    *   **Advancement of State-of-the-Art:** This work significantly advances the technical state-of-the-art in hallucination mitigation by providing a robust, label-free, and versatile fine-tuning approach for LLM abstention \\cite{tjandra2024umq}. It successfully addresses the critical limitations of prior methods, particularly their reliance on ground-truth labels and restriction to short-form responses.\n    *   **Potential Impact:** The ability to fine-tune LLMs to abstain appropriately without external labels and across diverse generation lengths enhances their trustworthiness and safety in high-stakes applications. The proposed AED metric also offers a more comprehensive evaluation standard for future research in uncertainty-aware LLMs, fostering the development of more reliable and helpful AI systems.",
      "intriguing_abstract": "Large Language Models (LLMs) are transforming industries, yet their pervasive tendency to hallucinate poses a critical challenge, hindering their deployment in high-stakes applications. Existing abstention fine-tuning methods, designed to make LLMs refuse to answer when uncertain, are often limited by their reliance on costly ground-truth labels or their inability to handle the lexical and syntactical variations inherent in longer generations.\n\nThis paper introduces a novel, label-free fine-tuning strategy that empowers LLMs to robustly abstain from answering questions beyond their knowledge. Our core innovation lies in leveraging **semantic entropy** to accurately quantify uncertainty across both short-form and complex, long-form generations. By computing entropy over the *semantic space* of model outputs rather than raw tokens, our method overcomes the limitations of classical entropy, enabling a versatile and scalable abstention mechanism. We construct training data by partitioning questions based on their computed semantic entropy, then fine-tune the model with supervised learning. Furthermore, we propose the **Accuracy-Engagement Distance (AED)**, a novel holistic metric that balances model accuracy with its willingness to answer. Experiments on LLAMA-3-8B-INSTRUCT demonstrate significant hallucination reductionsâ€”up to 30.1% for long-form and 8.7% for short-form generationsâ€”outperforming state-of-the-art baselines. This work significantly enhances LLM trustworthiness and safety, paving the way for more reliable AI systems in critical domains.",
      "keywords": [
        "Large Language Models (LLMs)",
        "hallucination mitigation",
        "abstention fine-tuning",
        "semantic entropy",
        "label-free fine-tuning",
        "Accuracy-Engagement Distance (AED)",
        "uncertainty estimation",
        "long-form generation",
        "bi-directional entailment",
        "trustworthiness and safety",
        "reduction in hallucination rates",
        "closed-book setting"
      ],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/3bb6f6a4cf672616bd49d8f4eb15d1b4df19972b.pdf",
      "citation_key": "tjandra2024umq",
      "metadata": {
        "title": "Fine-Tuning Large Language Models to Appropriately Abstain with Semantic Entropy",
        "authors": [
          "Benedict Aaron Tjandra",
          "Muhammed Razzak",
          "Jannik Kossen",
          "Kunal Handa",
          "Yarin Gal"
        ],
        "published_date": "2024",
        "abstract": "Large Language Models (LLMs) are known to hallucinate, whereby they generate plausible but inaccurate text. This phenomenon poses significant risks in critical applications, such as medicine or law, necessitating robust hallucination mitigation strategies. While recent works have proposed fine-tuning methods to teach LLMs to abstain from answering questions beyond their knowledge or capabilities, these methods rely on the existence of ground-truth labels or are limited to short-form responses. To address these limitations, we propose fine-tuning using semantic entropy, an uncertainty measure derived from introspection into the model which does not require external labels. We demonstrate that our approach matches or outperforms models fine-tuned using prior work and achieves strong performance for both short and long-form generations on a range of datasets.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/3bb6f6a4cf672616bd49d8f4eb15d1b4df19972b.pdf",
        "venue": "arXiv.org",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the paper for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem:** Large Language Models (LLMs) are prone to hallucination (generating plausible but inaccurate text), which poses significant risks in critical applications (e.g., medicine, law). While fine-tuning LLMs to abstain from answering questions beyond their knowledge is a promising mitigation strategy, existing methods suffer from two key limitations:\n        1.  They often rely on the availability of ground-truth labels, which are difficult or costly to obtain and can be noisy.\n        2.  They are typically limited to short-form responses, failing to account for the lexical and syntactical variations inherent in longer generations.\n    *   **Importance and Challenge:** Mitigating hallucinations is crucial for ensuring the safety, trustworthiness, and overall reliability of LLMs, especially as they are increasingly deployed in high-stakes real-world scenarios. Developing a label-free and versatile abstention mechanism applicable to diverse generation lengths is a significant challenge.\n\n*   **Related Work & Positioning**\n    *   **Relation to Existing Approaches:** This work builds upon existing hallucination mitigation strategies, particularly those involving uncertainty estimation and abstention fine-tuning. It specifically improves upon uncertainty-based fine-tuning methods like R-Tuning-U \\cite{zhang2024r} by addressing its limitations.\n    *   **Limitations of Previous Solutions:**\n        *   Many abstention fine-tuning methods (e.g., R-Tuning \\cite{zhang2024r}, \\cite{yang2023teaching}, \\cite{cheng2024uncertainty}, \\cite{wolfe2024abstain}) require ground-truth labels, making them resource-intensive and potentially susceptible to label noise.\n        *   Uncertainty-based methods like R-Tuning-U \\cite{zhang2024r} use classical entropy, which is sensitive to minor lexical and syntactical variations, thereby limiting its applicability to only short-form responses and hindering its usefulness for more complex, free-form generations.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method:** The paper proposes a fine-tuning strategy that leverages *semantic entropy* to enable LLMs to appropriately abstain from answering questions when uncertain \\cite{tjandra2024umq}. This approach is label-free and robust to variations in generation length.\n    *   **Novelty/Difference:**\n        *   **Semantic Entropy for Abstention:** Unlike R-Tuning-U which uses classical entropy over raw token sequences, \\cite{tjandra2024umq} computes entropy over the *semantic space* of model generations. This makes the uncertainty measure robust to lexical and syntactical variations, allowing it to accurately indicate hallucinations in both short-form and long-form generations. Semantic equivalence is operationalized using question-dependent bi-directional entailment to cluster responses.\n        *   **Label-Free Fine-Tuning:** The method constructs a training dataset by partitioning questions based on their computed semantic entropy. High-entropy questions are assigned an \"abstain\" label (\"I don't know the answer.\"), while low-entropy questions retain the model's standard response. The model is then fine-tuned using supervised learning with cross-entropy loss.\n        *   **Accuracy-Engagement Distance (AED):** A novel evaluation metric is introduced \\cite{tjandra2024umq} that holistically quantifies model hallucination by considering both accuracy and *engagement* (the number of questions a model willingly answers). AED is a normalized Euclidean distance from an ideal model (max accuracy, max engagement), penalizing models that abstain too frequently or inaccurately.\n\n*   **Key Technical Contributions**\n    *   **Novel Algorithm/Method:** A novel fine-tuning approach for LLMs that enables abstention based on *semantic entropy*, providing a label-free and robust mechanism for hallucination mitigation applicable to both short-form and long-form generations \\cite{tjandra2024umq}.\n    *   **Novel Evaluation Metric:** The introduction of the Accuracy-Engagement Distance (AED) \\cite{tjandra2024umq}, a more comprehensive metric for evaluating abstention-capable models by balancing accuracy with the willingness to answer.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted:** The proposed method was evaluated on LLAMA-3-8B-INSTRUCT across four datasets (TriviaQA, BioASQ, NQ, SQuAD) in a closed-book setting. Experiments covered two answering settings: Long-QA (free-form) and Short-QA (short answers). The method was compared against R-Tuning (label-dependent) and R-Tuning-U (label-independent, classical entropy) \\cite{zhang2024r}. Semantic entropy was computed using two different entailment models (DeBERTa and Llama-3-70B-Instruct).\n    *   **Key Performance Metrics and Comparison Results:**\n        *   The method using semantic entropy (SE) matched or outperformed models fine-tuned with R-Tuning and R-Tuning-U \\cite{tjandra2024umq}.\n        *   It achieved a significant reduction in hallucination rates: up to **30.1% for long-form generations** and up to **8.7% for short-form generations** compared to R-Tuning and R-Tuning-U.\n        *   The AED metric effectively demonstrated the improved balance between accuracy and engagement of the proposed method, penalizing models with low engagement that would otherwise appear equivalent based solely on accuracy.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations/Assumptions:** The effectiveness of semantic entropy relies on the quality and contextual understanding of the underlying entailment model used for semantic clustering. The uncertainty threshold (Ï„) for partitioning the dataset is a user-defined hyperparameter. Due to resource constraints, LoRA was used for fine-tuning, which might not capture the full potential of full fine-tuning.\n    *   **Scope of Applicability:** The method is demonstrated for factual QA tasks in a closed-book setting. Its label-free nature makes it highly scalable and applicable to a wide range of tasks requiring uncertainty-aware abstention, for both short and long-form text generation.\n\n*   **Technical Significance**\n    *   **Advancement of State-of-the-Art:** This work significantly advances the technical state-of-the-art in hallucination mitigation by providing a robust, label-free, and versatile fine-tuning approach for LLM abstention \\cite{tjandra2024umq}. It successfully addresses the critical limitations of prior methods, particularly their reliance on ground-truth labels and restriction to short-form responses.\n    *   **Potential Impact:** The ability to fine-tune LLMs to abstain appropriately without external labels and across diverse generation lengths enhances their trustworthiness and safety in high-stakes applications. The proposed AED metric also offers a more comprehensive evaluation standard for future research in uncertainty-aware LLMs, fostering the development of more reliable and helpful AI systems.",
        "keywords": [
          "Large Language Models (LLMs)",
          "hallucination mitigation",
          "abstention fine-tuning",
          "semantic entropy",
          "label-free fine-tuning",
          "Accuracy-Engagement Distance (AED)",
          "uncertainty estimation",
          "long-form generation",
          "bi-directional entailment",
          "trustworthiness and safety",
          "reduction in hallucination rates",
          "closed-book setting"
        ],
        "paper_type": "based on the abstract and introduction:\n\n*   the abstract explicitly states: \"to address these limitations, we **propose fine-tuning using semantic entropy**...\" and describes this as \"an uncertainty measure derived from introspection into the model which does not require external labels.\" this clearly indicates the development and presentation of a new method or algorithm.\n*   the abstract further mentions: \"we **demonstrate that our approach matches or outperforms** models fine-tuned using prior work and achieves strong performance for both short and long-form generations on a range of datasets.\" this describes the evaluation of the proposed method.\n*   the introduction sets up a technical problem (llm hallucinations and limitations of existing mitigation strategies) that the proposed solution aims to address.\n\nthese points align perfectly with the criteria for a **technical** paper: \"presents new methods, algorithms, or systems\" and \"abstract mentions: 'propose', 'develop', 'present', 'algorithm', 'method'\". while it includes empirical evaluation, the primary contribution is the *new method* itself.\n\n**classification: technical**"
      },
      "file_name": "3bb6f6a4cf672616bd49d8f4eb15d1b4df19972b.pdf"
    },
    {
      "success": true,
      "doc_id": "19992159d5fd3d251e348838579835e3",
      "summary": "Here's a focused summary of the paper for a literature review, emphasizing technical innovations and empirical validation:\n\n*   **Research Problem & Motivation**\n    *   The paper addresses the critical technical problem of \"hallucinations\" in Large Language Models (LLMs), specifically within the medical domain \\cite{umapathi2023puv}.\n    *   This problem is paramount because LLMs can generate plausible but incorrect or unverified medical information, which can lead to severe health consequences, misdiagnosis, or inappropriate treatment plans, making accuracy and reliability a matter of life or death in healthcare applications \\cite{umapathi2023puv}.\n    *   Existing efforts to mitigate LLM hallucinations have not adequately focused on the medical field, leaving a significant gap in evaluation and safety for this high-stakes domain \\cite{umapathi2023puv}.\n\n*   **Related Work & Positioning**\n    *   While general efforts exist to mitigate LLM hallucinations, this work explicitly states that it is the \"first of its kind to evaluate the hallucinations of LLMs in the medical domain\" \\cite{umapathi2023puv}.\n    *   Previous solutions lacked a dedicated, comprehensive benchmark and dataset tailored to the unique challenges and critical accuracy requirements of medical information and reasoning \\cite{umapathi2023puv}.\n\n*   **Technical Approach & Innovation**\n    *   The core technical method is the introduction of **Med-HALT (Medical Domain Hallucination Test)**, a novel benchmark and dataset designed to evaluate and reduce hallucinations in medical LLMs \\cite{umapathi2023puv}.\n    *   Med-HALT categorizes hallucination tests into two main types:\n        *   **Reasoning Hallucination Tests (RHTs)**: Assess an LLM's ability to reason about medical problems, generate logically coherent, and factually accurate output without creating fake information \\cite{umapathi2023puv}. This includes:\n            *   **False Confidence Test (FCT)**: Evaluates LLM's tendency to generate answers with undue certainty, even when lacking sufficient information, by presenting a medical MCQ with a randomly suggested \"correct\" answer for validation and explanation \\cite{umapathi2023puv}.\n            *   **None of the Above (NOTA) Test**: Challenges the LLM to identify when none of the provided options are correct in an MCQ, requiring justification \\cite{umapathi2023puv}.\n            *   **Fake Questions Test (FQT)**: Presents nonsensical or fake medical questions to test the LLM's ability to identify and appropriately handle such invalid queries \\cite{umapathi2023puv}.\n        *   **Memory Hallucination Tests (MHTs)**: Investigate the LLM's capacity to accurately recall and generate factual information from its encoded knowledge base, crucial for biomedical information retrieval \\cite{umapathi2023puv}. This includes four specific tests for PubMed articles: Abstract-to-Link, PMID-to-Title, Title-to-Link, and Link-to-Title \\cite{umapathi2023puv}.\n    *   The dataset is uniquely diverse, comprising multiple-choice questions from medical examinations across various countries (India, Spain, U.S., Taiwan) and a PubMed-derived dataset for memory tests, spanning numerous medical sub-disciplines \\cite{umapathi2023puv}.\n\n*   **Key Technical Contributions**\n    *   **Novel Benchmark and Dataset**: Med-HALT is the first dedicated benchmark and dataset for evaluating LLM hallucinations specifically in the medical domain \\cite{umapathi2023puv}.\n    *   **Innovative Testing Modalities**: Introduction of distinct reasoning-based (FCT, NOTA, FQT) and memory-based (Abstract-to-Link, PMID-to-Title, Title-to-Link, Link-to-Title) hallucination tests, providing a multi-faceted evaluation framework \\cite{umapathi2023puv}.\n    *   **Diverse Multinational Data**: Compilation of a comprehensive dataset from various international medical licensing exams and PubMed, ensuring broad applicability and challenging LLMs with diverse medical knowledge structures \\cite{umapathi2023puv}.\n    *   **Transparency and Reproducibility**: The framework, test designs, and dataset statistics are openly shared to facilitate further research and ensure reproducibility \\cite{umapathi2023puv}.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted**: The study evaluated leading commercial and open-source LLMs, including OpenAI's Text-Davinci and GPT-3.5 Turbo, and open-source models like LlaMa-2, MPT, and Falcon \\cite{umapathi2023puv}. Models were assessed in their default configurations without specific fine-tuning \\cite{umapathi2023puv}.\n    *   **Performance Metrics**: Evaluation was based on two key metrics:\n        *   **Accuracy**: The ratio of correct predictions to total predictions \\cite{umapathi2023puv}.\n        *   **Pointwise Score**: A more nuanced metric awarding +1 for correct answers and penalizing -0.25 for incorrect ones, reflecting common medical exam scoring \\cite{umapathi2023puv}.\n    *   **Key Results**:\n        *   Open-access models (Falcon and LlaMa-2) generally outperformed commercial variants (GPT-3.5 and Text-Davinci) across the hallucination tasks \\cite{umapathi2023puv}.\n        *   Llama-2 70B achieved the highest accuracy (42.21%) and Pointwise Score (52.37) in the Reasoning FCT task, but critically, *none of the models reached an acceptable level of accuracy*, highlighting the significant challenge of reasoning hallucination tests for current LLMs \\cite{umapathi2023puv}.\n        *   Falcon 40B demonstrated strong performance in the Reasoning Fake Questions Test, achieving 99.89% accuracy \\cite{umapathi2023puv}.\n\n*   **Limitations & Scope**\n    *   A significant technical limitation is the observed low performance of even state-of-the-art LLMs on reasoning-based hallucination tasks (e.g., FCT), indicating a fundamental challenge in complex medical reasoning and factual consistency \\cite{umapathi2023puv}.\n    *   The evaluation was conducted on models in their default configurations, meaning performance could potentially improve with domain-specific fine-tuning, which was outside the scope of this initial benchmark \\cite{umapathi2023puv}.\n    *   The scope of applicability is primarily focused on evaluating hallucinations in a question-answering and information retrieval context within the medical domain, using multiple-choice and structured retrieval tasks \\cite{umapathi2023puv}.\n\n*   **Technical Significance**\n    *   Med-HALT significantly advances the technical state-of-the-art by providing the first standardized, comprehensive, and diverse benchmark specifically designed to measure and evaluate hallucinations in LLMs for the critical medical domain \\cite{umapathi2023puv}.\n    *   It offers a robust framework for identifying specific types of hallucination patterns (reasoning vs. memory) and their impact, which is crucial for developing targeted mitigation strategies \\cite{umapathi2023puv}.\n    *   The empirical validation highlights the current limitations of leading LLMs in medical reasoning and factual recall, providing a clear roadmap for future research to develop safer, more reliable, and trustworthy language models for healthcare applications \\cite{umapathi2023puv}.\n    *   By openly sharing the benchmark and dataset, the work fosters transparency and reproducibility, enabling the broader research community to contribute to and build upon these findings \\cite{umapathi2023puv}.",
      "intriguing_abstract": "The promise of Large Language Models (LLMs) in healthcare is immense, yet the specter of \"hallucinations\"â€”generating plausible but incorrect medical informationâ€”poses a life-threatening risk. Current efforts to mitigate LLM hallucinations critically overlook the unique demands of the medical domain. We introduce **Med-HALT (Medical Domain Hallucination Test)**, the first comprehensive benchmark and dataset specifically designed to rigorously evaluate and reduce hallucinations in medical LLMs. Med-HALT pioneers novel **Reasoning Hallucination Tests (RHTs)**, including False Confidence, None of the Above, and Fake Questions, alongside **Memory Hallucination Tests (MHTs)** leveraging PubMed data. Our diverse, multinational dataset challenges models with real-world medical complexity. Experimental validation of leading LLMs (e.g., GPT-3.5, Llama-2) reveals a stark reality: even state-of-the-art models exhibit alarmingly low accuracy in complex medical reasoning tasks, underscoring a fundamental challenge. Med-HALT provides an indispensable, multi-faceted framework to identify specific hallucination patterns, offering a crucial roadmap for developing safer, more reliable, and trustworthy AI for critical healthcare applications.",
      "keywords": [
        "LLM hallucinations",
        "medical domain",
        "Med-HALT benchmark",
        "Reasoning Hallucination Tests",
        "Memory Hallucination Tests",
        "False Confidence Test (FCT)",
        "Fake Questions Test (FQT)",
        "novel benchmark and dataset",
        "medical reasoning",
        "factual consistency",
        "healthcare applications",
        "open-access LLMs performance",
        "evaluation framework",
        "accuracy and reliability",
        "multinational medical data"
      ],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/3b0792f6d7f6aa6aadd316e73943116afef2979b.pdf",
      "citation_key": "umapathi2023puv",
      "metadata": {
        "title": "Med-HALT: Medical Domain Hallucination Test for Large Language Models",
        "authors": [
          "Logesh Kumar Umapathi",
          "Ankit Pal",
          "Malaikannan Sankarasubbu"
        ],
        "published_date": "2023",
        "abstract": "This research paper focuses on the challenges posed by hallucinations in large language models (LLMs), particularly in the context of the medical domain. Hallucination, wherein these models generate plausible yet unverified or incorrect information, can have serious consequences in healthcare applications. We propose a new benchmark and dataset, Med-HALT (Medical Domain Hallucination Test), designed specifically to evaluate and reduce hallucinations. Med-HALT provides a diverse multinational dataset derived from medical examinations across various countries and includes multiple innovative testing modalities. Med-HALT includes two categories of tests reasoning and memory-based hallucination tests, designed to assess LLMsâ€™ problem-solving and information retrieval abilities. Our study evaluated leading LLMs, including Text Davinci, GPT-3.5, LlaMa-2, MPT, and Falcon, revealing significant differences in their performance. The paper provides detailed insights into the dataset, promoting transparency and reproducibility. Through this work, we aim to contribute to the development of safer and more reliable language models in healthcare. Our benchmark can be found at medhalt.github.io",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/3b0792f6d7f6aa6aadd316e73943116afef2979b.pdf",
        "venue": "Conference on Computational Natural Language Learning",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the paper for a literature review, emphasizing technical innovations and empirical validation:\n\n*   **Research Problem & Motivation**\n    *   The paper addresses the critical technical problem of \"hallucinations\" in Large Language Models (LLMs), specifically within the medical domain \\cite{umapathi2023puv}.\n    *   This problem is paramount because LLMs can generate plausible but incorrect or unverified medical information, which can lead to severe health consequences, misdiagnosis, or inappropriate treatment plans, making accuracy and reliability a matter of life or death in healthcare applications \\cite{umapathi2023puv}.\n    *   Existing efforts to mitigate LLM hallucinations have not adequately focused on the medical field, leaving a significant gap in evaluation and safety for this high-stakes domain \\cite{umapathi2023puv}.\n\n*   **Related Work & Positioning**\n    *   While general efforts exist to mitigate LLM hallucinations, this work explicitly states that it is the \"first of its kind to evaluate the hallucinations of LLMs in the medical domain\" \\cite{umapathi2023puv}.\n    *   Previous solutions lacked a dedicated, comprehensive benchmark and dataset tailored to the unique challenges and critical accuracy requirements of medical information and reasoning \\cite{umapathi2023puv}.\n\n*   **Technical Approach & Innovation**\n    *   The core technical method is the introduction of **Med-HALT (Medical Domain Hallucination Test)**, a novel benchmark and dataset designed to evaluate and reduce hallucinations in medical LLMs \\cite{umapathi2023puv}.\n    *   Med-HALT categorizes hallucination tests into two main types:\n        *   **Reasoning Hallucination Tests (RHTs)**: Assess an LLM's ability to reason about medical problems, generate logically coherent, and factually accurate output without creating fake information \\cite{umapathi2023puv}. This includes:\n            *   **False Confidence Test (FCT)**: Evaluates LLM's tendency to generate answers with undue certainty, even when lacking sufficient information, by presenting a medical MCQ with a randomly suggested \"correct\" answer for validation and explanation \\cite{umapathi2023puv}.\n            *   **None of the Above (NOTA) Test**: Challenges the LLM to identify when none of the provided options are correct in an MCQ, requiring justification \\cite{umapathi2023puv}.\n            *   **Fake Questions Test (FQT)**: Presents nonsensical or fake medical questions to test the LLM's ability to identify and appropriately handle such invalid queries \\cite{umapathi2023puv}.\n        *   **Memory Hallucination Tests (MHTs)**: Investigate the LLM's capacity to accurately recall and generate factual information from its encoded knowledge base, crucial for biomedical information retrieval \\cite{umapathi2023puv}. This includes four specific tests for PubMed articles: Abstract-to-Link, PMID-to-Title, Title-to-Link, and Link-to-Title \\cite{umapathi2023puv}.\n    *   The dataset is uniquely diverse, comprising multiple-choice questions from medical examinations across various countries (India, Spain, U.S., Taiwan) and a PubMed-derived dataset for memory tests, spanning numerous medical sub-disciplines \\cite{umapathi2023puv}.\n\n*   **Key Technical Contributions**\n    *   **Novel Benchmark and Dataset**: Med-HALT is the first dedicated benchmark and dataset for evaluating LLM hallucinations specifically in the medical domain \\cite{umapathi2023puv}.\n    *   **Innovative Testing Modalities**: Introduction of distinct reasoning-based (FCT, NOTA, FQT) and memory-based (Abstract-to-Link, PMID-to-Title, Title-to-Link, Link-to-Title) hallucination tests, providing a multi-faceted evaluation framework \\cite{umapathi2023puv}.\n    *   **Diverse Multinational Data**: Compilation of a comprehensive dataset from various international medical licensing exams and PubMed, ensuring broad applicability and challenging LLMs with diverse medical knowledge structures \\cite{umapathi2023puv}.\n    *   **Transparency and Reproducibility**: The framework, test designs, and dataset statistics are openly shared to facilitate further research and ensure reproducibility \\cite{umapathi2023puv}.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted**: The study evaluated leading commercial and open-source LLMs, including OpenAI's Text-Davinci and GPT-3.5 Turbo, and open-source models like LlaMa-2, MPT, and Falcon \\cite{umapathi2023puv}. Models were assessed in their default configurations without specific fine-tuning \\cite{umapathi2023puv}.\n    *   **Performance Metrics**: Evaluation was based on two key metrics:\n        *   **Accuracy**: The ratio of correct predictions to total predictions \\cite{umapathi2023puv}.\n        *   **Pointwise Score**: A more nuanced metric awarding +1 for correct answers and penalizing -0.25 for incorrect ones, reflecting common medical exam scoring \\cite{umapathi2023puv}.\n    *   **Key Results**:\n        *   Open-access models (Falcon and LlaMa-2) generally outperformed commercial variants (GPT-3.5 and Text-Davinci) across the hallucination tasks \\cite{umapathi2023puv}.\n        *   Llama-2 70B achieved the highest accuracy (42.21%) and Pointwise Score (52.37) in the Reasoning FCT task, but critically, *none of the models reached an acceptable level of accuracy*, highlighting the significant challenge of reasoning hallucination tests for current LLMs \\cite{umapathi2023puv}.\n        *   Falcon 40B demonstrated strong performance in the Reasoning Fake Questions Test, achieving 99.89% accuracy \\cite{umapathi2023puv}.\n\n*   **Limitations & Scope**\n    *   A significant technical limitation is the observed low performance of even state-of-the-art LLMs on reasoning-based hallucination tasks (e.g., FCT), indicating a fundamental challenge in complex medical reasoning and factual consistency \\cite{umapathi2023puv}.\n    *   The evaluation was conducted on models in their default configurations, meaning performance could potentially improve with domain-specific fine-tuning, which was outside the scope of this initial benchmark \\cite{umapathi2023puv}.\n    *   The scope of applicability is primarily focused on evaluating hallucinations in a question-answering and information retrieval context within the medical domain, using multiple-choice and structured retrieval tasks \\cite{umapathi2023puv}.\n\n*   **Technical Significance**\n    *   Med-HALT significantly advances the technical state-of-the-art by providing the first standardized, comprehensive, and diverse benchmark specifically designed to measure and evaluate hallucinations in LLMs for the critical medical domain \\cite{umapathi2023puv}.\n    *   It offers a robust framework for identifying specific types of hallucination patterns (reasoning vs. memory) and their impact, which is crucial for developing targeted mitigation strategies \\cite{umapathi2023puv}.\n    *   The empirical validation highlights the current limitations of leading LLMs in medical reasoning and factual recall, providing a clear roadmap for future research to develop safer, more reliable, and trustworthy language models for healthcare applications \\cite{umapathi2023puv}.\n    *   By openly sharing the benchmark and dataset, the work fosters transparency and reproducibility, enabling the broader research community to contribute to and build upon these findings \\cite{umapathi2023puv}.",
        "keywords": [
          "LLM hallucinations",
          "medical domain",
          "Med-HALT benchmark",
          "Reasoning Hallucination Tests",
          "Memory Hallucination Tests",
          "False Confidence Test (FCT)",
          "Fake Questions Test (FQT)",
          "novel benchmark and dataset",
          "medical reasoning",
          "factual consistency",
          "healthcare applications",
          "open-access LLMs performance",
          "evaluation framework",
          "accuracy and reliability",
          "multinational medical data"
        ],
        "paper_type": "based on the abstract and introduction:\n\nthe paper clearly states: \"we propose a new benchmark and dataset, med-halt (medical domain hallucination test), designed specifically to evaluate and reduce hallucinations.\" it then describes the features of this benchmark and dataset (\"diverse multinational dataset,\" \"multiple innovative testing modalities,\" \"two categories of tests\"). the introduction also reinforces this with \"med-halt: a new benchmark dataset for llm to test hallucination in medical domain.\"\n\nwhile the paper also mentions an evaluation (\"our study evaluated leading llms... revealing significant differences in their performance\"), this evaluation is conducted *using* the newly proposed benchmark. the primary contribution is the creation of this new tool/system/resource.\n\ntherefore, it best fits the **technical** category.\n\n**classification:** technical"
      },
      "file_name": "3b0792f6d7f6aa6aadd316e73943116afef2979b.pdf"
    },
    {
      "success": true,
      "doc_id": "5f7e22acb22c32ceea45f1ee1252849a",
      "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **Research Problem & Motivation**\n    *   Multimodal Large Language Models (MLLMs) are prone to \"hallucinations,\" generating content that is nonsensical or unfaithful to input sources, particularly visual information \\cite{zou2024dp7}.\n    *   This problem is critical because it undermines MLLM reliability in safety-sensitive domains like healthcare and autonomous driving.\n    *   The paper identifies that MLLM hallucinations often stem from the text decoder's sensitivity to visual tokens, leading to a phenomenon akin to \"amnesia\" about visual information, exacerbated by modality imbalance (visual vs. textual information density) and the autoregressive nature of decoding \\cite{zou2024dp7}.\n\n*   **Related Work & Positioning**\n    *   Existing approaches include Retrieval-Augmented Generation (RAG), extra fine-tuning, attention intervention (e.g., OPERA), and Contrastive Decoding (CD) strategies (e.g., VCD, ICD) \\cite{zou2024dp7}.\n    *   Limitations of previous solutions: RAG and fine-tuning incur substantial computational overhead or storage. Attention intervention methods suffer from high inference latency and large memory footprints. CD-based approaches, while not requiring extra training, often double inference costs, require task-specific input perturbation, can introduce noise, and typically do not improve general capabilities \\cite{zou2024dp7}.\n    *   `MemVR \\cite{zou2024dp7}` is positioned as a novel decoding paradigm that overcomes these limitations by offering superior performance, efficiency, and memory cost without requiring additional training or data, uniquely achieving both visual hallucination mitigation and general improvement with low latency.\n\n*   **Technical Approach & Innovation**\n    *   The core technical method, `MemVR \\cite{zou2024dp7}` (Memory-space Visual Retracing), is inspired by human cognition's \"look-twice\" mechanism.\n    *   It re-injects visual tokens as supplementary evidence into the MLLM through a Feed Forward Network (FFN) as \"key-value memory\" at a \"middle trigger layer\" \\cite{zou2024dp7}.\n    *   This \"look-twice\" mechanism is dynamically activated when the model exhibits high uncertainty (quantified by entropy) during inference, enhancing factual alignment where it's most needed \\cite{zou2024dp7}.\n    *   The approach is novel because it directly enhances the hidden states of intermediate layers with visual information, rather than modulating logits (like CD) or intervening in attention, and claims to do so without incurring additional inference time overhead \\cite{zou2024dp7}.\n\n*   **Key Technical Contributions**\n    *   **Novel Algorithm/Method**: `MemVR \\cite{zou2024dp7}`, an efficient, minimalist, and plug-and-play decoding paradigm that reinforces attention to visual information for modality balance during the forward pass.\n    *   **Techniques**: Introduction of static and dynamic Visual Retracing (VR) strategies that shift hidden states of intermediate layers for self-enhancement, avoiding the multi-round decoding common in CD-based methods \\cite{zou2024dp7}.\n    *   **Theoretical Insights/Analysis**: `MemVR \\cite{zou2024dp7}` provides experimental validation that hallucinations are triggered by the sensitivity of the text decoder (LLM) to non-text modalities and that refreshing visual memory alleviates this issue. It also demonstrates that uncertainty (entropy) is a reliable indicator of hallucination-prone tokens in MLLMs.\n\n*   **Experimental Validation**\n    *   `MemVR \\cite{zou2024dp7}` was comprehensively evaluated on various MLLMs (e.g., LLaVA-1.5-7B) and eight public benchmarks, including multimodal hallucination benchmarks (POPE, CHAIR I, MME) and general capabilities benchmarks (MME, MM-Bench, LLaVA-Bench, VizWiz, MM-Vet, etc.), with GPT-4o evaluations also conducted.\n    *   Key performance metrics included hallucination mitigation scores, general task performance, inference latency, throughput, time cost, and memory usage \\cite{zou2024dp7}.\n    *   Results showed `MemVR \\cite{zou2024dp7}` significantly mitigates hallucination and excels in general benchmarks, outperforming SOTA methods. Specific improvements include +7.0% on POPE, 15.6% on CHAIR I, and a +32.2 marks increase on MME total score.\n    *   Crucially, `MemVR \\cite{zou2024dp7}` achieved comparable inference speed and memory usage to standard greedy/sample decoding, demonstrating significantly better efficiency than OPERA (e.g., 3.66x faster latency) and VCD (e.g., 2.20x faster latency) \\cite{zou2024dp7}.\n\n*   **Limitations & Scope**\n    *   The paper implicitly assumes that uncertainty can be reliably quantified and that specific \"middle trigger layers\" are optimal for visual token re-injection.\n    *   `MemVR \\cite{zou2024dp7}` is a plug-and-play method applicable to various MLLMs and demonstrates generalizability across a wide range of hallucination and general benchmarks. Its primary scope is mitigating visual hallucinations.\n\n*   **Technical Significance**\n    *   `MemVR \\cite{zou2024dp7}` advances the technical state-of-the-art by offering a novel, efficient, and effective decoding paradigm for hallucination mitigation that directly addresses the \"amnesia\" of visual information in MLLMs, moving beyond the limitations of prior RAG, fine-tuning, attention intervention, and contrastive decoding methods.\n    *   Its potential impact includes enabling more reliable and factually aligned MLLM outputs, which is vital for safety-critical applications. The low-latency, plug-and-play nature makes it practical for real-world deployment. The insights into modality imbalance and uncertainty-triggered intervention could inspire future research into dynamic, cognition-inspired MLLM architectures \\cite{zou2024dp7}.",
      "intriguing_abstract": "Multimodal Large Language Models (MLLMs) are transforming AI, yet their propensity for visual hallucinations poses a critical barrier to reliable deployment in safety-sensitive domains. These fabrications often stem from the text decoder's \"amnesia\" regarding visual input, exacerbated by modality imbalance and autoregressive decoding. We introduce `MemVR` (Memory-space Visual Retracing), a novel, plug-and-play decoding paradigm inspired by human cognition's \"look-twice\" mechanism. `MemVR` dynamically re-injects visual tokens as supplementary \"key-value memory\" into the MLLM's Feed Forward Network at intermediate layers, specifically when high decoding uncertainty (entropy) is detected. This direct enhancement of hidden states, unlike prior logit- or attention-based interventions, efficiently reinforces visual fidelity without requiring additional training or data.\n\nEvaluated across eight benchmarks, `MemVR` significantly mitigates visual hallucinations (e.g., +7.0% POPE, +15.6% CHAIR I) and boosts general MLLM capabilities, outperforming state-of-the-art methods. Crucially, it achieves these gains with comparable inference speed and memory usage to standard decoding, vastly surpassing the efficiency of existing contrastive decoding and attention intervention techniques. `MemVR` offers a practical, low-latency solution for building more reliable and factually aligned MLLMs, paving the way for safer and more trustworthy AI applications.",
      "keywords": [
        "Multimodal Large Language Models (MLLMs)",
        "visual hallucinations",
        "modality imbalance",
        "MemVR (Memory-space Visual Retracing)",
        "decoding paradigm",
        "visual token re-injection",
        "uncertainty-triggered activation",
        "hidden states enhancement",
        "hallucination mitigation",
        "general capabilities improvement",
        "low inference latency",
        "plug-and-play method",
        "safety-critical applications",
        "text decoder sensitivity"
      ],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/c4d3c2516d24bd1c0eff93ea047147f3afd586ca.pdf",
      "citation_key": "zou2024dp7",
      "metadata": {
        "title": "Look Twice Before You Answer: Memory-Space Visual Retracing for Hallucination Mitigation in Multimodal Large Language Models",
        "authors": [
          "Xin Zou",
          "Yizhou Wang",
          "Yibo Yan",
          "Sirui Huang",
          "Kening Zheng",
          "Junkai Chen",
          "Chang Tang",
          "Xuming Hu"
        ],
        "published_date": "2024",
        "abstract": "Despite their impressive capabilities, multimodal large language models (MLLMs) are prone to hallucinations, i.e., the generated content that is nonsensical or unfaithful to input sources. Unlike in LLMs, hallucinations in MLLMs often stem from the sensitivity of text decoder to visual tokens, leading to a phenomenon akin to\"amnesia\"about visual information. To address this issue, we propose MemVR, a novel decoding paradigm inspired by common cognition: when the memory of an image seen the moment before is forgotten, people will look at it again for factual answers. Following this principle, we treat visual tokens as supplementary evidence, re-injecting them into the MLLM through Feed Forward Network (FFN) as\"key-value memory\"at the middle trigger layer. This\"look-twice\"mechanism occurs when the model exhibits high uncertainty during inference, effectively enhancing factual alignment. Comprehensive experimental evaluations demonstrate that MemVR significantly mitigates hallucination across various MLLMs and excels in general benchmarks without incurring additional time overhead. The implementation is available from https://github.com/1zhou-Wang/MemVR",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/c4d3c2516d24bd1c0eff93ea047147f3afd586ca.pdf",
        "venue": "arXiv.org",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **Research Problem & Motivation**\n    *   Multimodal Large Language Models (MLLMs) are prone to \"hallucinations,\" generating content that is nonsensical or unfaithful to input sources, particularly visual information \\cite{zou2024dp7}.\n    *   This problem is critical because it undermines MLLM reliability in safety-sensitive domains like healthcare and autonomous driving.\n    *   The paper identifies that MLLM hallucinations often stem from the text decoder's sensitivity to visual tokens, leading to a phenomenon akin to \"amnesia\" about visual information, exacerbated by modality imbalance (visual vs. textual information density) and the autoregressive nature of decoding \\cite{zou2024dp7}.\n\n*   **Related Work & Positioning**\n    *   Existing approaches include Retrieval-Augmented Generation (RAG), extra fine-tuning, attention intervention (e.g., OPERA), and Contrastive Decoding (CD) strategies (e.g., VCD, ICD) \\cite{zou2024dp7}.\n    *   Limitations of previous solutions: RAG and fine-tuning incur substantial computational overhead or storage. Attention intervention methods suffer from high inference latency and large memory footprints. CD-based approaches, while not requiring extra training, often double inference costs, require task-specific input perturbation, can introduce noise, and typically do not improve general capabilities \\cite{zou2024dp7}.\n    *   `MemVR \\cite{zou2024dp7}` is positioned as a novel decoding paradigm that overcomes these limitations by offering superior performance, efficiency, and memory cost without requiring additional training or data, uniquely achieving both visual hallucination mitigation and general improvement with low latency.\n\n*   **Technical Approach & Innovation**\n    *   The core technical method, `MemVR \\cite{zou2024dp7}` (Memory-space Visual Retracing), is inspired by human cognition's \"look-twice\" mechanism.\n    *   It re-injects visual tokens as supplementary evidence into the MLLM through a Feed Forward Network (FFN) as \"key-value memory\" at a \"middle trigger layer\" \\cite{zou2024dp7}.\n    *   This \"look-twice\" mechanism is dynamically activated when the model exhibits high uncertainty (quantified by entropy) during inference, enhancing factual alignment where it's most needed \\cite{zou2024dp7}.\n    *   The approach is novel because it directly enhances the hidden states of intermediate layers with visual information, rather than modulating logits (like CD) or intervening in attention, and claims to do so without incurring additional inference time overhead \\cite{zou2024dp7}.\n\n*   **Key Technical Contributions**\n    *   **Novel Algorithm/Method**: `MemVR \\cite{zou2024dp7}`, an efficient, minimalist, and plug-and-play decoding paradigm that reinforces attention to visual information for modality balance during the forward pass.\n    *   **Techniques**: Introduction of static and dynamic Visual Retracing (VR) strategies that shift hidden states of intermediate layers for self-enhancement, avoiding the multi-round decoding common in CD-based methods \\cite{zou2024dp7}.\n    *   **Theoretical Insights/Analysis**: `MemVR \\cite{zou2024dp7}` provides experimental validation that hallucinations are triggered by the sensitivity of the text decoder (LLM) to non-text modalities and that refreshing visual memory alleviates this issue. It also demonstrates that uncertainty (entropy) is a reliable indicator of hallucination-prone tokens in MLLMs.\n\n*   **Experimental Validation**\n    *   `MemVR \\cite{zou2024dp7}` was comprehensively evaluated on various MLLMs (e.g., LLaVA-1.5-7B) and eight public benchmarks, including multimodal hallucination benchmarks (POPE, CHAIR I, MME) and general capabilities benchmarks (MME, MM-Bench, LLaVA-Bench, VizWiz, MM-Vet, etc.), with GPT-4o evaluations also conducted.\n    *   Key performance metrics included hallucination mitigation scores, general task performance, inference latency, throughput, time cost, and memory usage \\cite{zou2024dp7}.\n    *   Results showed `MemVR \\cite{zou2024dp7}` significantly mitigates hallucination and excels in general benchmarks, outperforming SOTA methods. Specific improvements include +7.0% on POPE, 15.6% on CHAIR I, and a +32.2 marks increase on MME total score.\n    *   Crucially, `MemVR \\cite{zou2024dp7}` achieved comparable inference speed and memory usage to standard greedy/sample decoding, demonstrating significantly better efficiency than OPERA (e.g., 3.66x faster latency) and VCD (e.g., 2.20x faster latency) \\cite{zou2024dp7}.\n\n*   **Limitations & Scope**\n    *   The paper implicitly assumes that uncertainty can be reliably quantified and that specific \"middle trigger layers\" are optimal for visual token re-injection.\n    *   `MemVR \\cite{zou2024dp7}` is a plug-and-play method applicable to various MLLMs and demonstrates generalizability across a wide range of hallucination and general benchmarks. Its primary scope is mitigating visual hallucinations.\n\n*   **Technical Significance**\n    *   `MemVR \\cite{zou2024dp7}` advances the technical state-of-the-art by offering a novel, efficient, and effective decoding paradigm for hallucination mitigation that directly addresses the \"amnesia\" of visual information in MLLMs, moving beyond the limitations of prior RAG, fine-tuning, attention intervention, and contrastive decoding methods.\n    *   Its potential impact includes enabling more reliable and factually aligned MLLM outputs, which is vital for safety-critical applications. The low-latency, plug-and-play nature makes it practical for real-world deployment. The insights into modality imbalance and uncertainty-triggered intervention could inspire future research into dynamic, cognition-inspired MLLM architectures \\cite{zou2024dp7}.",
        "keywords": [
          "Multimodal Large Language Models (MLLMs)",
          "visual hallucinations",
          "modality imbalance",
          "MemVR (Memory-space Visual Retracing)",
          "decoding paradigm",
          "visual token re-injection",
          "uncertainty-triggered activation",
          "hidden states enhancement",
          "hallucination mitigation",
          "general capabilities improvement",
          "low inference latency",
          "plug-and-play method",
          "safety-critical applications",
          "text decoder sensitivity"
        ],
        "paper_type": "based on the abstract and introduction:\n\n*   the **abstract** explicitly states: \"we propose memvr, a novel decoding paradigm...\" and then describes the mechanism (\"re-injecting them into the mllm through feed forward network (ffn) as â€œkey-value memoryâ€\"). it also mentions \"comprehensive experimental evaluations demonstrate that memvr significantly mitigates hallucination,\" indicating the validation of the proposed method.\n*   the **introduction** sets up a technical problem: \"mllms still face challenges, notably the â€œhallucinationâ€ issue.\"\n\nthese elements strongly align with the criteria for a **technical** paper, which presents new methods, algorithms, or systems and often includes an empirical evaluation to demonstrate their effectiveness.\n\n**classification: technical**"
      },
      "file_name": "c4d3c2516d24bd1c0eff93ea047147f3afd586ca.pdf"
    },
    {
      "success": true,
      "doc_id": "b9144b5bcae7bc6476ee290f4d8ba220",
      "summary": "Here's a focused summary of the paper \"Drowzee: Metamorphic Testing for Fact-Conflicting Hallucination Detection in Large Language Models\" \\cite{li2024osp} for a literature review:\n\n*   **Research Problem & Motivation**\n    *   Large Language Models (LLMs) frequently generate \"hallucinations,\" which are coherent but factually inaccurate outputs.\n    *   The paper specifically addresses **Fact-Conflicting Hallucination (FCH)**, where LLMs produce content that directly contradicts established ground truth facts.\n    *   This problem is critical because FCH jeopardizes the safety, reliability, and trustworthiness of LLM applications, leading to user confusion.\n    *   **Challenges**:\n        1.  Difficulty in automatically constructing and updating comprehensive benchmark datasets, as existing methods rely on manually curated, static benchmarks that cannot cover the broad and evolving spectrum of FCH cases.\n        2.  Inherent difficulty in automatically validating the reasoning process behind LLM outputs, especially for complex logical relations, which can mask false understanding even if a final answer appears correct.\n\n*   **Related Work & Positioning**\n    *   Existing approaches for hallucination detection primarily rely on manually labeled datasets (e.g., TruthfulQA, HaluEval, KoLA) and naive/semi-automatic validation methods like string matching or manual review.\n    *   **Limitations of previous solutions**: These methods are static, resource-intensive, lack adaptability and scalability, and struggle to cover the dynamic nature of factual knowledge. They also lack dedicated frameworks for automatically testing FCH, particularly for validating complex logical reasoning.\n    *   **Positioning**: \\cite{li2024osp} introduces the first automatic logic-programming-aided metamorphic testing technique for FCH detection, aiming to overcome the limitations of manual benchmark creation and reasoning validation.\n\n*   **Technical Approach & Innovation**\n    *   **Core Method**: Drowzee, a novel logic-programming-aided metamorphic testing technique for FCH detection in LLMs \\cite{li2024osp}.\n    *   **Knowledge Base Construction**: Drowzee establishes a comprehensive and extensible factual knowledge base by crawling information from sources like Wikipedia. These knowledge pieces serve as \"seeds.\"\n    *   **Test Case Generation (Data Mutation)**: It leverages five unique logic reasoning rules to transform and augment these knowledge \"seeds\" into a large set of question-answer pairs. These pairs serve as test cases and ground truth, respectively. LLMs are prompted with these questions, instructed to provide reasoned answers.\n    *   **Validation Oracles**: Two semantic-aware and similarity-based metamorphic oracles are introduced. These oracles extract essential semantic elements and map logical relationships from both the LLM's answers and the ground truth. By assessing the similarity between these constructed logical and semantic structures, Drowzee reliably detects FCHs where LLM answers significantly diverge from the ground truth.\n\n*   **Key Technical Contributions**\n    *   Development of a novel FCH Testing Framework based on logic programming and metamorphic testing to automatically detect FCH issues in LLMs \\cite{li2024osp}.\n    *   Construction and release of an extensive factual knowledge base and benchmark dataset to facilitate future research.\n    *   Designing and implementing an innovative logic-reasoning-based method for data mutation, utilizing five unique logic reasoning rules to generate diverse and effective test scenarios.\n    *   Deployment of FCH-specific semantic-aware testing oracles (two automated verification mechanisms) that analyze semantic structure similarity to validate LLM reasoning logic and reliably detect FCHs.\n\n*   **Experimental Validation**\n    *   **Experiments**: Drowzee was deployed across a broad spectrum of topics sourced from diverse Wikipedia articles.\n    *   **LLMs Tested**: Evaluated on six different open-source and commercial LLMs.\n    *   **Domains**: Tested across nine distinct domains.\n    *   **Key Performance Metrics & Results**:\n        *   Drowzee successfully generated useful test cases and identified hallucinations.\n        *   Observed hallucination rates ranging from 24.7% to 59.8% across the tested LLMs.\n        *   Hallucination responses were categorized into four types.\n    *   **Key Findings**:\n        *   The lack of logical reasoning capabilities is the primary contributor to FCH issues in LLMs.\n        *   LLMs are particularly prone to generating hallucinations when dealing with temporal concepts and out-of-distribution knowledge.\n        *   The logic-based test cases generated by Drowzee effectively trigger and detect hallucinations.\n        *   Explored model editing techniques as a mitigation strategy, showing promising results on a small scale (edits to fewer than 1000 knowledge pieces).\n\n*   **Limitations & Scope**\n    *   **Mitigation Scope**: While model editing techniques were explored, their effectiveness was demonstrated only on a small scale (fewer than 1000 knowledge pieces), suggesting larger-scale mitigation remains a challenge.\n    *   **Focus**: The primary focus is on Fact-Conflicting Hallucinations (FCH), acknowledging other types (Input-Conflicting, Context-Conflicting) but not directly addressing them within the framework.\n\n*   **Technical Significance**\n    *   **Advances State-of-the-Art**: \\cite{li2024osp} significantly advances the technical state-of-the-art by introducing the first automatic, logic-programming-aided metamorphic testing framework for FCH detection, moving beyond manual and static evaluation methods.\n    *   **Addresses Core Challenges**: It provides a scalable and adaptable solution for automatically generating test cases and validating complex reasoning, which were major bottlenecks in LLM hallucination detection.\n    *   **Potential Impact**:\n        *   Provides a robust framework and a publicly released benchmark dataset to foster collaborative research and future advancements in FCH detection.\n        *   Offers critical insights into specific weaknesses of LLMs, such as logical reasoning deficiencies and struggles with temporal and out-of-distribution knowledge.\n        *   Emphasizes the ongoing need for community efforts to detect and mitigate hallucinations, paving the way for more reliable and trustworthy LLM applications.",
      "intriguing_abstract": "Large Language Models (LLMs) are revolutionizing AI, yet their pervasive **fact-conflicting hallucinations (FCHs)** severely undermine trustworthiness and reliability. Current detection methods, reliant on static benchmarks and manual validation, are inadequate for the dynamic and complex nature of factual knowledge and logical reasoning. We introduce **Drowzee**, the first automatic, **logic-programming-aided metamorphic testing** framework specifically designed to robustly detect FCHs in LLMs.\n\nDrowzee dynamically generates diverse, logic-based test cases by mutating an extensive factual **knowledge base** using five novel reasoning rules. It then employs two **semantic-aware metamorphic oracles** to precisely validate LLM outputs, comparing logical and semantic structures against ground truth. Our evaluation across six LLMs reveals alarming FCH rates (24.7-59.8%), exposing critical deficiencies in **logical reasoning**, especially with temporal and out-of-distribution knowledge. Drowzee provides a scalable solution, a comprehensive **benchmark dataset**, and crucial insights, significantly advancing the state-of-the-art towards more reliable and trustworthy LLM applications.",
      "keywords": [
        "Fact-Conflicting Hallucination (FCH)",
        "Large Language Models (LLMs)",
        "Metamorphic Testing",
        "Logic Programming",
        "Automatic Hallucination Detection",
        "FCH Testing Framework",
        "Logic-Reasoning-Based Data Mutation",
        "Semantic-Aware Testing Oracles",
        "Knowledge Base Construction",
        "Lack of Logical Reasoning",
        "Temporal and Out-of-Distribution Knowledge",
        "Model Editing (Mitigation)",
        "Benchmark Dataset"
      ],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/492e526ca2416a734f286da0efcfeda4672ea77f.pdf",
      "citation_key": "li2024osp",
      "metadata": {
        "title": "Drowzee: Metamorphic Testing for Fact-Conflicting Hallucination Detection in Large Language Models",
        "authors": [
          "Ningke Li",
          "Yuekang Li",
          "Yi Liu",
          "Ling Shi",
          "Kailong Wang",
          "Haoyu Wang"
        ],
        "published_date": "2024",
        "abstract": "Large language models (LLMs) have revolutionized language processing, but face critical challenges with security, privacy, and generating hallucinations â€” coherent but factually inaccurate outputs. A major issue is fact-conflicting hallucination (FCH), where LLMs produce content contradicting ground truth facts. Addressing FCH is difficult due to two key challenges: 1) Automatically constructing and updating benchmark datasets is hard, as existing methods rely on manually curated static benchmarks that cannot cover the broad, evolving spectrum of FCH cases. 2) Validating the reasoning behind LLM outputs is inherently difficult, especially for complex logical relations. To tackle these challenges, we introduce a novel logic-programming-aided metamorphic testing technique for FCH detection. We develop an extensive and extensible framework that constructs a comprehensive factual knowledge base by crawling sources like Wikipedia, seamlessly integrated into Drowzee. Using logical reasoning rules, we transform and augment this knowledge into a large set of test cases with ground truth answers. We test LLMs on these cases through template-based prompts, requiring them to provide reasoned answers. To validate their reasoning, we propose two semantic-aware oracles that assess the similarity between the semantic structures of the LLM answers and ground truth. Our approach automatically generates useful test cases and identifies hallucinations across six LLMs within nine domains, with hallucination rates ranging from 24.7% to 59.8%. Key findings include LLMs struggling with temporal concepts, out-of-distribution knowledge, and lack of logical reasoning capabilities. The results show that logic-based test cases generated by Drowzee effectively trigger and detect hallucinations. To further mitigate the identified FCHs, we explored model editing techniques, which proved effective on a small scale (with edits to fewer than 1000 knowledge pieces). Our findings emphasize the need for continued community efforts to detect and mitigate model hallucinations.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/492e526ca2416a734f286da0efcfeda4672ea77f.pdf",
        "venue": "Proc. ACM Program. Lang.",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the paper \"Drowzee: Metamorphic Testing for Fact-Conflicting Hallucination Detection in Large Language Models\" \\cite{li2024osp} for a literature review:\n\n*   **Research Problem & Motivation**\n    *   Large Language Models (LLMs) frequently generate \"hallucinations,\" which are coherent but factually inaccurate outputs.\n    *   The paper specifically addresses **Fact-Conflicting Hallucination (FCH)**, where LLMs produce content that directly contradicts established ground truth facts.\n    *   This problem is critical because FCH jeopardizes the safety, reliability, and trustworthiness of LLM applications, leading to user confusion.\n    *   **Challenges**:\n        1.  Difficulty in automatically constructing and updating comprehensive benchmark datasets, as existing methods rely on manually curated, static benchmarks that cannot cover the broad and evolving spectrum of FCH cases.\n        2.  Inherent difficulty in automatically validating the reasoning process behind LLM outputs, especially for complex logical relations, which can mask false understanding even if a final answer appears correct.\n\n*   **Related Work & Positioning**\n    *   Existing approaches for hallucination detection primarily rely on manually labeled datasets (e.g., TruthfulQA, HaluEval, KoLA) and naive/semi-automatic validation methods like string matching or manual review.\n    *   **Limitations of previous solutions**: These methods are static, resource-intensive, lack adaptability and scalability, and struggle to cover the dynamic nature of factual knowledge. They also lack dedicated frameworks for automatically testing FCH, particularly for validating complex logical reasoning.\n    *   **Positioning**: \\cite{li2024osp} introduces the first automatic logic-programming-aided metamorphic testing technique for FCH detection, aiming to overcome the limitations of manual benchmark creation and reasoning validation.\n\n*   **Technical Approach & Innovation**\n    *   **Core Method**: Drowzee, a novel logic-programming-aided metamorphic testing technique for FCH detection in LLMs \\cite{li2024osp}.\n    *   **Knowledge Base Construction**: Drowzee establishes a comprehensive and extensible factual knowledge base by crawling information from sources like Wikipedia. These knowledge pieces serve as \"seeds.\"\n    *   **Test Case Generation (Data Mutation)**: It leverages five unique logic reasoning rules to transform and augment these knowledge \"seeds\" into a large set of question-answer pairs. These pairs serve as test cases and ground truth, respectively. LLMs are prompted with these questions, instructed to provide reasoned answers.\n    *   **Validation Oracles**: Two semantic-aware and similarity-based metamorphic oracles are introduced. These oracles extract essential semantic elements and map logical relationships from both the LLM's answers and the ground truth. By assessing the similarity between these constructed logical and semantic structures, Drowzee reliably detects FCHs where LLM answers significantly diverge from the ground truth.\n\n*   **Key Technical Contributions**\n    *   Development of a novel FCH Testing Framework based on logic programming and metamorphic testing to automatically detect FCH issues in LLMs \\cite{li2024osp}.\n    *   Construction and release of an extensive factual knowledge base and benchmark dataset to facilitate future research.\n    *   Designing and implementing an innovative logic-reasoning-based method for data mutation, utilizing five unique logic reasoning rules to generate diverse and effective test scenarios.\n    *   Deployment of FCH-specific semantic-aware testing oracles (two automated verification mechanisms) that analyze semantic structure similarity to validate LLM reasoning logic and reliably detect FCHs.\n\n*   **Experimental Validation**\n    *   **Experiments**: Drowzee was deployed across a broad spectrum of topics sourced from diverse Wikipedia articles.\n    *   **LLMs Tested**: Evaluated on six different open-source and commercial LLMs.\n    *   **Domains**: Tested across nine distinct domains.\n    *   **Key Performance Metrics & Results**:\n        *   Drowzee successfully generated useful test cases and identified hallucinations.\n        *   Observed hallucination rates ranging from 24.7% to 59.8% across the tested LLMs.\n        *   Hallucination responses were categorized into four types.\n    *   **Key Findings**:\n        *   The lack of logical reasoning capabilities is the primary contributor to FCH issues in LLMs.\n        *   LLMs are particularly prone to generating hallucinations when dealing with temporal concepts and out-of-distribution knowledge.\n        *   The logic-based test cases generated by Drowzee effectively trigger and detect hallucinations.\n        *   Explored model editing techniques as a mitigation strategy, showing promising results on a small scale (edits to fewer than 1000 knowledge pieces).\n\n*   **Limitations & Scope**\n    *   **Mitigation Scope**: While model editing techniques were explored, their effectiveness was demonstrated only on a small scale (fewer than 1000 knowledge pieces), suggesting larger-scale mitigation remains a challenge.\n    *   **Focus**: The primary focus is on Fact-Conflicting Hallucinations (FCH), acknowledging other types (Input-Conflicting, Context-Conflicting) but not directly addressing them within the framework.\n\n*   **Technical Significance**\n    *   **Advances State-of-the-Art**: \\cite{li2024osp} significantly advances the technical state-of-the-art by introducing the first automatic, logic-programming-aided metamorphic testing framework for FCH detection, moving beyond manual and static evaluation methods.\n    *   **Addresses Core Challenges**: It provides a scalable and adaptable solution for automatically generating test cases and validating complex reasoning, which were major bottlenecks in LLM hallucination detection.\n    *   **Potential Impact**:\n        *   Provides a robust framework and a publicly released benchmark dataset to foster collaborative research and future advancements in FCH detection.\n        *   Offers critical insights into specific weaknesses of LLMs, such as logical reasoning deficiencies and struggles with temporal and out-of-distribution knowledge.\n        *   Emphasizes the ongoing need for community efforts to detect and mitigate hallucinations, paving the way for more reliable and trustworthy LLM applications.",
        "keywords": [
          "Fact-Conflicting Hallucination (FCH)",
          "Large Language Models (LLMs)",
          "Metamorphic Testing",
          "Logic Programming",
          "Automatic Hallucination Detection",
          "FCH Testing Framework",
          "Logic-Reasoning-Based Data Mutation",
          "Semantic-Aware Testing Oracles",
          "Knowledge Base Construction",
          "Lack of Logical Reasoning",
          "Temporal and Out-of-Distribution Knowledge",
          "Model Editing (Mitigation)",
          "Benchmark Dataset"
        ],
        "paper_type": "based on the provided title and introduction:\n\n*   **title:** \"drowzee: metamorphic testing for fact-conflicting hallucination detection in large language models\"\n    *   this title strongly suggests the introduction of a new system or method (\"drowzee\") that employs a specific technique (\"metamorphic testing\") to solve a technical problem (\"fact-conflicting hallucination detection\").\n*   **introduction:**\n    *   it identifies a critical technical challenge: \"hallucination\" in llms, specifically \"fact-conflicting hallucination\" (fch).\n    *   it explains the nature and impact of this problem, emphasizing its relevance to llm reliability and safety.\n    *   it sets the stage for a solution to this problem, stating that fch is the \"primary type of concern in this paper.\"\n\nthis content aligns best with the **technical** classification criteria:\n*   the introduction discusses a clear \"technical problem\" (fch in llms).\n*   the title implies a \"proposed solution\" (drowzee, using metamorphic testing) which would involve a \"method\" or \"algorithm\" for \"detection.\"\n\ntherefore, this paper is a **technical** paper."
      },
      "file_name": "492e526ca2416a734f286da0efcfeda4672ea77f.pdf"
    },
    {
      "success": true,
      "doc_id": "c105b6e5f2ccc1d7e6b97d7a68da33e9",
      "summary": "Here's a focused summary of the technical paper for a literature review:\n\n### Focused Summary for Literature Review: Lookback Lens\n\n**1. Research Problem & Motivation**\n*   **Specific Technical Problem:** Large Language Models (LLMs) frequently generate \"contextual hallucinations\"â€”details or answers unsubstantiated by the provided input contextâ€”in tasks like summarization and question answering, even when the correct information is present in the input.\n*   **Importance and Challenge:** Contextual hallucinations pose a significant challenge to the reliability and trustworthiness of LLMs in real-world applications (e.g., Retrieval-Augmented Generation, RAG). The problem is challenging because the model *has* the correct information but fails to utilize it faithfully, requiring methods that can discern when the model deviates from its given context.\n\n**2. Related Work & Positioning**\n*   **Relation to Existing Approaches:**\n    *   Most prior work on hallucination detection focuses on scenarios *without* input context, addressing hallucinations arising from the LLM's parametric knowledge.\n    *   Existing methods often leverage various internal LLM representations, such as hidden states, MLP outputs, or attention block/head outputs, to detect hallucinations.\n    *   Text-based entailment models are also used as a baseline for factual consistency.\n*   **Limitations of Previous Solutions:**\n    *   Many methods do not specifically target *contextual* hallucinations, where the input context is paramount.\n    *   Detectors based on raw hidden states or other complex internal representations can overfit to specific model architectures or training data, leading to poor generalization across tasks or models.\n    *   Text-based entailment models require extensive annotated datasets and may not directly capture the internal mechanisms leading to hallucinations.\n\n**3. Technical Approach & Innovation**\n*   **Core Technical Method:**\n    *   `\\cite{chuang20248ey}` proposes the \"Lookback Lens,\" a simple hallucination detection model based on a novel feature called the \"lookback ratio.\"\n    *   **Lookback Ratio:** For each attention head and at each decoding step, the lookback ratio is calculated as the proportion of attention weights focused on the *input context tokens* versus the *newly generated tokens*. This ratio quantifies how much the model \"looks back\" at the context during generation.\n    *   **Feature Vector:** These lookback ratios are concatenated across all attention heads and layers to form a feature vector for each time step. For a given text span, these vectors are averaged.\n    *   **Lookback Lens Classifier:** A linear classifier (logistic regression) is trained on these averaged lookback ratio features to predict whether a text span is factual or hallucinated.\n    *   **Lookback Lens Guided Decoding:** To mitigate hallucinations, `\\cite{chuang20248ey}` introduces a classifier-guided decoding strategy. During generation, multiple candidate text chunks are sampled. For each chunk, its lookback ratio features are computed and scored by the Lookback Lens classifier. The chunk predicted to be least hallucinated (highest factual probability) is selected and appended to the generated sequence.\n*   **Novelty or Differentiation:**\n    *   **Interpretable Feature:** The lookback ratio provides a simple, intuitive, and interpretable signal directly from attention maps, reflecting the model's focus on context versus its own output.\n    *   **Attention-Map-Only Approach:** Unlike methods relying on hidden states or other complex internal representations, `\\cite{chuang20248ey}` exclusively uses attention maps, which are hypothesized to be a more direct and generalizable indicator of contextual faithfulness.\n    *   **Cross-Task and Cross-Model Transferability:** The approach demonstrates strong generalization capabilities, transferring effectively across different tasks and even different LLM sizes (e.g., LLaMA-2-7B to LLaMA-2-13B) without retraining.\n    *   **Guided Decoding via Attention:** The guided decoding mechanism directly leverages attention patterns to steer generation, rather than relying on output token probabilities or external NLI models.\n\n**4. Key Technical Contributions**\n*   **Novel Algorithm/Method:** Introduction of the \"lookback ratio\" as a highly effective and interpretable feature for detecting contextual hallucinations in LLMs.\n*   **System Design/Architectural Innovation:** The \"Lookback Lens\" linear classifier, which leverages these attention-derived features, demonstrating competitive performance with simpler architecture compared to complex hidden state-based detectors.\n*   **Novel Algorithm/Method:** \"Lookback Lens Guided Decoding,\" a novel classifier-guided decoding strategy that effectively mitigates contextual hallucinations by selecting generation chunks based on their predicted factual consistency derived from attention patterns.\n*   **Empirical Insight:** Demonstrating that a simple, high-level feature extracted from attention maps can be more robust and generalizable for hallucination detection and mitigation than more complex internal representations or external NLI models.\n\n**5. Experimental Validation**\n*   **Experiments Conducted:**\n    *   **Detection:** Evaluated the Lookback Lens against text-based NLI models (SoTA and custom-trained) and hidden states-based classifiers (using LLaMA-2-7B-Chat's 24th, 28th, and 32nd layers). Experiments covered predefined span segmentation and sliding window settings, with both in-domain (two-fold validation) and out-of-domain (cross-task) transfer. GPT-4o was used for span-level annotation, validated by human review.\n    *   **Mitigation:** Applied Lookback Lens Guided Decoding to LLaMA-2-7B-Chat on XSum (summarization), Natural Questions (QA), and MT-bench (multi-turn conversations). Compared against greedy decoding and other classifier-guided decoding methods.\n    *   **Cross-Model Transfer:** Tested the transferability of a Lookback Lens trained on LLaMA-2-7B-Chat to LLaMA-2-13B-Chat without retraining.\n*   **Key Performance Metrics and Comparison Results:**\n    *   **Detection (AUROC):** Lookback Lens consistently achieved slightly better or comparable AUROC scores than hidden states-based classifiers and significantly outperformed NLI models, especially in out-of-domain transfer and sliding window settings (e.g., 66.1 AUROC for QA->Sum sliding window vs. 57.7 for 28th Layer hidden states). It showed superior generalization, with less performance drop on unseen tasks.\n    *   **Mitigation:** Lookback Lens Guided Decoding reduced contextual hallucinations by 9.6% in XSum (from 49.0% to 58.6% correct), reducing the number of hallucinated examples by 18.8%. It also improved NQ performance by 3% and boosted MT-bench scores specifically for hallucination detection, without compromising overall generation quality. This performance was on par with a SoTA NLI model trained on 700x more data.\n    *   **Cross-Model Transfer:** A Lookback Lens trained on LLaMA-2-7B-Chat successfully transferred to LLaMA-2-13B-Chat, achieving strong detection performance (e.g., 73.5 AUROC for QA->Sum predefined span) without any retraining on the larger model.\n\n**6. Limitations & Scope**\n*   **Technical Limitations/Assumptions:** The method assumes that contextual hallucinations are sufficiently reflected in the attention distribution between context and generated tokens. The guided decoding approach introduces computational overhead due to sampling multiple candidate chunks.\n*   **Scope of Applicability:** Primarily focused on detecting and mitigating *contextual* hallucinations in LLMs where an input context is provided. The empirical validation was conducted on LLaMA-2 models (7B and 13B) and specific tasks (summarization, QA, multi-turn conversations).\n\n**7. Technical Significance**\n*   **Advances State-of-the-Art:** `\\cite{chuang20248ey}` introduces a novel, interpretable, and highly effective approach for detecting and mitigating contextual hallucinations using only attention maps, outperforming more complex methods in generalization. It significantly advances the ability to improve factual consistency in LLM outputs.\n*   **Potential Impact on Future Research:**\n    *   Highlights the power of attention mechanisms as a direct and robust signal for understanding and controlling LLM behavior, opening new avenues for interpretability and steerability research.\n    *   Provides a practical and generalizable method for building hallucination detectors that can transfer across different LLM sizes and tasks, potentially reducing the need for extensive retraining.\n    *   Encourages further exploration of simple, high-level features derived from model internals for complex LLM control tasks.",
      "intriguing_abstract": "Large Language Models (LLMs) frequently undermine their impressive capabilities by generating \"contextual hallucinations\"â€”information unsubstantiated by their input context. This pervasive issue severely compromises their reliability in critical applications like Retrieval-Augmented Generation (RAG). We introduce the **Lookback Lens**, a novel and highly interpretable approach to detect and mitigate these errors.\n\nAt its core is the **lookback ratio**, a simple yet powerful feature derived directly from an LLM's **attention mechanisms**, quantifying how faithfully the model \"looks back\" at its input context versus its own generated tokens. Unlike complex hidden state-based methods, the Lookback Lens leverages only attention maps, demonstrating superior **generalization** across diverse tasks and even different LLM sizes (e.g., LLaMA-2-7B to LLaMA-2-13B) without retraining. We show it achieves competitive AUROC scores, particularly in challenging out-of-domain scenarios. Furthermore, our **Lookback Lens Guided Decoding** actively steers generation, reducing contextual hallucinations by up to 18.8% and significantly enhancing **factual consistency**. This work provides a robust, practical, and generalizable solution to a critical LLM reliability challenge, paving the way for more trustworthy and context-aware AI systems.",
      "keywords": [
        "Contextual hallucinations",
        "Large Language Models (LLMs)",
        "Lookback Lens",
        "lookback ratio",
        "attention maps",
        "hallucination detection",
        "hallucination mitigation",
        "classifier-guided decoding",
        "factual consistency",
        "cross-task transferability",
        "cross-model transferability",
        "interpretable feature",
        "Retrieval-Augmented Generation (RAG)",
        "generalization capabilities"
      ],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/143a05fb36be8198d7675b594c0656b5652da3cb.pdf",
      "citation_key": "chuang20248ey",
      "metadata": {
        "title": "Lookback Lens: Detecting and Mitigating Contextual Hallucinations in Large Language Models Using Only Attention Maps",
        "authors": [
          "Yung-Sung Chuang",
          "Linlu Qiu",
          "Cheng-Yu Hsieh",
          "Ranjay Krishna",
          "Yoon Kim",
          "James Glass"
        ],
        "published_date": "2024",
        "abstract": "When asked to summarize articles or answer questions given a passage, large language models (LLMs) can hallucinate details and respond with unsubstantiated answers that are inaccurate with respect to the input context. This paper describes a simple approach for detecting such **contextual hallucinations**. We hypothesize that contextual hallucinations are related to the extent to which an LLM attends to information in the provided context versus its own generations. Based on this intuition, we propose a simple hallucination detection model whose input features are given by the ratio of attention weights on the context versus newly generated tokens (for each attention head). We find that a linear classifier based on these _lookback ratio_ features is as effective as a richer detector that utilizes the entire hidden states of an LLM or a text-based entailment model. The lookback ratio-based detectorâ€”**Lookback Lens**â€”is found to transfer across tasks and even models, allowing a detector that is trained on a 7B model to be applied (without retraining) to a larger 13B model. We further apply this detector to mitigate contextual hallucinations, and find that a simple classifier-guided decoding approach is able to reduce the amount of hallucination, for example by 9.6% in the XSum summarization task.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/143a05fb36be8198d7675b594c0656b5652da3cb.pdf",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the technical paper for a literature review:\n\n### Focused Summary for Literature Review: Lookback Lens\n\n**1. Research Problem & Motivation**\n*   **Specific Technical Problem:** Large Language Models (LLMs) frequently generate \"contextual hallucinations\"â€”details or answers unsubstantiated by the provided input contextâ€”in tasks like summarization and question answering, even when the correct information is present in the input.\n*   **Importance and Challenge:** Contextual hallucinations pose a significant challenge to the reliability and trustworthiness of LLMs in real-world applications (e.g., Retrieval-Augmented Generation, RAG). The problem is challenging because the model *has* the correct information but fails to utilize it faithfully, requiring methods that can discern when the model deviates from its given context.\n\n**2. Related Work & Positioning**\n*   **Relation to Existing Approaches:**\n    *   Most prior work on hallucination detection focuses on scenarios *without* input context, addressing hallucinations arising from the LLM's parametric knowledge.\n    *   Existing methods often leverage various internal LLM representations, such as hidden states, MLP outputs, or attention block/head outputs, to detect hallucinations.\n    *   Text-based entailment models are also used as a baseline for factual consistency.\n*   **Limitations of Previous Solutions:**\n    *   Many methods do not specifically target *contextual* hallucinations, where the input context is paramount.\n    *   Detectors based on raw hidden states or other complex internal representations can overfit to specific model architectures or training data, leading to poor generalization across tasks or models.\n    *   Text-based entailment models require extensive annotated datasets and may not directly capture the internal mechanisms leading to hallucinations.\n\n**3. Technical Approach & Innovation**\n*   **Core Technical Method:**\n    *   `\\cite{chuang20248ey}` proposes the \"Lookback Lens,\" a simple hallucination detection model based on a novel feature called the \"lookback ratio.\"\n    *   **Lookback Ratio:** For each attention head and at each decoding step, the lookback ratio is calculated as the proportion of attention weights focused on the *input context tokens* versus the *newly generated tokens*. This ratio quantifies how much the model \"looks back\" at the context during generation.\n    *   **Feature Vector:** These lookback ratios are concatenated across all attention heads and layers to form a feature vector for each time step. For a given text span, these vectors are averaged.\n    *   **Lookback Lens Classifier:** A linear classifier (logistic regression) is trained on these averaged lookback ratio features to predict whether a text span is factual or hallucinated.\n    *   **Lookback Lens Guided Decoding:** To mitigate hallucinations, `\\cite{chuang20248ey}` introduces a classifier-guided decoding strategy. During generation, multiple candidate text chunks are sampled. For each chunk, its lookback ratio features are computed and scored by the Lookback Lens classifier. The chunk predicted to be least hallucinated (highest factual probability) is selected and appended to the generated sequence.\n*   **Novelty or Differentiation:**\n    *   **Interpretable Feature:** The lookback ratio provides a simple, intuitive, and interpretable signal directly from attention maps, reflecting the model's focus on context versus its own output.\n    *   **Attention-Map-Only Approach:** Unlike methods relying on hidden states or other complex internal representations, `\\cite{chuang20248ey}` exclusively uses attention maps, which are hypothesized to be a more direct and generalizable indicator of contextual faithfulness.\n    *   **Cross-Task and Cross-Model Transferability:** The approach demonstrates strong generalization capabilities, transferring effectively across different tasks and even different LLM sizes (e.g., LLaMA-2-7B to LLaMA-2-13B) without retraining.\n    *   **Guided Decoding via Attention:** The guided decoding mechanism directly leverages attention patterns to steer generation, rather than relying on output token probabilities or external NLI models.\n\n**4. Key Technical Contributions**\n*   **Novel Algorithm/Method:** Introduction of the \"lookback ratio\" as a highly effective and interpretable feature for detecting contextual hallucinations in LLMs.\n*   **System Design/Architectural Innovation:** The \"Lookback Lens\" linear classifier, which leverages these attention-derived features, demonstrating competitive performance with simpler architecture compared to complex hidden state-based detectors.\n*   **Novel Algorithm/Method:** \"Lookback Lens Guided Decoding,\" a novel classifier-guided decoding strategy that effectively mitigates contextual hallucinations by selecting generation chunks based on their predicted factual consistency derived from attention patterns.\n*   **Empirical Insight:** Demonstrating that a simple, high-level feature extracted from attention maps can be more robust and generalizable for hallucination detection and mitigation than more complex internal representations or external NLI models.\n\n**5. Experimental Validation**\n*   **Experiments Conducted:**\n    *   **Detection:** Evaluated the Lookback Lens against text-based NLI models (SoTA and custom-trained) and hidden states-based classifiers (using LLaMA-2-7B-Chat's 24th, 28th, and 32nd layers). Experiments covered predefined span segmentation and sliding window settings, with both in-domain (two-fold validation) and out-of-domain (cross-task) transfer. GPT-4o was used for span-level annotation, validated by human review.\n    *   **Mitigation:** Applied Lookback Lens Guided Decoding to LLaMA-2-7B-Chat on XSum (summarization), Natural Questions (QA), and MT-bench (multi-turn conversations). Compared against greedy decoding and other classifier-guided decoding methods.\n    *   **Cross-Model Transfer:** Tested the transferability of a Lookback Lens trained on LLaMA-2-7B-Chat to LLaMA-2-13B-Chat without retraining.\n*   **Key Performance Metrics and Comparison Results:**\n    *   **Detection (AUROC):** Lookback Lens consistently achieved slightly better or comparable AUROC scores than hidden states-based classifiers and significantly outperformed NLI models, especially in out-of-domain transfer and sliding window settings (e.g., 66.1 AUROC for QA->Sum sliding window vs. 57.7 for 28th Layer hidden states). It showed superior generalization, with less performance drop on unseen tasks.\n    *   **Mitigation:** Lookback Lens Guided Decoding reduced contextual hallucinations by 9.6% in XSum (from 49.0% to 58.6% correct), reducing the number of hallucinated examples by 18.8%. It also improved NQ performance by 3% and boosted MT-bench scores specifically for hallucination detection, without compromising overall generation quality. This performance was on par with a SoTA NLI model trained on 700x more data.\n    *   **Cross-Model Transfer:** A Lookback Lens trained on LLaMA-2-7B-Chat successfully transferred to LLaMA-2-13B-Chat, achieving strong detection performance (e.g., 73.5 AUROC for QA->Sum predefined span) without any retraining on the larger model.\n\n**6. Limitations & Scope**\n*   **Technical Limitations/Assumptions:** The method assumes that contextual hallucinations are sufficiently reflected in the attention distribution between context and generated tokens. The guided decoding approach introduces computational overhead due to sampling multiple candidate chunks.\n*   **Scope of Applicability:** Primarily focused on detecting and mitigating *contextual* hallucinations in LLMs where an input context is provided. The empirical validation was conducted on LLaMA-2 models (7B and 13B) and specific tasks (summarization, QA, multi-turn conversations).\n\n**7. Technical Significance**\n*   **Advances State-of-the-Art:** `\\cite{chuang20248ey}` introduces a novel, interpretable, and highly effective approach for detecting and mitigating contextual hallucinations using only attention maps, outperforming more complex methods in generalization. It significantly advances the ability to improve factual consistency in LLM outputs.\n*   **Potential Impact on Future Research:**\n    *   Highlights the power of attention mechanisms as a direct and robust signal for understanding and controlling LLM behavior, opening new avenues for interpretability and steerability research.\n    *   Provides a practical and generalizable method for building hallucination detectors that can transfer across different LLM sizes and tasks, potentially reducing the need for extensive retraining.\n    *   Encourages further exploration of simple, high-level features derived from model internals for complex LLM control tasks.",
        "keywords": [
          "Contextual hallucinations",
          "Large Language Models (LLMs)",
          "Lookback Lens",
          "lookback ratio",
          "attention maps",
          "hallucination detection",
          "hallucination mitigation",
          "classifier-guided decoding",
          "factual consistency",
          "cross-task transferability",
          "cross-model transferability",
          "interpretable feature",
          "Retrieval-Augmented Generation (RAG)",
          "generalization capabilities"
        ],
        "paper_type": "the paper should be classified as **empirical**.\n\nhere's the reasoning:\n\n1.  **technical aspects:** the abstract clearly states, \"we propose a simple hallucination detection model whose input features are given by the ratio of attention weights...\" this indicates the development of a new method or system, which aligns with the \"technical\" classification.\n\n2.  **empirical aspects (stronger emphasis):**\n    *   the abstract heavily emphasizes **findings** and **results** from experiments:\n        *   \"we find that a linear classifier based on these look-back ratio features is as effective as a richer detector...\"\n        *   \"the lookback ratio-based detectorâ€” lookback lens â€”is found to transfer across tasks and even models...\"\n        *   \"we further apply this detector to mitigate contextual hallucinations, and find that a simple classifier-guided decoding approach is able to reduce the amount of hallucination, for example by 9.6% in the xsum summarization task.\"\n    *   the mention of specific quantitative results (\"reduce... by 9.6% in the xsum summarization task\") is a hallmark of empirical studies.\n    *   the introduction discusses the problem and briefly contrasts with prior work, setting the stage for their proposed solution and its evaluation.\n    *   the venue, \"conference on empirical methods in natural language processing,\" strongly suggests an empirical focus.\n\nwhile the paper *proposes* a new technical method, the abstract's primary focus is on the *results* and *effectiveness* of that method as demonstrated through data-driven studies and experiments. the proposed model is the subject of the empirical investigation."
      },
      "file_name": "143a05fb36be8198d7675b594c0656b5652da3cb.pdf"
    },
    {
      "success": true,
      "doc_id": "caf12a4c9dad5a3798934d83de3fe280",
      "summary": "Here's a focused summary of the paper for a literature review, adhering to the specified citation requirements:\n\n### Analysis of \"The Dawn After the Dark: An Empirical Study on Factuality Hallucination in Large Language Models\" \\cite{li2024qrj}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: The paper addresses the pervasive issue of \"hallucination\" in Large Language Models (LLMs), defined as their tendency to generate factually incorrect content \\cite{li2024qrj}.\n    *   **Importance and Challenge**: This problem is critical because it severely hinders the trustworthy and reliable deployment of LLMs in real-world applications (e.g., clinical diagnoses) where factual accuracy is paramount. Existing research often focuses on individual aspects (detection, source, or mitigation) but lacks a systematic and in-depth empirical study across all three, and across different stages of LLM development and utilization \\cite{li2024qrj}.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: Previous studies have explored LLM hallucinations, but they typically analyze or address individual challenges (e.g., why LLMs hallucinate, how to detect them, or how to mitigate them) \\cite{li2024qrj}.\n    *   **Limitations of Previous Solutions**: Prior work often lacks a comprehensive, systematic, and in-depth experimental study that integrates detection, source analysis, and mitigation strategies across the entire LLM lifecycle (pre-training, supervised fine-tuning, RLHF, and inference) \\cite{li2024qrj}. This paper aims to fill that gap by providing a holistic empirical analysis.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper presents a systematic empirical study on LLM factuality hallucination, focusing on three key aspects: detection, source analysis, and mitigation.\n        *   **Hallucination Detection**: A simple yet effective framework is proposed, which decomposes detection into two sub-problems: 1) extracting factual statements from LLM responses (using GPT-4 for instruction-following) and 2) determining the truthfulness of each statement (using GPT-4, considering interrelations between statements) \\cite{li2024qrj}.\n        *   **Source Analysis**: The study zooms into different LLM stages (pre-training, supervised fine-tuning (SFT), reinforcement learning from human feedback (RLHF), and inference) and extensively analyzes potential factors leading to hallucinations, including prompt design \\cite{li2024qrj}.\n        *   **Mitigation**: A series of widely used techniques are implemented and examined, including RLHF, retrieval augmentation, self-reflexion, advanced decoding, and prompt improvement \\cite{li2024qrj}.\n    *   **Novelty/Differentiation**:\n        *   **Comprehensive Scope**: Unlike prior work, this study provides a systematic and integrated empirical analysis covering detection, source, and mitigation of factuality hallucination across the entire LLM lifecycle \\cite{li2024qrj}.\n        *   **New Benchmark**: Construction of HaluEval 2.0, an upgraded benchmark with 8,770 questions across five diverse domains (biomedicine, finance, science, education, open domain) specifically designed to evaluate factuality hallucination \\cite{li2024qrj}.\n        *   **Fine-grained Categorization**: Introduction of a detailed taxonomy for factuality hallucination, including Entity-error, Relation-error, Incompleteness, Outdatedness, Overclaim, and Unverifiability hallucinations \\cite{li2024qrj}.\n        *   **LLM-based Detection**: Leveraging advanced LLMs (GPT-4) for both factual statement extraction and judgment, with a specific consideration for interdependencies between statements, which is a refinement over independent assessment approaches \\cite{li2024qrj}.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**:\n        *   A simple yet effective LLM-based (GPT-4) framework for automatic factual hallucination detection, which extracts factual statements and then judges their truthfulness, accounting for interrelations \\cite{li2024qrj}.\n    *   **System Design/Architectural Innovations**:\n        *   The HaluEval 2.0 benchmark, a large-scale, multi-domain dataset specifically curated for evaluating factuality hallucination in LLMs \\cite{li2024qrj}.\n    *   **Theoretical Insights/Analysis**:\n        *   Empirical findings on the sources of hallucination:\n            *   Pre-training: Marginal effect of more tokens, but specialized data significantly alleviates domain-specific hallucinations; lower frequency of pre-training knowledge correlates with more hallucinations \\cite{li2024qrj}.\n            *   Fine-tuning: Improved instructions and balanced complexity in SFT reduce hallucinations; RLHF is effective but domain-dependent \\cite{li2024qrj}.\n            *   Inference: Diversity-oriented decoding increases hallucinations in professional domains, while greedy search exacerbates them in open-ended domains; token-by-token generation can lead to over-commitment to mistakes; quantization largely elicits hallucinations \\cite{li2024qrj}.\n            *   Prompt Design: More detailed task descriptions and in-context learning decrease hallucinations; rewriting questions or placing task descriptions after questions induces more hallucinations; easier-to-read, formal, and specific language reduces hallucination tendency \\cite{li2024qrj}.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**:\n        *   Construction and evaluation on the HaluEval 2.0 benchmark, comprising 8,770 questions across biomedicine, finance, science, education, and open domains \\cite{li2024qrj}.\n        *   Reliability test of the proposed detection approach by comparing LLM judgments (GPT-4) with human annotations on a subset of 1,000 questions \\cite{li2024qrj}.\n        *   Extensive evaluation of various open-source (Alpaca, Vicuna, YuLan-Chat, Llama 2-Chat) and closed-source (text-davinci-002/003, ChatGPT, Claude, Claude 2) LLMs \\cite{li2024qrj}.\n        *   Systematic experiments to analyze hallucination sources across pre-training, fine-tuning, inference, and prompt design, and to examine mitigation techniques \\cite{li2024qrj}.\n    *   **Key Performance Metrics and Comparison Results**:\n        *   **Detection Reliability**: The LLM-based detection method achieved high matching rates with human annotations, ranging from 91.5% to 94.7% across different domains, demonstrating its reliability \\cite{li2024qrj}.\n        *   **Hallucination Metrics**: Micro Hallucination Rate (MiHR) and Macro Hallucination Rate (MaHR) were used to quantify the degree of hallucination at statement and response levels, respectively \\cite{li2024qrj}.\n        *   **Empirical Findings**: The study yielded specific findings on how different factors (e.g., pre-training data, instruction complexity, decoding methods, quantization, prompt structure) impact hallucination rates, and the effectiveness of various mitigation techniques (e.g., RLHF, retrieval augmentation) \\cite{li2024qrj}.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The paper acknowledges that it is impossible to encompass all kinds of hallucination, and their taxonomy focuses on the most frequently occurring types of *factuality hallucination* \\cite{li2024qrj}. The LLM-based detection relies on the capabilities of GPT-4, which itself is an LLM and might have inherent biases or limitations.\n    *   **Scope of Applicability**: The study primarily focuses on *factuality hallucination* and its detection, sources, and mitigation. While comprehensive within this scope, it does not delve into other forms of hallucination (e.g., coherence, consistency) \\cite{li2024qrj}.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: This work significantly advances the technical state-of-the-art by providing the first systematic and comprehensive empirical study on factuality hallucination across the entire LLM lifecycle \\cite{li2024qrj}. It moves beyond isolated analyses to offer an integrated understanding.\n    *   **Potential Impact on Future Research**:\n        *   **Guidance for LLM Development**: The detailed empirical findings on hallucination sources (pre-training, fine-tuning, inference, prompting) offer concrete guidance for developers to design and train more factually robust LLMs \\cite{li2024qrj}.\n        *   **Improved Mitigation Strategies**: The examination of various mitigation techniques provides insights into their effectiveness and limitations, informing the development of more targeted and efficient debiasing methods \\cite{li2024qrj}.\n        *   **Standardized Evaluation**: The HaluEval 2.0 benchmark and the reliable LLM-based detection method provide valuable tools for future research to consistently evaluate and compare LLMs' factual accuracy \\cite{li2024qrj}.\n        *   **Deeper Understanding**: The fine-grained categorization of hallucination types contributes to a more nuanced understanding of this complex phenomenon, facilitating more precise analysis and intervention.",
      "intriguing_abstract": "The pervasive issue of factuality hallucination remains a critical barrier to the trustworthy deployment of Large Language Models (LLMs). Despite growing concern, a systematic, empirical understanding spanning detection, source analysis, and mitigation across the entire LLM lifecycle has been conspicuously absent. This paper presents the first comprehensive study to bridge this gap, offering an in-depth analysis from pre-training and supervised fine-tuning (SFT) to reinforcement learning from human feedback (RLHF) and inference.\n\nWe introduce HaluEval 2.0, a novel, multi-domain benchmark with 8,770 questions, alongside a reliable GPT-4-based detection framework that meticulously extracts and judges factual statements, even considering interdependencies. Our fine-grained taxonomy categorizes hallucination types, revealing nuanced insights. Empirical findings illuminate how factors like pre-training data, prompt engineering, decoding strategies, and quantization profoundly influence hallucination rates. We also rigorously evaluate mitigation techniques, including retrieval augmentation. This work provides invaluable guidance for developing more factually robust LLMs and offers essential tools for standardized evaluation, paving the way for a new dawn of reliable AI.",
      "keywords": [
        "Large Language Models (LLMs)",
        "factuality hallucination",
        "systematic empirical study",
        "hallucination detection",
        "source analysis",
        "mitigation strategies",
        "LLM lifecycle",
        "HaluEval 2.0 benchmark",
        "LLM-based detection framework",
        "prompt design",
        "retrieval augmentation",
        "fine-grained hallucination taxonomy",
        "trustworthy LLMs"
      ],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/1b387e3fbec0447c8bf2dcee21f6db59cdddf698.pdf",
      "citation_key": "li2024qrj",
      "metadata": {
        "title": "The Dawn After the Dark: An Empirical Study on Factuality Hallucination in Large Language Models",
        "authors": [
          "Junyi Li",
          "Jie Chen",
          "Ruiyang Ren",
          "Xiaoxue Cheng",
          "Wayne Xin Zhao",
          "Jian-yun Nie",
          "Ji-Rong Wen"
        ],
        "published_date": "2024",
        "abstract": "In the era of large language models (LLMs), hallucination (i.e., the tendency to generate factually incorrect content) poses great challenge to trustworthy and reliable deployment of LLMs in real-world applications. To tackle the LLM hallucination, three key questions should be well studied: how to detect hallucinations (detection), why do LLMs hallucinate (source), and what can be done to mitigate them (mitigation). To address these challenges, this work presents a systematic empirical study on LLM hallucination, focused on the the three aspects of hallucination detection, source and mitigation. Specially, we construct a new hallucination benchmark HaluEval 2.0, and designs a simple yet effective detection method for LLM hallucination. Furthermore, we zoom into the different training or utilization stages of LLMs and extensively analyze the potential factors that lead to the LLM hallucination. Finally, we implement and examine a series of widely used techniques to mitigate the hallucinations in LLMs. Our work has led to several important findings to understand the hallucination origin and mitigate the hallucinations in LLMs. Our code and data can be accessed at https://github.com/RUCAIBox/HaluEval-2.0.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/1b387e3fbec0447c8bf2dcee21f6db59cdddf698.pdf",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the paper for a literature review, adhering to the specified citation requirements:\n\n### Analysis of \"The Dawn After the Dark: An Empirical Study on Factuality Hallucination in Large Language Models\" \\cite{li2024qrj}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: The paper addresses the pervasive issue of \"hallucination\" in Large Language Models (LLMs), defined as their tendency to generate factually incorrect content \\cite{li2024qrj}.\n    *   **Importance and Challenge**: This problem is critical because it severely hinders the trustworthy and reliable deployment of LLMs in real-world applications (e.g., clinical diagnoses) where factual accuracy is paramount. Existing research often focuses on individual aspects (detection, source, or mitigation) but lacks a systematic and in-depth empirical study across all three, and across different stages of LLM development and utilization \\cite{li2024qrj}.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: Previous studies have explored LLM hallucinations, but they typically analyze or address individual challenges (e.g., why LLMs hallucinate, how to detect them, or how to mitigate them) \\cite{li2024qrj}.\n    *   **Limitations of Previous Solutions**: Prior work often lacks a comprehensive, systematic, and in-depth experimental study that integrates detection, source analysis, and mitigation strategies across the entire LLM lifecycle (pre-training, supervised fine-tuning, RLHF, and inference) \\cite{li2024qrj}. This paper aims to fill that gap by providing a holistic empirical analysis.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper presents a systematic empirical study on LLM factuality hallucination, focusing on three key aspects: detection, source analysis, and mitigation.\n        *   **Hallucination Detection**: A simple yet effective framework is proposed, which decomposes detection into two sub-problems: 1) extracting factual statements from LLM responses (using GPT-4 for instruction-following) and 2) determining the truthfulness of each statement (using GPT-4, considering interrelations between statements) \\cite{li2024qrj}.\n        *   **Source Analysis**: The study zooms into different LLM stages (pre-training, supervised fine-tuning (SFT), reinforcement learning from human feedback (RLHF), and inference) and extensively analyzes potential factors leading to hallucinations, including prompt design \\cite{li2024qrj}.\n        *   **Mitigation**: A series of widely used techniques are implemented and examined, including RLHF, retrieval augmentation, self-reflexion, advanced decoding, and prompt improvement \\cite{li2024qrj}.\n    *   **Novelty/Differentiation**:\n        *   **Comprehensive Scope**: Unlike prior work, this study provides a systematic and integrated empirical analysis covering detection, source, and mitigation of factuality hallucination across the entire LLM lifecycle \\cite{li2024qrj}.\n        *   **New Benchmark**: Construction of HaluEval 2.0, an upgraded benchmark with 8,770 questions across five diverse domains (biomedicine, finance, science, education, open domain) specifically designed to evaluate factuality hallucination \\cite{li2024qrj}.\n        *   **Fine-grained Categorization**: Introduction of a detailed taxonomy for factuality hallucination, including Entity-error, Relation-error, Incompleteness, Outdatedness, Overclaim, and Unverifiability hallucinations \\cite{li2024qrj}.\n        *   **LLM-based Detection**: Leveraging advanced LLMs (GPT-4) for both factual statement extraction and judgment, with a specific consideration for interdependencies between statements, which is a refinement over independent assessment approaches \\cite{li2024qrj}.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**:\n        *   A simple yet effective LLM-based (GPT-4) framework for automatic factual hallucination detection, which extracts factual statements and then judges their truthfulness, accounting for interrelations \\cite{li2024qrj}.\n    *   **System Design/Architectural Innovations**:\n        *   The HaluEval 2.0 benchmark, a large-scale, multi-domain dataset specifically curated for evaluating factuality hallucination in LLMs \\cite{li2024qrj}.\n    *   **Theoretical Insights/Analysis**:\n        *   Empirical findings on the sources of hallucination:\n            *   Pre-training: Marginal effect of more tokens, but specialized data significantly alleviates domain-specific hallucinations; lower frequency of pre-training knowledge correlates with more hallucinations \\cite{li2024qrj}.\n            *   Fine-tuning: Improved instructions and balanced complexity in SFT reduce hallucinations; RLHF is effective but domain-dependent \\cite{li2024qrj}.\n            *   Inference: Diversity-oriented decoding increases hallucinations in professional domains, while greedy search exacerbates them in open-ended domains; token-by-token generation can lead to over-commitment to mistakes; quantization largely elicits hallucinations \\cite{li2024qrj}.\n            *   Prompt Design: More detailed task descriptions and in-context learning decrease hallucinations; rewriting questions or placing task descriptions after questions induces more hallucinations; easier-to-read, formal, and specific language reduces hallucination tendency \\cite{li2024qrj}.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**:\n        *   Construction and evaluation on the HaluEval 2.0 benchmark, comprising 8,770 questions across biomedicine, finance, science, education, and open domains \\cite{li2024qrj}.\n        *   Reliability test of the proposed detection approach by comparing LLM judgments (GPT-4) with human annotations on a subset of 1,000 questions \\cite{li2024qrj}.\n        *   Extensive evaluation of various open-source (Alpaca, Vicuna, YuLan-Chat, Llama 2-Chat) and closed-source (text-davinci-002/003, ChatGPT, Claude, Claude 2) LLMs \\cite{li2024qrj}.\n        *   Systematic experiments to analyze hallucination sources across pre-training, fine-tuning, inference, and prompt design, and to examine mitigation techniques \\cite{li2024qrj}.\n    *   **Key Performance Metrics and Comparison Results**:\n        *   **Detection Reliability**: The LLM-based detection method achieved high matching rates with human annotations, ranging from 91.5% to 94.7% across different domains, demonstrating its reliability \\cite{li2024qrj}.\n        *   **Hallucination Metrics**: Micro Hallucination Rate (MiHR) and Macro Hallucination Rate (MaHR) were used to quantify the degree of hallucination at statement and response levels, respectively \\cite{li2024qrj}.\n        *   **Empirical Findings**: The study yielded specific findings on how different factors (e.g., pre-training data, instruction complexity, decoding methods, quantization, prompt structure) impact hallucination rates, and the effectiveness of various mitigation techniques (e.g., RLHF, retrieval augmentation) \\cite{li2024qrj}.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The paper acknowledges that it is impossible to encompass all kinds of hallucination, and their taxonomy focuses on the most frequently occurring types of *factuality hallucination* \\cite{li2024qrj}. The LLM-based detection relies on the capabilities of GPT-4, which itself is an LLM and might have inherent biases or limitations.\n    *   **Scope of Applicability**: The study primarily focuses on *factuality hallucination* and its detection, sources, and mitigation. While comprehensive within this scope, it does not delve into other forms of hallucination (e.g., coherence, consistency) \\cite{li2024qrj}.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: This work significantly advances the technical state-of-the-art by providing the first systematic and comprehensive empirical study on factuality hallucination across the entire LLM lifecycle \\cite{li2024qrj}. It moves beyond isolated analyses to offer an integrated understanding.\n    *   **Potential Impact on Future Research**:\n        *   **Guidance for LLM Development**: The detailed empirical findings on hallucination sources (pre-training, fine-tuning, inference, prompting) offer concrete guidance for developers to design and train more factually robust LLMs \\cite{li2024qrj}.\n        *   **Improved Mitigation Strategies**: The examination of various mitigation techniques provides insights into their effectiveness and limitations, informing the development of more targeted and efficient debiasing methods \\cite{li2024qrj}.\n        *   **Standardized Evaluation**: The HaluEval 2.0 benchmark and the reliable LLM-based detection method provide valuable tools for future research to consistently evaluate and compare LLMs' factual accuracy \\cite{li2024qrj}.\n        *   **Deeper Understanding**: The fine-grained categorization of hallucination types contributes to a more nuanced understanding of this complex phenomenon, facilitating more precise analysis and intervention.",
        "keywords": [
          "Large Language Models (LLMs)",
          "factuality hallucination",
          "systematic empirical study",
          "hallucination detection",
          "source analysis",
          "mitigation strategies",
          "LLM lifecycle",
          "HaluEval 2.0 benchmark",
          "LLM-based detection framework",
          "prompt design",
          "retrieval augmentation",
          "fine-grained hallucination taxonomy",
          "trustworthy LLMs"
        ],
        "paper_type": "the paper type is **empirical**.\n\nhere's why:\n\n1.  **explicit mention:** the title explicitly states \"an **empirical study**\". the abstract also states \"this work presents a systematic **empirical study** on llm hallucination\". the introduction mentions \"in-depth **experimental study**\".\n2.  **methodology:** the abstract describes constructing a new benchmark (\"halueval 2.0\"), designing a detection method, extensively analyzing potential factors, and implementing and examining mitigation techniques. these are all characteristics of data-driven studies and experiments.\n3.  **outcomes:** the abstract highlights \"several important **findings** to understand the hallucination origin and mitigate the hallucinations\". this aligns with the \"findings\" aspect of empirical research.\n4.  **data/code availability:** the mention of \"our code and data can be accessed at...\" further supports the empirical nature, as it implies data collection and analysis."
      },
      "file_name": "1b387e3fbec0447c8bf2dcee21f6db59cdddf698.pdf"
    },
    {
      "success": true,
      "doc_id": "806a78533044c7793e26f5019dea8413",
      "summary": "Here's a focused summary of the paper for a literature review, emphasizing technical innovations and empirical validation:\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem**: The paper addresses the critical issue of \"hallucination\" in Large Vision-Language Models (LVLMs), where models generate information that does not exist in the visual input.\n    *   **Importance and Challenge**: Hallucination limits the practical applicability of LVLMs and poses significant risks due to potentially misleading or incorrect information. Existing evaluation methods, particularly object-based approaches like POPE \\cite{wang2023zop}, are shown to be highly susceptible to prompt bias, leading to inaccurate assessments that do not reflect real-world hallucination behavior. The complexity of LVLM responses in real-world scenarios also makes traditional match-based evaluation methods inadequate.\n\n*   **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: The work directly critiques and positions itself against prior object-based hallucination evaluation frameworks (e.g., POPE \\cite{wang2023zop}).\n    *   **Limitations of Previous Solutions**: Previous methods are demonstrated to exploit LVLMs' judgment bias (e.g., answering \"yes\" to over 80% of queries about non-existent objects when prompted with \"Is there a {object}?\"), rather than accurately measuring actual hallucination in descriptive contexts (where less than 10% hallucination was observed for the same items). This prompt sensitivity renders existing evaluations unreliable for real-world scenarios.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes **HaELM (Hallucination Evaluation based on Large Language Models)**, an LLM-based framework for evaluating hallucination in LVLMs. HaELM operates by:\n        1.  **Identifying Hallucination Patterns**: Manually annotating realistic hallucination responses from LVLMs when prompted to \"Describe this image.\"\n        2.  **Data Collection**: Generating a substantial dataset of both hallucinatory and non-hallucinatory responses. Simulated hallucination data is created by crafting prompts for ChatGPT to generate responses based on reference captions, iteratively refined through human similarity assessment to align with realistic hallucination patterns. Non-hallucination data is generated by ChatGPT strictly adhering to reference captions.\n        3.  **Model Training**: Fine-tuning an open-source LLM (LLaMA \\cite{wang2023zop}) using the LoRA-based methodology \\cite{wang2023zop} on the collected data. The trained model takes reference captions and LVLM responses as input and outputs a \"yes\" or \"no\" judgment on hallucination.\n    *   **Novelty**: HaELM is novel in being the first to leverage the powerful text-understanding capabilities of LLMs for hallucination evaluation in LVLMs, specifically designed for real-world, complex descriptive responses. It directly addresses and mitigates the prompt bias observed in previous object-based evaluation methods. Its design also emphasizes low cost, reproducibility, privacy preservation, and local deployment.\n\n*   **Key Technical Contributions**\n    *   **Novel Discovery**: Empirical demonstration that existing object-based hallucination evaluation methods are flawed due to LVLMs' prompt sensitivity and judgment bias, failing to reflect real-world hallucination.\n    *   **Novel Framework**: Introduction of HaELM, the first LLM-based framework for robust and realistic hallucination evaluation in LVLMs.\n    *   **Methodological Innovation**: A systematic data collection strategy involving both manual annotation of real hallucination patterns and bulk generation of simulated hallucination data using ChatGPT, followed by LoRA-based fine-tuning of LLaMA.\n    *   **Practical Advantages**: HaELM offers significant advantages over commercial LLM baselines like ChatGPT in terms of cost-effectiveness, reproducibility, and data privacy through local deployment.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted**:\n        *   Evaluation of HaELM's performance against ChatGPT using a human-annotated benchmark derived from LVLM responses to \"Describe this image\" prompts on MS-COCO 2014 images.\n        *   Comparison of time and cost efficiency between HaELM and ChatGPT for evaluation.\n        *   Application of HaELM to evaluate hallucination in prominent open-source LVLMs: mPLUG-Owl, MiniGPT-4, and LLaVA.\n    *   **Key Performance Metrics**: Accuracy, Precision, Recall, and F1 Score. Time and monetary cost.\n    *   **Comparison Results**:\n        *   HaELM achieved an overall accuracy of 61%, closely comparable to ChatGPT's 64%, demonstrating approximately 95% of ChatGPT's performance level.\n        *   While HaELM showed a bias towards non-hallucination responses (higher accuracy on non-hallucination, lower on hallucination compared to ChatGPT), it achieved competitive average F1 scores (e.g., 77.5% for LLaVA, 51.8% for MiniGPT-4, 65.6% for mPLUG-Owl on non-hallucination; 37.7% for LLaVA, 64.3% for MiniGPT-4, 42.7% for mPLUG-Owl on hallucination).\n        *   HaELM significantly reduced evaluation costs and time after initial setup (3.8 hours and $4.3 for one-time collection/training vs. $6.6 and 1.6 hours *per evaluation* for ChatGPT), making it more scalable for repeated analyses.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations**: HaELM exhibits a bias towards classifying responses as non-hallucinatory. This is attributed to the simulated hallucination data not fully matching the distribution of actual hallucination responses, leading to misclassification in certain patterns.\n    *   **Scope of Applicability**: The current work primarily focuses on hallucination evaluation within the real-world scenario of image description.\n\n*   **Technical Significance**\n    *   **Advancement of State-of-the-Art**: HaELM significantly advances the technical state-of-the-art by providing a more reliable, robust, and practical framework for evaluating hallucination in LVLMs, overcoming the limitations of prompt-sensitive object-based methods.\n    *   **Potential Impact**: It enables more accurate and comprehensive analysis of hallucination factors in LVLMs, paving the way for future research into effective mitigation strategies. The cost-effectiveness, reproducibility, and privacy features of HaELM make it a valuable tool for the broader research community and practical applications.",
      "intriguing_abstract": "The pervasive challenge of hallucination in Large Vision-Language Models (LVLMs) critically undermines their reliability, yet current evaluation methods are fundamentally flawed. We reveal that prevailing object-based hallucination benchmarks, such as POPE, are highly susceptible to prompt bias, yielding misleading assessments that fail to reflect real-world descriptive hallucination. To overcome this, we introduce **HaELM (Hallucination Evaluation based on Large Language Models)**, a novel, LLM-based framework for robust and realistic hallucination assessment in complex LVLM responses. HaELM employs a unique data generation strategy, combining manual annotation with simulated hallucination data crafted by ChatGPT, to fine-tune an open-source LLM (LLaMA) using LoRA. This enables accurate discernment of hallucinatory content without prompt sensitivity. Achieving performance comparable to commercial LLM baselines at significantly reduced cost and enhanced reproducibility, HaELM offers a scalable and private solution. Our work not only exposes inherent flaws in prevailing evaluation paradigms but also provides a critical framework to truly understand and mitigate hallucination, paving the way for safer and more trustworthy LVLMs.",
      "keywords": [
        "Hallucination in LVLMs",
        "Large Vision-Language Models (LVLMs)",
        "Prompt bias",
        "Object-based evaluation limitations",
        "HaELM framework",
        "LLM-based evaluation",
        "Simulated hallucination data",
        "LoRA fine-tuning",
        "Real-world image description",
        "Cost-effective evaluation",
        "Reproducible evaluation",
        "Empirical validation"
      ],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/bb1083425517bdac8d9a6438fcf5032543acb20e.pdf",
      "citation_key": "wang2023zop",
      "metadata": {
        "title": "Evaluation and Analysis of Hallucination in Large Vision-Language Models",
        "authors": [
          "Junyan Wang",
          "Yi Zhou",
          "Guohai Xu",
          "Pengcheng Shi",
          "Chenlin Zhao",
          "Haiyang Xu",
          "Qinghao Ye",
          "Mingshi Yan",
          "Ji Zhang",
          "Jihua Zhu",
          "Jitao Sang",
          "Haoyu Tang"
        ],
        "published_date": "2023",
        "abstract": "Large Vision-Language Models (LVLMs) have recently achieved remarkable success. However, LVLMs are still plagued by the hallucination problem, which limits the practicality in many scenarios. Hallucination refers to the information of LVLMs' responses that does not exist in the visual input, which poses potential risks of substantial consequences. There has been limited work studying hallucination evaluation in LVLMs. In this paper, we propose Hallucination Evaluation based on Large Language Models (HaELM), an LLM-based hallucination evaluation framework. HaELM achieves an approximate 95% performance comparable to ChatGPT and has additional advantages including low cost, reproducibility, privacy preservation and local deployment. Leveraging the HaELM, we evaluate the hallucination in current LVLMs. Furthermore, we analyze the factors contributing to hallucination in LVLMs and offer helpful suggestions to mitigate the hallucination problem. Our training data and human annotation hallucination data will be made public soon.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/bb1083425517bdac8d9a6438fcf5032543acb20e.pdf",
        "venue": "arXiv.org",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the paper for a literature review, emphasizing technical innovations and empirical validation:\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem**: The paper addresses the critical issue of \"hallucination\" in Large Vision-Language Models (LVLMs), where models generate information that does not exist in the visual input.\n    *   **Importance and Challenge**: Hallucination limits the practical applicability of LVLMs and poses significant risks due to potentially misleading or incorrect information. Existing evaluation methods, particularly object-based approaches like POPE \\cite{wang2023zop}, are shown to be highly susceptible to prompt bias, leading to inaccurate assessments that do not reflect real-world hallucination behavior. The complexity of LVLM responses in real-world scenarios also makes traditional match-based evaluation methods inadequate.\n\n*   **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: The work directly critiques and positions itself against prior object-based hallucination evaluation frameworks (e.g., POPE \\cite{wang2023zop}).\n    *   **Limitations of Previous Solutions**: Previous methods are demonstrated to exploit LVLMs' judgment bias (e.g., answering \"yes\" to over 80% of queries about non-existent objects when prompted with \"Is there a {object}?\"), rather than accurately measuring actual hallucination in descriptive contexts (where less than 10% hallucination was observed for the same items). This prompt sensitivity renders existing evaluations unreliable for real-world scenarios.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes **HaELM (Hallucination Evaluation based on Large Language Models)**, an LLM-based framework for evaluating hallucination in LVLMs. HaELM operates by:\n        1.  **Identifying Hallucination Patterns**: Manually annotating realistic hallucination responses from LVLMs when prompted to \"Describe this image.\"\n        2.  **Data Collection**: Generating a substantial dataset of both hallucinatory and non-hallucinatory responses. Simulated hallucination data is created by crafting prompts for ChatGPT to generate responses based on reference captions, iteratively refined through human similarity assessment to align with realistic hallucination patterns. Non-hallucination data is generated by ChatGPT strictly adhering to reference captions.\n        3.  **Model Training**: Fine-tuning an open-source LLM (LLaMA \\cite{wang2023zop}) using the LoRA-based methodology \\cite{wang2023zop} on the collected data. The trained model takes reference captions and LVLM responses as input and outputs a \"yes\" or \"no\" judgment on hallucination.\n    *   **Novelty**: HaELM is novel in being the first to leverage the powerful text-understanding capabilities of LLMs for hallucination evaluation in LVLMs, specifically designed for real-world, complex descriptive responses. It directly addresses and mitigates the prompt bias observed in previous object-based evaluation methods. Its design also emphasizes low cost, reproducibility, privacy preservation, and local deployment.\n\n*   **Key Technical Contributions**\n    *   **Novel Discovery**: Empirical demonstration that existing object-based hallucination evaluation methods are flawed due to LVLMs' prompt sensitivity and judgment bias, failing to reflect real-world hallucination.\n    *   **Novel Framework**: Introduction of HaELM, the first LLM-based framework for robust and realistic hallucination evaluation in LVLMs.\n    *   **Methodological Innovation**: A systematic data collection strategy involving both manual annotation of real hallucination patterns and bulk generation of simulated hallucination data using ChatGPT, followed by LoRA-based fine-tuning of LLaMA.\n    *   **Practical Advantages**: HaELM offers significant advantages over commercial LLM baselines like ChatGPT in terms of cost-effectiveness, reproducibility, and data privacy through local deployment.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted**:\n        *   Evaluation of HaELM's performance against ChatGPT using a human-annotated benchmark derived from LVLM responses to \"Describe this image\" prompts on MS-COCO 2014 images.\n        *   Comparison of time and cost efficiency between HaELM and ChatGPT for evaluation.\n        *   Application of HaELM to evaluate hallucination in prominent open-source LVLMs: mPLUG-Owl, MiniGPT-4, and LLaVA.\n    *   **Key Performance Metrics**: Accuracy, Precision, Recall, and F1 Score. Time and monetary cost.\n    *   **Comparison Results**:\n        *   HaELM achieved an overall accuracy of 61%, closely comparable to ChatGPT's 64%, demonstrating approximately 95% of ChatGPT's performance level.\n        *   While HaELM showed a bias towards non-hallucination responses (higher accuracy on non-hallucination, lower on hallucination compared to ChatGPT), it achieved competitive average F1 scores (e.g., 77.5% for LLaVA, 51.8% for MiniGPT-4, 65.6% for mPLUG-Owl on non-hallucination; 37.7% for LLaVA, 64.3% for MiniGPT-4, 42.7% for mPLUG-Owl on hallucination).\n        *   HaELM significantly reduced evaluation costs and time after initial setup (3.8 hours and $4.3 for one-time collection/training vs. $6.6 and 1.6 hours *per evaluation* for ChatGPT), making it more scalable for repeated analyses.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations**: HaELM exhibits a bias towards classifying responses as non-hallucinatory. This is attributed to the simulated hallucination data not fully matching the distribution of actual hallucination responses, leading to misclassification in certain patterns.\n    *   **Scope of Applicability**: The current work primarily focuses on hallucination evaluation within the real-world scenario of image description.\n\n*   **Technical Significance**\n    *   **Advancement of State-of-the-Art**: HaELM significantly advances the technical state-of-the-art by providing a more reliable, robust, and practical framework for evaluating hallucination in LVLMs, overcoming the limitations of prompt-sensitive object-based methods.\n    *   **Potential Impact**: It enables more accurate and comprehensive analysis of hallucination factors in LVLMs, paving the way for future research into effective mitigation strategies. The cost-effectiveness, reproducibility, and privacy features of HaELM make it a valuable tool for the broader research community and practical applications.",
        "keywords": [
          "Hallucination in LVLMs",
          "Large Vision-Language Models (LVLMs)",
          "Prompt bias",
          "Object-based evaluation limitations",
          "HaELM framework",
          "LLM-based evaluation",
          "Simulated hallucination data",
          "LoRA fine-tuning",
          "Real-world image description",
          "Cost-effective evaluation",
          "Reproducible evaluation",
          "Empirical validation"
        ],
        "paper_type": "the paper type is **technical**.\n\n**reasoning:**\n\n1.  **\"propose\", \"develop\", \"present\", \"algorithm\", \"method\"**: the abstract explicitly states, \"we propose hallucination evaluation based on large language models (haelm), an llm-based hallucination evaluation framework.\" this directly indicates the development and presentation of a new method/system.\n2.  **technical problem, proposed solution**: the introduction and abstract clearly identify a technical problem (\"lvlms are still plagued by the hallucination problem\") and present a specific technical solution (the haelm framework) to address it.\n3.  while the paper also includes an empirical component (\"leveraging the haelm, we evaluate the hallucination in current lvlms. furthermore, we analyze the factors contributing to hallucination...\"), this empirical work is *enabled by* and *demonstrates* the proposed technical framework. the primary contribution described is the creation of the haelm framework itself."
      },
      "file_name": "bb1083425517bdac8d9a6438fcf5032543acb20e.pdf"
    },
    {
      "success": true,
      "doc_id": "524b998e6d1c8496b1d20c16e983b0d5",
      "summary": "Here's a focused summary of the technical paper \\cite{xu2024n76} for a literature review:\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem:** The paper addresses the fundamental question of whether hallucination in Large Language Models (LLMs) can be completely eliminated.\n    *   **Importance & Challenge:** Hallucination, where LLMs generate plausible but factually incorrect or nonsensical information, is a critical challenge impacting the safety and ethics of LLM deployment. Prior research has been largely empirical, focusing on mitigation, but has not provided a formal answer to its ultimate eliminability, which is crucial for understanding the inherent limitations and capabilities of LLMs.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches:** Previous works have primarily focused on empirically identifying sources of hallucination (e.g., data collection, training, inference issues) and proposing mitigation strategies (e.g., factual-centered metrics, retrieval-based methods, prompting for reasoning/verification).\n    *   **Limitations of Previous Solutions:** These empirical studies, while useful, cannot answer the fundamental question of whether hallucination can be *completely eliminated* due to the impossibility of exhaustively testing all inputs. A formal definition and analysis of hallucination were lacking.\n    *   **Positioning:** \\cite{xu2024n76} distinguishes itself by providing the first formal definition of hallucination and a theoretical proof of its inevitability for all computable LLMs. It notes a parallel work by Kalai and Vempala \\cite{kalai2023statistical} which provides a statistical lower bound on hallucination rates for calibrated LLMs, but positions its own results as more general, applicable to all computable LLMs.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method:** The paper formalizes the problem by defining a \"formal world\" where hallucination is precisely defined as inconsistencies between a computable LLM and a computable ground truth function. It then employs results from learning theory, specifically the diagonalization argument (Cantor's diagonalization), to prove the inevitability of hallucination.\n    *   **Novelty:** The core innovation lies in shifting from empirical observation to a rigorous, formal proof. By abstracting LLMs as total computable functions and defining a ground truth function, the paper provides a theoretical upper limit to LLMs' abilities, demonstrating that they cannot learn all computable functions and will thus inevitably hallucinate if used as general problem solvers. This formalization allows for discussions independent of specific model architectures, training algorithms, or data.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms, Methods, or Techniques:**\n        *   A formal definition of hallucination within a \"formal world\" of computable functions (Definition 4).\n        *   A formal procedure for training and deploying an LLM (Procedure 1) that abstracts away implementation details.\n        *   Proof of inevitability of hallucination for any computably enumerable set of LLMs (Theorem 1), showing that all states of such LLMs will hallucinate on some inputs.\n        *   Proof that hallucination will occur on *infinitely many* inputs for any computably enumerable set of LLMs (Theorem 2).\n        *   Generalization of these proofs to show that hallucination is inevitable for *any individual computable LLM* (Theorem 3), both on some inputs and on infinitely many inputs.\n    *   **Theoretical Insights or Analysis:** The application of the diagonalization argument from learning theory to demonstrate a fundamental, inherent limitation of LLMs, regardless of their training or architecture.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted:** The abstract and contributions section state that the paper \"show examples of hallucination-prone problems and validate the claims by empirical study.\" However, the provided text content *ends* before detailing these specific experiments, performance metrics, or comparison results. The paper mentions it will describe \"hallucination-prone tasks\" and empirically validate claims for real-world LLMs constrained by provable time complexity.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions:**\n        *   The primary proof is conducted in a \"formal world\" where LLMs are considered total computable functions and ground truth is a computable function. While argued to apply to the more complex real world, this abstraction is a simplification.\n        *   The definition of hallucination relies on a unique \"correct output\" for any input string, and in some theoretical discussions, assumes an unbounded time limit for LLM computation.\n    *   **Scope of Applicability:** The findings apply to all computable LLMs, including those constrained by polynomial time complexity, which encompasses all currently proposed real-world LLMs.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art:** This paper fundamentally shifts the understanding of LLM hallucination from a solvable engineering problem to an inherent, inevitable limitation. It provides a theoretical foundation for why complete elimination of hallucination is impossible.\n    *   **Potential Impact on Future Research:**\n        *   Informs realistic expectations for LLM capabilities and guides research away from the pursuit of \"hallucination-free\" LLMs towards more effective mitigation, detection, and robust deployment strategies.\n        *   Highlights the need for LLMs to be used as specialized tools rather than general problem solvers without external verification.\n        *   Provides a framework for discussing the mechanisms and efficacies of existing and future hallucination mitigators within a formal context.",
      "intriguing_abstract": "Can Large Language Model (LLM) hallucination truly be eliminated? This fundamental question, critical for safe and ethical AI deployment, has remained unanswered by empirical mitigation efforts. We present the first formal definition of hallucination, rigorously framing it as an inconsistency between a computable LLM and a computable ground truth function. Leveraging a powerful diagonalization argument from learning theory, we provide a groundbreaking theoretical proof demonstrating the *inevitability* of hallucination for *all computable LLMs*. Our findings reveal that no LLM, regardless of architecture or training, can entirely escape generating factually incorrect or nonsensical outputs on infinitely many inputs. This paradigm shift moves beyond empirical observation to establish an inherent, fundamental limitation of LLMs. It necessitates a re-evaluation of research priorities, guiding efforts towards robust detection and mitigation strategies rather than the pursuit of a hallucination-free ideal. This work fundamentally advances our understanding of LLM capabilities and inherent limitations, impacting future AI development and deployment.",
      "keywords": [
        "Hallucination in LLMs",
        "Formal definition of hallucination",
        "Theoretical proof of inevitability",
        "Computable functions",
        "Diagonalization argument",
        "Inherent LLM limitation",
        "Learning theory",
        "LLMs as general problem solvers",
        "Hallucination mitigation strategies",
        "Formal world abstraction",
        "Robust LLM deployment",
        "Cannot be completely eliminated",
        "Safety and ethics of LLMs"
      ],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/5cd671efa2af8456c615c5faf54d1be4950f3819.pdf",
      "citation_key": "xu2024n76",
      "metadata": {
        "title": "Hallucination is Inevitable: An Innate Limitation of Large Language Models",
        "authors": [
          "Ziwei Xu",
          "Sanjay Jain",
          "Mohan Kankanhalli"
        ],
        "published_date": "2024",
        "abstract": "Hallucination has been widely recognized to be a significant drawback for large language models (LLMs). There have been many works that attempt to reduce the extent of hallucination. These efforts have mostly been empirical so far, which cannot answer the fundamental question whether it can be completely eliminated. In this paper, we formalize the problem and show that it is impossible to eliminate hallucination in LLMs. Specifically, we define a formal world where hallucination is defined as inconsistencies between a computable LLM and a computable ground truth function. By employing results from learning theory, we show that LLMs cannot learn all the computable functions and will therefore inevitably hallucinate if used as general problem solvers. Since the formal world is a part of the real world which is much more complicated, hallucinations are also inevitable for real world LLMs. Furthermore, for real world LLMs constrained by provable time complexity, we describe the hallucination-prone tasks and empirically validate our claims. Finally, using the formal world framework, we discuss the possible mechanisms and efficacies of existing hallucination mitigators as well as the practical implications on the safe deployment of LLMs.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/5cd671efa2af8456c615c5faf54d1be4950f3819.pdf",
        "venue": "arXiv.org",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the technical paper \\cite{xu2024n76} for a literature review:\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem:** The paper addresses the fundamental question of whether hallucination in Large Language Models (LLMs) can be completely eliminated.\n    *   **Importance & Challenge:** Hallucination, where LLMs generate plausible but factually incorrect or nonsensical information, is a critical challenge impacting the safety and ethics of LLM deployment. Prior research has been largely empirical, focusing on mitigation, but has not provided a formal answer to its ultimate eliminability, which is crucial for understanding the inherent limitations and capabilities of LLMs.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches:** Previous works have primarily focused on empirically identifying sources of hallucination (e.g., data collection, training, inference issues) and proposing mitigation strategies (e.g., factual-centered metrics, retrieval-based methods, prompting for reasoning/verification).\n    *   **Limitations of Previous Solutions:** These empirical studies, while useful, cannot answer the fundamental question of whether hallucination can be *completely eliminated* due to the impossibility of exhaustively testing all inputs. A formal definition and analysis of hallucination were lacking.\n    *   **Positioning:** \\cite{xu2024n76} distinguishes itself by providing the first formal definition of hallucination and a theoretical proof of its inevitability for all computable LLMs. It notes a parallel work by Kalai and Vempala \\cite{kalai2023statistical} which provides a statistical lower bound on hallucination rates for calibrated LLMs, but positions its own results as more general, applicable to all computable LLMs.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method:** The paper formalizes the problem by defining a \"formal world\" where hallucination is precisely defined as inconsistencies between a computable LLM and a computable ground truth function. It then employs results from learning theory, specifically the diagonalization argument (Cantor's diagonalization), to prove the inevitability of hallucination.\n    *   **Novelty:** The core innovation lies in shifting from empirical observation to a rigorous, formal proof. By abstracting LLMs as total computable functions and defining a ground truth function, the paper provides a theoretical upper limit to LLMs' abilities, demonstrating that they cannot learn all computable functions and will thus inevitably hallucinate if used as general problem solvers. This formalization allows for discussions independent of specific model architectures, training algorithms, or data.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms, Methods, or Techniques:**\n        *   A formal definition of hallucination within a \"formal world\" of computable functions (Definition 4).\n        *   A formal procedure for training and deploying an LLM (Procedure 1) that abstracts away implementation details.\n        *   Proof of inevitability of hallucination for any computably enumerable set of LLMs (Theorem 1), showing that all states of such LLMs will hallucinate on some inputs.\n        *   Proof that hallucination will occur on *infinitely many* inputs for any computably enumerable set of LLMs (Theorem 2).\n        *   Generalization of these proofs to show that hallucination is inevitable for *any individual computable LLM* (Theorem 3), both on some inputs and on infinitely many inputs.\n    *   **Theoretical Insights or Analysis:** The application of the diagonalization argument from learning theory to demonstrate a fundamental, inherent limitation of LLMs, regardless of their training or architecture.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted:** The abstract and contributions section state that the paper \"show examples of hallucination-prone problems and validate the claims by empirical study.\" However, the provided text content *ends* before detailing these specific experiments, performance metrics, or comparison results. The paper mentions it will describe \"hallucination-prone tasks\" and empirically validate claims for real-world LLMs constrained by provable time complexity.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions:**\n        *   The primary proof is conducted in a \"formal world\" where LLMs are considered total computable functions and ground truth is a computable function. While argued to apply to the more complex real world, this abstraction is a simplification.\n        *   The definition of hallucination relies on a unique \"correct output\" for any input string, and in some theoretical discussions, assumes an unbounded time limit for LLM computation.\n    *   **Scope of Applicability:** The findings apply to all computable LLMs, including those constrained by polynomial time complexity, which encompasses all currently proposed real-world LLMs.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art:** This paper fundamentally shifts the understanding of LLM hallucination from a solvable engineering problem to an inherent, inevitable limitation. It provides a theoretical foundation for why complete elimination of hallucination is impossible.\n    *   **Potential Impact on Future Research:**\n        *   Informs realistic expectations for LLM capabilities and guides research away from the pursuit of \"hallucination-free\" LLMs towards more effective mitigation, detection, and robust deployment strategies.\n        *   Highlights the need for LLMs to be used as specialized tools rather than general problem solvers without external verification.\n        *   Provides a framework for discussing the mechanisms and efficacies of existing and future hallucination mitigators within a formal context.",
        "keywords": [
          "Hallucination in LLMs",
          "Formal definition of hallucination",
          "Theoretical proof of inevitability",
          "Computable functions",
          "Diagonalization argument",
          "Inherent LLM limitation",
          "Learning theory",
          "LLMs as general problem solvers",
          "Hallucination mitigation strategies",
          "Formal world abstraction",
          "Robust LLM deployment",
          "Cannot be completely eliminated",
          "Safety and ethics of LLMs"
        ],
        "paper_type": "based on the abstract and introduction, this paper is a **theoretical** paper.\n\nhere's why:\n\n*   **abstract keywords and phrases:**\n    *   \"we **formalize the problem** and **show that it is impossible to eliminate hallucination** in llms.\"\n    *   \"specifically, we **define a formal world** where hallucination is defined as inconsistencies between a computable llm and a computable ground truth function.\"\n    *   \"by employing **results from learning theory**, we show that llms cannot learn all the computable functions and will therefore inevitably hallucinate...\"\n    *   these phrases directly align with the \"theoretical\" criteria: \"mathematical analysis, proofs, formal models\" and \"prove\", \"theorem\", \"analysis\", \"mathematical\", \"formal\".\n\n*   **primary goal:** the core contribution is a fundamental proof of an inherent limitation, rather than proposing a new method (technical), reviewing literature (survey), or presenting experimental results as the main finding (empirical). while it mentions empirical validation, it's to support the *theoretical claims*, not the primary focus of the paper."
      },
      "file_name": "5cd671efa2af8456c615c5faf54d1be4950f3819.pdf"
    },
    {
      "success": true,
      "doc_id": "fc49a8c903bfb5eb35e3ae76559b61ac",
      "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **Research Problem & Motivation**\n    *   The paper addresses the critical problem of \"hallucination\" in large pretrained generative models (e.g., GPT-3), where models generate non-existent or factually incorrect content \\cite{liu2021mo6}.\n    *   This problem is important because it undermines the trustworthiness and potential merits of NLG systems in real-world applications, especially in real-time generation scenarios where post-examination is impossible \\cite{liu2021mo6}.\n    *   It is challenging because existing hallucination detection methods are often reference-based (requiring a ground-truth reference, which is often unavailable for free-form text generation) or operate at a coarse sentence/document level, failing to provide the fine-grained signals needed to prevent fallacious content in real-time \\cite{liu2021mo6}.\n\n*   **Related Work & Positioning**\n    *   Existing approaches typically detect hallucination by comparing generated text against a provided reference, common in tasks like abstractive summarization, machine translation, and data-to-text generation \\cite{liu2021mo6}.\n    *   Limitations of previous solutions include:\n        *   **Reference dependency**: Ground-truth references are often not readily available for many free-form text generation applications (e.g., chatbots, auto-completion), making reference-based methods inapplicable \\cite{liu2021mo6}.\n        *   **Granularity**: Sentence- or document-level detection lacks the high-resolution signals necessary to pinpoint the exact locus of hallucination, making it insufficient for real-time prevention or fine-grained correction \\cite{liu2021mo6}.\n\n*   **Technical Approach & Innovation**\n    *   The core technical method is the proposal of a novel **token-level, reference-free hallucination detection task** \\cite{liu2021mo6}.\n    *   To support this task, the authors introduce an associated annotated dataset named **HADES** (HAllucination DEtection data Set) \\cite{liu2021mo6}.\n    *   The dataset creation involves:\n        *   **Contextual Perturbation**: Raw text segments from English Wikipedia are perturbed using a BERT-based pipeline (MASK, REPLACE, RANK) to simulate machine-generated text. This process ensures fluency, syntactic correctness, and lexical diversity while introducing potential inconsistencies \\cite{liu2021mo6}.\n            *   **MASK**: Randomly masks word spans (excluding stop words/punctuation, treating named entities as units).\n            *   **REPLACE**: A pretrained BERT-base model predicts replacements for masked spans, using top-k (k=10) sampling for diversity and coherence.\n            *   **RANK**: GPT-2 (117M) is used to rank 20 perturbed candidates by perplexity, selecting the most fluent one \\cite{liu2021mo6}.\n        *   **Iterative Model-in-the-loop Annotation**: To mitigate label imbalance and reduce annotation cost, a strategy conceptually related to active learning is employed. A detection model (initiated with BERT) is iteratively retrained on previously annotated data to select less trivial and more balanced instances for subsequent human annotation rounds \\cite{liu2021mo6}. This involves filtering out obvious cases (e.g., paraphrases, simple date/name changes) and subsampling predicted \"hallucination\" cases to balance the dataset \\cite{liu2021mo6}.\n    *   The approach is novel due to its **reference-free nature** and **token-level granularity**, which allows for more flexible application in diverse generation scenarios and provides finer-grained signals for real-time hallucination mitigation \\cite{liu2021mo6}.\n\n*   **Key Technical Contributions**\n    *   **Novel Task Definition**: Introduction of the first reference-free, token-level hallucination detection task for free-form text generation \\cite{liu2021mo6}.\n    *   **HADES Dataset**: Creation of a unique, large-scale (âˆ¼11k instances) annotated dataset specifically designed for this task, extracted from English Wikipedia \\cite{liu2021mo6}.\n    *   **Innovative Data Collection Strategy**: Development and application of an iterative model-in-the-loop annotation approach to efficiently create a balanced and non-trivial dataset, addressing challenges of cost and label imbalance \\cite{liu2021mo6}.\n    *   **Comprehensive Data Analysis**: Detailed statistical analysis of the HADES dataset, characterizing different types of hallucinations and their distribution across Part-of-Speech and Named Entity Recognition tags \\cite{liu2021mo6}.\n\n*   **Experimental Validation**\n    *   The paper describes the creation of the HADES dataset, which serves as the primary validation artifact for the proposed task \\cite{liu2021mo6}.\n    *   **Dataset Statistics**: HADES comprises 10,954 instances (8754 train, 1000 val, 1200 test) with a balanced distribution of \"hallucination\" (54.5%) and \"not hallucination\" (45.5%) cases \\cite{liu2021mo6}.\n    *   **Annotation Quality**: High inter-annotator agreement (Krippendorfâ€™s alpha of 0.87) demonstrates the reliability of the human labels \\cite{liu2021mo6}.\n    *   **Hallucination Characterization**: The paper provides a qualitative analysis of hallucination types observed in HADES, including domain-specific knowledge errors, commonsense knowledge conflicts, incoherence, unrelated content, and conflicts with preceding/succeeding context \\cite{liu2021mo6}.\n    *   **Linguistic Feature Analysis**: Analysis of POS and NER tags reveals that verbs/verbal phrases are less prone to hallucination, while adverbs, adjectives, and proper nouns (acronyms) are more frequently associated with hallucinated content \\cite{liu2021mo6}.\n    *   **Baseline Models**: The authors state they created multiple baseline models, including feature-based and pretrained models, as a first step towards addressing the proposed task, though specific performance results for these baselines are not detailed in the provided text \\cite{liu2021mo6}.\n\n*   **Limitations & Scope**\n    *   **Data Source**: The HADES dataset is primarily derived from Wikipedia, which is a high-quality, formal text source. This might limit its direct applicability to more informal or diverse text generation domains \\cite{liu2021mo6}.\n    *   **Perturbation Method**: The contextual perturbation method using BERT, while effective, might not fully capture the diverse error modes of all large generative models, as it focuses on single-token or small span replacements \\cite{liu2021mo6}.\n    *   **Definition of Hallucination**: The paper's definition of \"hallucination\" is tied to factualness and semantic coherence relative to an original text, which might not encompass all forms of \"non-existent or incorrect content\" that a free-form generator could produce without a clear reference \\cite{liu2021mo6}.\n    *   **Scope of Applicability**: While the task is designed for free-form text generation, the current dataset is based on perturbed existing text rather than purely generative outputs, which could influence the types of hallucinations observed \\cite{liu2021mo6}.\n\n*   **Technical Significance**\n    *   This work significantly advances the technical state-of-the-art by defining and providing resources for a **reference-free, token-level hallucination detection task**, moving beyond the limitations of reference-dependent and coarse-grained methods \\cite{liu2021mo6}.\n    *   The HADES dataset and its innovative creation methodology provide a crucial benchmark for developing and evaluating models capable of fine-grained hallucination detection \\cite{liu2021mo6}.\n    *   The proposed task has the potential to enable **real-time hallucination mitigation** in NLG systems by providing signals at the decoding stage, allowing for proactive prevention rather than post-hoc correction \\cite{liu2021mo6}.\n    *   It opens new avenues for research into understanding the linguistic and factual characteristics of hallucinations at a granular level, fostering the development of more trustworthy and reliable generative AI \\cite{liu2021mo6}.",
      "intriguing_abstract": "The pervasive issue of 'hallucination' in large pretrained generative models severely compromises the trustworthiness and real-world applicability of Natural Language Generation (NLG) systems. Current detection methods, often reference-dependent and coarse-grained, fall short in providing the fine-grained, real-time signals crucial for preventing factually incorrect or non-existent content. This paper introduces a groundbreaking solution: the first **token-level, reference-free hallucination detection task**.\n\nTo facilitate this novel task, we present **HADES** (HAllucination DEtection data Set), a meticulously annotated dataset of nearly 11,000 instances. HADES is constructed through an innovative process combining **contextual perturbation** using BERT and GPT-2 to simulate diverse machine-generated inconsistencies, with an **iterative model-in-the-loop annotation** strategy to ensure data balance and quality. This unique methodology, conceptually related to active learning, efficiently curates a dataset with high inter-annotator agreement. Our work offers a critical benchmark and a pathway towards enabling **real-time hallucination mitigation**, fundamentally enhancing the reliability and trustworthiness of generative AI. This advancement is pivotal for developing robust NLG applications that can operate confidently without human post-examination.",
      "keywords": [
        "Hallucination detection",
        "large pretrained generative models",
        "reference-free hallucination detection",
        "token-level granularity",
        "HADES dataset",
        "contextual perturbation",
        "iterative model-in-the-loop annotation",
        "NLG systems trustworthiness",
        "real-time hallucination mitigation",
        "free-form text generation",
        "factual incorrectness",
        "linguistic feature analysis"
      ],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/c6bf48f25e0a65d64d658b47326de5922ea7dd44.pdf",
      "citation_key": "liu2021mo6",
      "metadata": {
        "title": "A Token-level Reference-free Hallucination Detection Benchmark for Free-form Text Generation",
        "authors": [
          "Tianyu Liu",
          "Yizhe Zhang",
          "C. Brockett",
          "Yi Mao",
          "Zhifang Sui",
          "Weizhu Chen",
          "W. Dolan"
        ],
        "published_date": "2021",
        "abstract": "Large pretrained generative models like GPT-3 often suffer from hallucinating non-existent or incorrect content, which undermines their potential merits in real applications. Existing work usually attempts to detect these hallucinations based on a corresponding oracle reference at a sentence or document level. However ground-truth references may not be readily available for many free-form text generation applications, and sentence- or document-level detection may fail to provide the fine-grained signals that would prevent fallacious content in real time. As a first step to addressing these issues, we propose a novel token-level, reference-free hallucination detection task and an associated annotated dataset named HaDeS (HAllucination DEtection dataSet). To create this dataset, we first perturb a large number of text segments extracted from English language Wikipedia, and then verify these with crowd-sourced annotations. To mitigate label imbalance during annotation, we utilize an iterative model-in-loop strategy. We conduct comprehensive data analyses and create multiple baseline models.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/c6bf48f25e0a65d64d658b47326de5922ea7dd44.pdf",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **Research Problem & Motivation**\n    *   The paper addresses the critical problem of \"hallucination\" in large pretrained generative models (e.g., GPT-3), where models generate non-existent or factually incorrect content \\cite{liu2021mo6}.\n    *   This problem is important because it undermines the trustworthiness and potential merits of NLG systems in real-world applications, especially in real-time generation scenarios where post-examination is impossible \\cite{liu2021mo6}.\n    *   It is challenging because existing hallucination detection methods are often reference-based (requiring a ground-truth reference, which is often unavailable for free-form text generation) or operate at a coarse sentence/document level, failing to provide the fine-grained signals needed to prevent fallacious content in real-time \\cite{liu2021mo6}.\n\n*   **Related Work & Positioning**\n    *   Existing approaches typically detect hallucination by comparing generated text against a provided reference, common in tasks like abstractive summarization, machine translation, and data-to-text generation \\cite{liu2021mo6}.\n    *   Limitations of previous solutions include:\n        *   **Reference dependency**: Ground-truth references are often not readily available for many free-form text generation applications (e.g., chatbots, auto-completion), making reference-based methods inapplicable \\cite{liu2021mo6}.\n        *   **Granularity**: Sentence- or document-level detection lacks the high-resolution signals necessary to pinpoint the exact locus of hallucination, making it insufficient for real-time prevention or fine-grained correction \\cite{liu2021mo6}.\n\n*   **Technical Approach & Innovation**\n    *   The core technical method is the proposal of a novel **token-level, reference-free hallucination detection task** \\cite{liu2021mo6}.\n    *   To support this task, the authors introduce an associated annotated dataset named **HADES** (HAllucination DEtection data Set) \\cite{liu2021mo6}.\n    *   The dataset creation involves:\n        *   **Contextual Perturbation**: Raw text segments from English Wikipedia are perturbed using a BERT-based pipeline (MASK, REPLACE, RANK) to simulate machine-generated text. This process ensures fluency, syntactic correctness, and lexical diversity while introducing potential inconsistencies \\cite{liu2021mo6}.\n            *   **MASK**: Randomly masks word spans (excluding stop words/punctuation, treating named entities as units).\n            *   **REPLACE**: A pretrained BERT-base model predicts replacements for masked spans, using top-k (k=10) sampling for diversity and coherence.\n            *   **RANK**: GPT-2 (117M) is used to rank 20 perturbed candidates by perplexity, selecting the most fluent one \\cite{liu2021mo6}.\n        *   **Iterative Model-in-the-loop Annotation**: To mitigate label imbalance and reduce annotation cost, a strategy conceptually related to active learning is employed. A detection model (initiated with BERT) is iteratively retrained on previously annotated data to select less trivial and more balanced instances for subsequent human annotation rounds \\cite{liu2021mo6}. This involves filtering out obvious cases (e.g., paraphrases, simple date/name changes) and subsampling predicted \"hallucination\" cases to balance the dataset \\cite{liu2021mo6}.\n    *   The approach is novel due to its **reference-free nature** and **token-level granularity**, which allows for more flexible application in diverse generation scenarios and provides finer-grained signals for real-time hallucination mitigation \\cite{liu2021mo6}.\n\n*   **Key Technical Contributions**\n    *   **Novel Task Definition**: Introduction of the first reference-free, token-level hallucination detection task for free-form text generation \\cite{liu2021mo6}.\n    *   **HADES Dataset**: Creation of a unique, large-scale (âˆ¼11k instances) annotated dataset specifically designed for this task, extracted from English Wikipedia \\cite{liu2021mo6}.\n    *   **Innovative Data Collection Strategy**: Development and application of an iterative model-in-the-loop annotation approach to efficiently create a balanced and non-trivial dataset, addressing challenges of cost and label imbalance \\cite{liu2021mo6}.\n    *   **Comprehensive Data Analysis**: Detailed statistical analysis of the HADES dataset, characterizing different types of hallucinations and their distribution across Part-of-Speech and Named Entity Recognition tags \\cite{liu2021mo6}.\n\n*   **Experimental Validation**\n    *   The paper describes the creation of the HADES dataset, which serves as the primary validation artifact for the proposed task \\cite{liu2021mo6}.\n    *   **Dataset Statistics**: HADES comprises 10,954 instances (8754 train, 1000 val, 1200 test) with a balanced distribution of \"hallucination\" (54.5%) and \"not hallucination\" (45.5%) cases \\cite{liu2021mo6}.\n    *   **Annotation Quality**: High inter-annotator agreement (Krippendorfâ€™s alpha of 0.87) demonstrates the reliability of the human labels \\cite{liu2021mo6}.\n    *   **Hallucination Characterization**: The paper provides a qualitative analysis of hallucination types observed in HADES, including domain-specific knowledge errors, commonsense knowledge conflicts, incoherence, unrelated content, and conflicts with preceding/succeeding context \\cite{liu2021mo6}.\n    *   **Linguistic Feature Analysis**: Analysis of POS and NER tags reveals that verbs/verbal phrases are less prone to hallucination, while adverbs, adjectives, and proper nouns (acronyms) are more frequently associated with hallucinated content \\cite{liu2021mo6}.\n    *   **Baseline Models**: The authors state they created multiple baseline models, including feature-based and pretrained models, as a first step towards addressing the proposed task, though specific performance results for these baselines are not detailed in the provided text \\cite{liu2021mo6}.\n\n*   **Limitations & Scope**\n    *   **Data Source**: The HADES dataset is primarily derived from Wikipedia, which is a high-quality, formal text source. This might limit its direct applicability to more informal or diverse text generation domains \\cite{liu2021mo6}.\n    *   **Perturbation Method**: The contextual perturbation method using BERT, while effective, might not fully capture the diverse error modes of all large generative models, as it focuses on single-token or small span replacements \\cite{liu2021mo6}.\n    *   **Definition of Hallucination**: The paper's definition of \"hallucination\" is tied to factualness and semantic coherence relative to an original text, which might not encompass all forms of \"non-existent or incorrect content\" that a free-form generator could produce without a clear reference \\cite{liu2021mo6}.\n    *   **Scope of Applicability**: While the task is designed for free-form text generation, the current dataset is based on perturbed existing text rather than purely generative outputs, which could influence the types of hallucinations observed \\cite{liu2021mo6}.\n\n*   **Technical Significance**\n    *   This work significantly advances the technical state-of-the-art by defining and providing resources for a **reference-free, token-level hallucination detection task**, moving beyond the limitations of reference-dependent and coarse-grained methods \\cite{liu2021mo6}.\n    *   The HADES dataset and its innovative creation methodology provide a crucial benchmark for developing and evaluating models capable of fine-grained hallucination detection \\cite{liu2021mo6}.\n    *   The proposed task has the potential to enable **real-time hallucination mitigation** in NLG systems by providing signals at the decoding stage, allowing for proactive prevention rather than post-hoc correction \\cite{liu2021mo6}.\n    *   It opens new avenues for research into understanding the linguistic and factual characteristics of hallucinations at a granular level, fostering the development of more trustworthy and reliable generative AI \\cite{liu2021mo6}.",
        "keywords": [
          "Hallucination detection",
          "large pretrained generative models",
          "reference-free hallucination detection",
          "token-level granularity",
          "HADES dataset",
          "contextual perturbation",
          "iterative model-in-the-loop annotation",
          "NLG systems trustworthiness",
          "real-time hallucination mitigation",
          "free-form text generation",
          "factual incorrectness",
          "linguistic feature analysis"
        ],
        "paper_type": "the paper should be classified as **technical**.\n\nhere's why:\n\n*   **proposes new methods/systems:** the abstract explicitly states, \"we **propose a novel token-level, reference-free hallucination detection task and an associated annotated dataset named hades**\". it also mentions creating \"multiple baseline models.\" this directly aligns with the \"technical\" criterion of presenting new methods, algorithms, or systems.\n*   **addresses a technical problem with a proposed solution:** the introduction details the problem of hallucination in large generative models and the limitations of existing reference-based detection. the abstract then presents the paper's solution: a new task and dataset to address these issues.\n*   **methodology for creation:** the abstract describes the methodology for creating the hades dataset (\"perturb a large number of text segments,\" \"verify with crowd-sourced annotations,\" \"iterative model-in-loop strategy\"), which is a technical process.\n\nwhile the paper also involves data (\"annotated dataset,\" \"comprehensive data analyses\") and could have empirical elements, its core contribution is the *proposal and creation* of a new task, dataset, and baselines, which are fundamental technical contributions to the field. it's not primarily a review, theoretical analysis, or a detailed study of an existing application."
      },
      "file_name": "c6bf48f25e0a65d64d658b47326de5922ea7dd44.pdf"
    },
    {
      "success": true,
      "doc_id": "4dabc72b65aead7e05177aa280ab5b71",
      "summary": "Here is a focused summary of the technical paper for literature review:\n\n*   **Research Problem & Motivation**\n    *   Generative Large Language Models (LLMs) frequently \"hallucinate\" facts, producing non-factual statements that undermine trust in their outputs \\cite{manakul20236ex}.\n    *   This problem is critical because LLMs are widely used in applications like report drafting and virtual assistants, where factual accuracy is paramount.\n    *   Existing hallucination detection methods are limited:\n        *   Some require access to the LLM's internal output probability distributions (e.g., token probabilities or entropy), which are often unavailable for black-box models accessed via APIs (e.g., ChatGPT).\n        *   Others rely on external databases for fact verification, which can be complex, costly, and only assess facts relative to the database's knowledge, not general LLM hallucinations.\n\n*   **Related Work & Positioning**\n    *   Previous work on hallucination detection includes intrinsic uncertainty metrics (e.g., token probability, entropy) and fact-verification approaches using external databases \\cite{manakul20236ex}.\n    *   Uncertainty metrics (e.g., Yuan et al., 2021; Fu et al., 2023) are \"grey-box\" methods requiring access to token-level probabilities, which is a significant limitation for many commercial LLMs.\n    *   Fact-verification methods (e.g., Thorne et al., 2018; Guo et al., 2022) are \"resource-intensive\" as they depend on external knowledge bases.\n    *   Other approaches, like using hidden representations for truthfulness prediction (Azaria and Mitchell, 2023) or self-evaluation (Kadavath et al., 2022), are either white-box or require specific prompting strategies.\n    *   SelfCheckGPT is positioned as the first zero-resource, black-box hallucination detection solution for general LLM responses, addressing the limitations of prior methods by not requiring internal model access or external databases.\n\n*   **Technical Approach & Innovation**\n    *   The core idea of SelfCheckGPT is that if an LLM genuinely \"knows\" a concept, stochastically sampled responses to the same prompt will be consistent and contain similar facts \\cite{manakul20236ex}. Conversely, for hallucinated facts, sampled responses are likely to diverge and contradict one another.\n    *   It operates by generating multiple stochastic samples (N samples) from the black-box LLM using the same query as the original response to be evaluated.\n    *   The method then measures the informational consistency between the original response's sentences and these generated samples.\n    *   Five variants for measuring consistency are proposed:\n        *   **BERTScore**: Compares the i-th sentence of the original response with the most similar sentence from each sampled response using BERTScore. A lower score indicates higher inconsistency.\n        *   **Question Answering (QA)**: Generates multiple-choice questions from the original response's sentences and uses an independent QA system to answer them based on the original response and each sample. Inconsistency is measured by how often answers based on samples mismatch the answer based on the original response.\n        *   **n-gram**: Trains a simple n-gram language model on the collected samples (and the original response) to approximate the LLM's token probabilities. Hallucinations are indicated by lower (more negative) log-probabilities.\n        *   **Natural Language Inference (NLI)**: Uses an NLI model (e.g., DeBERTa-v3-large fine-tuned on MNLI) to determine if a sampled passage contradicts the sentence being assessed. The average contradiction probability across samples serves as the score.\n        *   **LLM Prompting**: Queries another LLM (e.g., GPT-3) to directly assess whether a given sentence is supported by a sampled response (context) using a simple \"Yes/No\" prompt. The proportion of \"No\" answers indicates inconsistency.\n\n*   **Key Technical Contributions**\n    *   **Novel Sampling-Based Paradigm**: Introduces a novel sampling-based approach for hallucination detection that leverages the inherent stochasticity of LLMs.\n    *   **Zero-Resource & Black-Box Applicability**: Develops a method that works without external databases and without requiring access to the LLM's internal states or token probabilities, making it applicable to proprietary black-box models.\n    *   **Multiple Consistency Measures**: Proposes and evaluates five distinct technical methods (BERTScore, QA, n-gram, NLI, LLM Prompting) for quantifying informational consistency, offering flexibility and robustness.\n    *   **Empirical Demonstration**: Provides a strong first baseline for black-box hallucination detection, demonstrating its effectiveness even against grey-box methods.\n\n*   **Experimental Validation**\n    *   **Dataset Creation**: A custom dataset was created by using GPT-3 (text-davinci-003) to generate synthetic Wikipedia articles based on concepts from the WikiBio dataset.\n    *   **Manual Annotation**: 1908 generated sentences were manually annotated for factuality into three categories: Major Inaccurate (1), Minor Inaccurate (0.5), and Accurate (0). Inter-annotator agreement (Cohen's Îº) was moderate to substantial (0.595 for 3-class, 0.748 for 2-class).\n    *   **Metrics**:\n        *   **Sentence-level hallucination detection**: Evaluated using Area Under the Precision-Recall Curve (AUC-PR).\n        *   **Passage-level factuality assessment**: Evaluated using correlation scores (e.g., Pearson, Spearman) between predicted and annotated factuality scores.\n    *   **Key Results**:\n        *   SelfCheckGPT variants, particularly SelfCheckGPT-Prompt and SelfCheckGPT-NLI, achieved considerably higher AUC-PR scores for sentence-level hallucination detection compared to grey-box methods (e.g., token probability, entropy) and a proxy LLM approach.\n        *   SelfCheckGPT also showed higher correlation scores for passage-level factuality assessment.\n        *   The approach demonstrated the ability to detect both non-factual and factual sentences and rank passages by factuality.\n\n*   **Limitations & Scope**\n    *   **Sampling Quality**: The effectiveness relies on the quality and diversity of the stochastically generated samples. A limited number of samples (N=20 in experiments) might not fully capture the LLM's knowledge distribution.\n    *   **LLM Capability for Prompting**: The LLM Prompting variant requires a sufficiently capable LLM to perform consistency assessment effectively; less capable models (e.g., GPT-3 text-curie-001, LLaMA) failed in initial investigations.\n    *   **Synthetic Dataset**: The evaluation was conducted on a synthetically generated dataset (GPT-3 on WikiBio), which might not fully represent the diversity and complexity of real-world LLM outputs across all tasks.\n    *   **Computational Cost**: Generating multiple samples and performing consistency checks (especially with QA or LLM prompting) can incur significant computational and API costs.\n\n*   **Technical Significance**\n    *   SelfCheckGPT significantly advances the technical state-of-the-art in LLM hallucination detection by providing a practical, zero-resource, and black-box solution.\n    *   It offers a robust method for assessing the trustworthiness of LLM outputs without requiring privileged access to model internals or reliance on external knowledge bases.\n    *   This work establishes a strong baseline for future research in black-box factuality assessment and opens avenues for developing more sophisticated consistency-based detection mechanisms.\n    *   Its potential impact includes improving the reliability of LLM applications, fostering greater user trust, and enabling broader deployment of LLMs in sensitive domains.",
      "intriguing_abstract": "Large Language Models (LLMs) are transforming countless applications, yet their pervasive tendency to \"hallucinate\" facts critically undermines user trust and limits deployment in sensitive domains. Current hallucination detection methods are often impractical, demanding access to internal model probabilities or relying on costly external knowledge bases, making them unsuitable for proprietary black-box LLMs.\n\nWe introduce **SelfCheckGPT**, a novel, zero-resource, and black-box solution that fundamentally redefines LLM factuality assessment. Our core insight is that an LLM's genuine knowledge yields consistent responses across multiple stochastic samples to the same prompt, whereas hallucinations manifest as divergent and contradictory outputs. SelfCheckGPT leverages this by generating multiple samples from the target LLM and quantifying their informational consistency using diverse measures, including BERTScore, Natural Language Inference (NLI), and LLM Prompting. Empirical evaluations demonstrate SelfCheckGPT's superior performance in sentence-level hallucination detection and passage-level factuality assessment compared to existing grey-box and proxy methods. This work provides a robust, practical framework to enhance the trustworthiness of LLM-generated content, paving the way for more reliable and impactful LLM applications.",
      "keywords": [
        "Generative Large Language Models",
        "LLM hallucination detection",
        "black-box LLMs",
        "zero-resource",
        "sampling-based paradigm",
        "informational consistency",
        "BERTScore",
        "Natural Language Inference (NLI)",
        "LLM Prompting",
        "sentence-level hallucination detection",
        "passage-level factuality assessment",
        "trustworthiness of LLM outputs",
        "computational cost"
      ],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/7c1707db9aafd209aa93db3251e7ebd593d55876.pdf",
      "citation_key": "manakul20236ex",
      "metadata": {
        "title": "SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models",
        "authors": [
          "Potsawee Manakul",
          "Adian Liusie",
          "M. Gales"
        ],
        "published_date": "2023",
        "abstract": "Generative Large Language Models (LLMs) such as GPT-3 are capable of generating highly fluent responses to a wide variety of user prompts. However, LLMs are known to hallucinate facts and make non-factual statements which can undermine trust in their output. Existing fact-checking approaches either require access to the output probability distribution (which may not be available for systems such as ChatGPT) or external databases that are interfaced via separate, often complex, modules. In this work, we propose\"SelfCheckGPT\", a simple sampling-based approach that can be used to fact-check the responses of black-box models in a zero-resource fashion, i.e. without an external database. SelfCheckGPT leverages the simple idea that if an LLM has knowledge of a given concept, sampled responses are likely to be similar and contain consistent facts. However, for hallucinated facts, stochastically sampled responses are likely to diverge and contradict one another. We investigate this approach by using GPT-3 to generate passages about individuals from the WikiBio dataset, and manually annotate the factuality of the generated passages. We demonstrate that SelfCheckGPT can: i) detect non-factual and factual sentences; and ii) rank passages in terms of factuality. We compare our approach to several baselines and show that our approach has considerably higher AUC-PR scores in sentence-level hallucination detection and higher correlation scores in passage-level factuality assessment compared to grey-box methods.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/7c1707db9aafd209aa93db3251e7ebd593d55876.pdf",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "citationCount": 0,
        "score": 0,
        "summary": "Here is a focused summary of the technical paper for literature review:\n\n*   **Research Problem & Motivation**\n    *   Generative Large Language Models (LLMs) frequently \"hallucinate\" facts, producing non-factual statements that undermine trust in their outputs \\cite{manakul20236ex}.\n    *   This problem is critical because LLMs are widely used in applications like report drafting and virtual assistants, where factual accuracy is paramount.\n    *   Existing hallucination detection methods are limited:\n        *   Some require access to the LLM's internal output probability distributions (e.g., token probabilities or entropy), which are often unavailable for black-box models accessed via APIs (e.g., ChatGPT).\n        *   Others rely on external databases for fact verification, which can be complex, costly, and only assess facts relative to the database's knowledge, not general LLM hallucinations.\n\n*   **Related Work & Positioning**\n    *   Previous work on hallucination detection includes intrinsic uncertainty metrics (e.g., token probability, entropy) and fact-verification approaches using external databases \\cite{manakul20236ex}.\n    *   Uncertainty metrics (e.g., Yuan et al., 2021; Fu et al., 2023) are \"grey-box\" methods requiring access to token-level probabilities, which is a significant limitation for many commercial LLMs.\n    *   Fact-verification methods (e.g., Thorne et al., 2018; Guo et al., 2022) are \"resource-intensive\" as they depend on external knowledge bases.\n    *   Other approaches, like using hidden representations for truthfulness prediction (Azaria and Mitchell, 2023) or self-evaluation (Kadavath et al., 2022), are either white-box or require specific prompting strategies.\n    *   SelfCheckGPT is positioned as the first zero-resource, black-box hallucination detection solution for general LLM responses, addressing the limitations of prior methods by not requiring internal model access or external databases.\n\n*   **Technical Approach & Innovation**\n    *   The core idea of SelfCheckGPT is that if an LLM genuinely \"knows\" a concept, stochastically sampled responses to the same prompt will be consistent and contain similar facts \\cite{manakul20236ex}. Conversely, for hallucinated facts, sampled responses are likely to diverge and contradict one another.\n    *   It operates by generating multiple stochastic samples (N samples) from the black-box LLM using the same query as the original response to be evaluated.\n    *   The method then measures the informational consistency between the original response's sentences and these generated samples.\n    *   Five variants for measuring consistency are proposed:\n        *   **BERTScore**: Compares the i-th sentence of the original response with the most similar sentence from each sampled response using BERTScore. A lower score indicates higher inconsistency.\n        *   **Question Answering (QA)**: Generates multiple-choice questions from the original response's sentences and uses an independent QA system to answer them based on the original response and each sample. Inconsistency is measured by how often answers based on samples mismatch the answer based on the original response.\n        *   **n-gram**: Trains a simple n-gram language model on the collected samples (and the original response) to approximate the LLM's token probabilities. Hallucinations are indicated by lower (more negative) log-probabilities.\n        *   **Natural Language Inference (NLI)**: Uses an NLI model (e.g., DeBERTa-v3-large fine-tuned on MNLI) to determine if a sampled passage contradicts the sentence being assessed. The average contradiction probability across samples serves as the score.\n        *   **LLM Prompting**: Queries another LLM (e.g., GPT-3) to directly assess whether a given sentence is supported by a sampled response (context) using a simple \"Yes/No\" prompt. The proportion of \"No\" answers indicates inconsistency.\n\n*   **Key Technical Contributions**\n    *   **Novel Sampling-Based Paradigm**: Introduces a novel sampling-based approach for hallucination detection that leverages the inherent stochasticity of LLMs.\n    *   **Zero-Resource & Black-Box Applicability**: Develops a method that works without external databases and without requiring access to the LLM's internal states or token probabilities, making it applicable to proprietary black-box models.\n    *   **Multiple Consistency Measures**: Proposes and evaluates five distinct technical methods (BERTScore, QA, n-gram, NLI, LLM Prompting) for quantifying informational consistency, offering flexibility and robustness.\n    *   **Empirical Demonstration**: Provides a strong first baseline for black-box hallucination detection, demonstrating its effectiveness even against grey-box methods.\n\n*   **Experimental Validation**\n    *   **Dataset Creation**: A custom dataset was created by using GPT-3 (text-davinci-003) to generate synthetic Wikipedia articles based on concepts from the WikiBio dataset.\n    *   **Manual Annotation**: 1908 generated sentences were manually annotated for factuality into three categories: Major Inaccurate (1), Minor Inaccurate (0.5), and Accurate (0). Inter-annotator agreement (Cohen's Îº) was moderate to substantial (0.595 for 3-class, 0.748 for 2-class).\n    *   **Metrics**:\n        *   **Sentence-level hallucination detection**: Evaluated using Area Under the Precision-Recall Curve (AUC-PR).\n        *   **Passage-level factuality assessment**: Evaluated using correlation scores (e.g., Pearson, Spearman) between predicted and annotated factuality scores.\n    *   **Key Results**:\n        *   SelfCheckGPT variants, particularly SelfCheckGPT-Prompt and SelfCheckGPT-NLI, achieved considerably higher AUC-PR scores for sentence-level hallucination detection compared to grey-box methods (e.g., token probability, entropy) and a proxy LLM approach.\n        *   SelfCheckGPT also showed higher correlation scores for passage-level factuality assessment.\n        *   The approach demonstrated the ability to detect both non-factual and factual sentences and rank passages by factuality.\n\n*   **Limitations & Scope**\n    *   **Sampling Quality**: The effectiveness relies on the quality and diversity of the stochastically generated samples. A limited number of samples (N=20 in experiments) might not fully capture the LLM's knowledge distribution.\n    *   **LLM Capability for Prompting**: The LLM Prompting variant requires a sufficiently capable LLM to perform consistency assessment effectively; less capable models (e.g., GPT-3 text-curie-001, LLaMA) failed in initial investigations.\n    *   **Synthetic Dataset**: The evaluation was conducted on a synthetically generated dataset (GPT-3 on WikiBio), which might not fully represent the diversity and complexity of real-world LLM outputs across all tasks.\n    *   **Computational Cost**: Generating multiple samples and performing consistency checks (especially with QA or LLM prompting) can incur significant computational and API costs.\n\n*   **Technical Significance**\n    *   SelfCheckGPT significantly advances the technical state-of-the-art in LLM hallucination detection by providing a practical, zero-resource, and black-box solution.\n    *   It offers a robust method for assessing the trustworthiness of LLM outputs without requiring privileged access to model internals or reliance on external knowledge bases.\n    *   This work establishes a strong baseline for future research in black-box factuality assessment and opens avenues for developing more sophisticated consistency-based detection mechanisms.\n    *   Its potential impact includes improving the reliability of LLM applications, fostering greater user trust, and enabling broader deployment of LLMs in sensitive domains.",
        "keywords": [
          "Generative Large Language Models",
          "LLM hallucination detection",
          "black-box LLMs",
          "zero-resource",
          "sampling-based paradigm",
          "informational consistency",
          "BERTScore",
          "Natural Language Inference (NLI)",
          "LLM Prompting",
          "sentence-level hallucination detection",
          "passage-level factuality assessment",
          "trustworthiness of LLM outputs",
          "computational cost"
        ],
        "paper_type": "the paper should be classified as **technical**.\n\nhere's why:\n\n*   **abstract:** explicitly states \"we **propose** 'selfcheckgpt', a simple sampling-based **approach**\". it then describes the mechanism of this approach (\"leverages the simple idea that if an llm has knowledge...\"). it also details the experimental validation of this new approach (\"we investigate this approach by using gpt-3...\", \"we **demonstrate** that selfcheckgpt can...\", \"we **compare our approach** to several baselines and show that our approach has considerably higher auc-pr scores...\").\n*   **introduction:** sets up the technical problem (llm hallucination) and immediately presents figure 1, which is a diagram illustrating the proposed \"selfcheckgpt\" method.\n*   **keywords from criteria:** the abstract uses \"propose\", \"approach\", \"method\" (implicitly, by describing the approach). the introduction discusses a \"technical problem\" (hallucination) and a \"proposed solution\" (selfcheckgpt).\n\nwhile the paper also contains strong **empirical** elements (experiments, data annotation, comparison to baselines, auc-pr scores), its primary contribution is the *development and proposal of a new method* (selfcheckgpt), which is then empirically validated. papers that introduce novel methods, algorithms, or systems and then evaluate them are typically classified as technical."
      },
      "file_name": "7c1707db9aafd209aa93db3251e7ebd593d55876.pdf"
    },
    {
      "success": true,
      "doc_id": "266e9d2d75f8c8b898a61331e1848fbd",
      "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **Research Problem & Motivation**\n    *   Large Vision-Language Models (LVLMs) are susceptible to multimodal hallucinations, where their generated responses contradict visual information \\cite{zhong2024mfi}.\n    *   A critical, yet underexplored, problem is whether these generated hallucinations can \"snowball,\" influencing an LVLM's subsequent generations and leading to further incorrect responses, even when ground visual information is available. This phenomenon, termed \"Multimodal Hallucination Snowballing,\" is important because it undermines the reliability of LVLMs in interactive, conversational settings.\n\n*   **Related Work & Positioning**\n    *   Previous research has identified similar \"snowballing\" effects in Large Language Models (LLMs), where models over-commit to early mistakes, leading to accumulated errors in conversational contexts \\cite{zhong2024mfi}.\n    *   However, this paper highlights that the extent to which *multimodal* hallucinations accumulate and mislead LVLMs into generating false claims had not been systematically investigated prior to this work \\cite{zhong2024mfi}. This work positions itself as the first to conduct such an investigation.\n\n*   **Technical Approach & Innovation**\n    *   **MMHalSnowball Framework**: \\cite{zhong2024mfi} proposes a novel framework to systematically evaluate multimodal hallucination snowballing. It constructs curated hallucinatory visual conversations where LVLMs are first prompted to describe an image, and if a hallucination is observed, a subsequent question relevant to that hallucination is posed. The framework compares LVLM performance in these hallucinatory conversations against hallucination-free contexts.\n    *   **Hallucination Creation**: The framework meticulously creates hallucinations by categorizing them into Existence, Attribute, Relation, and Imagination types. It uses ChatGPT to rewrite facts and generate image descriptions that semantically entail a specific hallucination, ensuring the hallucinated information directly contradicts the visual ground truth and invalidates the correct answer to a target question.\n    *   **Residual Visual Decoding (RVD)**: To mitigate snowballing, \\cite{zhong2024mfi} introduces a training-free decoding method. RVD works by \"residual connecting\" the visual information with the current user instruction. This process derives output distributions that emphasize direct visual evidence, which are then used to revise the LVLM's original output distribution, providing models with more direct access to visual information during generation.\n\n*   **Key Technical Contributions**\n    *   **Identification and Characterization of Multimodal Hallucination Snowballing**: The paper formally identifies and defines this novel phenomenon, demonstrating how LVLMs can be misled by their own previously generated hallucinations \\cite{zhong2024mfi}.\n    *   **MMHalSnowball Evaluation Framework**: A systematic framework for constructing hallucinatory visual conversations and evaluating LVLMs' susceptibility to snowballing, including a detailed methodology for hallucination creation and allocation across different types \\cite{zhong2024mfi}.\n    *   **Residual Visual Decoding (RVD)**: A novel, training-free decoding method designed to mitigate multimodal hallucination snowballing by integrating residual visual input to revise output distributions \\cite{zhong2024mfi}.\n\n*   **Experimental Validation**\n    *   **Models Tested**: Experiments were conducted on prominent open-source LVLMs, including GPT-4V, LLaVA 1.5, and mPLUG-Owl2 \\cite{zhong2024mfi}.\n    *   **Dataset**: The validation set of the GQA dataset, augmented with regional descriptions from Visual Genome, was used as the data source to ensure objective perceptional questions and minimize dataset contamination \\cite{zhong2024mfi}.\n    *   **Key Findings on Snowballing**: The experiments revealed that the performance of open-source LVLMs dropped by at least 31% when exposed to hallucinatory conversational contexts compared to hallucination-free conversations. Specifically, over 59% of answers were semantically aligned with previously generated hallucinations, indicating models were misled \\cite{zhong2024mfi}.\n    *   **RVD Performance**: Residual Visual Decoding demonstrated significant mitigation, reducing snowballed multimodal hallucinations by more than 24% while successfully maintaining the models' contextual modeling capabilities \\cite{zhong2024mfi}.\n\n*   **Limitations & Scope**\n    *   The study primarily focuses on specific types of hallucinations (Existence, Attribute, Relation, Imagination) and their impact in a question-answering conversational setting \\cite{zhong2024mfi}.\n    *   While the MMHalSnowball framework is systematic, the hallucination creation process relies on ChatGPT, which introduces a dependency on another LLM's capabilities \\cite{zhong2024mfi}.\n    *   The RVD method is training-free, which is an advantage, but its generalizability across all possible LVLM architectures and tasks beyond conversational QA needs further exploration.\n\n*   **Technical Significance**\n    *   This paper significantly advances the technical state-of-the-art by being the first to systematically investigate and quantify the \"Multimodal Hallucination Snowballing\" phenomenon in LVLMs \\cite{zhong2024mfi}.\n    *   The proposed MMHalSnowball framework provides a robust methodology for future research to evaluate LVLM robustness against self-generated errors.\n    *   The introduction of Residual Visual Decoding offers a practical, training-free mitigation strategy, paving the way for more reliable and trustworthy LVLM interactions, particularly in safety-critical applications \\cite{zhong2024mfi}. This work highlights the need for LVLMs to maintain direct access to visual information to prevent over-reliance on potentially hallucinated textual context.",
      "intriguing_abstract": "Can Large Vision-Language Models (LVLMs) be misled by their *own* generated falsehoods? We uncover and systematically investigate **Multimodal Hallucination Snowballing**, a critical, underexplored phenomenon where an LVLM's initial multimodal hallucinations propagate, corrupting subsequent generations even when visual ground truth is available. This self-reinforcing error profoundly undermines LVLM reliability in interactive, conversational settings.\n\nTo quantify this risk, we introduce the novel **MMHalSnowball framework**, meticulously constructing hallucinatory visual conversations to evaluate LVLMs' susceptibility. Our experiments reveal that leading open-source LVLMs suffer a performance drop of over 31% in such contexts, with more than 59% of answers aligning with prior hallucinations. To combat this, we propose **Residual Visual Decoding (RVD)**, a training-free method that 'residual connects' visual information to revise output distributions, significantly reducing snowballed hallucinations by over 24% while preserving contextual understanding. This work is the first to characterize multimodal hallucination snowballing, offering both a robust evaluation framework and a practical mitigation strategy, paving the way for more trustworthy and visually grounded LVLM interactions.",
      "keywords": [
        "Large Vision-Language Models (LVLMs)",
        "multimodal hallucinations",
        "Multimodal Hallucination Snowballing",
        "MMHalSnowball Framework",
        "Residual Visual Decoding (RVD)",
        "training-free decoding",
        "conversational AI reliability",
        "systematic evaluation framework",
        "self-generated errors",
        "direct visual evidence",
        "mitigation strategies",
        "safety-critical applications"
      ],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/968bd4cf71c66bb153527778836e54c85ee6162c.pdf",
      "citation_key": "zhong2024mfi",
      "metadata": {
        "title": "Investigating and Mitigating the Multimodal Hallucination Snowballing in Large Vision-Language Models",
        "authors": [
          "Weihong Zhong",
          "Xiaocheng Feng",
          "Liang Zhao",
          "Qiming Li",
          "Lei Huang",
          "Yuxuan Gu",
          "Weitao Ma",
          "Yuan Xu",
          "Bing Qin"
        ],
        "published_date": "2024",
        "abstract": "Though advanced in understanding visual information with human languages, Large Vision-Language Models (LVLMs) still suffer from multimodal hallucinations. A natural concern is that during multimodal interaction, the generated hallucinations could influence the LVLMs' subsequent generation. Thus, we raise a question: When presented with a query relevant to the previously generated hallucination, will LVLMs be misled and respond incorrectly, even though the ground visual information exists? To answer this, we propose a framework called MMHalSnowball to evaluate LVLMs' behaviors when encountering generated hallucinations, where LVLMs are required to answer specific visual questions within a curated hallucinatory conversation. Crucially, our experiment shows that the performance of open-source LVLMs drops by at least $31\\%$, indicating that LVLMs are prone to accept the generated hallucinations and make false claims that they would not have supported without distractions. We term this phenomenon Multimodal Hallucination Snowballing. To mitigate this, we further propose a training-free method called Residual Visual Decoding, where we revise the output distribution of LVLMs with the one derived from the residual visual input, providing models with direct access to the visual information. Experiments show that our method can mitigate more than $24\\%$ of the snowballed multimodal hallucination while maintaining capabilities.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/968bd4cf71c66bb153527778836e54c85ee6162c.pdf",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **Research Problem & Motivation**\n    *   Large Vision-Language Models (LVLMs) are susceptible to multimodal hallucinations, where their generated responses contradict visual information \\cite{zhong2024mfi}.\n    *   A critical, yet underexplored, problem is whether these generated hallucinations can \"snowball,\" influencing an LVLM's subsequent generations and leading to further incorrect responses, even when ground visual information is available. This phenomenon, termed \"Multimodal Hallucination Snowballing,\" is important because it undermines the reliability of LVLMs in interactive, conversational settings.\n\n*   **Related Work & Positioning**\n    *   Previous research has identified similar \"snowballing\" effects in Large Language Models (LLMs), where models over-commit to early mistakes, leading to accumulated errors in conversational contexts \\cite{zhong2024mfi}.\n    *   However, this paper highlights that the extent to which *multimodal* hallucinations accumulate and mislead LVLMs into generating false claims had not been systematically investigated prior to this work \\cite{zhong2024mfi}. This work positions itself as the first to conduct such an investigation.\n\n*   **Technical Approach & Innovation**\n    *   **MMHalSnowball Framework**: \\cite{zhong2024mfi} proposes a novel framework to systematically evaluate multimodal hallucination snowballing. It constructs curated hallucinatory visual conversations where LVLMs are first prompted to describe an image, and if a hallucination is observed, a subsequent question relevant to that hallucination is posed. The framework compares LVLM performance in these hallucinatory conversations against hallucination-free contexts.\n    *   **Hallucination Creation**: The framework meticulously creates hallucinations by categorizing them into Existence, Attribute, Relation, and Imagination types. It uses ChatGPT to rewrite facts and generate image descriptions that semantically entail a specific hallucination, ensuring the hallucinated information directly contradicts the visual ground truth and invalidates the correct answer to a target question.\n    *   **Residual Visual Decoding (RVD)**: To mitigate snowballing, \\cite{zhong2024mfi} introduces a training-free decoding method. RVD works by \"residual connecting\" the visual information with the current user instruction. This process derives output distributions that emphasize direct visual evidence, which are then used to revise the LVLM's original output distribution, providing models with more direct access to visual information during generation.\n\n*   **Key Technical Contributions**\n    *   **Identification and Characterization of Multimodal Hallucination Snowballing**: The paper formally identifies and defines this novel phenomenon, demonstrating how LVLMs can be misled by their own previously generated hallucinations \\cite{zhong2024mfi}.\n    *   **MMHalSnowball Evaluation Framework**: A systematic framework for constructing hallucinatory visual conversations and evaluating LVLMs' susceptibility to snowballing, including a detailed methodology for hallucination creation and allocation across different types \\cite{zhong2024mfi}.\n    *   **Residual Visual Decoding (RVD)**: A novel, training-free decoding method designed to mitigate multimodal hallucination snowballing by integrating residual visual input to revise output distributions \\cite{zhong2024mfi}.\n\n*   **Experimental Validation**\n    *   **Models Tested**: Experiments were conducted on prominent open-source LVLMs, including GPT-4V, LLaVA 1.5, and mPLUG-Owl2 \\cite{zhong2024mfi}.\n    *   **Dataset**: The validation set of the GQA dataset, augmented with regional descriptions from Visual Genome, was used as the data source to ensure objective perceptional questions and minimize dataset contamination \\cite{zhong2024mfi}.\n    *   **Key Findings on Snowballing**: The experiments revealed that the performance of open-source LVLMs dropped by at least 31% when exposed to hallucinatory conversational contexts compared to hallucination-free conversations. Specifically, over 59% of answers were semantically aligned with previously generated hallucinations, indicating models were misled \\cite{zhong2024mfi}.\n    *   **RVD Performance**: Residual Visual Decoding demonstrated significant mitigation, reducing snowballed multimodal hallucinations by more than 24% while successfully maintaining the models' contextual modeling capabilities \\cite{zhong2024mfi}.\n\n*   **Limitations & Scope**\n    *   The study primarily focuses on specific types of hallucinations (Existence, Attribute, Relation, Imagination) and their impact in a question-answering conversational setting \\cite{zhong2024mfi}.\n    *   While the MMHalSnowball framework is systematic, the hallucination creation process relies on ChatGPT, which introduces a dependency on another LLM's capabilities \\cite{zhong2024mfi}.\n    *   The RVD method is training-free, which is an advantage, but its generalizability across all possible LVLM architectures and tasks beyond conversational QA needs further exploration.\n\n*   **Technical Significance**\n    *   This paper significantly advances the technical state-of-the-art by being the first to systematically investigate and quantify the \"Multimodal Hallucination Snowballing\" phenomenon in LVLMs \\cite{zhong2024mfi}.\n    *   The proposed MMHalSnowball framework provides a robust methodology for future research to evaluate LVLM robustness against self-generated errors.\n    *   The introduction of Residual Visual Decoding offers a practical, training-free mitigation strategy, paving the way for more reliable and trustworthy LVLM interactions, particularly in safety-critical applications \\cite{zhong2024mfi}. This work highlights the need for LVLMs to maintain direct access to visual information to prevent over-reliance on potentially hallucinated textual context.",
        "keywords": [
          "Large Vision-Language Models (LVLMs)",
          "multimodal hallucinations",
          "Multimodal Hallucination Snowballing",
          "MMHalSnowball Framework",
          "Residual Visual Decoding (RVD)",
          "training-free decoding",
          "conversational AI reliability",
          "systematic evaluation framework",
          "self-generated errors",
          "direct visual evidence",
          "mitigation strategies",
          "safety-critical applications"
        ],
        "paper_type": "the paper should be classified as **technical**.\n\nhere's why:\n\n1.  **proposes new methods/systems:** the abstract explicitly states:\n    *   \"we **propose a framework** called mmhalsnowball to evaluate lvlmsâ€™ behaviors...\"\n    *   \"we further **propose a training-free method** called residual visual decoding...\"\n    this directly aligns with the \"technical\" criterion: \"presents new methods, algorithms, or systems. abstract mentions: 'propose', 'develop', 'present', 'algorithm', 'method'.\"\n\n2.  **addresses a technical problem with a proposed solution:** the introduction and abstract clearly identify a technical problem (multimodal hallucinations and their snowballing effect in lvlms) and then present specific technical solutions (an evaluation framework and a mitigation method).\n\nwhile the paper also involves significant empirical work (\"our experiment shows...\", \"experiments show that our method can mitigate...\"), this empirical analysis is primarily used to validate the proposed technical framework and mitigation method. the core contribution lies in the *development and presentation* of these new methods."
      },
      "file_name": "968bd4cf71c66bb153527778836e54c85ee6162c.pdf"
    },
    {
      "success": true,
      "doc_id": "031a1b5817e7a61cab60139d44c31b73",
      "summary": "Here's a focused summary of the paper for a literature review, emphasizing technical innovations and empirical validation:\n\n### Focused Summary for Literature Review: Knowledge Overshadowing Causes Amalgamated Hallucination in Large Language Models \\cite{zhang2024qq9}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: The paper addresses a specific type of hallucination in Large Language Models (LLMs) called \"amalgamated hallucination,\" which occurs even when training data is factually correct. This phenomenon is coined \"knowledge overshadowing,\" where, given multiple conditions in a query, some dominant conditions overshadow others, leading to incorrect, amalgamated outputs \\cite{zhang2024qq9}.\n    *   **Importance & Challenge**: Hallucination is a major impediment to LLM reliability, especially for knowledge-intensive tasks. Existing explanations often point to low-quality data or decoding issues, but this work identifies a persistent hallucination type stemming from data imbalance and over-generalization, posing a fundamental challenge to LLM factual consistency \\cite{zhang2024qq9}.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: Previous studies attribute hallucination to low-quality training data or discrepancies between input/output in decoding \\cite{zhang2024qq9}.\n    *   **Limitations of Previous Solutions**: This work distinguishes itself by showing that amalgamated hallucinations due to knowledge overshadowing persist even with factually correct training data, indicating that these prior explanations do not fully cover the observed phenomenon \\cite{zhang2024qq9}. It positions knowledge overshadowing as a case of over-generalization, a distinct root cause.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes that knowledge overshadowing is a case of over-generalization of dominant conditions (patterns) in the training data, often stemming from data imbalance.\n    *   **Novelty**:\n        *   **Discovery of Knowledge Overshadowing**: Identifying and formally defining \"knowledge overshadowing\" as a prevalent cause of amalgamated hallucinations in LLMs \\cite{zhang2024qq9}.\n        *   **Theoretical Interpretation**: Interpreting this phenomenon as over-generalization and deriving a generalization bound that connects hallucination rate with imbalance ratio and condition length \\cite{zhang2024qq9}.\n        *   **Inference-time Mitigation**: Proposing a novel, training-free, two-step inference-time approach:\n            1.  **Detection**: Utilizing Pointwise Mutual Information (PMI) to pre-identify possible overshadowed conditions in the generation prompt.\n            2.  **Alleviation**: Employing a self-contrastive decoding method over dominant conditions to reduce hallucination during inference \\cite{zhang2024qq9}.\n\n4.  **Key Technical Contributions**\n    *   **Novel Phenomenon & Analysis**: Discovery and characterization of \"knowledge overshadowing\" as a specific, prevalent type of hallucination caused by popular conditions suppressing less popular ones, leading to factually incorrect outputs \\cite{zhang2024qq9}.\n    *   **Theoretical Insights**: Quantification of the relationship between imbalance ratio, condition length, and hallucination rate, supported by a derived generalization bound for auto-regressive language modeling that aligns with empirical observations \\cite{zhang2024qq9}.\n    *   **Novel Mitigation Method**: A simple yet effective training-free self-contrastive decoding method, combined with PMI-based detection, to anticipate and alleviate knowledge overshadowing-induced hallucinations during inference \\cite{zhang2024qq9}.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**:\n        *   **Verification of Overshadowing**: Probing experiments on pretrained models (Olmo-7b) using natural language queries to demonstrate overshadowing across various conditions (gender, race, occupation, negation) and its correlation with training data mentions \\cite{zhang2024qq9}.\n        *   **Controlled Fine-tuning Experiments**: Fine-tuning various LLM families (Llama-2-7B, Mistral-7B, GPT-J-6B, Phi-2-2.8B, Pythia-410m) on natural language tasks (Event-Time, Event-Location, Gender Bias, Negation) with controlled data imbalance ratios (10:1 to 100:1) \\cite{zhang2024qq9}.\n        *   **Synthetic Data Experiments**: Using synthetic datasets with varying imbalance ratios and condition lengths to quantitatively analyze their impact on hallucination rates across Pythia model family sizes (160m to 2.8b) \\cite{zhang2024qq9}.\n        *   **Generalization Correlation**: Experiments correlating hallucination rate with Gradient Signal-to-Noise Ratio (GSNR) and varying weight decay to demonstrate that overshadowing is linked to over-generalization \\cite{zhang2024qq9}.\n        *   **Mitigation Method Evaluation**: Extensive experiments evaluating the proposed PMI-detection and self-contrastive decoding method on a mixture of datasets and models \\cite{zhang2024qq9}.\n    *   **Key Performance Metrics & Results**:\n        *   **Hallucination Rate (HR) / Relative Hallucination Rate (rHR)**: Consistently observed high hallucination rates across all tasks and models.\n        *   **Impact of Imbalance Ratio**: Hallucination rate consistently increased with higher imbalance ratios (e.g., up to 88.3% for Mistral-7B on Negation with 100:1 ratio) \\cite{zhang2024qq9}.\n        *   **Impact of Model Size**: Larger models exhibited higher hallucination rates, suggesting inverse scaling for this specific hallucination type \\cite{zhang2024qq9}.\n        *   **Impact of Condition Length**: Longer dominant condition descriptions led to higher hallucination rates \\cite{zhang2024qq9}.\n        *   **Proposed Method Performance**: The proposed approach achieved up to **82% F1 for hallucination anticipation** and demonstrated **11.2% to 39.4% hallucination control** (reduction in hallucination rate) across different models and datasets \\cite{zhang2024qq9}.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations**: The proposed mitigation method is an inference-time solution, which might not address the fundamental training-time causes of over-generalization. The detection mechanism relies on identifying \"overshadowing conditions\" which might be challenging in highly complex, open-ended prompts \\cite{zhang2024qq9}.\n    *   **Scope of Applicability**: The work primarily focuses on \"amalgamated hallucinations\" caused by \"knowledge overshadowing\" due to data imbalance and over-generalization. It does not claim to address all forms of LLM hallucination \\cite{zhang2024qq9}.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: This paper significantly advances the understanding of LLM hallucination by identifying a novel and prevalent mechanism (\"knowledge overshadowing\") that persists even with factual training data. It provides a theoretical framework (over-generalization and generalization bound) to explain this phenomenon \\cite{zhang2024qq9}.\n    *   **Potential Impact**: The proposed training-free, inference-time detection and mitigation method offers a practical and immediate solution for improving the factual consistency of LLMs in knowledge-intensive applications. This work opens new avenues for research into data-centric approaches and decoding strategies to combat specific types of hallucinations \\cite{zhang2024qq9}.",
      "intriguing_abstract": "Large Language Models (LLMs) frequently suffer from hallucination, yet its persistence even with factually correct training data remains a critical challenge. This paper uncovers \"knowledge overshadowing,\" a novel phenomenon causing \"amalgamated hallucination\" where dominant conditions in a query suppress less prominent ones, leading to factually incorrect outputs. We reveal this stems from over-generalization due to data imbalance, providing a theoretical generalization bound that quantifies its relationship with imbalance ratio and condition length. Our empirical studies across various LLMs demonstrate that hallucination rates escalate with increased imbalance and model size. To combat this, we propose a novel, training-free, two-step inference-time mitigation strategy: Pointwise Mutual Information (PMI) for detecting overshadowed conditions, followed by self-contrastive decoding for alleviation. This approach achieves up to 82% F1 for anticipation and significantly reduces hallucination rates (11.2-39.4%). This work fundamentally advances our understanding of LLM factual inconsistency and offers a practical, immediate solution to enhance reliability in knowledge-intensive applications.",
      "keywords": [
        "amalgamated hallucination",
        "knowledge overshadowing",
        "Large Language Models (LLMs)",
        "over-generalization",
        "data imbalance",
        "generalization bound",
        "Pointwise Mutual Information (PMI)",
        "self-contrastive decoding",
        "inference-time mitigation",
        "factual consistency",
        "imbalance ratio impact",
        "inverse scaling (hallucination)",
        "empirical validation"
      ],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/01f3b1809035a593b9dd6fb0b2cabdc8e216542f.pdf",
      "citation_key": "zhang2024qq9",
      "metadata": {
        "title": "Knowledge Overshadowing Causes Amalgamated Hallucination in Large Language Models",
        "authors": [
          "Yuji Zhang",
          "Sha Li",
          "Jiateng Liu",
          "Pengfei Yu",
          "Y. Fung",
          "Jing Li",
          "Manling Li",
          "Heng Ji"
        ],
        "published_date": "2024",
        "abstract": "Hallucination is often regarded as a major impediment for using large language models (LLMs), especially for knowledge-intensive tasks. Even when the training corpus consists solely of true statements, language models still generate hallucinations in the form of amalgamations of multiple facts. We coin this phenomenon as ``knowledge overshadowing'': when we query knowledge from a language model with multiple conditions, some conditions overshadow others, leading to hallucinated outputs. This phenomenon partially stems from training data imbalance, which we verify on both pretrained models and fine-tuned models, over a wide range of LM model families and sizes.From a theoretical point of view, knowledge overshadowing can be interpreted as over-generalization of the dominant conditions (patterns). We show that the hallucination rate grows with both the imbalance ratio (between the popular and unpopular condition) and the length of dominant condition description, consistent with our derived generalization bound. Finally, we propose to utilize overshadowing conditions as a signal to catch hallucination before it is produced, along with a training-free self-contrastive decoding method to alleviate hallucination during inference. Our proposed approach showcases up to 82% F1 for hallucination anticipation and 11.2% to 39.4% hallucination control, with different models and datasets.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/01f3b1809035a593b9dd6fb0b2cabdc8e216542f.pdf",
        "venue": "arXiv.org",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the paper for a literature review, emphasizing technical innovations and empirical validation:\n\n### Focused Summary for Literature Review: Knowledge Overshadowing Causes Amalgamated Hallucination in Large Language Models \\cite{zhang2024qq9}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: The paper addresses a specific type of hallucination in Large Language Models (LLMs) called \"amalgamated hallucination,\" which occurs even when training data is factually correct. This phenomenon is coined \"knowledge overshadowing,\" where, given multiple conditions in a query, some dominant conditions overshadow others, leading to incorrect, amalgamated outputs \\cite{zhang2024qq9}.\n    *   **Importance & Challenge**: Hallucination is a major impediment to LLM reliability, especially for knowledge-intensive tasks. Existing explanations often point to low-quality data or decoding issues, but this work identifies a persistent hallucination type stemming from data imbalance and over-generalization, posing a fundamental challenge to LLM factual consistency \\cite{zhang2024qq9}.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: Previous studies attribute hallucination to low-quality training data or discrepancies between input/output in decoding \\cite{zhang2024qq9}.\n    *   **Limitations of Previous Solutions**: This work distinguishes itself by showing that amalgamated hallucinations due to knowledge overshadowing persist even with factually correct training data, indicating that these prior explanations do not fully cover the observed phenomenon \\cite{zhang2024qq9}. It positions knowledge overshadowing as a case of over-generalization, a distinct root cause.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes that knowledge overshadowing is a case of over-generalization of dominant conditions (patterns) in the training data, often stemming from data imbalance.\n    *   **Novelty**:\n        *   **Discovery of Knowledge Overshadowing**: Identifying and formally defining \"knowledge overshadowing\" as a prevalent cause of amalgamated hallucinations in LLMs \\cite{zhang2024qq9}.\n        *   **Theoretical Interpretation**: Interpreting this phenomenon as over-generalization and deriving a generalization bound that connects hallucination rate with imbalance ratio and condition length \\cite{zhang2024qq9}.\n        *   **Inference-time Mitigation**: Proposing a novel, training-free, two-step inference-time approach:\n            1.  **Detection**: Utilizing Pointwise Mutual Information (PMI) to pre-identify possible overshadowed conditions in the generation prompt.\n            2.  **Alleviation**: Employing a self-contrastive decoding method over dominant conditions to reduce hallucination during inference \\cite{zhang2024qq9}.\n\n4.  **Key Technical Contributions**\n    *   **Novel Phenomenon & Analysis**: Discovery and characterization of \"knowledge overshadowing\" as a specific, prevalent type of hallucination caused by popular conditions suppressing less popular ones, leading to factually incorrect outputs \\cite{zhang2024qq9}.\n    *   **Theoretical Insights**: Quantification of the relationship between imbalance ratio, condition length, and hallucination rate, supported by a derived generalization bound for auto-regressive language modeling that aligns with empirical observations \\cite{zhang2024qq9}.\n    *   **Novel Mitigation Method**: A simple yet effective training-free self-contrastive decoding method, combined with PMI-based detection, to anticipate and alleviate knowledge overshadowing-induced hallucinations during inference \\cite{zhang2024qq9}.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**:\n        *   **Verification of Overshadowing**: Probing experiments on pretrained models (Olmo-7b) using natural language queries to demonstrate overshadowing across various conditions (gender, race, occupation, negation) and its correlation with training data mentions \\cite{zhang2024qq9}.\n        *   **Controlled Fine-tuning Experiments**: Fine-tuning various LLM families (Llama-2-7B, Mistral-7B, GPT-J-6B, Phi-2-2.8B, Pythia-410m) on natural language tasks (Event-Time, Event-Location, Gender Bias, Negation) with controlled data imbalance ratios (10:1 to 100:1) \\cite{zhang2024qq9}.\n        *   **Synthetic Data Experiments**: Using synthetic datasets with varying imbalance ratios and condition lengths to quantitatively analyze their impact on hallucination rates across Pythia model family sizes (160m to 2.8b) \\cite{zhang2024qq9}.\n        *   **Generalization Correlation**: Experiments correlating hallucination rate with Gradient Signal-to-Noise Ratio (GSNR) and varying weight decay to demonstrate that overshadowing is linked to over-generalization \\cite{zhang2024qq9}.\n        *   **Mitigation Method Evaluation**: Extensive experiments evaluating the proposed PMI-detection and self-contrastive decoding method on a mixture of datasets and models \\cite{zhang2024qq9}.\n    *   **Key Performance Metrics & Results**:\n        *   **Hallucination Rate (HR) / Relative Hallucination Rate (rHR)**: Consistently observed high hallucination rates across all tasks and models.\n        *   **Impact of Imbalance Ratio**: Hallucination rate consistently increased with higher imbalance ratios (e.g., up to 88.3% for Mistral-7B on Negation with 100:1 ratio) \\cite{zhang2024qq9}.\n        *   **Impact of Model Size**: Larger models exhibited higher hallucination rates, suggesting inverse scaling for this specific hallucination type \\cite{zhang2024qq9}.\n        *   **Impact of Condition Length**: Longer dominant condition descriptions led to higher hallucination rates \\cite{zhang2024qq9}.\n        *   **Proposed Method Performance**: The proposed approach achieved up to **82% F1 for hallucination anticipation** and demonstrated **11.2% to 39.4% hallucination control** (reduction in hallucination rate) across different models and datasets \\cite{zhang2024qq9}.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations**: The proposed mitigation method is an inference-time solution, which might not address the fundamental training-time causes of over-generalization. The detection mechanism relies on identifying \"overshadowing conditions\" which might be challenging in highly complex, open-ended prompts \\cite{zhang2024qq9}.\n    *   **Scope of Applicability**: The work primarily focuses on \"amalgamated hallucinations\" caused by \"knowledge overshadowing\" due to data imbalance and over-generalization. It does not claim to address all forms of LLM hallucination \\cite{zhang2024qq9}.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: This paper significantly advances the understanding of LLM hallucination by identifying a novel and prevalent mechanism (\"knowledge overshadowing\") that persists even with factual training data. It provides a theoretical framework (over-generalization and generalization bound) to explain this phenomenon \\cite{zhang2024qq9}.\n    *   **Potential Impact**: The proposed training-free, inference-time detection and mitigation method offers a practical and immediate solution for improving the factual consistency of LLMs in knowledge-intensive applications. This work opens new avenues for research into data-centric approaches and decoding strategies to combat specific types of hallucinations \\cite{zhang2024qq9}.",
        "keywords": [
          "amalgamated hallucination",
          "knowledge overshadowing",
          "Large Language Models (LLMs)",
          "over-generalization",
          "data imbalance",
          "generalization bound",
          "Pointwise Mutual Information (PMI)",
          "self-contrastive decoding",
          "inference-time mitigation",
          "factual consistency",
          "imbalance ratio impact",
          "inverse scaling (hallucination)",
          "empirical validation"
        ],
        "paper_type": "based on the abstract and introduction, this paper is best classified as **technical**.\n\nhere's why:\n\n*   **abstract mentions:** \"we coin this phenomenon as 'knowledge overshadowing'...\", \"from a theoretical point of view...\", \"we show that the hallucination rate grows...\", and most importantly, \"finally, we **propose** to utilize overshadowing conditions as a signal to catch hallucination... along with a training-free self-contrastive **decoding method** to alleviate hallucination during inference. our **proposed approach showcases** up to 82% f1 for hallucination anticipation and 11.2% to 39.4% hallucination control...\"\n*   **introduction discusses:** it identifies a critical issue (hallucination), points out a specific manifestation of it (\"knowledge overshadowing\"), and sets the stage for understanding and addressing this problem. while it discusses existing literature, its primary focus is not a comprehensive review.\n\nthe paper clearly presents a new phenomenon, provides a theoretical interpretation, but crucially, it **proposes a new method/system** (the self-contrastive decoding method) to address the problem and then evaluates its performance with empirical results. the empirical results serve to validate the effectiveness of the proposed technical solution. while it has theoretical and empirical components, the core contribution highlighted is the development and evaluation of a new method."
      },
      "file_name": "01f3b1809035a593b9dd6fb0b2cabdc8e216542f.pdf"
    },
    {
      "success": true,
      "doc_id": "486fb74c01f264ed1e95256ee9a5acb7",
      "summary": "This paper \\cite{wu20241us} introduces a novel framework and benchmark for evaluating hallucination in Large Vision-Language Models (LVLMs), with a particular focus on previously underexplored relation hallucinations.\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Existing LVLM hallucination benchmarks primarily focus on *object-related* hallucinations, neglecting the critical issue of *relation hallucination* (where LVLMs correctly identify objects but misrepresent their relationships).\n    *   **Importance and Challenge**: Hallucinations severely impact the reliability of LVLMs. Relation hallucination is shown to be a more severe and previously overlooked problem. Current evaluation methods are often biased (e.g., requiring short \"Yes/No\" answers) or limited in applicability, failing to provide a unified, fine-grained, and unbiased assessment across diverse vision-language tasks.\n\n*   **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: Previous works either solely evaluate object hallucinations or use \"yes/no\" questions for both object and relation types.\n    *   **Limitations of Previous Solutions**:\n        *   **Incompleteness**: Neglect other hallucination types like relation hallucination, leading to non-comprehensive evaluations.\n        *   **Bias**: Methods requiring short answers (e.g., \"Yes/No\" or template-driven like \"What is the relation between A and B?\") introduce bias due to varying LVLM capabilities in generating brief responses, inflating performance for some models.\n        *   **Limited Applicability**: These benchmarks often require transforming general vision-language tasks into specific formats, restricting their generalizability.\n        *   **Discriminator Limitations**: Concurrent work like Reefknot \\cite{zheng2024reef} uses simpler template-based questions and a single entailment-based discriminator, limiting task variety, response length, and the comprehensiveness of hallucination identification.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes a unified framework that evaluates hallucinations via **(object, relation, object) triplets** extracted from LVLM responses.\n        *   **Definitions**: Clearly defines object hallucination (object not in image), relation hallucination (objects in image, but relation not), and distinguishes them from prediction errors.\n        *   **Evaluation Metrics**: Introduces **question-level (Hallu Q)** and **image-level (Hallu I)** hallucination rates, calculated as the proportion of hallucinated triplets, ensuring comparability across different LVLMs.\n        *   **Pipeline**:\n            1.  **Knowledge Graph Extraction**: Uses GPT-4 to extract triplets from LVLM-generated responses.\n            2.  **Hallucination Judgment**: Employs two strategies:\n                *   **NLI Judge**: A Natural Language Inference model compares extracted triplets with ground truth scene graph triplets based on similarity and NLI scores.\n                *   **LLM Judge**: A powerful LLM (GPT-4) determines if an extracted triplet can be directly obtained or inferred from the image's scene graph. It further clarifies if the hallucination is object- or relation-specific.\n    *   **Novelty/Differentiation**:\n        *   **Unified Triplet-Level Evaluation**: First framework to simultaneously and explicitly measure both object and relation hallucinations at a fine-grained triplet level, generalizable across diverse vision-language tasks.\n        *   **Unbiased Metrics**: Addresses the comparability issue of previous metrics by normalizing hallucination rates at question and image levels.\n        *   **Robust Judge**: Leverages powerful LLMs (GPT-4) for accurate and nuanced hallucination discrimination, demonstrating high alignment with human judgment, and distinguishing it from simpler NLI or entailment-based methods.\n        *   **Task-Agnostic Design**: The framework is designed to be applicable to any vision-language task, provided scene graph annotations are available.\n\n*   **Key Technical Contributions**\n    *   **Novel Framework**: A unified triplet-level hallucination evaluation framework capable of jointly assessing object and relation hallucinations in LVLM responses across diverse vision-language tasks.\n    *   **Novel Benchmark (Tri-HE)**: Introduction of Tri-HE, a fine-grained triplet-level hallucination evaluation benchmark specifically designed for LVLMs, constructed from GQA images with GPT-4V generated questions, answers, and refined scene graphs.\n    *   **Training-Free Mitigation Approach**: A simple yet effective training-free method that mitigates hallucinations by explicitly incorporating scene graph triplets into LVLM prompts.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted**:\n        *   Comprehensive evaluation of various open-source (LLaVA, LLaVA-1.5, MiniGPT-4, InstructBLIP, Shikra, InternLM2) and closed-source (GPT-4V) LVLMs on the Tri-HE benchmark.\n        *   Ablation studies comparing human vs. LLM judge agreement and NLI vs. LLM judge performance.\n        *   Validation of the proposed training-free hallucination mitigation approach.\n    *   **Key Performance Metrics**: Hallu Q and Hallu I (overall, object, and relation hallucination rates).\n    *   **Comparison Results**:\n        *   **Severity of Relation Hallucination**: Experiments consistently show that relation hallucination is *more severe* than object hallucination across all evaluated LVLMs, highlighting a critical, previously neglected problem.\n        *   **Model Performance**: GPT-4V generally exhibits the lowest hallucination rates but still suffers from relation hallucination. Open-source models like LLaVA-1.5 and InternLM2 perform relatively well.\n        *   **Judge Reliability**: The LLM judge (GPT-4) demonstrates high agreement with human judgments (80.2% for object, 78.5% for relation), outperforming the NLI judge. LLaMA-3.3 also shows comparable performance as a judge.\n        *   **Mitigation Effectiveness**: The proposed training-free mitigation method significantly reduces both object and relation hallucinations across various LVLMs, outperforming existing open-source competitors without mitigation.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The framework relies on the availability of scene graph annotations for test images or the ability to extract them using pre-trained expert models. The effectiveness of the LLM judge depends on the capabilities of the underlying LLM (e.g., GPT-4).\n    *   **Scope of Applicability**: While formulated primarily for VQA tasks, the framework is designed to be task-agnostic and generalizable to any natural-language-based vision-language task, provided scene graph information is available. Detailed exploration for other tasks is left for future work.\n\n*   **Technical Significance**\n    *   **Advancement of State-of-the-Art**: This work significantly advances the technical state-of-the-art in LVLM reliability by providing the first unified, fine-grained evaluation framework for both object and relation hallucinations. It uncovers relation hallucination as a major, previously underestimated challenge.\n    *   **Potential Impact on Future Research**:\n        *   **New Research Direction**: Highlights the critical need for LVLM research to address relation hallucination, moving beyond just object-level issues.\n        *   **Improved Evaluation**: Provides a robust, unbiased, and generalizable benchmark (Tri-HE) and evaluation methodology that can be adopted for future LVLM development and comparison.\n        *   **Mitigation Strategies**: The simple yet effective training-free mitigation approach offers a practical baseline and motivates further research into more advanced hallucination reduction techniques.\n        *   **Foundation for Reliability**: Lays a foundation for building more reliable and trustworthy LVLMs by enabling a deeper understanding and measurement of different hallucination types.",
      "intriguing_abstract": "Large Vision-Language Models (LVLMs) are transforming AI, yet their pervasive *hallucinations* remain a critical barrier to trustworthiness. While *object hallucination* has received considerable attention, the insidious problem of *relation hallucination*â€”where objects are correctly identified but their relationships are misrepresentedâ€”has been largely overlooked and underestimated. This paper introduces a novel, unified framework for fine-grained *triplet-level* evaluation, simultaneously assessing both *object and relation hallucinations* in LVLM responses across diverse vision-language tasks.\n\nWe present *Tri-HE*, a comprehensive benchmark featuring *GPT-4V*-generated questions and refined *scene graph* annotations, enabling unbiased *question-level (Hallu Q)* and *image-level (Hallu I)* metrics. Leveraging powerful *LLM Judges*, our framework demonstrates high human alignment. Crucially, our extensive experiments across diverse LVLMs reveal that *relation hallucination is consistently more severe* than object hallucination, even in advanced models like *GPT-4V*. Furthermore, we propose a simple yet effective *training-free mitigation* strategy that significantly reduces these errors. This work fundamentally advances LVLM reliability, providing essential tools and insights to build more accurate and trustworthy vision-language systems.",
      "keywords": [
        "Large Vision-Language Models (LVLMs)",
        "hallucination evaluation",
        "relation hallucination",
        "object hallucination",
        "unified triplet-level framework",
        "Tri-HE benchmark",
        "scene graph annotations",
        "LLM Judge (GPT-4)",
        "training-free hallucination mitigation",
        "severity of relation hallucination",
        "unbiased hallucination metrics",
        "vision-language tasks",
        "LVLM reliability"
      ],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/f6d4c76b21539aadc2ca8d813fe631be7149231e.pdf",
      "citation_key": "wu20241us",
      "metadata": {
        "title": "Unified Triplet-Level Hallucination Evaluation for Large Vision-Language Models",
        "authors": [
          "J. Wu",
          "Tsz Ting Chung",
          "Kai Chen",
          "Dit-Yan Yeung"
        ],
        "published_date": "2024",
        "abstract": "Despite the outstanding performance in vision-language reasoning, Large Vision-Language Models (LVLMs) might generate hallucinated contents that do not exist in the given image. Most existing LVLM hallucination benchmarks are constrained to evaluate the object-related hallucinations. However, the potential hallucination on the relations between two objects, i.e., relation hallucination, still lacks investigation. To remedy that, we design a unified framework to measure the object and relation hallucination in LVLMs simultaneously. The core idea of our framework is to evaluate hallucinations via (object, relation, object) triplets extracted from LVLMs'responses, making it easily generalizable to different vision-language tasks. Based on our framework, we further introduce Tri-HE, a novel Triplet-level Hallucination Evaluation benchmark which can be used to study both object and relation hallucination at the same time. With comprehensive evaluations on Tri-HE, we observe that the relation hallucination issue is even more serious than object hallucination among existing LVLMs, highlighting a previously neglected problem towards reliable LVLMs. Moreover, based on our findings, we design a simple training-free approach that effectively mitigates hallucinations for LVLMs. Our dataset and code for the reproduction of our experiments are available publicly at https://github.com/wujunjie1998/Tri-HE.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/f6d4c76b21539aadc2ca8d813fe631be7149231e.pdf",
        "venue": "Trans. Mach. Learn. Res.",
        "citationCount": 0,
        "score": 0,
        "summary": "This paper \\cite{wu20241us} introduces a novel framework and benchmark for evaluating hallucination in Large Vision-Language Models (LVLMs), with a particular focus on previously underexplored relation hallucinations.\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Existing LVLM hallucination benchmarks primarily focus on *object-related* hallucinations, neglecting the critical issue of *relation hallucination* (where LVLMs correctly identify objects but misrepresent their relationships).\n    *   **Importance and Challenge**: Hallucinations severely impact the reliability of LVLMs. Relation hallucination is shown to be a more severe and previously overlooked problem. Current evaluation methods are often biased (e.g., requiring short \"Yes/No\" answers) or limited in applicability, failing to provide a unified, fine-grained, and unbiased assessment across diverse vision-language tasks.\n\n*   **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: Previous works either solely evaluate object hallucinations or use \"yes/no\" questions for both object and relation types.\n    *   **Limitations of Previous Solutions**:\n        *   **Incompleteness**: Neglect other hallucination types like relation hallucination, leading to non-comprehensive evaluations.\n        *   **Bias**: Methods requiring short answers (e.g., \"Yes/No\" or template-driven like \"What is the relation between A and B?\") introduce bias due to varying LVLM capabilities in generating brief responses, inflating performance for some models.\n        *   **Limited Applicability**: These benchmarks often require transforming general vision-language tasks into specific formats, restricting their generalizability.\n        *   **Discriminator Limitations**: Concurrent work like Reefknot \\cite{zheng2024reef} uses simpler template-based questions and a single entailment-based discriminator, limiting task variety, response length, and the comprehensiveness of hallucination identification.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes a unified framework that evaluates hallucinations via **(object, relation, object) triplets** extracted from LVLM responses.\n        *   **Definitions**: Clearly defines object hallucination (object not in image), relation hallucination (objects in image, but relation not), and distinguishes them from prediction errors.\n        *   **Evaluation Metrics**: Introduces **question-level (Hallu Q)** and **image-level (Hallu I)** hallucination rates, calculated as the proportion of hallucinated triplets, ensuring comparability across different LVLMs.\n        *   **Pipeline**:\n            1.  **Knowledge Graph Extraction**: Uses GPT-4 to extract triplets from LVLM-generated responses.\n            2.  **Hallucination Judgment**: Employs two strategies:\n                *   **NLI Judge**: A Natural Language Inference model compares extracted triplets with ground truth scene graph triplets based on similarity and NLI scores.\n                *   **LLM Judge**: A powerful LLM (GPT-4) determines if an extracted triplet can be directly obtained or inferred from the image's scene graph. It further clarifies if the hallucination is object- or relation-specific.\n    *   **Novelty/Differentiation**:\n        *   **Unified Triplet-Level Evaluation**: First framework to simultaneously and explicitly measure both object and relation hallucinations at a fine-grained triplet level, generalizable across diverse vision-language tasks.\n        *   **Unbiased Metrics**: Addresses the comparability issue of previous metrics by normalizing hallucination rates at question and image levels.\n        *   **Robust Judge**: Leverages powerful LLMs (GPT-4) for accurate and nuanced hallucination discrimination, demonstrating high alignment with human judgment, and distinguishing it from simpler NLI or entailment-based methods.\n        *   **Task-Agnostic Design**: The framework is designed to be applicable to any vision-language task, provided scene graph annotations are available.\n\n*   **Key Technical Contributions**\n    *   **Novel Framework**: A unified triplet-level hallucination evaluation framework capable of jointly assessing object and relation hallucinations in LVLM responses across diverse vision-language tasks.\n    *   **Novel Benchmark (Tri-HE)**: Introduction of Tri-HE, a fine-grained triplet-level hallucination evaluation benchmark specifically designed for LVLMs, constructed from GQA images with GPT-4V generated questions, answers, and refined scene graphs.\n    *   **Training-Free Mitigation Approach**: A simple yet effective training-free method that mitigates hallucinations by explicitly incorporating scene graph triplets into LVLM prompts.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted**:\n        *   Comprehensive evaluation of various open-source (LLaVA, LLaVA-1.5, MiniGPT-4, InstructBLIP, Shikra, InternLM2) and closed-source (GPT-4V) LVLMs on the Tri-HE benchmark.\n        *   Ablation studies comparing human vs. LLM judge agreement and NLI vs. LLM judge performance.\n        *   Validation of the proposed training-free hallucination mitigation approach.\n    *   **Key Performance Metrics**: Hallu Q and Hallu I (overall, object, and relation hallucination rates).\n    *   **Comparison Results**:\n        *   **Severity of Relation Hallucination**: Experiments consistently show that relation hallucination is *more severe* than object hallucination across all evaluated LVLMs, highlighting a critical, previously neglected problem.\n        *   **Model Performance**: GPT-4V generally exhibits the lowest hallucination rates but still suffers from relation hallucination. Open-source models like LLaVA-1.5 and InternLM2 perform relatively well.\n        *   **Judge Reliability**: The LLM judge (GPT-4) demonstrates high agreement with human judgments (80.2% for object, 78.5% for relation), outperforming the NLI judge. LLaMA-3.3 also shows comparable performance as a judge.\n        *   **Mitigation Effectiveness**: The proposed training-free mitigation method significantly reduces both object and relation hallucinations across various LVLMs, outperforming existing open-source competitors without mitigation.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The framework relies on the availability of scene graph annotations for test images or the ability to extract them using pre-trained expert models. The effectiveness of the LLM judge depends on the capabilities of the underlying LLM (e.g., GPT-4).\n    *   **Scope of Applicability**: While formulated primarily for VQA tasks, the framework is designed to be task-agnostic and generalizable to any natural-language-based vision-language task, provided scene graph information is available. Detailed exploration for other tasks is left for future work.\n\n*   **Technical Significance**\n    *   **Advancement of State-of-the-Art**: This work significantly advances the technical state-of-the-art in LVLM reliability by providing the first unified, fine-grained evaluation framework for both object and relation hallucinations. It uncovers relation hallucination as a major, previously underestimated challenge.\n    *   **Potential Impact on Future Research**:\n        *   **New Research Direction**: Highlights the critical need for LVLM research to address relation hallucination, moving beyond just object-level issues.\n        *   **Improved Evaluation**: Provides a robust, unbiased, and generalizable benchmark (Tri-HE) and evaluation methodology that can be adopted for future LVLM development and comparison.\n        *   **Mitigation Strategies**: The simple yet effective training-free mitigation approach offers a practical baseline and motivates further research into more advanced hallucination reduction techniques.\n        *   **Foundation for Reliability**: Lays a foundation for building more reliable and trustworthy LVLMs by enabling a deeper understanding and measurement of different hallucination types.",
        "keywords": [
          "Large Vision-Language Models (LVLMs)",
          "hallucination evaluation",
          "relation hallucination",
          "object hallucination",
          "unified triplet-level framework",
          "Tri-HE benchmark",
          "scene graph annotations",
          "LLM Judge (GPT-4)",
          "training-free hallucination mitigation",
          "severity of relation hallucination",
          "unbiased hallucination metrics",
          "vision-language tasks",
          "LVLM reliability"
        ],
        "paper_type": "the paper should be classified as **technical**.\n\nhere's why:\n\n*   **proposes new methods/systems:** the abstract explicitly states, \"we design a unified framework to measure the object and relation hallucination,\" and \"we further introduce tri-he, a novel triplet-level hallucination evaluation benchmark.\" it also mentions designing \"a simple training-free approach that effectively mitigates hallucinations.\" these are all new methods, systems, or algorithms.\n*   **focus on technical problem and solution:** the introduction highlights a technical problem (relation hallucination in lvlms) and the abstract presents a proposed solution (the unified framework, tri-he benchmark, and mitigation approach).\n*   **empirical evaluation as support:** while the paper includes \"comprehensive evaluations\" and presents \"findings,\" these are used to demonstrate the effectiveness and insights gained from the *newly developed* framework and benchmark. the primary contribution is the creation of these technical tools and approaches, not solely a data-driven study of an existing phenomenon."
      },
      "file_name": "f6d4c76b21539aadc2ca8d813fe631be7149231e.pdf"
    },
    {
      "success": true,
      "doc_id": "a419a055a9f7af08a54e972cd0448bdc",
      "summary": "Here's a focused summary of the paper \"A Comprehensive Survey of Hallucination Mitigation Techniques in Large Language Models\" by S.M Towhidul Islam Tonmoy et al. for a literature review:\n\n*   **1. Research Problem & Motivation**\n    *   **Specific Technical Problem:** Large Language Models (LLMs) frequently \"hallucinate,\" generating content that appears factual but is ungrounded, factually erroneous, or inconsistent with real-world knowledge \\cite{tonmoy20244e4}. This includes misinterpreting ambiguous prompts, extrapolating biases from training data, or modifying information.\n    *   **Importance and Challenge:**\n        *   Hallucination is the primary obstacle to safely deploying powerful LLMs in real-world production systems, especially in sensitive applications like medical records, financial analysis, or legal advice, where small errors can lead to significant harm \\cite{tonmoy20244e4}.\n        *   LLMs, despite their fluency, lack true comprehension, and their static training data makes them unable to adapt to evolving world knowledge, leading to outdated or false information \\cite{tonmoy20244e4}.\n        *   The issue can \"snowball\" in complex reasoning tasks, making outputs increasingly unreliable \\cite{tonmoy20244e4}.\n\n*   **2. Related Work & Positioning**\n    *   **Relation to Existing Approaches:** This paper is a comprehensive survey that consolidates and organizes over thirty-two diverse techniques developed to mitigate hallucination in LLMs \\cite{tonmoy20244e4}. It introduces a systematic taxonomy to categorize these methods.\n    *   **Limitations of Previous Solutions (as identified by the survey):**\n        *   Many existing methods rectify hallucinations *post-hoc* (after generation), which can be less effective than real-time detection and rectification during the generation process \\cite{tonmoy20244e4}.\n        *   The closed-source nature of many contemporary LLMs limits the applicability of techniques that require access to internal model outputs (e.g., logit values for high entropy word detection) \\cite{tonmoy20244e4}.\n        *   Traditional Retrieval-Augmented Generation (RAG) approaches often required building non-parametric memory architectures from scratch for specific tasks, contrasting with end-to-end RAG's use of pre-trained components \\cite{tonmoy20244e4}.\n        *   LLMs' inherent static knowledge base struggles with dynamic, evolving real-world information \\cite{tonmoy20244e4}.\n\n*   **3. Technical Approach & Innovation**\n    *   **Core Technical Method:** The paper's core approach is a systematic review and classification of existing hallucination mitigation techniques. It introduces a detailed taxonomy that categorizes methods based on parameters such as dataset utilization, common tasks, feedback mechanisms, and retriever types \\cite{tonmoy20244e4}.\n    *   **Novelty/Difference:**\n        *   It provides a comprehensive synthesis of over thirty-two techniques, including those for Vision Language Models (VLMs), which is broader than many existing reviews \\cite{tonmoy20244e4}.\n        *   The proposed taxonomy offers a structured framework for understanding and distinguishing diverse approaches to hallucination mitigation, organizing them into categories like Prompt Engineering (Retrieval Augmented Generation, Self-Refinement, Prompt Tuning) and Developing Models (Decoding Strategy, Knowledge Graph, Loss Function, Supervised Finetuning) \\cite{tonmoy20244e4}.\n        *   It analyzes the challenges and limitations inherent in these techniques, providing a foundation for future research \\cite{tonmoy20244e4}.\n\n*   **4. Key Technical Contributions**\n    *   **Novel Algorithms, Methods, or Techniques:** The primary contribution is not a new algorithm, but a novel *systematic taxonomy* for categorizing hallucination mitigation techniques in LLMs, encompassing VLMs \\cite{tonmoy20244e4}.\n    *   **System Design or Architectural Innovations:** The paper's structured classification system (Figure 1) serves as an architectural innovation for organizing the field, distinguishing methods based on whether they involve model development or prompt engineering, and further sub-categorizing them \\cite{tonmoy20244e4}.\n    *   **Theoretical Insights or Analysis:** It synthesizes the essential features of various mitigation techniques, offering a structured understanding of their mechanisms and guiding future research \\cite{tonmoy20244e4}. It also deliberates on the limitations and challenges, proposing potential solutions and future directions \\cite{tonmoy20244e4}.\n\n*   **5. Experimental Validation**\n    *   As a survey paper, this work does not conduct its own experiments. Instead, it reports on the empirical validation and performance metrics of the *surveyed techniques* \\cite{tonmoy20244e4}. Examples include:\n        *   **D&Q framework:** Achieved competitive performance against GPT-3.5 on ChitChatQA and a 59.6% F1 score on HotPotQA \\cite{tonmoy20244e4}.\n        *   **EVER:** Outperformed both retrieval-based and non-retrieval-based baselines in tasks like short-form QA, biography generation, and multi-hop reasoning by detecting and rectifying hallucinations in real-time \\cite{tonmoy20244e4}.\n        *   **RARR:** Demonstrated the ability to enhance attribution in LLM outputs while preserving essential text properties \\cite{tonmoy20244e4}.\n        *   **High Entropy Word Spotting and Replacement:** Showed that `albert-large-v2` excels in detecting high entropy words, and `distilroberta-base` is superior in replacing them to reduce hallucinations \\cite{tonmoy20244e4}.\n        *   **End-to-End RAG:** Demonstrated enhanced performance on various knowledge-intensive tasks by jointly training the generator and retriever \\cite{tonmoy20244e4}.\n        *   **Self-Reflection Methodology:** Empirically proven effective, generalizable, and scalable in reducing hallucinations in medical generative QA systems through iterative feedback \\cite{tonmoy20244e4}.\n\n*   **6. Limitations & Scope**\n    *   **Technical Limitations/Assumptions (of surveyed techniques):**\n        *   The reliance on external knowledge bases (e.g., in RAG) introduces dependencies on the quality and coverage of those bases \\cite{tonmoy20244e4}.\n        *   Techniques requiring access to internal model states (e.g., logit outputs) are often limited by the black-box nature of commercial LLMs \\cite{tonmoy20244e4}.\n        *   The \"snowballing\" effect of hallucinations in multi-step reasoning remains a challenge for many approaches \\cite{tonmoy20244e4}.\n    *   **Scope of Applicability:** The survey covers a broad range of applications where LLMs are used, including academic research, programming, creative writing, technical advisement, and sensitive domains like medical and financial analysis \\cite{tonmoy20244e4}. The techniques discussed are applicable to various LLM architectures and tasks, categorized into prompt engineering and model development strategies \\cite{tonmoy20244e4}.\n\n*   **7. Technical Significance**\n    *   **Advancement of State-of-the-Art:** This paper significantly advances the technical state-of-the-art by providing the first comprehensive and systematically categorized overview of hallucination mitigation techniques in LLMs, including VLMs \\cite{tonmoy20244e4}. It synthesizes a vast and rapidly evolving body of work into an accessible framework.\n    *   **Potential Impact on Future Research:**\n        *   It offers a robust foundation for future research by clearly delineating existing methods, their strengths, and their weaknesses \\cite{tonmoy20244e4}.\n        *   By highlighting inherent challenges and limitations, it guides researchers toward critical unsolved problems and promising directions for developing more robust and reliable LLMs \\cite{tonmoy20244e4}.\n        *   The detailed taxonomy facilitates structured research endeavors, enabling better comparison and development of new mitigation strategies \\cite{tonmoy20244e4}.",
      "intriguing_abstract": "The pervasive issue of hallucination in Large Language Models (LLMs) poses a critical barrier to their safe and reliable deployment in sensitive real-world applications. Generating factually erroneous or ungrounded content, LLMs undermine trust and can lead to significant harm. This paper presents the first comprehensive survey of over thirty-two diverse hallucination mitigation techniques, including those for Vision Language Models (VLMs). We introduce a novel, systematic taxonomy that meticulously categorizes these methods based on parameters like dataset utilization, feedback mechanisms, and retriever types. Our framework organizes approaches into key strategies such as Prompt Engineering (e.g., Retrieval-Augmented Generation, Self-Refinement) and Developing Models (e.g., Decoding Strategy, Knowledge Graph integration, Supervised Finetuning). By synthesizing the strengths and limitations of existing solutions, this work provides a structured understanding of the current landscape. This survey not only offers a robust foundation for researchers but also highlights critical challenges and promising future directions, paving the way for more trustworthy and robust LLM systems.",
      "keywords": [
        "Large Language Models (LLMs)",
        "Hallucination mitigation",
        "Systematic taxonomy",
        "Comprehensive survey",
        "Retrieval-Augmented Generation (RAG)",
        "Prompt engineering",
        "Model development strategies",
        "Vision Language Models (VLMs)",
        "Real-world deployment challenges",
        "Black-box LLMs",
        "Snowballing effect",
        "Sensitive applications",
        "Future research directions"
      ],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/5272acad9e4201e93dabe3fd99bd7ead9b1a544d.pdf",
      "citation_key": "tonmoy20244e4",
      "metadata": {
        "title": "A Comprehensive Survey of Hallucination Mitigation Techniques in Large Language Models",
        "authors": [
          "S. Tonmoy",
          "S. M. M. Zaman",
          "Vinija Jain",
          "Anku Rani",
          "Vipula Rawte",
          "Aman Chadha",
          "Amitava Das"
        ],
        "published_date": "2024",
        "abstract": "As Large Language Models (LLMs) continue to advance in their ability to write human-like text, a key challenge remains around their tendency to hallucinate generating content that appears factual but is ungrounded. This issue of hallucination is arguably the biggest hindrance to safely deploying these powerful LLMs into real-world production systems that impact people's lives. The journey toward widespread adoption of LLMs in practical settings heavily relies on addressing and mitigating hallucinations. Unlike traditional AI systems focused on limited tasks, LLMs have been exposed to vast amounts of online text data during training. While this allows them to display impressive language fluency, it also means they are capable of extrapolating information from the biases in training data, misinterpreting ambiguous prompts, or modifying the information to align superficially with the input. This becomes hugely alarming when we rely on language generation capabilities for sensitive applications, such as summarizing medical records, financial analysis reports, etc. This paper presents a comprehensive survey of over 32 techniques developed to mitigate hallucination in LLMs. Notable among these are Retrieval Augmented Generation (Lewis et al, 2021), Knowledge Retrieval (Varshney et al,2023), CoNLI (Lei et al, 2023), and CoVe (Dhuliawala et al, 2023). Furthermore, we introduce a detailed taxonomy categorizing these methods based on various parameters, such as dataset utilization, common tasks, feedback mechanisms, and retriever types. This classification helps distinguish the diverse approaches specifically designed to tackle hallucination issues in LLMs. Additionally, we analyze the challenges and limitations inherent in these techniques, providing a solid foundation for future research in addressing hallucinations and related phenomena within the realm of LLMs.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/5272acad9e4201e93dabe3fd99bd7ead9b1a544d.pdf",
        "venue": "arXiv.org",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the paper \"A Comprehensive Survey of Hallucination Mitigation Techniques in Large Language Models\" by S.M Towhidul Islam Tonmoy et al. for a literature review:\n\n*   **1. Research Problem & Motivation**\n    *   **Specific Technical Problem:** Large Language Models (LLMs) frequently \"hallucinate,\" generating content that appears factual but is ungrounded, factually erroneous, or inconsistent with real-world knowledge \\cite{tonmoy20244e4}. This includes misinterpreting ambiguous prompts, extrapolating biases from training data, or modifying information.\n    *   **Importance and Challenge:**\n        *   Hallucination is the primary obstacle to safely deploying powerful LLMs in real-world production systems, especially in sensitive applications like medical records, financial analysis, or legal advice, where small errors can lead to significant harm \\cite{tonmoy20244e4}.\n        *   LLMs, despite their fluency, lack true comprehension, and their static training data makes them unable to adapt to evolving world knowledge, leading to outdated or false information \\cite{tonmoy20244e4}.\n        *   The issue can \"snowball\" in complex reasoning tasks, making outputs increasingly unreliable \\cite{tonmoy20244e4}.\n\n*   **2. Related Work & Positioning**\n    *   **Relation to Existing Approaches:** This paper is a comprehensive survey that consolidates and organizes over thirty-two diverse techniques developed to mitigate hallucination in LLMs \\cite{tonmoy20244e4}. It introduces a systematic taxonomy to categorize these methods.\n    *   **Limitations of Previous Solutions (as identified by the survey):**\n        *   Many existing methods rectify hallucinations *post-hoc* (after generation), which can be less effective than real-time detection and rectification during the generation process \\cite{tonmoy20244e4}.\n        *   The closed-source nature of many contemporary LLMs limits the applicability of techniques that require access to internal model outputs (e.g., logit values for high entropy word detection) \\cite{tonmoy20244e4}.\n        *   Traditional Retrieval-Augmented Generation (RAG) approaches often required building non-parametric memory architectures from scratch for specific tasks, contrasting with end-to-end RAG's use of pre-trained components \\cite{tonmoy20244e4}.\n        *   LLMs' inherent static knowledge base struggles with dynamic, evolving real-world information \\cite{tonmoy20244e4}.\n\n*   **3. Technical Approach & Innovation**\n    *   **Core Technical Method:** The paper's core approach is a systematic review and classification of existing hallucination mitigation techniques. It introduces a detailed taxonomy that categorizes methods based on parameters such as dataset utilization, common tasks, feedback mechanisms, and retriever types \\cite{tonmoy20244e4}.\n    *   **Novelty/Difference:**\n        *   It provides a comprehensive synthesis of over thirty-two techniques, including those for Vision Language Models (VLMs), which is broader than many existing reviews \\cite{tonmoy20244e4}.\n        *   The proposed taxonomy offers a structured framework for understanding and distinguishing diverse approaches to hallucination mitigation, organizing them into categories like Prompt Engineering (Retrieval Augmented Generation, Self-Refinement, Prompt Tuning) and Developing Models (Decoding Strategy, Knowledge Graph, Loss Function, Supervised Finetuning) \\cite{tonmoy20244e4}.\n        *   It analyzes the challenges and limitations inherent in these techniques, providing a foundation for future research \\cite{tonmoy20244e4}.\n\n*   **4. Key Technical Contributions**\n    *   **Novel Algorithms, Methods, or Techniques:** The primary contribution is not a new algorithm, but a novel *systematic taxonomy* for categorizing hallucination mitigation techniques in LLMs, encompassing VLMs \\cite{tonmoy20244e4}.\n    *   **System Design or Architectural Innovations:** The paper's structured classification system (Figure 1) serves as an architectural innovation for organizing the field, distinguishing methods based on whether they involve model development or prompt engineering, and further sub-categorizing them \\cite{tonmoy20244e4}.\n    *   **Theoretical Insights or Analysis:** It synthesizes the essential features of various mitigation techniques, offering a structured understanding of their mechanisms and guiding future research \\cite{tonmoy20244e4}. It also deliberates on the limitations and challenges, proposing potential solutions and future directions \\cite{tonmoy20244e4}.\n\n*   **5. Experimental Validation**\n    *   As a survey paper, this work does not conduct its own experiments. Instead, it reports on the empirical validation and performance metrics of the *surveyed techniques* \\cite{tonmoy20244e4}. Examples include:\n        *   **D&Q framework:** Achieved competitive performance against GPT-3.5 on ChitChatQA and a 59.6% F1 score on HotPotQA \\cite{tonmoy20244e4}.\n        *   **EVER:** Outperformed both retrieval-based and non-retrieval-based baselines in tasks like short-form QA, biography generation, and multi-hop reasoning by detecting and rectifying hallucinations in real-time \\cite{tonmoy20244e4}.\n        *   **RARR:** Demonstrated the ability to enhance attribution in LLM outputs while preserving essential text properties \\cite{tonmoy20244e4}.\n        *   **High Entropy Word Spotting and Replacement:** Showed that `albert-large-v2` excels in detecting high entropy words, and `distilroberta-base` is superior in replacing them to reduce hallucinations \\cite{tonmoy20244e4}.\n        *   **End-to-End RAG:** Demonstrated enhanced performance on various knowledge-intensive tasks by jointly training the generator and retriever \\cite{tonmoy20244e4}.\n        *   **Self-Reflection Methodology:** Empirically proven effective, generalizable, and scalable in reducing hallucinations in medical generative QA systems through iterative feedback \\cite{tonmoy20244e4}.\n\n*   **6. Limitations & Scope**\n    *   **Technical Limitations/Assumptions (of surveyed techniques):**\n        *   The reliance on external knowledge bases (e.g., in RAG) introduces dependencies on the quality and coverage of those bases \\cite{tonmoy20244e4}.\n        *   Techniques requiring access to internal model states (e.g., logit outputs) are often limited by the black-box nature of commercial LLMs \\cite{tonmoy20244e4}.\n        *   The \"snowballing\" effect of hallucinations in multi-step reasoning remains a challenge for many approaches \\cite{tonmoy20244e4}.\n    *   **Scope of Applicability:** The survey covers a broad range of applications where LLMs are used, including academic research, programming, creative writing, technical advisement, and sensitive domains like medical and financial analysis \\cite{tonmoy20244e4}. The techniques discussed are applicable to various LLM architectures and tasks, categorized into prompt engineering and model development strategies \\cite{tonmoy20244e4}.\n\n*   **7. Technical Significance**\n    *   **Advancement of State-of-the-Art:** This paper significantly advances the technical state-of-the-art by providing the first comprehensive and systematically categorized overview of hallucination mitigation techniques in LLMs, including VLMs \\cite{tonmoy20244e4}. It synthesizes a vast and rapidly evolving body of work into an accessible framework.\n    *   **Potential Impact on Future Research:**\n        *   It offers a robust foundation for future research by clearly delineating existing methods, their strengths, and their weaknesses \\cite{tonmoy20244e4}.\n        *   By highlighting inherent challenges and limitations, it guides researchers toward critical unsolved problems and promising directions for developing more robust and reliable LLMs \\cite{tonmoy20244e4}.\n        *   The detailed taxonomy facilitates structured research endeavors, enabling better comparison and development of new mitigation strategies \\cite{tonmoy20244e4}.",
        "keywords": [
          "Large Language Models (LLMs)",
          "Hallucination mitigation",
          "Systematic taxonomy",
          "Comprehensive survey",
          "Retrieval-Augmented Generation (RAG)",
          "Prompt engineering",
          "Model development strategies",
          "Vision Language Models (VLMs)",
          "Real-world deployment challenges",
          "Black-box LLMs",
          "Snowballing effect",
          "Sensitive applications",
          "Future research directions"
        ],
        "paper_type": "based on the provided abstract and introduction, the paper type is:\n\n**survey**\n\n**reasoning:**\n\n*   the **title** explicitly states: \"a comprehensive survey of hallucination mitigation techniques in large language models.\"\n*   the **abstract** directly mentions: \"this paper presents a comprehensive survey of over thirty-two techniques developed to mitigate hallucination in llms.\" it also discusses introducing \"a detailed taxonomy categorizing these methods\" and analyzing \"challenges and limitations inherent in these techniques,\" which are all hallmarks of a survey paper.\n*   the **introduction** sets the stage by discussing the problem of hallucination and noting that \"researchers have proposed various strategies,\" indicating that the paper will review these existing strategies.\n\nthese points align perfectly with the criteria for a \"survey\" paper, which reviews existing literature comprehensively and often discusses classification schemes and future directions."
      },
      "file_name": "5272acad9e4201e93dabe3fd99bd7ead9b1a544d.pdf"
    },
    {
      "success": true,
      "doc_id": "fb22f15cf366e791a0aae450098875df",
      "summary": "Here's a focused summary of the paper \"NoiseBoost: Alleviating Hallucination with Noise Perturbation for Multimodal Large Language Models\" \\cite{wu2024n00} for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Multimodal Large Language Models (MLLMs) suffer from hallucinations, particularly when generating lengthy, detailed image descriptions.\n    *   **Importance & Challenge**: Hallucinations limit MLLMs' applicability in real-world scenarios. The problem stems from MLLMs' inherent summarization mechanism, leading to an excessive dependence on linguistic tokens and neglect of visual information. This over-reliance on language priors is exacerbated by the feature disparity between separately pre-trained visual encoders and LLMs, worsening with longer generation lengths.\n\n*   **Related Work & Positioning**\n    *   **Existing Approaches**: Previous work on mitigating hallucination primarily focuses on developing tailored decoders (e.g., OPERA \\cite{wu2024n00}, visual contrastive decoding \\cite{wu2024n00}) or annotating hallucination-specific data (e.g., HallDoctor \\cite{wu2024n00}, Fine-grained PPO \\cite{wu2024n00}).\n    *   **Limitations of Previous Solutions**: Re-decoding methods significantly increase inference time (doubling or tripling), making MLLMs challenging for deployment on personal devices. Manually curated reward datasets for training are often limited in scale, differ in distribution from real-world usage, and cannot encompass all scenarios, contradicting the scaling law for large models.\n    *   **Positioning**: \\cite{wu2024n00} aims to address the fundamental cause of hallucination by enhancing MLLM training *without* introducing additional datasets, complex decoder schemes, or incurring significant inference/training costs.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes **NoiseBoost**, a simple and broadly applicable method that integrates noise feature perturbations into MLLMs.\n    *   **Mechanism**: NoiseBoost injects Gaussian noise into the *projected visual tokens*. This perturbation increases the \"hardship\" in visual understanding, compelling the MLLM to distribute its attention weights more evenly between visual and linguistic tokens, thereby reducing over-reliance on language priors. It acts as a regularizer.\n    *   **Novelty**:\n        *   It's a simple, generalizable method applied directly to visual features, avoiding complex architectural changes or iterative decoding.\n        *   It's pioneer in enabling semi-supervised learning (SSL) for MLLMs by using noise perturbation to create a \"noisy student\" for consistency regularization, addressing the lack of visual augmentation methods for MLLMs.\n    *   **Application across Training Stages**:\n        *   **Supervised Fine-tuning (SFT)**: Gaussian noise is directly added to projected visual tokens during training.\n        *   **Reinforcement Learning (RL)** (e.g., DPO): Noise perturbation is primarily applied to visual tokens associated with *preferred* responses ($y_w$) to make learning more challenging for desired outcomes, while less/no noise is applied to less preferred responses ($y_l$).\n        *   **Semi-Supervised Learning (SSL)**: A frozen MLLM acts as a teacher to generate pseudo labels from original images. NoiseBoost then perturbs the visual features for the student model, which learns to produce consistent and robust results, enabling the use of unlabeled data.\n\n*   **Key Technical Contributions**\n    *   **Novel Algorithm/Method**: Introduction of NoiseBoost, a noise feature perturbation method that effectively alleviates hallucination in MLLMs by balancing attention weights between visual and linguistic tokens.\n    *   **Training Paradigm Innovation**: Pioneering the integration of semi-supervised learning for MLLMs using NoiseBoost, enabling the effective utilization of unlabeled data.\n    *   **Generalizability**: Demonstrating NoiseBoost as a consistent performance enhancer across various MLLM training strategies (SFT, RL) and different MLLM backbones (LLaVA-1.5, QwenVL).\n\n*   **Experimental Validation**\n    *   **Experiments Conducted**: Evaluated NoiseBoost on state-of-the-art MLLMs, LLaVA-1.5 (with Llama-7B/13B backbones) and QwenVL (Qwen-7B). Experiments covered Supervised Fine-tuning, Reinforcement Learning, and Semi-Supervised Learning.\n    *   **Key Performance Metrics & Results**:\n        *   **SFT**: NoiseBoost consistently improved performance across hallucination datasets (POPE) and question-answer datasets (GQA, VixWiz, Text-VQA, MME, SEED-Bench), showing gains exceeding 1% on most. Human evaluation of dense captions on 1k images showed an **8.1% improvement in accuracy**.\n        *   **RL**: Demonstrated consistent performance gains across all evaluated datasets (POPE, GQA, VixWiz, Text-VQA, MME, SEED-Bench) when integrated with DPO.\n        *   **SSL**: Achieved comparable performance with only **50% of the labeled data** by effectively mining unlabeled data.\n        *   All improvements were achieved with **negligible additional training costs**.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations**: The noise scale (0.5) and perturbation chance (50%) were set without extensive tuning, suggesting potential for further optimization. The HA-DPO dataset for RL was limited in scale. Automated evaluation for QwenVL was suboptimal due to prompt sensitivity, leading to reliance on human evaluation for this model.\n    *   **Scope of Applicability**: NoiseBoost is primarily designed to mitigate hallucination by addressing attention distribution in MLLMs. It is applicable to common MLLM training paradigms including SFT, RL, and SSL.\n\n*   **Technical Significance**\n    *   **Advancement of State-of-the-Art**: NoiseBoost offers a simple, broadly applicable, and cost-effective solution to a critical problem (hallucination) in MLLMs, advancing their reliability without complex architectural changes or expensive data annotation.\n    *   **Potential Impact on Future Research**: By enabling semi-supervised learning for MLLMs, NoiseBoost opens new avenues for leveraging vast amounts of unlabeled data, potentially reducing the reliance on labor-intensive data cleaning and labeling. This could lead to more robust, honest, and deployable MLLMs for real-world applications.",
      "intriguing_abstract": "Multimodal Large Language Models (MLLMs) are revolutionizing AI, yet their pervasive tendency to \"hallucinate\"â€”generating factually incorrect or non-existent details, particularly in lengthy descriptionsâ€”severely limits their real-world deployment. Current mitigation strategies often incur prohibitive inference costs or rely on scarce, manually annotated data. We introduce **NoiseBoost**, a remarkably simple yet profoundly effective noise feature perturbation method designed to fundamentally address MLLM hallucination. By strategically injecting Gaussian noise into projected visual tokens during training, NoiseBoost compels MLLMs to distribute attention more evenly between visual and linguistic priors, thereby enhancing visual grounding and reducing over-reliance on language.\n\nNoiseBoost is highly generalizable, demonstrating consistent performance gains across Supervised Fine-tuning (SFT) and Reinforcement Learning (RL) paradigms, including DPO, and various MLLM backbones like LLaVA-1.5 and QwenVL. Crucially, it pioneers semi-supervised learning (SSL) for MLLMs, enabling robust performance with just 50% of labeled data by leveraging unlabeled visual information. Our experiments show significant improvements, including an **8.1% accuracy boost** in human evaluations for dense captioning, all with negligible additional training cost. NoiseBoost paves the way for more reliable, honest, and deployable MLLMs, unlocking their full potential in diverse applications.",
      "keywords": [
        "Multimodal Large Language Models (MLLMs)",
        "Hallucination mitigation",
        "NoiseBoost",
        "Noise perturbation",
        "Projected visual tokens",
        "Language priors",
        "Attention weights balancing",
        "Semi-supervised learning (SSL) for MLLMs",
        "Supervised Fine-tuning (SFT)",
        "Reinforcement Learning (RL)",
        "Consistency regularization",
        "Cost-effective solution",
        "Generalizable method",
        "Unlabeled data utilization"
      ],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/2126e045f81b831da34c185e2b51a49194bf4aa4.pdf",
      "citation_key": "wu2024n00",
      "metadata": {
        "title": "NoiseBoost: Alleviating Hallucination with Noise Perturbation for Multimodal Large Language Models",
        "authors": [
          "Kai Wu",
          "Boyuan Jiang",
          "Zhengkai Jiang",
          "Qingdong He",
          "Donghao Luo",
          "Shengzhi Wang",
          "Qingwen Liu",
          "Chengjie Wang"
        ],
        "published_date": "2024",
        "abstract": "Multimodal large language models (MLLMs) contribute a powerful mechanism to understanding visual information building on large language models. However, MLLMs are notorious for suffering from hallucinations, especially when generating lengthy, detailed descriptions for images. Our analysis reveals that hallucinations stem from the inherent summarization mechanism of large language models, leading to excessive dependence on linguistic tokens while neglecting vision information. In this paper, we propose NoiseBoost, a broadly applicable and simple method for alleviating hallucinations for MLLMs through the integration of noise feature perturbations. Noise perturbation acts as a regularizer, facilitating a balanced distribution of attention weights among visual and linguistic tokens. Despite its simplicity, NoiseBoost consistently enhances the performance of MLLMs across common training strategies, including supervised fine-tuning and reinforcement learning. Further, NoiseBoost pioneerly enables semi-supervised learning for MLLMs, unleashing the power of unlabeled data. Comprehensive experiments demonstrate that NoiseBoost improves dense caption accuracy by 8.1% with human evaluation and achieves comparable results with 50% of the data by mining unlabeled data. Code and models are available at https://kaiwu5.github.io/noiseboost.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/2126e045f81b831da34c185e2b51a49194bf4aa4.pdf",
        "venue": "arXiv.org",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the paper \"NoiseBoost: Alleviating Hallucination with Noise Perturbation for Multimodal Large Language Models\" \\cite{wu2024n00} for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Multimodal Large Language Models (MLLMs) suffer from hallucinations, particularly when generating lengthy, detailed image descriptions.\n    *   **Importance & Challenge**: Hallucinations limit MLLMs' applicability in real-world scenarios. The problem stems from MLLMs' inherent summarization mechanism, leading to an excessive dependence on linguistic tokens and neglect of visual information. This over-reliance on language priors is exacerbated by the feature disparity between separately pre-trained visual encoders and LLMs, worsening with longer generation lengths.\n\n*   **Related Work & Positioning**\n    *   **Existing Approaches**: Previous work on mitigating hallucination primarily focuses on developing tailored decoders (e.g., OPERA \\cite{wu2024n00}, visual contrastive decoding \\cite{wu2024n00}) or annotating hallucination-specific data (e.g., HallDoctor \\cite{wu2024n00}, Fine-grained PPO \\cite{wu2024n00}).\n    *   **Limitations of Previous Solutions**: Re-decoding methods significantly increase inference time (doubling or tripling), making MLLMs challenging for deployment on personal devices. Manually curated reward datasets for training are often limited in scale, differ in distribution from real-world usage, and cannot encompass all scenarios, contradicting the scaling law for large models.\n    *   **Positioning**: \\cite{wu2024n00} aims to address the fundamental cause of hallucination by enhancing MLLM training *without* introducing additional datasets, complex decoder schemes, or incurring significant inference/training costs.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes **NoiseBoost**, a simple and broadly applicable method that integrates noise feature perturbations into MLLMs.\n    *   **Mechanism**: NoiseBoost injects Gaussian noise into the *projected visual tokens*. This perturbation increases the \"hardship\" in visual understanding, compelling the MLLM to distribute its attention weights more evenly between visual and linguistic tokens, thereby reducing over-reliance on language priors. It acts as a regularizer.\n    *   **Novelty**:\n        *   It's a simple, generalizable method applied directly to visual features, avoiding complex architectural changes or iterative decoding.\n        *   It's pioneer in enabling semi-supervised learning (SSL) for MLLMs by using noise perturbation to create a \"noisy student\" for consistency regularization, addressing the lack of visual augmentation methods for MLLMs.\n    *   **Application across Training Stages**:\n        *   **Supervised Fine-tuning (SFT)**: Gaussian noise is directly added to projected visual tokens during training.\n        *   **Reinforcement Learning (RL)** (e.g., DPO): Noise perturbation is primarily applied to visual tokens associated with *preferred* responses ($y_w$) to make learning more challenging for desired outcomes, while less/no noise is applied to less preferred responses ($y_l$).\n        *   **Semi-Supervised Learning (SSL)**: A frozen MLLM acts as a teacher to generate pseudo labels from original images. NoiseBoost then perturbs the visual features for the student model, which learns to produce consistent and robust results, enabling the use of unlabeled data.\n\n*   **Key Technical Contributions**\n    *   **Novel Algorithm/Method**: Introduction of NoiseBoost, a noise feature perturbation method that effectively alleviates hallucination in MLLMs by balancing attention weights between visual and linguistic tokens.\n    *   **Training Paradigm Innovation**: Pioneering the integration of semi-supervised learning for MLLMs using NoiseBoost, enabling the effective utilization of unlabeled data.\n    *   **Generalizability**: Demonstrating NoiseBoost as a consistent performance enhancer across various MLLM training strategies (SFT, RL) and different MLLM backbones (LLaVA-1.5, QwenVL).\n\n*   **Experimental Validation**\n    *   **Experiments Conducted**: Evaluated NoiseBoost on state-of-the-art MLLMs, LLaVA-1.5 (with Llama-7B/13B backbones) and QwenVL (Qwen-7B). Experiments covered Supervised Fine-tuning, Reinforcement Learning, and Semi-Supervised Learning.\n    *   **Key Performance Metrics & Results**:\n        *   **SFT**: NoiseBoost consistently improved performance across hallucination datasets (POPE) and question-answer datasets (GQA, VixWiz, Text-VQA, MME, SEED-Bench), showing gains exceeding 1% on most. Human evaluation of dense captions on 1k images showed an **8.1% improvement in accuracy**.\n        *   **RL**: Demonstrated consistent performance gains across all evaluated datasets (POPE, GQA, VixWiz, Text-VQA, MME, SEED-Bench) when integrated with DPO.\n        *   **SSL**: Achieved comparable performance with only **50% of the labeled data** by effectively mining unlabeled data.\n        *   All improvements were achieved with **negligible additional training costs**.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations**: The noise scale (0.5) and perturbation chance (50%) were set without extensive tuning, suggesting potential for further optimization. The HA-DPO dataset for RL was limited in scale. Automated evaluation for QwenVL was suboptimal due to prompt sensitivity, leading to reliance on human evaluation for this model.\n    *   **Scope of Applicability**: NoiseBoost is primarily designed to mitigate hallucination by addressing attention distribution in MLLMs. It is applicable to common MLLM training paradigms including SFT, RL, and SSL.\n\n*   **Technical Significance**\n    *   **Advancement of State-of-the-Art**: NoiseBoost offers a simple, broadly applicable, and cost-effective solution to a critical problem (hallucination) in MLLMs, advancing their reliability without complex architectural changes or expensive data annotation.\n    *   **Potential Impact on Future Research**: By enabling semi-supervised learning for MLLMs, NoiseBoost opens new avenues for leveraging vast amounts of unlabeled data, potentially reducing the reliance on labor-intensive data cleaning and labeling. This could lead to more robust, honest, and deployable MLLMs for real-world applications.",
        "keywords": [
          "Multimodal Large Language Models (MLLMs)",
          "Hallucination mitigation",
          "NoiseBoost",
          "Noise perturbation",
          "Projected visual tokens",
          "Language priors",
          "Attention weights balancing",
          "Semi-supervised learning (SSL) for MLLMs",
          "Supervised Fine-tuning (SFT)",
          "Reinforcement Learning (RL)",
          "Consistency regularization",
          "Cost-effective solution",
          "Generalizable method",
          "Unlabeled data utilization"
        ],
        "paper_type": "based on the abstract and introduction:\n\n1.  **problem identification:** the paper identifies a significant problem: hallucinations in multimodal large language models (mllms).\n2.  **proposed solution:** the abstract explicitly states: \"in this paper, we **propose noiseboost**, a broadly applicable and simple **method** for alleviating hallucinations for mllms through the integration of noise feature perturbations.\" this is a clear indicator of presenting a new method or system.\n3.  **mechanism explanation:** it explains how noiseboost works (\"noise perturbation acts as a regularizer, facilitating a balanced distribution of attention weights...\").\n4.  **empirical validation:** the abstract also mentions \"comprehensive **experiments demonstrate** that noiseboost improves dense caption accuracy by 8.1% with human evaluation...\" this indicates a strong empirical component.\n\nwhile the paper includes significant empirical evaluation, its core contribution is the **proposal and development of a new method (noiseboost)** to solve a technical problem. the experiments serve to validate this new method. therefore, the primary classification is **technical**.\n\n**classification:** technical"
      },
      "file_name": "2126e045f81b831da34c185e2b51a49194bf4aa4.pdf"
    },
    {
      "success": true,
      "doc_id": "f63fb503943f9694263394e65923bd7c",
      "summary": "Here's a focused summary of the paper for a literature review:\n\n### MindMap : Knowledge Graph Prompting Sparks Graph of Thoughts in Large Language Models \\cite{wen2023t6v}\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Large Language Models (LLMs) suffer from inflexibility (outdated knowledge, costly updates, catastrophic forgetting), hallucination (generating plausible but incorrect information), and lack of transparency (black-box nature, implicit knowledge).\n    *   **Importance and Challenge**: These limitations hinder LLMs' deployment in high-stakes applications, such as medical diagnosis, where factual correctness, explainability, and up-to-date knowledge are critical. Knowledge Graphs (KGs) offer explicit, interpretable, and modifiable knowledge, making them a promising complement to LLMs.\n\n*   **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: This work builds upon prompt engineering and KG-augmented LLM research.\n    *   **Limitations of Previous Solutions**:\n        *   **Retrieval-Augmented LLM Inference**: Document retrieval suffers from inaccurate retrieval and lengthy documents that LLMs struggle to process effectively, often leading to hallucinations. KG retrieval approaches often treat KG inputs as plain text, ignoring their graphical structure, making responses hard to validate and vulnerable to hallucinations.\n        *   **Graph Mining with LLMs**: Prior work primarily focused on graph mining tasks (e.g., edge detection, summarization) rather than complex text generation requiring multi-evidence reasoning grounded in KGs.\n        *   **KG Integration into LLM Pre-training**: While effective, this is intractable for web-scale KGs and doesn't fundamentally mitigate LLM limits in flexibility, reliability, or transparency for fixed, pre-trained models.\n        *   **Prompting for Implicit Reasoning**: Methods like Chain-of-Thought or Tree-of-Thought focus on eliciting LLM's implicit knowledge, but don't integrate external structured knowledge effectively.\n    *   **Positioning**: MindMap distinguishes itself by focusing on synergistic inference between fixed, powerful LLMs (like commercial APIs) and KGs, enabling LLMs to comprehend graphical inputs and reason over a \"mind map\" that combines explicit KG facts and implicit LLM knowledge, rather than just rephrasing retrieved text.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method**: MindMap is a novel prompting pipeline comprising three main components:\n        1.  **Evidence Graph Mining**:\n            *   **Entity Recognition**: Uses an LLM to identify key entities from the input query.\n            *   **Evidence Sub-graphs Exploration**: Queries a source KG to build two types of sub-graphs:\n                *   *Path-based exploration*: Traces intermediary paths (up to `k` hops) to connect important query entities.\n                *   *Neighbor-based exploration*: Expands each query-related node by 1-hop to its neighbors.\n            *   **Pruning**: Clusters and samples sub-graphs to manage information overhead and maintain diversity.\n        2.  **Evidence Graph Aggregation**: An LLM is prompted to comprehend and aggregate the retrieved evidence sub-graphs. Each subgraph is formatted as an entity chain (e.g., \"(Fatigue, Nausea) - IsSymptomOf - LiverProblem\") and then converted into a natural language description, forming a unified reasoning graph (`Gm`). This leverages LLM's NLU/NLG to unify semantically similar entities and resolve ambiguities.\n        3.  **LLM Reasoning on the Mind Map**: LLMs are prompted with the aggregated reasoning graphs (`Gm`) to produce final outputs. This involves:\n            *   **Graph-of-Thought Instruction**: Guides LLMs to comprehend and enhance the input, build their own \"mind map\" for reasoning, and index knowledge sources.\n            *   **Synergistic Inference**: LLMs consolidate the built reasoning graph and their implicit knowledge to generate answers, an inference process, and a visual \"mind map\" (decision tree) explaining the reasoning pathways.\n    *   **Novelty/Difference**: MindMap's novelty lies in enabling LLMs to *comprehend and reason over structured graphical inputs* (not just plain text representations of KGs), *synergistically combine explicit KG knowledge with implicit LLM knowledge* (including enhancement when KG is inaccurate), and *elicit a transparent \"mind map\" of the reasoning process*. This moves beyond simple retrieval-augmentation to true knowledge integration and explainable reasoning.\n\n*   **Key Technical Contributions**\n    *   **Novel Prompting Pipeline**: Introduction of MindMap, a three-stage pipeline for integrating KGs with LLMs for enhanced inference and transparency.\n    *   **Structured KG Input Comprehension**: A method for LLMs to effectively process and aggregate structured evidence sub-graphs from KGs, converting them into a natural language reasoning graph.\n    *   **Eliciting Graph-of-Thoughts Reasoning**: A mechanism to prompt LLMs to build an internal \"mind map\" that consolidates retrieved facts and implicit knowledge, discovers new patterns, and reasons over this map.\n    *   **Synergistic Knowledge Inference**: Enabling LLMs to perform \"knowledge enhancement\" by leveraging their implicit knowledge to expand, connect, and improve information, especially when external KG input is inaccurate.\n    *   **Transparency through Mind Map Generation**: Generating an explicit \"mind map\" (decision tree) that reveals the LLM's reasoning pathways, grounded in both KG evidence and LLM's internal knowledge.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted**: Evaluated on diverse question-answering tasks requiring sophisticated reasoning and domain knowledge, specifically in medical domains.\n    *   **Datasets**: Three medical Q&A datasets: GenMedGPT-5k (English clinical Q&A), CMCQA (Chinese long dialogue), and ExplainCPE (Chinese 5-way choice questions, including knowledge mismatches to test robustness).\n    *   **Knowledge Graphs**: EMCKG and CMCKG, constructed for medical concepts.\n    *   **Baselines**: Vanilla GPT-3.5 and GPT-4, Tree-of-Thought (TOT), and three retrieval-augmented baselines (BM25 Retriever, Text Embedding Retriever, KG Retriever), all using `gpt-3.5-turbo-0613` as backbone for retrieval methods.\n    *   **Key Performance Metrics**: BERTScore (Precision, Recall, F1) for semantic similarity and GPT-4 Rating (average rank, assessing quality, factual correctness, diversity, integrity) for human-like evaluation. A \"Hallucination Quantify\" metric was also used.\n    *   **Comparison Results**: MindMap consistently achieved significant improvements over all baselines across all datasets and metrics. For GenMedGPT-5k, MindMap showed the highest BERTScore F1 (0.7954) and the best GPT-4 Ranking (1.8725 average), indicating superior answer quality and factual correctness. It also demonstrated robustness to incorrect retrieval knowledge (on ExplainCPE) and reduced hallucinations compared to baselines.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations**: The paper does not explicitly state technical limitations of MindMap itself, but rather positions it as a solution to existing LLM limitations. Implicitly, the quality of the initial entity recognition and the underlying KG are crucial. The complexity of constructing and maintaining high-quality KGs for diverse domains remains a challenge.\n    *   **Scope of Applicability**: Primarily validated in medical question-answering tasks. While the framework is general, its effectiveness might vary in domains where KGs are less structured or comprehensive. The reliance on LLM for entity recognition and aggregation could introduce its own set of biases or errors.\n\n*   **Technical Significance**\n    *   **Advancement of State-of-the-Art**: MindMap significantly advances the state-of-the-art in LLM-KG integration by enabling LLMs to move beyond simple fact retrieval to perform synergistic, graph-of-thoughts reasoning. It addresses critical issues of hallucination and transparency in LLMs.\n    *   **Potential Impact**: This work paves the way for more reliable, explainable, and knowledge-grounded LLM applications, particularly in high-stakes domains like healthcare. It highlights a promising direction for future research in combining the powerful reasoning capabilities of LLMs with the structured, verifiable knowledge of KGs.",
      "intriguing_abstract": "Large Language Models (LLMs) are revolutionizing AI, yet their widespread deployment in high-stakes domains like medical diagnosis is hindered by persistent challenges: factual hallucination, knowledge inflexibility, and a critical lack of transparency. We introduce **MindMap**, a novel prompting pipeline that fundamentally transforms LLM-Knowledge Graph (KG) integration, moving beyond simple retrieval to enable true synergistic reasoning.\n\nMindMap empowers LLMs to *comprehend and reason directly over structured graphical inputs*, rather than treating KGs as mere text. Through intelligent evidence graph mining and aggregation, our method constructs a unified \"mind map\" that guides LLM inference. This unique approach elicits a \"Graph-of-Thought\" reasoning process, where LLMs synergistically combine explicit KG facts with their vast implicit knowledge, even performing knowledge enhancement when external data is incomplete. Crucially, MindMap generates an explicit, visual \"mind map\" (decision tree) that transparently reveals the LLM's reasoning pathways. Evaluated on complex medical Q&A tasks, MindMap consistently achieves significant improvements over state-of-the-art baselines, demonstrating superior factual correctness, robustness to noisy data, and drastically reduced hallucination. This work paves the way for reliable, explainable, and knowledge-grounded LLM applications essential for critical decision-making.",
      "keywords": [
        "MindMap",
        "Knowledge Graph Prompting",
        "Large Language Models (LLMs)",
        "Graph-of-Thoughts Reasoning",
        "Synergistic Knowledge Inference",
        "Structured KG Input Comprehension",
        "Evidence Graph Mining",
        "Reasoning Graph Aggregation",
        "Explainable AI",
        "Hallucination Reduction",
        "Medical Question-Answering",
        "Factual Correctness",
        "Decision Tree Visualization"
      ],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/ca261cb681b082e90ca6c7a9d325b4265ed1dc28.pdf",
      "citation_key": "wen2023t6v",
      "metadata": {
        "title": "MindMap: Knowledge Graph Prompting Sparks Graph of Thoughts in Large Language Models",
        "authors": [
          "Yilin Wen",
          "Zifeng Wang",
          "Jimeng Sun"
        ],
        "published_date": "2023",
        "abstract": "Large language models (LLMs) have achieved remarkable performance in natural language understanding and generation tasks. However, they often suffer from limitations such as difficulty in incorporating new knowledge, generating hallucinations, and explaining their reasoning process. To address these challenges, we propose a novel prompting pipeline, named \\method, that leverages knowledge graphs (KGs) to enhance LLMs' inference and transparency. Our method enables LLMs to comprehend KG inputs and infer with a combination of implicit and external knowledge. Moreover, our method elicits the mind map of LLMs, which reveals their reasoning pathways based on the ontology of knowledge. We evaluate our method on diverse question \\&answering tasks, especially in medical domains, and show significant improvements over baselines. We also introduce a new hallucination evaluation benchmark and analyze the effects of different components of our method. Our results demonstrate the effectiveness and robustness of our method in merging knowledge from LLMs and KGs for combined inference. To reproduce our results and extend the framework further, we make our codebase available at https://github.com/wyl-willing/MindMap.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/ca261cb681b082e90ca6c7a9d325b4265ed1dc28.pdf",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the paper for a literature review:\n\n### MindMap : Knowledge Graph Prompting Sparks Graph of Thoughts in Large Language Models \\cite{wen2023t6v}\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Large Language Models (LLMs) suffer from inflexibility (outdated knowledge, costly updates, catastrophic forgetting), hallucination (generating plausible but incorrect information), and lack of transparency (black-box nature, implicit knowledge).\n    *   **Importance and Challenge**: These limitations hinder LLMs' deployment in high-stakes applications, such as medical diagnosis, where factual correctness, explainability, and up-to-date knowledge are critical. Knowledge Graphs (KGs) offer explicit, interpretable, and modifiable knowledge, making them a promising complement to LLMs.\n\n*   **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: This work builds upon prompt engineering and KG-augmented LLM research.\n    *   **Limitations of Previous Solutions**:\n        *   **Retrieval-Augmented LLM Inference**: Document retrieval suffers from inaccurate retrieval and lengthy documents that LLMs struggle to process effectively, often leading to hallucinations. KG retrieval approaches often treat KG inputs as plain text, ignoring their graphical structure, making responses hard to validate and vulnerable to hallucinations.\n        *   **Graph Mining with LLMs**: Prior work primarily focused on graph mining tasks (e.g., edge detection, summarization) rather than complex text generation requiring multi-evidence reasoning grounded in KGs.\n        *   **KG Integration into LLM Pre-training**: While effective, this is intractable for web-scale KGs and doesn't fundamentally mitigate LLM limits in flexibility, reliability, or transparency for fixed, pre-trained models.\n        *   **Prompting for Implicit Reasoning**: Methods like Chain-of-Thought or Tree-of-Thought focus on eliciting LLM's implicit knowledge, but don't integrate external structured knowledge effectively.\n    *   **Positioning**: MindMap distinguishes itself by focusing on synergistic inference between fixed, powerful LLMs (like commercial APIs) and KGs, enabling LLMs to comprehend graphical inputs and reason over a \"mind map\" that combines explicit KG facts and implicit LLM knowledge, rather than just rephrasing retrieved text.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method**: MindMap is a novel prompting pipeline comprising three main components:\n        1.  **Evidence Graph Mining**:\n            *   **Entity Recognition**: Uses an LLM to identify key entities from the input query.\n            *   **Evidence Sub-graphs Exploration**: Queries a source KG to build two types of sub-graphs:\n                *   *Path-based exploration*: Traces intermediary paths (up to `k` hops) to connect important query entities.\n                *   *Neighbor-based exploration*: Expands each query-related node by 1-hop to its neighbors.\n            *   **Pruning**: Clusters and samples sub-graphs to manage information overhead and maintain diversity.\n        2.  **Evidence Graph Aggregation**: An LLM is prompted to comprehend and aggregate the retrieved evidence sub-graphs. Each subgraph is formatted as an entity chain (e.g., \"(Fatigue, Nausea) - IsSymptomOf - LiverProblem\") and then converted into a natural language description, forming a unified reasoning graph (`Gm`). This leverages LLM's NLU/NLG to unify semantically similar entities and resolve ambiguities.\n        3.  **LLM Reasoning on the Mind Map**: LLMs are prompted with the aggregated reasoning graphs (`Gm`) to produce final outputs. This involves:\n            *   **Graph-of-Thought Instruction**: Guides LLMs to comprehend and enhance the input, build their own \"mind map\" for reasoning, and index knowledge sources.\n            *   **Synergistic Inference**: LLMs consolidate the built reasoning graph and their implicit knowledge to generate answers, an inference process, and a visual \"mind map\" (decision tree) explaining the reasoning pathways.\n    *   **Novelty/Difference**: MindMap's novelty lies in enabling LLMs to *comprehend and reason over structured graphical inputs* (not just plain text representations of KGs), *synergistically combine explicit KG knowledge with implicit LLM knowledge* (including enhancement when KG is inaccurate), and *elicit a transparent \"mind map\" of the reasoning process*. This moves beyond simple retrieval-augmentation to true knowledge integration and explainable reasoning.\n\n*   **Key Technical Contributions**\n    *   **Novel Prompting Pipeline**: Introduction of MindMap, a three-stage pipeline for integrating KGs with LLMs for enhanced inference and transparency.\n    *   **Structured KG Input Comprehension**: A method for LLMs to effectively process and aggregate structured evidence sub-graphs from KGs, converting them into a natural language reasoning graph.\n    *   **Eliciting Graph-of-Thoughts Reasoning**: A mechanism to prompt LLMs to build an internal \"mind map\" that consolidates retrieved facts and implicit knowledge, discovers new patterns, and reasons over this map.\n    *   **Synergistic Knowledge Inference**: Enabling LLMs to perform \"knowledge enhancement\" by leveraging their implicit knowledge to expand, connect, and improve information, especially when external KG input is inaccurate.\n    *   **Transparency through Mind Map Generation**: Generating an explicit \"mind map\" (decision tree) that reveals the LLM's reasoning pathways, grounded in both KG evidence and LLM's internal knowledge.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted**: Evaluated on diverse question-answering tasks requiring sophisticated reasoning and domain knowledge, specifically in medical domains.\n    *   **Datasets**: Three medical Q&A datasets: GenMedGPT-5k (English clinical Q&A), CMCQA (Chinese long dialogue), and ExplainCPE (Chinese 5-way choice questions, including knowledge mismatches to test robustness).\n    *   **Knowledge Graphs**: EMCKG and CMCKG, constructed for medical concepts.\n    *   **Baselines**: Vanilla GPT-3.5 and GPT-4, Tree-of-Thought (TOT), and three retrieval-augmented baselines (BM25 Retriever, Text Embedding Retriever, KG Retriever), all using `gpt-3.5-turbo-0613` as backbone for retrieval methods.\n    *   **Key Performance Metrics**: BERTScore (Precision, Recall, F1) for semantic similarity and GPT-4 Rating (average rank, assessing quality, factual correctness, diversity, integrity) for human-like evaluation. A \"Hallucination Quantify\" metric was also used.\n    *   **Comparison Results**: MindMap consistently achieved significant improvements over all baselines across all datasets and metrics. For GenMedGPT-5k, MindMap showed the highest BERTScore F1 (0.7954) and the best GPT-4 Ranking (1.8725 average), indicating superior answer quality and factual correctness. It also demonstrated robustness to incorrect retrieval knowledge (on ExplainCPE) and reduced hallucinations compared to baselines.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations**: The paper does not explicitly state technical limitations of MindMap itself, but rather positions it as a solution to existing LLM limitations. Implicitly, the quality of the initial entity recognition and the underlying KG are crucial. The complexity of constructing and maintaining high-quality KGs for diverse domains remains a challenge.\n    *   **Scope of Applicability**: Primarily validated in medical question-answering tasks. While the framework is general, its effectiveness might vary in domains where KGs are less structured or comprehensive. The reliance on LLM for entity recognition and aggregation could introduce its own set of biases or errors.\n\n*   **Technical Significance**\n    *   **Advancement of State-of-the-Art**: MindMap significantly advances the state-of-the-art in LLM-KG integration by enabling LLMs to move beyond simple fact retrieval to perform synergistic, graph-of-thoughts reasoning. It addresses critical issues of hallucination and transparency in LLMs.\n    *   **Potential Impact**: This work paves the way for more reliable, explainable, and knowledge-grounded LLM applications, particularly in high-stakes domains like healthcare. It highlights a promising direction for future research in combining the powerful reasoning capabilities of LLMs with the structured, verifiable knowledge of KGs.",
        "keywords": [
          "MindMap",
          "Knowledge Graph Prompting",
          "Large Language Models (LLMs)",
          "Graph-of-Thoughts Reasoning",
          "Synergistic Knowledge Inference",
          "Structured KG Input Comprehension",
          "Evidence Graph Mining",
          "Reasoning Graph Aggregation",
          "Explainable AI",
          "Hallucination Reduction",
          "Medical Question-Answering",
          "Factual Correctness",
          "Decision Tree Visualization"
        ],
        "paper_type": "based on the abstract and introduction:\n\n*   the abstract explicitly states: \"we **propose a novel prompting pipeline**, named mindmap\". it then describes \"our **method** enables llms...\" and \"our **method** elicits the mind map...\". it also mentions introducing a \"new hallucination evaluation benchmark\" and making a \"codebase available\".\n*   the introduction sets up a problem with current llms and figure 1 conceptually compares \"our **method**\" with other baselines.\n\nthese phrases strongly align with the criteria for a **technical** paper, which presents new methods, algorithms, or systems. while there is also an empirical component (evaluation, significant improvements, analyzing effects), the core contribution is the *proposal and development of a new method/system*.\n\ntherefore, the paper type is: **technical**"
      },
      "file_name": "ca261cb681b082e90ca6c7a9d325b4265ed1dc28.pdf"
    },
    {
      "success": true,
      "doc_id": "b6be9763821e3cf09866ce7bbfdc3cd9",
      "summary": "Here's a focused summary of the paper for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Large Vision-Language Models (LVLMs) suffer from \"object hallucination,\" where they generate descriptions containing objects inconsistent with or absent from the target images.\n    *   **Importance & Challenge**: This problem degrades model performance and harms user experience, posing significant risks in real-world applications (e.g., autonomous driving safety). Existing evaluation methods like CHAIR are unstable, sensitive to instruction design, and biased, making it difficult to accurately assess and understand hallucination in LVLMs.\n\n*   **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: Builds upon the success of Large Language Models (LLMs) and Vision-Language Pre-trained Models (VLPMs), which are integrated to form LVLMs. Acknowledges that both LLMs (text hallucination) and VLPMs (object hallucination) have shown hallucination tendencies.\n    *   **Limitations of Previous Solutions**:\n        *   The widely used CHAIR (Caption Hallucination Assessment with Image Relevance) metric is shown to be unstable, highly sensitive to instruction phrasing, and influenced by caption length.\n        *   CHAIR relies on complex, human-crafted parsing rules for object extraction, which may not adapt well to the diverse generation styles of LVLMs and can lead to misclassification errors.\n        *   Previous work lacked a systematic study of object hallucination specifically in LVLMs.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method**:\n        *   Conducts the first systematic empirical study of object hallucination in representative LVLMs using the CHAIR metric as a baseline.\n        *   Proposes **Polling-based Object Probing Evaluation (POPE)**, a novel method that converts hallucination evaluation into a binary classification task by prompting LVLMs with simple \"Yes-or-No\" questions about probing objects (e.g., \"Is there a car in the image?\").\n        *   Investigates the influence of visual instruction data, hypothesizing that LVLMs are prone to hallucinate objects that frequently appear or co-occur in their training datasets.\n        *   POPE incorporates different object sampling strategies: \"Random,\" \"Popular\" (sampling frequently appearing/co-occurring objects), and \"Adversarial\" (sampling objects that are visually similar or semantically related but absent).\n    *   **Novelty**:\n        *   Presents the first systematic and comprehensive empirical study of object hallucination in LVLMs.\n        *   Introduces POPE as a more stable, flexible, and scalable evaluation method for object hallucination, overcoming the limitations of traditional caption-based metrics.\n        *   Identifies and quantitatively validates the role of object frequency and co-occurrence in visual instruction datasets as a significant driver of object hallucination in LVLMs.\n\n*   **Key Technical Contributions** \\cite{li2023249}\n    *   **Novel Evaluation Method**: Introduction of POPE, a polling-based approach that reframes object hallucination assessment as a binary classification task, offering enhanced stability and flexibility compared to existing methods.\n    *   **Empirical Study**: The first systematic empirical study demonstrating the severe prevalence of object hallucination in state-of-the-art LVLMs, often exceeding that of smaller vision-language models.\n    *   **Identification of Hallucination Drivers**: Quantitative and qualitative analysis revealing that LVLMs are significantly prone to hallucinate objects that frequently appear or co-occur within their visual instruction datasets.\n    *   **Scalability and Flexibility**: POPE's design allows for easy extension to unannotated datasets by leveraging automatic segmentation tools (e.g., SEEM), broadening its applicability.\n\n*   **Experimental Validation** \\cite{li2023249}\n    *   **Models Evaluated**: Five representative LVLMs (mPLUG-Owl, LLaVA, MultiModal-GPT, MiniGPT-4, InstructBLIP) and four smaller VLPMs (OSCAR Base, VinVL Large, OFA Large, BLIP Large) for comparison.\n    *   **Datasets**: MSCOCO dataset was used for evaluating hallucination and analyzing object frequencies.\n    *   **Metrics**:\n        *   CHAIR I (instance-level) and CHAIR S (sentence-level) were used for initial evaluation and to highlight their limitations.\n        *   HR@k (Hit Ratio at k), specifically HRA@k (for appearing frequency) and HRC@k (for co-occurring frequency), was introduced to quantify the correlation between object frequency/co-occurrence and hallucination times.\n        *   POPE's performance is implicitly measured by its ability to stably and flexibly evaluate hallucination through \"Yes/No\" responses.\n    *   **Key Results**:\n        *   Most LVLMs exhibit severe object hallucination, with some (e.g., LLaVA) showing significantly higher CHAIR S scores (32.7) than smaller VLPMs (e.g., OSCAR base at 13.0).\n        *   CHAIR metrics were shown to be unstable, with results varying significantly based on minor instruction changes (e.g., Instruction 2 doubling CHAIR values compared to Instruction 1).\n        *   Qualitative (bar charts) and quantitative (HR@k) analyses confirmed that approximately half of hallucinated objects belong to the top 10 frequently appearing COCO objects (HRA@10 near 0.5), and more than half are among the top 10 frequently co-occurring objects (HRC@10 near 0.6).\n        *   POPE demonstrated superior stability and flexibility in evaluating object hallucination compared to CHAIR.\n\n*   **Limitations & Scope** \\cite{li2023249}\n    *   **CHAIR Limitations**: The paper explicitly highlights the instability, instruction sensitivity, caption length bias, and reliance on complex parsing rules of the CHAIR metric.\n    *   **POPE Scope**: The current study focuses on coarse-grained object hallucinations (presence/absence of objects) and leaves fine-grained hallucinations (e.g., number, attributes, positions of objects) for future work.\n    *   **Assumptions**: The analysis of hallucination drivers assumes that the object distributions in visual instruction datasets directly influence LVLM's hallucination tendencies.\n\n*   **Technical Significance** \\cite{li2023249}\n    *   **Advances State-of-the-Art**: Provides the first systematic investigation into object hallucination in LVLMs and introduces POPE, a more robust and practical evaluation methodology.\n    *   **Potential Impact**:\n        *   Highlights a critical safety and reliability concern for the deployment of LVLMs in sensitive real-world applications.\n        *   Offers crucial insights into the underlying mechanisms of hallucination in LVLMs, specifically linking it to the statistical properties (frequency and co-occurrence) of training data.\n        *   POPE establishes a new, stable benchmark for future research, enabling more accurate comparison of LVLMs and fostering the development of effective mitigation strategies against object hallucination.\n        *   Encourages further research into fine-grained hallucination and methods to de-bias LVLMs from generating common but absent objects.",
      "intriguing_abstract": "Large Vision-Language Models (LVLMs) are transforming AI, yet their pervasive \"object hallucination\"â€”generating descriptions of non-existent objectsâ€”poses critical safety and reliability risks, particularly in sensitive applications like autonomous driving. Current evaluation metrics, such as CHAIR, are unstable, instruction-sensitive, and biased, hindering accurate assessment. This paper presents the first systematic empirical study revealing the severe prevalence of object hallucination in state-of-the-art LVLMs. We introduce **POPE (Polling-based Object Probing Evaluation)**, a novel, stable, and flexible method that reframes hallucination assessment as a binary classification task, overcoming the limitations of previous approaches. Our comprehensive analysis quantitatively demonstrates that LVLMs are significantly prone to hallucinate objects that frequently appear or co-occur within their visual instruction datasets. POPE provides a robust benchmark for future research, offering crucial insights into hallucination drivers and paving the way for developing effective mitigation strategies to enhance LVLM trustworthiness and real-world applicability.",
      "keywords": [
        "Large Vision-Language Models (LVLMs)",
        "object hallucination",
        "Polling-based Object Probing Evaluation (POPE)",
        "CHAIR metric instability",
        "systematic empirical study",
        "visual instruction data",
        "object frequency and co-occurrence",
        "binary classification evaluation",
        "scalable hallucination assessment",
        "real-world application safety",
        "hallucination drivers identification",
        "severe hallucination prevalence"
      ],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/206400aba5f12f734cdd2e4ab48ef6014ea60773.pdf",
      "citation_key": "li2023249",
      "metadata": {
        "title": "Evaluating Object Hallucination in Large Vision-Language Models",
        "authors": [
          "Yifan Li",
          "Yifan Du",
          "Kun Zhou",
          "Jinpeng Wang",
          "Wayne Xin Zhao",
          "Ji-rong Wen"
        ],
        "published_date": "2023",
        "abstract": "Inspired by the superior language abilities of large language models (LLM), large vision-language models (LVLM) have been recently explored by integrating powerful LLMs for improving the performance on complex multimodal tasks. Despite the promising progress on LVLMs, we find that LVLMs suffer from the hallucination problem, i.e. they tend to generate objects that are inconsistent with the target images in the descriptions. To investigate it, this work presents the first systematic study on object hallucination of LVLMs. We conduct the evaluation experiments on several representative LVLMs, and show that they mostly suffer from severe object hallucination issue. We further discuss that the visual instructions may influence the hallucination, and find that: objects that frequently occur in the visual instructions or co-occur with the image objects, are obviously prone to be hallucinated by LVLMs. Besides, we find that existing evaluation methods might be affected by the input instructions and generation styles of LVLMs. Thus, we further design an improved evaluation method for object hallucination by proposing a polling-based query method called POPE. Experiment results demonstrate that our POPE can evaluate the object hallucination in a more stable and flexible way. Our codes and data are publicly available at https://github.com/RUCAIBox/POPE.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/206400aba5f12f734cdd2e4ab48ef6014ea60773.pdf",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the paper for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Large Vision-Language Models (LVLMs) suffer from \"object hallucination,\" where they generate descriptions containing objects inconsistent with or absent from the target images.\n    *   **Importance & Challenge**: This problem degrades model performance and harms user experience, posing significant risks in real-world applications (e.g., autonomous driving safety). Existing evaluation methods like CHAIR are unstable, sensitive to instruction design, and biased, making it difficult to accurately assess and understand hallucination in LVLMs.\n\n*   **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: Builds upon the success of Large Language Models (LLMs) and Vision-Language Pre-trained Models (VLPMs), which are integrated to form LVLMs. Acknowledges that both LLMs (text hallucination) and VLPMs (object hallucination) have shown hallucination tendencies.\n    *   **Limitations of Previous Solutions**:\n        *   The widely used CHAIR (Caption Hallucination Assessment with Image Relevance) metric is shown to be unstable, highly sensitive to instruction phrasing, and influenced by caption length.\n        *   CHAIR relies on complex, human-crafted parsing rules for object extraction, which may not adapt well to the diverse generation styles of LVLMs and can lead to misclassification errors.\n        *   Previous work lacked a systematic study of object hallucination specifically in LVLMs.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method**:\n        *   Conducts the first systematic empirical study of object hallucination in representative LVLMs using the CHAIR metric as a baseline.\n        *   Proposes **Polling-based Object Probing Evaluation (POPE)**, a novel method that converts hallucination evaluation into a binary classification task by prompting LVLMs with simple \"Yes-or-No\" questions about probing objects (e.g., \"Is there a car in the image?\").\n        *   Investigates the influence of visual instruction data, hypothesizing that LVLMs are prone to hallucinate objects that frequently appear or co-occur in their training datasets.\n        *   POPE incorporates different object sampling strategies: \"Random,\" \"Popular\" (sampling frequently appearing/co-occurring objects), and \"Adversarial\" (sampling objects that are visually similar or semantically related but absent).\n    *   **Novelty**:\n        *   Presents the first systematic and comprehensive empirical study of object hallucination in LVLMs.\n        *   Introduces POPE as a more stable, flexible, and scalable evaluation method for object hallucination, overcoming the limitations of traditional caption-based metrics.\n        *   Identifies and quantitatively validates the role of object frequency and co-occurrence in visual instruction datasets as a significant driver of object hallucination in LVLMs.\n\n*   **Key Technical Contributions** \\cite{li2023249}\n    *   **Novel Evaluation Method**: Introduction of POPE, a polling-based approach that reframes object hallucination assessment as a binary classification task, offering enhanced stability and flexibility compared to existing methods.\n    *   **Empirical Study**: The first systematic empirical study demonstrating the severe prevalence of object hallucination in state-of-the-art LVLMs, often exceeding that of smaller vision-language models.\n    *   **Identification of Hallucination Drivers**: Quantitative and qualitative analysis revealing that LVLMs are significantly prone to hallucinate objects that frequently appear or co-occur within their visual instruction datasets.\n    *   **Scalability and Flexibility**: POPE's design allows for easy extension to unannotated datasets by leveraging automatic segmentation tools (e.g., SEEM), broadening its applicability.\n\n*   **Experimental Validation** \\cite{li2023249}\n    *   **Models Evaluated**: Five representative LVLMs (mPLUG-Owl, LLaVA, MultiModal-GPT, MiniGPT-4, InstructBLIP) and four smaller VLPMs (OSCAR Base, VinVL Large, OFA Large, BLIP Large) for comparison.\n    *   **Datasets**: MSCOCO dataset was used for evaluating hallucination and analyzing object frequencies.\n    *   **Metrics**:\n        *   CHAIR I (instance-level) and CHAIR S (sentence-level) were used for initial evaluation and to highlight their limitations.\n        *   HR@k (Hit Ratio at k), specifically HRA@k (for appearing frequency) and HRC@k (for co-occurring frequency), was introduced to quantify the correlation between object frequency/co-occurrence and hallucination times.\n        *   POPE's performance is implicitly measured by its ability to stably and flexibly evaluate hallucination through \"Yes/No\" responses.\n    *   **Key Results**:\n        *   Most LVLMs exhibit severe object hallucination, with some (e.g., LLaVA) showing significantly higher CHAIR S scores (32.7) than smaller VLPMs (e.g., OSCAR base at 13.0).\n        *   CHAIR metrics were shown to be unstable, with results varying significantly based on minor instruction changes (e.g., Instruction 2 doubling CHAIR values compared to Instruction 1).\n        *   Qualitative (bar charts) and quantitative (HR@k) analyses confirmed that approximately half of hallucinated objects belong to the top 10 frequently appearing COCO objects (HRA@10 near 0.5), and more than half are among the top 10 frequently co-occurring objects (HRC@10 near 0.6).\n        *   POPE demonstrated superior stability and flexibility in evaluating object hallucination compared to CHAIR.\n\n*   **Limitations & Scope** \\cite{li2023249}\n    *   **CHAIR Limitations**: The paper explicitly highlights the instability, instruction sensitivity, caption length bias, and reliance on complex parsing rules of the CHAIR metric.\n    *   **POPE Scope**: The current study focuses on coarse-grained object hallucinations (presence/absence of objects) and leaves fine-grained hallucinations (e.g., number, attributes, positions of objects) for future work.\n    *   **Assumptions**: The analysis of hallucination drivers assumes that the object distributions in visual instruction datasets directly influence LVLM's hallucination tendencies.\n\n*   **Technical Significance** \\cite{li2023249}\n    *   **Advances State-of-the-Art**: Provides the first systematic investigation into object hallucination in LVLMs and introduces POPE, a more robust and practical evaluation methodology.\n    *   **Potential Impact**:\n        *   Highlights a critical safety and reliability concern for the deployment of LVLMs in sensitive real-world applications.\n        *   Offers crucial insights into the underlying mechanisms of hallucination in LVLMs, specifically linking it to the statistical properties (frequency and co-occurrence) of training data.\n        *   POPE establishes a new, stable benchmark for future research, enabling more accurate comparison of LVLMs and fostering the development of effective mitigation strategies against object hallucination.\n        *   Encourages further research into fine-grained hallucination and methods to de-bias LVLMs from generating common but absent objects.",
        "keywords": [
          "Large Vision-Language Models (LVLMs)",
          "object hallucination",
          "Polling-based Object Probing Evaluation (POPE)",
          "CHAIR metric instability",
          "systematic empirical study",
          "visual instruction data",
          "object frequency and co-occurrence",
          "binary classification evaluation",
          "scalable hallucination assessment",
          "real-world application safety",
          "hallucination drivers identification",
          "severe hallucination prevalence"
        ],
        "paper_type": "based on the abstract and introduction:\n\n1.  **\"presents the first systematic study on object hallucination of lvlms.\"** - this is a strong indicator of an empirical study.\n2.  **\"we conduct the evaluation experiments on several representative lvlms\"** - explicitly mentions \"evaluation experiments,\" which is a hallmark of empirical research.\n3.  **\"show that they mostly suffer from severe object hallucination issues.\"** - presents findings from the experiments.\n4.  **\"we further discuss that the visual instructions may influence the hallucination, and find that...\"** - more findings and observations from the study.\n5.  **\"besides, we further design a polling-based query method called pope for better evaluation of object hallucination.\"** - while this describes a technical contribution (a new method), it's presented as a tool for \"better evaluation\" within the larger study.\n6.  **\"experiment results show that our pope can evaluate object hallucination in a more stable and flexible way.\"** - this is the empirical validation of the proposed method, fitting within the overall empirical framework.\n7.  the title \"evaluating object hallucination...\" also points directly to an empirical focus.\n8.  the venue \"conference on empirical methods in natural language processing\" (emnlp) further supports an empirical classification.\n\nthe paper's primary contribution is the systematic investigation and evaluation of object hallucination in lvlms through experiments and data analysis. the pope method is a secondary contribution designed to facilitate this empirical evaluation.\n\ntherefore, the most fitting classification is **empirical**."
      },
      "file_name": "206400aba5f12f734cdd2e4ab48ef6014ea60773.pdf"
    },
    {
      "success": true,
      "doc_id": "4753ef909a3f923d689a9bd3cb31040d",
      "summary": "Here's a focused summary of the paper \\cite{lan20240yz} for a literature review:\n\n### Analysis of \"A Survey of Hallucination in Large Visual Language Models\" \\cite{lan20240yz}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: The paper addresses the pervasive issue of \"hallucination\" in Large Visual Language Models (LVLMs), where models generate factually incorrect but seemingly plausible information (e.g., misreporting non-existent objects, properties, behaviors, or relationships in images).\n    *   **Importance and Challenge**: Hallucinations severely limit the potential and practical effectiveness of LVLMs, especially in high-accuracy and reliability scenarios. They can mislead users, disseminate misinformation, and erode trust, hindering the widespread adoption and development of LVLMs. Correcting or mitigating these hallucinations is crucial for building trustworthy LVLMs.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: While existing surveys have summarized hallucination correction in Large Language Models (LLMs) \\cite{lan20240yz} and partially covered multimodal large language models \\cite{lan20240yz}, this work distinguishes itself by proposing a \"distinctly different taxonomic strategy.\"\n    *   **Limitations of Previous Solutions (as addressed by this survey)**: The paper implies that previous reviews lacked a comprehensive and structured categorization specifically tailored to the unique challenges of hallucination in LVLMs, encompassing both correction efforts and evaluation benchmarks.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method/Algorithm**: As a survey, the paper's core approach is a structured review and categorization of existing research. It first introduces LVLM architecture (perceptual, cross-modal, response modules) and identifies the main causes of hallucination:\n        *   **Modality Gap**: Differences in data distribution, features, and semantics between visual and textual modalities, leading to biased understanding.\n        *   **Toxicity in Dataset**: Presence of misleading or hallucinatory samples in large-scale training datasets, often generated by other LLMs/LVLMs.\n        *   **LLM Hallucinations**: Inherited hallucination tendencies from the underlying LLMs used as the \"brain\" of LVLMs, including conflicts with parametric knowledge and randomness in decoding strategies.\n    *   **Novelty/Differentiation**: The primary innovation lies in its novel taxonomic strategy for organizing hallucination correction methods and evaluation benchmarks. Correction methods are categorized into three core ideas:\n        *   **Dataset Dehallucination**: Focuses on improving training data quality.\n        *   **Modalities Gap**: Aims to enhance visual comprehension and bridge the gap between visual and textual representations.\n        *   **Output Correction**: Addresses hallucinations during or after the generation process.\n    *   It also provides a structured overview of evaluation benchmarks, classifying them as \"Judgmental\" or \"Generative.\"\n\n4.  **Key Technical Contributions**\n    *   **Novel Taxonomy**: Proposes a novel and comprehensive taxonomy for hallucination correction methods in LVLMs, categorizing them into Dataset Dehallucination, Modalities Gap, and Output Correction.\n    *   **Categorization of Causes**: Clearly identifies and attributes the main technical causes of hallucination in LVLMs to the modality gap, dataset toxicity, and inherent LLM hallucinations.\n    *   **Survey of Evaluation Benchmarks**: Systematically presents available hallucination evaluation benchmarks for LVLMs, distinguishing between judgmental and generative perspectives.\n    *   **Future Research Directions**: Suggests future research avenues to enhance the dependability and utility of LVLMs by addressing the identified challenges.\n\n5.  **Experimental Validation**\n    *   The paper itself is a survey and does not conduct new experiments. Instead, it *reviews* the experimental validation strategies employed by the research it summarizes.\n    *   **Types of Experiments/Benchmarks Reviewed**:\n        *   **Judgmental Benchmarks**: Evaluate LVLM responses based on human judgment or specific factual checks. Examples include:\n            *   **Object Hallucination**: POPE \\cite{lan20240yz}, CIEM \\cite{lan20240yz}, EMMA \\cite{lan20240yz}, Merlim \\cite{lan20240yz} (focus on existence, properties, inter-relationships of objects).\n            *   **Parametric Knowledge**: MME \\cite{lan20240yz}, Hallusionbench \\cite{lan20240yz} (assess factual consistency with world knowledge).\n            *   **Self-awareness**: MM-SAP \\cite{lan20240yz}.\n            *   **Special Phenomenon**: VHTest \\cite{lan20240yz}.\n        *   **Generative Benchmarks**: Use automated metrics or specific setups to quantify hallucination in generated text. Examples include:\n            *   **Metrics**: CHAIR \\cite{lan20240yz}, AMBER \\cite{lan20240yz}.\n            *   **Fraudulent Input**: MAD-Bench \\cite{lan20240yz}, CorrelationQA \\cite{lan20240yz}.\n            *   **Visual Drift**: GenCeption \\cite{lan20240yz}.\n            *   **Image Sequences**: Mementos \\cite{lan20240yz}.\n            *   **Reverse Expansion**: UniHD \\cite{lan20240yz}.\n    *   **Key Performance Metrics**: The reviewed papers typically use metrics related to factual accuracy, object existence verification, consistency, and human preference scores to evaluate hallucination mitigation.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations (of LVLMs, as discussed)**: The paper highlights inherent limitations of current LVLMs, such as their reliance on parametric knowledge when visual information is insufficient, the difficulty in bridging the modality gap, and the propagation of biases/hallucinations from training datasets and underlying LLMs.\n    *   **Scope of Applicability**: The survey focuses specifically on hallucination in *Large Visual Language Models*, covering their structure, causes, correction methods, and evaluation. It does not delve into other potential issues of LVLMs or hallucination in other AI model types beyond its direct relevance to LVLMs.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: This survey significantly advances the technical state-of-the-art by providing a much-needed, structured, and comprehensive overview of hallucination in LVLMs. Its novel taxonomy helps organize a rapidly growing field, making it easier for researchers to understand the landscape of existing solutions and identify gaps.\n    *   **Potential Impact on Future Research**: By clearly outlining the causes, correction strategies, and evaluation methods, the paper serves as a foundational resource. It guides future research by suggesting promising directions for developing more robust, reliable, and trustworthy LVLMs, particularly in areas like improving data quality, enhancing cross-modal understanding, and refining output generation.",
      "intriguing_abstract": "The pervasive issue of **hallucination**â€”where Large Visual Language Models (LVLMs) generate factually incorrect yet seemingly plausible informationâ€”severely undermines their trustworthiness and limits their real-world applicability. This critical challenge, ranging from misreporting non-existent objects to propagating misinformation, demands a systematic understanding and mitigation strategy.\n\nThis survey provides the first comprehensive and novel taxonomic framework to navigate the complex landscape of hallucination in LVLMs. We meticulously categorize the root causes, attributing them to the **modality gap**, **dataset toxicity**, and inherited **LLM hallucinations**. Crucially, we introduce a distinct taxonomy for **hallucination correction methods**, classifying them into **Dataset Dehallucination**, **Modalities Gap** bridging, and **Output Correction** strategies. Furthermore, we systematically review existing **evaluation benchmarks**, distinguishing between **judgmental** and **generative** approaches. By offering a structured overview of causes, solutions, and assessment tools, this paper serves as an indispensable guide, illuminating pathways for future research to build more reliable, accurate, and trustworthy **multimodal AI** systems.",
      "keywords": [
        "Hallucination in LVLMs",
        "Novel Taxonomic Strategy",
        "Modality Gap",
        "Dataset Dehallucination",
        "Output Correction",
        "Evaluation Benchmarks",
        "Judgmental Benchmarks",
        "Generative Benchmarks",
        "LLM Hallucinations",
        "Cross-modal Understanding",
        "Trustworthy LVLMs",
        "Dataset Toxicity"
      ],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/d6da914d0c8021df6622857aba23b794fc7e6a40.pdf",
      "citation_key": "lan20240yz",
      "metadata": {
        "title": "A Survey of Hallucination in Large Visual Language Models",
        "authors": [
          "Wei Lan",
          "Wenyi Chen",
          "Qingfeng Chen",
          "Shirui Pan",
          "Huiyu Zhou",
          "Yi Pan"
        ],
        "published_date": "2024",
        "abstract": "The Large Visual Language Models (LVLMs) enhances user interaction and enriches user experience by integrating visual modality on the basis of the Large Language Models (LLMs). It has demonstrated their powerful information processing and generation capabilities. However, the existence of hallucinations has limited the potential and practical effectiveness of LVLM in various fields. Although lots of work has been devoted to the issue of hallucination mitigation and correction, there are few reviews to summary this issue. In this survey, we first introduce the background of LVLMs and hallucinations. Then, the structure of LVLMs and main causes of hallucination generation are introduced. Further, we summary recent works on hallucination correction and mitigation. In addition, the available hallucination evaluation benchmarks for LVLMs are presented from judgmental and generative perspectives. Finally, we suggest some future research directions to enhance the dependability and utility of LVLMs.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/d6da914d0c8021df6622857aba23b794fc7e6a40.pdf",
        "venue": "arXiv.org",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the paper \\cite{lan20240yz} for a literature review:\n\n### Analysis of \"A Survey of Hallucination in Large Visual Language Models\" \\cite{lan20240yz}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: The paper addresses the pervasive issue of \"hallucination\" in Large Visual Language Models (LVLMs), where models generate factually incorrect but seemingly plausible information (e.g., misreporting non-existent objects, properties, behaviors, or relationships in images).\n    *   **Importance and Challenge**: Hallucinations severely limit the potential and practical effectiveness of LVLMs, especially in high-accuracy and reliability scenarios. They can mislead users, disseminate misinformation, and erode trust, hindering the widespread adoption and development of LVLMs. Correcting or mitigating these hallucinations is crucial for building trustworthy LVLMs.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: While existing surveys have summarized hallucination correction in Large Language Models (LLMs) \\cite{lan20240yz} and partially covered multimodal large language models \\cite{lan20240yz}, this work distinguishes itself by proposing a \"distinctly different taxonomic strategy.\"\n    *   **Limitations of Previous Solutions (as addressed by this survey)**: The paper implies that previous reviews lacked a comprehensive and structured categorization specifically tailored to the unique challenges of hallucination in LVLMs, encompassing both correction efforts and evaluation benchmarks.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method/Algorithm**: As a survey, the paper's core approach is a structured review and categorization of existing research. It first introduces LVLM architecture (perceptual, cross-modal, response modules) and identifies the main causes of hallucination:\n        *   **Modality Gap**: Differences in data distribution, features, and semantics between visual and textual modalities, leading to biased understanding.\n        *   **Toxicity in Dataset**: Presence of misleading or hallucinatory samples in large-scale training datasets, often generated by other LLMs/LVLMs.\n        *   **LLM Hallucinations**: Inherited hallucination tendencies from the underlying LLMs used as the \"brain\" of LVLMs, including conflicts with parametric knowledge and randomness in decoding strategies.\n    *   **Novelty/Differentiation**: The primary innovation lies in its novel taxonomic strategy for organizing hallucination correction methods and evaluation benchmarks. Correction methods are categorized into three core ideas:\n        *   **Dataset Dehallucination**: Focuses on improving training data quality.\n        *   **Modalities Gap**: Aims to enhance visual comprehension and bridge the gap between visual and textual representations.\n        *   **Output Correction**: Addresses hallucinations during or after the generation process.\n    *   It also provides a structured overview of evaluation benchmarks, classifying them as \"Judgmental\" or \"Generative.\"\n\n4.  **Key Technical Contributions**\n    *   **Novel Taxonomy**: Proposes a novel and comprehensive taxonomy for hallucination correction methods in LVLMs, categorizing them into Dataset Dehallucination, Modalities Gap, and Output Correction.\n    *   **Categorization of Causes**: Clearly identifies and attributes the main technical causes of hallucination in LVLMs to the modality gap, dataset toxicity, and inherent LLM hallucinations.\n    *   **Survey of Evaluation Benchmarks**: Systematically presents available hallucination evaluation benchmarks for LVLMs, distinguishing between judgmental and generative perspectives.\n    *   **Future Research Directions**: Suggests future research avenues to enhance the dependability and utility of LVLMs by addressing the identified challenges.\n\n5.  **Experimental Validation**\n    *   The paper itself is a survey and does not conduct new experiments. Instead, it *reviews* the experimental validation strategies employed by the research it summarizes.\n    *   **Types of Experiments/Benchmarks Reviewed**:\n        *   **Judgmental Benchmarks**: Evaluate LVLM responses based on human judgment or specific factual checks. Examples include:\n            *   **Object Hallucination**: POPE \\cite{lan20240yz}, CIEM \\cite{lan20240yz}, EMMA \\cite{lan20240yz}, Merlim \\cite{lan20240yz} (focus on existence, properties, inter-relationships of objects).\n            *   **Parametric Knowledge**: MME \\cite{lan20240yz}, Hallusionbench \\cite{lan20240yz} (assess factual consistency with world knowledge).\n            *   **Self-awareness**: MM-SAP \\cite{lan20240yz}.\n            *   **Special Phenomenon**: VHTest \\cite{lan20240yz}.\n        *   **Generative Benchmarks**: Use automated metrics or specific setups to quantify hallucination in generated text. Examples include:\n            *   **Metrics**: CHAIR \\cite{lan20240yz}, AMBER \\cite{lan20240yz}.\n            *   **Fraudulent Input**: MAD-Bench \\cite{lan20240yz}, CorrelationQA \\cite{lan20240yz}.\n            *   **Visual Drift**: GenCeption \\cite{lan20240yz}.\n            *   **Image Sequences**: Mementos \\cite{lan20240yz}.\n            *   **Reverse Expansion**: UniHD \\cite{lan20240yz}.\n    *   **Key Performance Metrics**: The reviewed papers typically use metrics related to factual accuracy, object existence verification, consistency, and human preference scores to evaluate hallucination mitigation.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations (of LVLMs, as discussed)**: The paper highlights inherent limitations of current LVLMs, such as their reliance on parametric knowledge when visual information is insufficient, the difficulty in bridging the modality gap, and the propagation of biases/hallucinations from training datasets and underlying LLMs.\n    *   **Scope of Applicability**: The survey focuses specifically on hallucination in *Large Visual Language Models*, covering their structure, causes, correction methods, and evaluation. It does not delve into other potential issues of LVLMs or hallucination in other AI model types beyond its direct relevance to LVLMs.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: This survey significantly advances the technical state-of-the-art by providing a much-needed, structured, and comprehensive overview of hallucination in LVLMs. Its novel taxonomy helps organize a rapidly growing field, making it easier for researchers to understand the landscape of existing solutions and identify gaps.\n    *   **Potential Impact on Future Research**: By clearly outlining the causes, correction strategies, and evaluation methods, the paper serves as a foundational resource. It guides future research by suggesting promising directions for developing more robust, reliable, and trustworthy LVLMs, particularly in areas like improving data quality, enhancing cross-modal understanding, and refining output generation.",
        "keywords": [
          "Hallucination in LVLMs",
          "Novel Taxonomic Strategy",
          "Modality Gap",
          "Dataset Dehallucination",
          "Output Correction",
          "Evaluation Benchmarks",
          "Judgmental Benchmarks",
          "Generative Benchmarks",
          "LLM Hallucinations",
          "Cross-modal Understanding",
          "Trustworthy LVLMs",
          "Dataset Toxicity"
        ],
        "paper_type": "**classification:** survey\n\n**reasoning:**\n\n1.  **direct keyword match:** the abstract explicitly states, \"in this **survey**, we first introduce...\" and mentions \"few **reviews** to summary this issue.\" the title of the paper is also \"a **survey** of hallucination in large visual language models.\"\n2.  **content description:** the abstract outlines a structure typical of a survey paper:\n    *   introducing background (lvlms and hallucinations).\n    *   explaining causes (structure of lvlms and hallucination generation).\n    *   summarizing recent works (hallucination correction and mitigation).\n    *   presenting existing evaluation benchmarks.\n    *   suggesting future research directions.\n    3. **criteria alignment:** these points directly match the \"survey\" classification criteria: \"reviews existing literature comprehensively,\" \"abstract mentions: 'survey', 'review', 'comprehensive analysis', 'state-of-the-art',\" and \"introduction discusses: literature organization, classification schemes\" (implicitly, by organizing the discussion of existing work)."
      },
      "file_name": "d6da914d0c8021df6622857aba23b794fc7e6a40.pdf"
    },
    {
      "success": true,
      "doc_id": "7a1fcea8f9cb1aee1c9a159035339864",
      "summary": "Here's a focused summary of the technical paper for a literature review:\n\n### Technical Paper Analysis: Chain-of-Verification Reduces Hallucination in Large Language Models \\cite{dhuliawala2023rqn}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: The paper addresses the persistent issue of \"hallucination\" in Large Language Models (LLMs), where models generate plausible but factually incorrect information. This is particularly problematic for lesser-known facts and in longform text generation where errors can compound.\n    *   **Importance and Challenge**: Hallucinations undermine the trustworthiness and reliability of LLM outputs, limiting their applicability in critical domains. It's challenging because even the largest models, despite improved reasoning capabilities, still exhibit this behavior, and simply scaling up doesn't resolve it.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: This work falls under \"generation-time correction\" methods, which aim to improve LLM reliability by adding reasoning decisions on top of the base model. It builds on research encouraging LLMs to generate internal thoughts or reasoning chains (e.g., Chain-of-Thought) and self-critique.\n    *   **Limitations of Previous Solutions**:\n        *   **Training-time correction**: Requires adjusting model weights, which can be complex.\n        *   **Generation-time correction**: Some methods rely on token probabilities or multiple samples, which may not always be robust. Approaches using inconsistencies (e.g., LM vs LM) are related but often involve multi-agent setups.\n        *   **Augmentation (tool-use)**: While effective (e.g., retrieval-augmented generation), CoVe specifically focuses on *self-correction* using only the LLM's internal knowledge, without external tools like search engines.\n        *   **Self-critique/verification**: Previous self-verification methods often focus on logical/mathematical tasks or specific question-answer formats, whereas CoVe targets factual hallucination across diverse generation tasks. CoVe's \"factored\" approach specifically addresses the issue of LLMs repeating their own hallucinations when conditioning on prior incorrect generations.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method (Chain-of-Verification - CoVe)**: CoVe is a four-step process designed to enable an LLM to deliberate on and correct its own responses:\n        1.  **Generate Baseline Response**: The LLM produces an initial draft.\n        2.  **Plan Verifications**: Conditioned on the query and baseline response, the LLM generates a list of specific verification questions to fact-check claims in the draft.\n        3.  **Execute Verifications**: The LLM answers these verification questions. Crucially, several variants are explored:\n            *   **Joint**: Planning and execution in a single prompt, potentially leading to repetition.\n            *   **2-Step**: Separates planning (conditions on baseline) and execution (answers questions without conditioning on baseline) to reduce repetition.\n            *   **Factored**: Answers *each* verification question independently as a separate prompt, without conditioning on the original baseline response or other verification answers. This is the most sophisticated and aims to minimize bias and repetition.\n            *   **Factor+Revise**: Adds an explicit cross-checking step after factored execution to detect inconsistencies.\n        4.  **Generate Final Verified Response**: The LLM produces a revised response, incorporating the insights and corrections from the verification steps.\n    *   **Novelty/Difference**:\n        *   **Systematic Self-Deliberation**: CoVe provides a structured, multi-step framework for LLMs to self-critique and verify factual claims *internally*, without external tools.\n        *   **Independent Verification Execution**: The \"factored\" variant is particularly novel, ensuring that verification questions are answered independently of the potentially hallucinated baseline response and other verification answers, thereby preventing the model from repeating its own mistakes. This addresses the \"exposure bias\" problem.\n        *   **LLM-only Verification**: Unlike retrieval-augmented methods, CoVe relies solely on the LLM's inherent knowledge and reasoning capabilities for verification.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithm/Method**: Introduction of the Chain-of-Verification (CoVe) method, a multi-stage self-correction pipeline for LLMs.\n    *   **Algorithmic Innovations**: Development of \"factored\" variants for executing verification questions, which significantly reduce the likelihood of repeating hallucinations by answering questions independently of the initial response and other verification contexts.\n    *   **System Design**: A modular design where each step (draft, plan, execute, revise) is performed by prompting the same base LLM, allowing for flexible integration and variant exploration.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**: CoVe was evaluated across a variety of tasks:\n        *   **List-based questions**: Wikidata (easier) and Wiki-Category list (harder), requiring generation of sets of entities.\n        *   **Closed-book MultiSpanQA**: Factoid-based questions with multiple independent answers.\n        *   **Longform text generation**: Generating biographies of entities.\n    *   **Key Performance Metrics & Comparison Results**:\n        *   **Metrics**: Precision (for list-based tasks), FACTSCORE (for longform biographies), and average number of positive/negative (hallucinated) entities.\n        *   **Baselines**: Llama 65B (few-shot), Llama 2 70B Chat (zero-shot, CoT).\n        *   **Results**:\n            *   CoVe consistently decreased hallucinations and improved precision across all tasks compared to baseline LLM generations.\n            *   On Wikidata, Llama 65B CoVe (two-step) achieved 0.36 precision vs. 0.17 for Llama 65B Few-shot, significantly reducing negative entities (0.68 vs 2.95).\n            *   On Wiki-Category list, Llama 65B CoVe (factored) achieved 0.22 precision vs. 0.12 for Llama 65B Few-shot.\n            *   The \"factored\" and \"two-step\" variants generally outperformed the \"joint\" method, demonstrating the importance of separating verification steps and avoiding conditioning on the initial response.\n            *   For longform biography generation, CoVe significantly improved FACTSCORE, indicating higher factual correctness.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**:\n        *   Relies on the base LLM's ability to generate and execute verification plans, and to reason about inconsistencies.\n        *   Does not explore tool-use (e.g., retrieval-augmentation via search engines) for verification, relying solely on the LLM's internal knowledge.\n        *   The \"factored\" approach, while effective, can be computationally more expensive due to requiring many separate LLM prompts, though these can be batched.\n    *   **Scope of Applicability**: Primarily demonstrated for factual hallucination in text generation tasks (list generation, QA, biographies). While the principles might extend, its direct applicability to other types of LLM errors (e.g., logical fallacies, coherence issues) is not explicitly explored.\n\n7.  **Technical Significance**\n    *   **Advance State-of-the-Art**: CoVe significantly advances the state-of-the-art in reducing factual hallucinations in LLMs by introducing a systematic, self-deliberative framework. It demonstrates that LLMs can effectively self-correct by planning and executing internal verification steps.\n    *   **Potential Impact on Future Research**:\n        *   **Enhanced LLM Reliability**: Provides a robust method for improving the factual accuracy of LLM outputs, making them more trustworthy for various applications.\n        *   **Self-Correction Paradigms**: Opens avenues for further research into more sophisticated LLM self-correction mechanisms, potentially integrating external tools with internal deliberation.\n        *   **Understanding LLM Reasoning**: Offers insights into how LLMs can be prompted to perform complex reasoning and self-critique, moving beyond simple next-token prediction.\n        *   **Mitigating Exposure Bias**: The success of factored variants highlights the importance of careful context management in multi-step LLM reasoning to prevent error propagation.",
      "intriguing_abstract": "The pervasive challenge of factual hallucination in Large Language Models (LLMs) severely undermines their trustworthiness and limits their application in critical domains. We introduce **Chain-of-Verification (CoVe)**, a novel generation-time correction framework that empowers LLMs to systematically self-deliberate and correct their own outputs without external tools. CoVe guides an LLM through a four-step process: generating a baseline response, planning specific verification questions, executing these verifications, and finally, revising the initial output.\n\nOur key innovation lies in the \"factored\" execution of verifications, where each factual claim is independently cross-checked, crucially preventing the model from repeating its own errors and mitigating **exposure bias**. Across diverse tasks, including list generation, multi-span QA, and longform biography synthesis, CoVe consistently and significantly reduces hallucinations, improving factual accuracy and precision. This work advances the state-of-the-art in LLM reliability, offering a robust paradigm for internal self-correction and paving the way for more trustworthy and sophisticated AI systems.",
      "keywords": [
        "Hallucination in Large Language Models",
        "Chain-of-Verification (CoVe)",
        "LLM self-correction",
        "Factored verification",
        "Independent verification execution",
        "Generation-time correction",
        "Self-deliberation",
        "Reduced factual hallucinations",
        "Enhanced LLM reliability",
        "Exposure bias mitigation",
        "Multi-step reasoning",
        "Longform text generation",
        "Factual accuracy"
      ],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/4b0b56be0ae9479d2bd5c2f0943db1906343c10f.pdf",
      "citation_key": "dhuliawala2023rqn",
      "metadata": {
        "title": "Chain-of-Verification Reduces Hallucination in Large Language Models",
        "authors": [
          "S. Dhuliawala",
          "M. Komeili",
          "Jing Xu",
          "R. Raileanu",
          "Xian Li",
          "Asli Celikyilmaz",
          "Jason Weston"
        ],
        "published_date": "2023",
        "abstract": "Generation of plausible yet incorrect factual information, termed hallucination, is an unsolved issue in large language models. We study the ability of language models to deliberate on the responses they give in order to correct their mistakes. We develop the Chain-of-Verification (CoVe) method whereby the model first (i) drafts an initial response; then (ii) plans verification questions to fact-check its draft; (iii) answers those questions independently so the answers are not biased by other responses; and (iv) generates its final verified response. In experiments, we show CoVe decreases hallucinations across a variety of tasks, from list-based questions from Wikidata, closed book MultiSpanQA and longform text generation.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/4b0b56be0ae9479d2bd5c2f0943db1906343c10f.pdf",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the technical paper for a literature review:\n\n### Technical Paper Analysis: Chain-of-Verification Reduces Hallucination in Large Language Models \\cite{dhuliawala2023rqn}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: The paper addresses the persistent issue of \"hallucination\" in Large Language Models (LLMs), where models generate plausible but factually incorrect information. This is particularly problematic for lesser-known facts and in longform text generation where errors can compound.\n    *   **Importance and Challenge**: Hallucinations undermine the trustworthiness and reliability of LLM outputs, limiting their applicability in critical domains. It's challenging because even the largest models, despite improved reasoning capabilities, still exhibit this behavior, and simply scaling up doesn't resolve it.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: This work falls under \"generation-time correction\" methods, which aim to improve LLM reliability by adding reasoning decisions on top of the base model. It builds on research encouraging LLMs to generate internal thoughts or reasoning chains (e.g., Chain-of-Thought) and self-critique.\n    *   **Limitations of Previous Solutions**:\n        *   **Training-time correction**: Requires adjusting model weights, which can be complex.\n        *   **Generation-time correction**: Some methods rely on token probabilities or multiple samples, which may not always be robust. Approaches using inconsistencies (e.g., LM vs LM) are related but often involve multi-agent setups.\n        *   **Augmentation (tool-use)**: While effective (e.g., retrieval-augmented generation), CoVe specifically focuses on *self-correction* using only the LLM's internal knowledge, without external tools like search engines.\n        *   **Self-critique/verification**: Previous self-verification methods often focus on logical/mathematical tasks or specific question-answer formats, whereas CoVe targets factual hallucination across diverse generation tasks. CoVe's \"factored\" approach specifically addresses the issue of LLMs repeating their own hallucinations when conditioning on prior incorrect generations.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method (Chain-of-Verification - CoVe)**: CoVe is a four-step process designed to enable an LLM to deliberate on and correct its own responses:\n        1.  **Generate Baseline Response**: The LLM produces an initial draft.\n        2.  **Plan Verifications**: Conditioned on the query and baseline response, the LLM generates a list of specific verification questions to fact-check claims in the draft.\n        3.  **Execute Verifications**: The LLM answers these verification questions. Crucially, several variants are explored:\n            *   **Joint**: Planning and execution in a single prompt, potentially leading to repetition.\n            *   **2-Step**: Separates planning (conditions on baseline) and execution (answers questions without conditioning on baseline) to reduce repetition.\n            *   **Factored**: Answers *each* verification question independently as a separate prompt, without conditioning on the original baseline response or other verification answers. This is the most sophisticated and aims to minimize bias and repetition.\n            *   **Factor+Revise**: Adds an explicit cross-checking step after factored execution to detect inconsistencies.\n        4.  **Generate Final Verified Response**: The LLM produces a revised response, incorporating the insights and corrections from the verification steps.\n    *   **Novelty/Difference**:\n        *   **Systematic Self-Deliberation**: CoVe provides a structured, multi-step framework for LLMs to self-critique and verify factual claims *internally*, without external tools.\n        *   **Independent Verification Execution**: The \"factored\" variant is particularly novel, ensuring that verification questions are answered independently of the potentially hallucinated baseline response and other verification answers, thereby preventing the model from repeating its own mistakes. This addresses the \"exposure bias\" problem.\n        *   **LLM-only Verification**: Unlike retrieval-augmented methods, CoVe relies solely on the LLM's inherent knowledge and reasoning capabilities for verification.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithm/Method**: Introduction of the Chain-of-Verification (CoVe) method, a multi-stage self-correction pipeline for LLMs.\n    *   **Algorithmic Innovations**: Development of \"factored\" variants for executing verification questions, which significantly reduce the likelihood of repeating hallucinations by answering questions independently of the initial response and other verification contexts.\n    *   **System Design**: A modular design where each step (draft, plan, execute, revise) is performed by prompting the same base LLM, allowing for flexible integration and variant exploration.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**: CoVe was evaluated across a variety of tasks:\n        *   **List-based questions**: Wikidata (easier) and Wiki-Category list (harder), requiring generation of sets of entities.\n        *   **Closed-book MultiSpanQA**: Factoid-based questions with multiple independent answers.\n        *   **Longform text generation**: Generating biographies of entities.\n    *   **Key Performance Metrics & Comparison Results**:\n        *   **Metrics**: Precision (for list-based tasks), FACTSCORE (for longform biographies), and average number of positive/negative (hallucinated) entities.\n        *   **Baselines**: Llama 65B (few-shot), Llama 2 70B Chat (zero-shot, CoT).\n        *   **Results**:\n            *   CoVe consistently decreased hallucinations and improved precision across all tasks compared to baseline LLM generations.\n            *   On Wikidata, Llama 65B CoVe (two-step) achieved 0.36 precision vs. 0.17 for Llama 65B Few-shot, significantly reducing negative entities (0.68 vs 2.95).\n            *   On Wiki-Category list, Llama 65B CoVe (factored) achieved 0.22 precision vs. 0.12 for Llama 65B Few-shot.\n            *   The \"factored\" and \"two-step\" variants generally outperformed the \"joint\" method, demonstrating the importance of separating verification steps and avoiding conditioning on the initial response.\n            *   For longform biography generation, CoVe significantly improved FACTSCORE, indicating higher factual correctness.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**:\n        *   Relies on the base LLM's ability to generate and execute verification plans, and to reason about inconsistencies.\n        *   Does not explore tool-use (e.g., retrieval-augmentation via search engines) for verification, relying solely on the LLM's internal knowledge.\n        *   The \"factored\" approach, while effective, can be computationally more expensive due to requiring many separate LLM prompts, though these can be batched.\n    *   **Scope of Applicability**: Primarily demonstrated for factual hallucination in text generation tasks (list generation, QA, biographies). While the principles might extend, its direct applicability to other types of LLM errors (e.g., logical fallacies, coherence issues) is not explicitly explored.\n\n7.  **Technical Significance**\n    *   **Advance State-of-the-Art**: CoVe significantly advances the state-of-the-art in reducing factual hallucinations in LLMs by introducing a systematic, self-deliberative framework. It demonstrates that LLMs can effectively self-correct by planning and executing internal verification steps.\n    *   **Potential Impact on Future Research**:\n        *   **Enhanced LLM Reliability**: Provides a robust method for improving the factual accuracy of LLM outputs, making them more trustworthy for various applications.\n        *   **Self-Correction Paradigms**: Opens avenues for further research into more sophisticated LLM self-correction mechanisms, potentially integrating external tools with internal deliberation.\n        *   **Understanding LLM Reasoning**: Offers insights into how LLMs can be prompted to perform complex reasoning and self-critique, moving beyond simple next-token prediction.\n        *   **Mitigating Exposure Bias**: The success of factored variants highlights the importance of careful context management in multi-step LLM reasoning to prevent error propagation.",
        "keywords": [
          "Hallucination in Large Language Models",
          "Chain-of-Verification (CoVe)",
          "LLM self-correction",
          "Factored verification",
          "Independent verification execution",
          "Generation-time correction",
          "Self-deliberation",
          "Reduced factual hallucinations",
          "Enhanced LLM reliability",
          "Exposure bias mitigation",
          "Multi-step reasoning",
          "Longform text generation",
          "Factual accuracy"
        ],
        "paper_type": "**technical**\n\n**reasoning:**\n\n1.  **abstract:** explicitly states \"we develop the chain-of-verification (cove) method.\" it then describes the steps of this new method and mentions experiments to show its effectiveness. this directly aligns with the \"technical\" criteria: \"presents new methods, algorithms, or systems\" and \"abstract mentions: 'propose', 'develop', 'present', 'algorithm', 'method'\".\n2.  **introduction:** discusses a technical problem (hallucination in llms) and sets the stage for a proposed solution, which is the cove method. this aligns with \"introduction discusses: technical problem, proposed solution\".\n3.  while the paper also includes empirical evaluation (\"in experiments, we show cove decreases hallucinations...\"), the primary contribution highlighted is the *development of the new method* itself, making it fundamentally a technical paper that validates its proposed solution empirically."
      },
      "file_name": "4b0b56be0ae9479d2bd5c2f0943db1906343c10f.pdf"
    },
    {
      "success": true,
      "doc_id": "fb34c216376e960c6704e8bd0bb9f3c0",
      "summary": "Here's a focused summary of the technical paper for a literature review:\n\n---\n\n### Technical Paper Analysis: Can Knowledge Graphs Make Large Language Models More Trustworthy? An Empirical Study Over Open-ended Question Answering \\cite{sui20242u1}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem:** Large Language Models (LLMs) are prone to generating \"hallucinations\" (plausible but incorrect or irrelevant outputs), especially in complex, real-world scenarios. Existing benchmarks for evaluating LLM+Knowledge Graph (KG) integration primarily focus on closed-ended tasks, which are insufficient for thoroughly assessing hallucination reduction and nuanced reasoning in open-ended settings.\n    *   **Importance and Challenge:** Hallucinations pose significant risks in high-stakes domains (e.g., healthcare, scientific research). KGs offer structured, explicit, and traceable factual information, making them a promising avenue for improving LLM reliability. However, evaluating their effectiveness in complex, open-ended question answering (QA) and under realistic conditions (where KGs might contain errors) is challenging due to the lack of appropriate benchmarks and metrics.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches:** The work builds on the growing field of augmenting LLMs with external KGs to enhance reasoning and reduce factual errors. It also relates to Retrieval Augmented Generation (RAG) paradigms.\n    *   **Limitations of Previous Solutions:**\n        *   Existing LLM+KG benchmarks predominantly focus on closed-ended tasks (e.g., fixed sets of entities, relations, or logical forms).\n        *   These closed-ended benchmarks are limited in their ability to detect and quantify hallucinations, as conventional metrics (like accuracy) cannot distinguish between errors from incorrect retrieval and fabricated answers.\n        *   They fall short in evaluating performance for complex, real-world applications that demand nuanced, paragraph-long answers.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method:** The paper introduces **OKGQA**, a novel benchmark specifically designed for evaluating LLMs augmented with KGs in open-ended, real-world question answering settings. It also proposes **OKGQA-P**, a variant to assess model robustness under deliberately perturbed and contaminated KGs.\n    *   **Novelty/Difference:**\n        *   **Open-ended Focus:** Unlike prior work, OKGQA emphasizes generating detailed, paragraph-long answers that include explicit reasoning paths and supporting facts from the KG, enabling direct measurement of hallucination.\n        *   **Diverse Question Types:** OKGQA incorporates a wide range of complex question types (descriptive, explanatory, predictive, comparative, critical) that cannot be answered by simple fact retrieval.\n        *   **KG Perturbation (OKGQA-P):** It introduces a unique mechanism to simulate real-world KG imperfections by perturbing KG semantics and structure (e.g., relation swapping, replacement, edge rewiring, deletion), allowing for robustness evaluation.\n        *   **Unified KG-augmented Framework:** A RAG-like framework is proposed, comprising a \"Graph-guided retrieval\" component (G-retrieval) and a \"Graph-guided generator\" component (G-Generator), with various algorithmic design choices for each.\n        *   **Prize-Cost Trade-off for Retrieval:** G-retrieval employs a novel prize-cost strategy to extract compact yet informative subgraphs, paths, or triplets from the KG, balancing relevance and size.\n\n4.  **Key Technical Contributions**\n    *   **Novel Benchmark (OKGQA):** A new, comprehensive benchmark for open-ended KGQA, featuring diverse, complex query types and designed to evaluate hallucination rates and reasoning improvements in LLM+KG models.\n    *   **Robustness Benchmark (OKGQA-P):** A variant of OKGQA that systematically introduces noise and perturbations into KGs to evaluate model performance under imperfect knowledge conditions. This includes specific edge-based perturbation heuristics (Relation Swapping, Relation Replacement, Edge Rewiring, Edge Deletion) and metrics (ATS, SC2D, SD2) to quantify perturbation levels.\n    *   **KG-Augmented Framework:** A unified retrieval-augmented generation (RAG) framework for integrating KGs with LLMs, featuring:\n        *   **Graph-guided Retrieval:** Incorporates a prize-cost trade-off strategy for efficient and relevant knowledge extraction, offering three variants: triplet-retrieval, path-retrieval, and subgraph-retrieval (using Prize-Collecting Steiner Tree).\n        *   **Graph-guided Generation:** Utilizes the retrieved knowledge to generate coherent, factually grounded responses.\n    *   **Empirical Analysis:** Provides a comparative analysis of different KG integration strategies and their impact on hallucination reduction and overall response quality.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted:**\n        *   Evaluation of various LLM+KG integration methods on the OKGQA benchmark to assess hallucination reduction and reasoning improvements.\n        *   Experiments on OKGQA-P to study how KG-aware methods are affected by different levels and types of noise/perturbations in KGs.\n    *   **Key Performance Metrics:**\n        *   **Hallucination Metrics:** FActScore \\cite{sui20242u1} (measures factual precision by validating atomic facts against Wikipedia) and SAFE \\cite{sui20242u1} (uses an LLM as an investigative agent with Google Search to assess factual support). Both report the proportion of supported atomic facts.\n        *   **Response Quality Metrics:** Four metrics using an LLM-as-evaluator approach, specifically the G-Eval framework \\cite{sui20242u1}.\n    *   **Comparison Results (Key Findings):**\n        *   Integrating KG information generally reduces factual errors, particularly for queries requiring deeper reasoning.\n        *   Relying solely on internal LLM reasoning strategies (e.g., Chain-of-Thought, Self-Consistency) can introduce biases and hallucinations.\n        *   Subgraph-based retrieval methods often achieve the best performance for simpler query types.\n        *   KGs effectively reduce hallucinations in LLMs even when the KG itself is partially contaminated or perturbed.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions:**\n        *   KG subgraph extraction is limited to a 2-hop neighborhood to balance coverage and computational feasibility, which might miss relevant information further away.\n        *   The current benchmark uses DBpedia; applicability to other KGs or domain-specific KGs might require further validation.\n        *   The perturbation methods are edge-based heuristics; more sophisticated or realistic KG error models could be explored.\n    *   **Scope of Applicability:** The benchmark and findings are primarily focused on open-ended question answering tasks where detailed, paragraph-long responses are expected. While valuable, the insights might not directly translate to all LLM applications (e.g., highly constrained generation tasks).\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art:**\n        *   Provides the first dedicated benchmark (OKGQA) for evaluating LLM+KG models in open-ended QA, addressing a critical gap in existing evaluation methodologies.\n        *   Introduces a novel approach (OKGQA-P) to systematically assess the robustness of LLM+KG systems to imperfect KGs, which is crucial for real-world deployment.\n        *   Offers empirical evidence and comparative analysis on how different KG integration strategies impact hallucination reduction, guiding future method design.\n    *   **Potential Impact on Future Research:**\n        *   Facilitates more complete performance comparisons and encourages continuous improvement in integrating KGs with LLMs to mitigate hallucination and enhance trustworthiness.\n        *   Promotes research into more robust KG-augmented LLM architectures that can handle noisy or incomplete knowledge sources.\n        *   Inspires the development of advanced KG retrieval and generation techniques that leverage the structured nature of KGs more effectively for complex reasoning.",
      "intriguing_abstract": "The pervasive challenge of **hallucinations** in Large Language Models (LLMs) severely limits their trustworthiness, especially in complex, **open-ended question answering (QA)** scenarios where existing evaluation benchmarks fall critically short. We introduce **OKGQA**, a novel and comprehensive benchmark specifically designed to rigorously assess LLM performance when augmented with **Knowledge Graphs (KGs)**, focusing on generating detailed, factually grounded, paragraph-long responses and explicitly measuring hallucination rates.\n\nTo simulate real-world imperfections, we further present **OKGQA-P**, a unique variant that systematically perturbs KG semantics and structure, enabling crucial **robustness** evaluations. Our unified **Retrieval Augmented Generation (RAG)** framework, featuring a novel prize-cost strategy for efficient graph-guided knowledge extraction, demonstrates that KGs significantly mitigate hallucinations, even under noisy conditions. This work provides critical empirical insights into effective KG integration strategies, guiding future research and advancing the state-of-the-art in building more reliable and trustworthy LLMs for high-stakes applications.",
      "keywords": [
        "Large Language Models",
        "Knowledge Graphs",
        "LLM Hallucinations",
        "Open-ended Question Answering",
        "OKGQA benchmark",
        "KG Perturbation",
        "Model Robustness",
        "Retrieval Augmented Generation (RAG)",
        "Graph-guided Retrieval",
        "Prize-cost Trade-off Retrieval",
        "Factual Error Reduction",
        "Subgraph-based Retrieval",
        "Hallucination Metrics (FActScore",
        "SAFE)",
        "Imperfect Knowledge Graphs"
      ],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/89fccb4b70d0a072d9c874dddfab0afb3676d1b8.pdf",
      "citation_key": "sui20242u1",
      "metadata": {
        "title": "Can Knowledge Graphs Make Large Language Models More Trustworthy? An Empirical Study over Open-ended Question Answering",
        "authors": [
          "Yuan Sui",
          "Bryan Hooi"
        ],
        "published_date": "2024",
        "abstract": "Recent works integrating Knowledge Graphs (KGs) have shown promising improvements in enhancing the reasoning capabilities of Large Language Models (LLMs). However, existing benchmarks primarily focus on closed-ended tasks, leaving a gap in evaluating performance on more complex, real-world scenarios. This limitation also hinders a thorough assessment of KGs' potential to reduce hallucinations in LLMs. To address this, we introduce OKGQA, a new benchmark specifically designed to evaluate LLMs augmented with KGs in open-ended, real-world question answering settings. OKGQA reflects practical complexities through diverse question types and incorporates metrics to quantify both hallucination rates and reasoning improvements in LLM+KG models. To consider the scenarios in which KGs may contain varying levels of errors, we propose a benchmark variant, OKGQA-P, to assess model performance when the semantics and structure of KGs are deliberately perturbed and contaminated. In this paper, we aims to (1) explore whether KGs can make LLMs more trustworthy in an open-ended setting, and (2) conduct a comparative analysis to shed light on method design. We believe this study can facilitate a more complete performance comparison and encourages continuous improvement in integrating KGs with LLMs to mitigate hallucination, and make LLMs more trustworthy. Code and data are released at https://github.com/Y-Sui/OKGQA.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/89fccb4b70d0a072d9c874dddfab0afb3676d1b8.pdf",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the technical paper for a literature review:\n\n---\n\n### Technical Paper Analysis: Can Knowledge Graphs Make Large Language Models More Trustworthy? An Empirical Study Over Open-ended Question Answering \\cite{sui20242u1}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem:** Large Language Models (LLMs) are prone to generating \"hallucinations\" (plausible but incorrect or irrelevant outputs), especially in complex, real-world scenarios. Existing benchmarks for evaluating LLM+Knowledge Graph (KG) integration primarily focus on closed-ended tasks, which are insufficient for thoroughly assessing hallucination reduction and nuanced reasoning in open-ended settings.\n    *   **Importance and Challenge:** Hallucinations pose significant risks in high-stakes domains (e.g., healthcare, scientific research). KGs offer structured, explicit, and traceable factual information, making them a promising avenue for improving LLM reliability. However, evaluating their effectiveness in complex, open-ended question answering (QA) and under realistic conditions (where KGs might contain errors) is challenging due to the lack of appropriate benchmarks and metrics.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches:** The work builds on the growing field of augmenting LLMs with external KGs to enhance reasoning and reduce factual errors. It also relates to Retrieval Augmented Generation (RAG) paradigms.\n    *   **Limitations of Previous Solutions:**\n        *   Existing LLM+KG benchmarks predominantly focus on closed-ended tasks (e.g., fixed sets of entities, relations, or logical forms).\n        *   These closed-ended benchmarks are limited in their ability to detect and quantify hallucinations, as conventional metrics (like accuracy) cannot distinguish between errors from incorrect retrieval and fabricated answers.\n        *   They fall short in evaluating performance for complex, real-world applications that demand nuanced, paragraph-long answers.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method:** The paper introduces **OKGQA**, a novel benchmark specifically designed for evaluating LLMs augmented with KGs in open-ended, real-world question answering settings. It also proposes **OKGQA-P**, a variant to assess model robustness under deliberately perturbed and contaminated KGs.\n    *   **Novelty/Difference:**\n        *   **Open-ended Focus:** Unlike prior work, OKGQA emphasizes generating detailed, paragraph-long answers that include explicit reasoning paths and supporting facts from the KG, enabling direct measurement of hallucination.\n        *   **Diverse Question Types:** OKGQA incorporates a wide range of complex question types (descriptive, explanatory, predictive, comparative, critical) that cannot be answered by simple fact retrieval.\n        *   **KG Perturbation (OKGQA-P):** It introduces a unique mechanism to simulate real-world KG imperfections by perturbing KG semantics and structure (e.g., relation swapping, replacement, edge rewiring, deletion), allowing for robustness evaluation.\n        *   **Unified KG-augmented Framework:** A RAG-like framework is proposed, comprising a \"Graph-guided retrieval\" component (G-retrieval) and a \"Graph-guided generator\" component (G-Generator), with various algorithmic design choices for each.\n        *   **Prize-Cost Trade-off for Retrieval:** G-retrieval employs a novel prize-cost strategy to extract compact yet informative subgraphs, paths, or triplets from the KG, balancing relevance and size.\n\n4.  **Key Technical Contributions**\n    *   **Novel Benchmark (OKGQA):** A new, comprehensive benchmark for open-ended KGQA, featuring diverse, complex query types and designed to evaluate hallucination rates and reasoning improvements in LLM+KG models.\n    *   **Robustness Benchmark (OKGQA-P):** A variant of OKGQA that systematically introduces noise and perturbations into KGs to evaluate model performance under imperfect knowledge conditions. This includes specific edge-based perturbation heuristics (Relation Swapping, Relation Replacement, Edge Rewiring, Edge Deletion) and metrics (ATS, SC2D, SD2) to quantify perturbation levels.\n    *   **KG-Augmented Framework:** A unified retrieval-augmented generation (RAG) framework for integrating KGs with LLMs, featuring:\n        *   **Graph-guided Retrieval:** Incorporates a prize-cost trade-off strategy for efficient and relevant knowledge extraction, offering three variants: triplet-retrieval, path-retrieval, and subgraph-retrieval (using Prize-Collecting Steiner Tree).\n        *   **Graph-guided Generation:** Utilizes the retrieved knowledge to generate coherent, factually grounded responses.\n    *   **Empirical Analysis:** Provides a comparative analysis of different KG integration strategies and their impact on hallucination reduction and overall response quality.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted:**\n        *   Evaluation of various LLM+KG integration methods on the OKGQA benchmark to assess hallucination reduction and reasoning improvements.\n        *   Experiments on OKGQA-P to study how KG-aware methods are affected by different levels and types of noise/perturbations in KGs.\n    *   **Key Performance Metrics:**\n        *   **Hallucination Metrics:** FActScore \\cite{sui20242u1} (measures factual precision by validating atomic facts against Wikipedia) and SAFE \\cite{sui20242u1} (uses an LLM as an investigative agent with Google Search to assess factual support). Both report the proportion of supported atomic facts.\n        *   **Response Quality Metrics:** Four metrics using an LLM-as-evaluator approach, specifically the G-Eval framework \\cite{sui20242u1}.\n    *   **Comparison Results (Key Findings):**\n        *   Integrating KG information generally reduces factual errors, particularly for queries requiring deeper reasoning.\n        *   Relying solely on internal LLM reasoning strategies (e.g., Chain-of-Thought, Self-Consistency) can introduce biases and hallucinations.\n        *   Subgraph-based retrieval methods often achieve the best performance for simpler query types.\n        *   KGs effectively reduce hallucinations in LLMs even when the KG itself is partially contaminated or perturbed.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions:**\n        *   KG subgraph extraction is limited to a 2-hop neighborhood to balance coverage and computational feasibility, which might miss relevant information further away.\n        *   The current benchmark uses DBpedia; applicability to other KGs or domain-specific KGs might require further validation.\n        *   The perturbation methods are edge-based heuristics; more sophisticated or realistic KG error models could be explored.\n    *   **Scope of Applicability:** The benchmark and findings are primarily focused on open-ended question answering tasks where detailed, paragraph-long responses are expected. While valuable, the insights might not directly translate to all LLM applications (e.g., highly constrained generation tasks).\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art:**\n        *   Provides the first dedicated benchmark (OKGQA) for evaluating LLM+KG models in open-ended QA, addressing a critical gap in existing evaluation methodologies.\n        *   Introduces a novel approach (OKGQA-P) to systematically assess the robustness of LLM+KG systems to imperfect KGs, which is crucial for real-world deployment.\n        *   Offers empirical evidence and comparative analysis on how different KG integration strategies impact hallucination reduction, guiding future method design.\n    *   **Potential Impact on Future Research:**\n        *   Facilitates more complete performance comparisons and encourages continuous improvement in integrating KGs with LLMs to mitigate hallucination and enhance trustworthiness.\n        *   Promotes research into more robust KG-augmented LLM architectures that can handle noisy or incomplete knowledge sources.\n        *   Inspires the development of advanced KG retrieval and generation techniques that leverage the structured nature of KGs more effectively for complex reasoning.",
        "keywords": [
          "Large Language Models",
          "Knowledge Graphs",
          "LLM Hallucinations",
          "Open-ended Question Answering",
          "OKGQA benchmark",
          "KG Perturbation",
          "Model Robustness",
          "Retrieval Augmented Generation (RAG)",
          "Graph-guided Retrieval",
          "Prize-cost Trade-off Retrieval",
          "Factual Error Reduction",
          "Subgraph-based Retrieval",
          "Hallucination Metrics (FActScore",
          "SAFE)",
          "Imperfect Knowledge Graphs"
        ],
        "paper_type": "the paper should be classified as **empirical**.\n\nhere's why:\n\n1.  **title:** the title explicitly states \"an empirical study.\"\n2.  **abstract:**\n    *   it introduces a new benchmark (okgqa) which is a technical contribution, but the *purpose* of this benchmark is \"to evaluate llms augmented with kgs.\"\n    *   it mentions \"incorporates metrics to quantify both hallucination rates and reasoning improvements,\" indicating data collection and measurement.\n    *   it clearly states the aims: \"(1) explore whether kgs can make llms more trustworthy in an open-ended setting, and (2) conduct a comparative analysis to shed light on method design.\" these are classic goals of an empirical study.\n    *   it concludes by saying \"we believe this study can facilitate a more complete performance comparison.\"\n3.  **introduction:** it sets up the problem (llm hallucinations) and the proposed solution (augmenting with kgs), leading into the need for their evaluation framework and study.\n\nwhile the paper *develops a new benchmark* (a technical contribution), the primary focus and stated goal are to *conduct a study* using this benchmark to answer specific research questions and perform a comparative analysis, which falls squarely under the definition of an empirical paper."
      },
      "file_name": "89fccb4b70d0a072d9c874dddfab0afb3676d1b8.pdf"
    },
    {
      "success": true,
      "doc_id": "6f7ffd94af6b10bf57908e180af09540",
      "summary": "Here's a focused summary of the paper for a literature review, adhering to your citation requirements:\n\n### Technical Paper Analysis: Detecting and Mitigating Hallucination in Large Vision Language Models via Fine-Grained AI Feedback \\cite{xiao2024hv1}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Large Vision Language Models (LVLMs) frequently suffer from \"hallucination,\" where generated responses contain incorrect objects, attributes, or relationships that do not align with the given visual and linguistic contexts.\n    *   **Importance and Challenge**: This hallucination phenomenon significantly restricts the practical utility and trustworthiness of LVLMs. Existing solutions are often limited by:\n        *   Coarse-grained detection and mitigation (e.g., response-level feedback).\n        *   High costs associated with human expert annotation or reliance on expensive proprietary models for fine-grained feedback.\n        *   Treating all hallucinations equally, neglecting to prioritize the mitigation of more critical errors.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**:\n        *   **Hallucination Detection**: Previous work utilizes off-the-shelf tools like closed-source LLMs (e.g., GPT-4 in GAVIE, SHR) or LVLMs (e.g., GPT-4V/Gemini in UNIHD) to identify hallucinations, sometimes at a sentence level.\n        *   **Hallucination Mitigation**: Approaches include training-free methods (post-processing outputs) and training-based methods (instruction fine-tuning, preference learning). Preference learning methods like LLaVA-RLHF, RLHF-V, POVID, and Silkie use human or model feedback to bias LVLMs towards non-hallucinatory responses.\n    *   **Limitations of Previous Solutions**:\n        *   Most preference data is at a coarse, response-level, which is sub-optimal for thorough detection and mitigation.\n        *   Constructing fine-grained preference data is prohibitively expensive, requiring extensive human or proprietary model annotation.\n        *   Existing mitigation strategies often treat all hallucinations with equal importance, failing to differentiate and prioritize more severe errors. Silkie, for instance, uses LVLM feedback but at a coarse granularity.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes a three-component framework for detecting and mitigating hallucinations via fine-grained AI feedback \\cite{xiao2024hv1}:\n        1.  **Fine-Grained AI Feedback Generation**: A small-scale, sentence-level hallucination annotation dataset is generated using proprietary models (GPT-4/GPT-4V). This feedback includes hallucination type (object, attribute, relationship), severity (Minor, Moderate, Major), and rationale.\n        2.  **Fine-Grained AI Feedback for Hallucination Detection**: An open-source hallucination detection model (`Mdet`) is trained on this fine-grained AI feedback, enabling sentence-level detection with severity assessment. This model is then integrated into a \"detect-then-rewrite\" pipeline for automatic preference dataset construction.\n        3.  **Hallucination Severity-Aware Direct Preference Optimization (HSA-DPO)**: This novel preference learning approach differentiates the severity of hallucinations. It aggregates sentence-wise severity scores into a response-level score and incorporates this into the DPO objective, prioritizing the mitigation of critical hallucinations.\n    *   **Novelty/Differentiation**:\n        *   **Fine-grained, sentence-level analysis**: Unlike coarse-grained approaches, \\cite{xiao2024hv1} provides detailed feedback on hallucination type, severity, and rationale at the sentence level.\n        *   **Automated preference dataset construction**: The \"detect-then-rewrite\" pipeline, leveraging an open-source detection model and a rewriting model (LLaVA), significantly reduces the cost of creating large-scale, fine-grained preference datasets, aligning with the concept of scalable oversight.\n        *   **Severity-aware mitigation**: HSA-DPO is the first to explicitly incorporate hallucination severity into preference learning, allowing the model to prioritize fixing more impactful errors.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**:\n        *   A method for generating fine-grained, sentence-level hallucination annotations (type, severity, rationale) using proprietary LVLMs.\n        *   A \"detect-then-rewrite\" pipeline for cost-effectively constructing large-scale, fine-grained preference datasets for hallucination mitigation.\n        *   Hallucination Severity-Aware Direct Preference Optimization (HSA-DPO), which integrates hallucination severity scores into the DPO objective to prioritize critical hallucination mitigation.\n    *   **System Design/Architectural Innovations**: A modular framework comprising fine-grained feedback generation, a trained detection model, and a novel preference optimization algorithm.\n    *   **Theoretical Insights/Analysis**: The \"detect-then-rewrite\" pipeline is explicitly linked to the concept of \"scalable oversight,\" demonstrating how machines can assist in evaluating and correcting model outputs by breaking down complex problems.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**: Extensive experiments were performed on both hallucination detection and mitigation benchmarks.\n        *   **Detection**: Evaluated the trained detection model on MHaluBench.\n        *   **Mitigation**: Evaluated the HSA-DPO method on AMBER and Object HalBench benchmarks, comparing against state-of-the-art models like Silkie, GPT-4V, and RLHF-V.\n    *   **Key Performance Metrics and Comparison Results**:\n        *   **Hallucination Detection**: The proposed detection model achieved new state-of-the-art results on MHaluBench, outperforming powerful proprietary models like GPT-4V and Gemini.\n        *   **Hallucination Mitigation**: HSA-DPO significantly improved the base LVLM's performance:\n            *   Reduced the Hallucination Rate on AMBER by 36.1%.\n            *   Reduced the CHAIR Score on Object HalBench by 76.3%.\n            *   Outperformed state-of-the-art mitigation models (Silkie, GPT-4V, RLHF-V) across all metrics on both benchmarks.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**:\n        *   The initial fine-grained AI feedback generation relies on the capabilities and accuracy of proprietary models (GPT-4/GPT-4V). The quality of the trained detection model is thus dependent on the quality of this initial feedback.\n        *   The paper primarily focuses on Detailed Description Generation (DDG) and Visual Complex Reasoning (VCR) tasks, though it states the method can be extended to other visual-language tasks.\n    *   **Scope of Applicability**: The method is designed for Large Vision Language Models (LVLMs) and is particularly beneficial for scenarios requiring high fidelity and trustworthiness in generated content, especially where fine-grained control over hallucination mitigation is desired.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: \\cite{xiao2024hv1} significantly advances the technical state-of-the-art in LVLM hallucination by introducing fine-grained, sentence-level detection and a novel severity-aware mitigation strategy. It demonstrates that open-source models can be trained to surpass proprietary models in specific hallucination detection tasks and achieve superior mitigation performance.\n    *   **Potential Impact on Future Research**:\n        *   **Cost-effective training**: The automated \"detect-then-rewrite\" pipeline offers a scalable and budget-friendly approach to generate high-quality preference datasets, reducing reliance on expensive human or proprietary model annotations.\n        *   **More reliable LVLMs**: By prioritizing critical hallucinations, HSA-DPO can lead to more robust and trustworthy LVLMs, enhancing their applicability in sensitive domains.\n        *   **Foundation for nuanced feedback**: The fine-grained feedback mechanism (type, severity, rationale) provides a richer basis for understanding and addressing model failures, opening avenues for more targeted model improvements.",
      "intriguing_abstract": "Large Vision Language Models (LVLMs) are increasingly powerful, yet their widespread adoption is hampered by persistent hallucinationsâ€”generating incorrect objects, attributes, or relationships that erode trustworthiness. Existing mitigation strategies are often coarse-grained, expensive, and fail to differentiate critical errors from minor ones.\n\nWe introduce a novel framework that revolutionizes hallucination detection and mitigation through **fine-grained AI feedback**. Our approach first generates detailed, sentence-level annotations, including hallucination type, severity, and rationale. This rich data trains an open-source detection model (`Mdet`) that achieves state-of-the-art performance, even surpassing powerful proprietary models like GPT-4V. Leveraging this detector, we propose a cost-effective \"detect-then-rewrite\" pipeline for automated, large-scale preference dataset construction, embodying the principle of scalable oversight. Crucially, we present **Hallucination Severity-Aware Direct Preference Optimization (HSA-DPO)**, a pioneering preference learning method that integrates severity scores directly into the DPO objective, prioritizing the mitigation of the most impactful hallucinations. Experiments demonstrate HSA-DPO's superior performance, significantly reducing hallucination rates and outperforming leading models. This work paves the way for more reliable, trustworthy, and economically trainable LVLMs.",
      "keywords": [
        "Large Vision Language Models (LVLMs)",
        "hallucination detection and mitigation",
        "fine-grained AI feedback",
        "sentence-level hallucination analysis",
        "Hallucination Severity-Aware Direct Preference Optimization (HSA-DPO)",
        "automated preference dataset construction",
        "detect-then-rewrite pipeline",
        "scalable oversight",
        "open-source hallucination detection",
        "state-of-the-art performance",
        "trustworthy AI",
        "hallucination types and severity"
      ],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/27d55a944b5c02b8c10eb250773d8eb082e06476.pdf",
      "citation_key": "xiao2024hv1",
      "metadata": {
        "title": "Detecting and Mitigating Hallucination in Large Vision Language Models via Fine-Grained AI Feedback",
        "authors": [
          "Wenyi Xiao",
          "Ziwei Huang",
          "Leilei Gan",
          "Wanggui He",
          "Haoyuan Li",
          "Zhelun Yu",
          "Hao Jiang",
          "Fei Wu",
          "Linchao Zhu"
        ],
        "published_date": "2024",
        "abstract": "The rapidly developing Large Vision Language Models (LVLMs) have shown notable capabilities on a range of multi-modal tasks, but still face the hallucination phenomena where the generated texts do not align with the given contexts, significantly restricting the usages of LVLMs. Most previous work detects and mitigates hallucination at the coarse-grained level or requires expensive annotation (e.g., labeling by proprietary models or human experts). To address these issues, we propose detecting and mitigating hallucinations in LVLMs via fine-grained AI feedback. The basic idea is that we generate a small-size sentence-level hallucination annotation dataset by proprietary models, whereby we train a hallucination detection model which can perform sentence-level hallucination detection, covering primary hallucination types (i.e., object, attribute, and relationship). Then, we propose a detect-then-rewrite pipeline to automatically construct preference dataset for training hallucination mitigating model. Furthermore, we propose differentiating the severity of hallucinations, and introducing a Hallucination Severity-Aware Direct Preference Optimization (HSA-DPO) for mitigating hallucination in LVLMs by incorporating the severity of hallucinations into preference learning. Extensive experiments demonstrate the effectiveness of our method.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/27d55a944b5c02b8c10eb250773d8eb082e06476.pdf",
        "venue": "arXiv.org",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the paper for a literature review, adhering to your citation requirements:\n\n### Technical Paper Analysis: Detecting and Mitigating Hallucination in Large Vision Language Models via Fine-Grained AI Feedback \\cite{xiao2024hv1}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Large Vision Language Models (LVLMs) frequently suffer from \"hallucination,\" where generated responses contain incorrect objects, attributes, or relationships that do not align with the given visual and linguistic contexts.\n    *   **Importance and Challenge**: This hallucination phenomenon significantly restricts the practical utility and trustworthiness of LVLMs. Existing solutions are often limited by:\n        *   Coarse-grained detection and mitigation (e.g., response-level feedback).\n        *   High costs associated with human expert annotation or reliance on expensive proprietary models for fine-grained feedback.\n        *   Treating all hallucinations equally, neglecting to prioritize the mitigation of more critical errors.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**:\n        *   **Hallucination Detection**: Previous work utilizes off-the-shelf tools like closed-source LLMs (e.g., GPT-4 in GAVIE, SHR) or LVLMs (e.g., GPT-4V/Gemini in UNIHD) to identify hallucinations, sometimes at a sentence level.\n        *   **Hallucination Mitigation**: Approaches include training-free methods (post-processing outputs) and training-based methods (instruction fine-tuning, preference learning). Preference learning methods like LLaVA-RLHF, RLHF-V, POVID, and Silkie use human or model feedback to bias LVLMs towards non-hallucinatory responses.\n    *   **Limitations of Previous Solutions**:\n        *   Most preference data is at a coarse, response-level, which is sub-optimal for thorough detection and mitigation.\n        *   Constructing fine-grained preference data is prohibitively expensive, requiring extensive human or proprietary model annotation.\n        *   Existing mitigation strategies often treat all hallucinations with equal importance, failing to differentiate and prioritize more severe errors. Silkie, for instance, uses LVLM feedback but at a coarse granularity.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes a three-component framework for detecting and mitigating hallucinations via fine-grained AI feedback \\cite{xiao2024hv1}:\n        1.  **Fine-Grained AI Feedback Generation**: A small-scale, sentence-level hallucination annotation dataset is generated using proprietary models (GPT-4/GPT-4V). This feedback includes hallucination type (object, attribute, relationship), severity (Minor, Moderate, Major), and rationale.\n        2.  **Fine-Grained AI Feedback for Hallucination Detection**: An open-source hallucination detection model (`Mdet`) is trained on this fine-grained AI feedback, enabling sentence-level detection with severity assessment. This model is then integrated into a \"detect-then-rewrite\" pipeline for automatic preference dataset construction.\n        3.  **Hallucination Severity-Aware Direct Preference Optimization (HSA-DPO)**: This novel preference learning approach differentiates the severity of hallucinations. It aggregates sentence-wise severity scores into a response-level score and incorporates this into the DPO objective, prioritizing the mitigation of critical hallucinations.\n    *   **Novelty/Differentiation**:\n        *   **Fine-grained, sentence-level analysis**: Unlike coarse-grained approaches, \\cite{xiao2024hv1} provides detailed feedback on hallucination type, severity, and rationale at the sentence level.\n        *   **Automated preference dataset construction**: The \"detect-then-rewrite\" pipeline, leveraging an open-source detection model and a rewriting model (LLaVA), significantly reduces the cost of creating large-scale, fine-grained preference datasets, aligning with the concept of scalable oversight.\n        *   **Severity-aware mitigation**: HSA-DPO is the first to explicitly incorporate hallucination severity into preference learning, allowing the model to prioritize fixing more impactful errors.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**:\n        *   A method for generating fine-grained, sentence-level hallucination annotations (type, severity, rationale) using proprietary LVLMs.\n        *   A \"detect-then-rewrite\" pipeline for cost-effectively constructing large-scale, fine-grained preference datasets for hallucination mitigation.\n        *   Hallucination Severity-Aware Direct Preference Optimization (HSA-DPO), which integrates hallucination severity scores into the DPO objective to prioritize critical hallucination mitigation.\n    *   **System Design/Architectural Innovations**: A modular framework comprising fine-grained feedback generation, a trained detection model, and a novel preference optimization algorithm.\n    *   **Theoretical Insights/Analysis**: The \"detect-then-rewrite\" pipeline is explicitly linked to the concept of \"scalable oversight,\" demonstrating how machines can assist in evaluating and correcting model outputs by breaking down complex problems.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**: Extensive experiments were performed on both hallucination detection and mitigation benchmarks.\n        *   **Detection**: Evaluated the trained detection model on MHaluBench.\n        *   **Mitigation**: Evaluated the HSA-DPO method on AMBER and Object HalBench benchmarks, comparing against state-of-the-art models like Silkie, GPT-4V, and RLHF-V.\n    *   **Key Performance Metrics and Comparison Results**:\n        *   **Hallucination Detection**: The proposed detection model achieved new state-of-the-art results on MHaluBench, outperforming powerful proprietary models like GPT-4V and Gemini.\n        *   **Hallucination Mitigation**: HSA-DPO significantly improved the base LVLM's performance:\n            *   Reduced the Hallucination Rate on AMBER by 36.1%.\n            *   Reduced the CHAIR Score on Object HalBench by 76.3%.\n            *   Outperformed state-of-the-art mitigation models (Silkie, GPT-4V, RLHF-V) across all metrics on both benchmarks.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**:\n        *   The initial fine-grained AI feedback generation relies on the capabilities and accuracy of proprietary models (GPT-4/GPT-4V). The quality of the trained detection model is thus dependent on the quality of this initial feedback.\n        *   The paper primarily focuses on Detailed Description Generation (DDG) and Visual Complex Reasoning (VCR) tasks, though it states the method can be extended to other visual-language tasks.\n    *   **Scope of Applicability**: The method is designed for Large Vision Language Models (LVLMs) and is particularly beneficial for scenarios requiring high fidelity and trustworthiness in generated content, especially where fine-grained control over hallucination mitigation is desired.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: \\cite{xiao2024hv1} significantly advances the technical state-of-the-art in LVLM hallucination by introducing fine-grained, sentence-level detection and a novel severity-aware mitigation strategy. It demonstrates that open-source models can be trained to surpass proprietary models in specific hallucination detection tasks and achieve superior mitigation performance.\n    *   **Potential Impact on Future Research**:\n        *   **Cost-effective training**: The automated \"detect-then-rewrite\" pipeline offers a scalable and budget-friendly approach to generate high-quality preference datasets, reducing reliance on expensive human or proprietary model annotations.\n        *   **More reliable LVLMs**: By prioritizing critical hallucinations, HSA-DPO can lead to more robust and trustworthy LVLMs, enhancing their applicability in sensitive domains.\n        *   **Foundation for nuanced feedback**: The fine-grained feedback mechanism (type, severity, rationale) provides a richer basis for understanding and addressing model failures, opening avenues for more targeted model improvements.",
        "keywords": [
          "Large Vision Language Models (LVLMs)",
          "hallucination detection and mitigation",
          "fine-grained AI feedback",
          "sentence-level hallucination analysis",
          "Hallucination Severity-Aware Direct Preference Optimization (HSA-DPO)",
          "automated preference dataset construction",
          "detect-then-rewrite pipeline",
          "scalable oversight",
          "open-source hallucination detection",
          "state-of-the-art performance",
          "trustworthy AI",
          "hallucination types and severity"
        ],
        "paper_type": "based on the abstract and introduction, this paper is best classified as **technical**.\n\nhere's why:\n\n*   **abstract mentions:** \"we **propose** detecting and mitigating hallucinations...\", \"we **train a detection model**\", \"we **propose a detect-then-rewrite pipeline**\", \"we **propose differentiating... and introducing a hallucination severity-aware direct preference optimization (hsa-dpo)**\". these phrases clearly indicate the development and presentation of new methods, algorithms, and systems.\n*   **introduction discusses:** the problem of hallucination in lvlms (a technical problem) and sets the stage for their proposed solution, which is then detailed in the abstract. figure 1 also highlights the performance of their proposed model (hsa-dpo) against state-of-the-art models.\n*   **empirical validation:** while the paper includes \"extensive experiments\" and \"demonstrate that our method sets a new state-of-the-art\", these experiments serve to validate the effectiveness of the *proposed technical methods*. the primary contribution is the novel approach itself, rather than solely a data-driven study of an existing phenomenon."
      },
      "file_name": "27d55a944b5c02b8c10eb250773d8eb082e06476.pdf"
    },
    {
      "success": true,
      "doc_id": "6422218d991d7515f6f17b967b3fc197",
      "summary": "Here's a focused summary of the paper \"Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions\" \\cite{trivedi2022qsf} for a literature review:\n\n---\n\n### Focused Summary for Literature Review: Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions \\cite{trivedi2022qsf}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Large Language Models (LLMs) using Chain-of-Thought (CoT) reasoning struggle with knowledge-intensive, multi-step question answering (QA) when the necessary information is not available or up-to-date within their parameters \\cite{trivedi2022qsf}.\n    *   **Importance and Challenge**: A simple one-step retrieve-and-read approach (retrieving based solely on the initial question) is insufficient for multi-step QA. For complex questions, what needs to be retrieved often depends on intermediate reasoning steps already derived, which in turn might depend on previously retrieved information. This creates a circular dependency where reasoning needs facts, and facts need reasoning to be identified, leading to potential hallucination without proper grounding \\cite{trivedi2022qsf}.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**:\n        *   **Prompting-based Open-Domain QA**: Approaches like SelfAsk \\cite{trivedi2022qsf}, DecomP \\cite{trivedi2022qsf}, and ReAct \\cite{trivedi2022qsf} decompose questions or generate action sequences.\n        *   **Supervised Multi-Step Open-Domain QA**: Prior work (e.g., Das et al., 2019; Xiong et al., 2021; Qi et al., 2019) explored iterative retrieval, often using neural query reformulation or entity linking.\n    *   **Limitations of Previous Solutions**:\n        *   **Prompting-based**: SelfAsk and DecomP are not designed for CoT reasoning, do not focus on the dynamic retrieval problem during reasoning, and often require single-hop QA models. ReAct relies on much larger models (e.g., PaLM-540B) and often requires fine-tuning \\cite{trivedi2022qsf}. These methods are not shown to be effective for smaller models without training.\n        *   **Supervised Iterative Retrieval**: These methods typically rely on extensive supervised training on large-scale datasets, making them unsuitable for few-shot settings \\cite{trivedi2022qsf}.\n    *   **Positioning**: IRCoT distinguishes itself by proposing a novel, few-shot, training-free approach that *interleaves* CoT generation with retrieval, allowing each to dynamically inform the other, specifically addressing the limitations of one-shot retrieval and the training requirements of supervised iterative methods \\cite{trivedi2022qsf}.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method (IRCoT)**: IRCoT (Interleaved Retrieval guided by Chain-of-Thought) is an iterative process that alternates between generating a CoT step and retrieving knowledge.\n        1.  **Initial Retrieval**: Begins by retrieving a base set of `K` paragraphs using the original question as the query \\cite{trivedi2022qsf}.\n        2.  **Iterative Interleaving**:\n            *   **Reason Step**: A language model generates the next CoT sentence, conditioned on the original question, all paragraphs collected so far, and all CoT sentences generated so far \\cite{trivedi2022qsf}.\n            *   **Retrieve Step**: The *last generated CoT sentence* is then used as a new query to retrieve `K` additional paragraphs, which are added to the cumulative set of retrieved documents \\cite{trivedi2022qsf}.\n        3.  **Termination**: This loop continues until the CoT explicitly states an answer (e.g., \"answer is:\") or a maximum number of reasoning steps is reached \\cite{trivedi2022qsf}.\n        4.  **Final QA**: All collected paragraphs are then passed to a separate QA reader (using either CoT or Direct Prompting) to generate the final answer \\cite{trivedi2022qsf}.\n    *   **Novelty**: The key innovation is the *dynamic, bidirectional interleaving* of CoT reasoning and retrieval. Instead of a static, one-time retrieval or pre-defined sub-questions, IRCoT uses the evolving CoT to generate more precise and context-aware queries for subsequent retrieval, and the newly retrieved information in turn grounds and improves the next CoT step \\cite{trivedi2022qsf}. This allows for adaptive information seeking during complex multi-step reasoning.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithm**: Introduction of IRCoT, a novel retrieval method that dynamically interleaves CoT generation with knowledge retrieval for multi-step QA \\cite{trivedi2022qsf}.\n    *   **Dynamic Query Generation**: Leverages intermediate CoT sentences as queries for subsequent retrieval, enabling more targeted and relevant information gathering as the reasoning process unfolds \\cite{trivedi2022qsf}.\n    *   **Improved Factual Accuracy**: By grounding reasoning steps in retrieved external knowledge, IRCoT significantly reduces model hallucination, leading to more factually accurate CoT reasoning \\cite{trivedi2022qsf}.\n    *   **Few-shot and Training-free**: Achieves substantial performance gains in a few-shot setting without requiring any additional training or fine-tuning of the underlying LLMs \\cite{trivedi2022qsf}.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**: Evaluated IRCoT on four multi-step, open-domain QA datasets: HotpotQA, 2WikiMultihopQA, MuSiQue, and IIRC \\cite{trivedi2022qsf}. Comparisons were made against a one-step retriever (OneR) and a retriever-less baseline (NoR QA).\n    *   **Models Used**: OpenAI GPT3 (code-davinci-002, 175B parameters) and various Flan-T5 models (0.7B, 3B, 11B parameters) were used as CoT generators and QA readers. BM25 served as the base retriever \\cite{trivedi2022qsf}.\n    *   **Key Performance Metrics**: Retrieval performance was measured by fixed-budget optimal recall (max 15 paragraphs). QA performance was measured by F1 score. Factual error reduction in CoT was also assessed \\cite{trivedi2022qsf}.\n    *   **Comparison Results**:\n        *   **Retrieval**: IRCoT substantially improved retrieval recall by 11-21 points compared to the one-step retriever \\cite{trivedi2022qsf}.\n        *   **QA Performance**: Led to significant improvements in downstream few-shot QA performance, up to 15 F1 points \\cite{trivedi2022qsf}.\n        *   **Factual Accuracy**: Reduced factual errors in generated CoTs by up to 50% \\cite{trivedi2022qsf}.\n        *   **Model Scalability**: Demonstrated effectiveness with both large (GPT3) and smaller (Flan-T5-*) models. Notably, Flan-T5-XL (3B) with IRCoT outperformed the 58X larger GPT3 using a one-step retriever \\cite{trivedi2022qsf}.\n        *   **Robustness**: Improvements were consistent in out-of-distribution (OOD) settings \\cite{trivedi2022qsf}.\n        *   **State-of-the-Art**: QA scores exceeded those reported by recent few-shot ODQA works like SelfAsk and ReAct \\cite{trivedi2022qsf}.\n\n6.  **Limitations & Scope**\n    *   **Fixed Budget**: The retrieval process is constrained by a maximum number of retrieved paragraphs (e.g., 15) and a maximum number of reasoning steps (e.g., 8) \\cite{trivedi2022qsf}.\n    *   **Base Retriever Dependency**: Performance is somewhat dependent on the quality of the underlying base retriever (BM25 in this work) \\cite{trivedi2022qsf}.\n    *   **Prompt Engineering**: Requires a small set of annotated questions with reasoning steps for in-context demonstrations \\cite{trivedi2022qsf}.\n    *   **Scope of Applicability**: Primarily focused on knowledge-intensive, multi-step QA in open-domain, few-shot settings. It is not designed for tasks where all information is provided in the context or where purely parametric knowledge is sufficient \\cite{trivedi2022qsf}.\n\n7.  **Technical Significance**\n    *   **Advances State-of-the-Art**: Significantly pushes the boundaries of few-shot, open-domain QA for complex, multi-step questions by effectively integrating dynamic knowledge retrieval with LLM reasoning \\cite{trivedi2022qsf}.\n    *   **Reduces LLM Hallucination**: Provides a robust mechanism to ground LLM-generated reasoning in external, verifiable facts, leading to more reliable and accurate outputs \\cite{trivedi2022qsf}.\n    *   **Democratizes Advanced QA**: Demonstrates that sophisticated multi-step QA capabilities can be achieved even with much smaller LLMs (like Flan-T5) when coupled with an intelligent retrieval strategy, potentially reducing the reliance on extremely large, costly models \\cite{trivedi2022qsf}.\n    *   **Impact on Future Research**: Establishes a strong paradigm for future research into more tightly integrated and adaptive reasoning-retrieval systems, moving beyond static information retrieval to truly dynamic knowledge seeking during complex cognitive tasks \\cite{trivedi2022qsf}.",
      "intriguing_abstract": "Large Language Models (LLMs) often falter on knowledge-intensive, multi-step question answering, struggling with outdated internal knowledge and the limitations of static retrieval. We introduce IRCoT (Interleaved Retrieval guided by Chain-of-Thought), a novel, few-shot, training-free approach that dynamically interleaves Chain-of-Thought (CoT) reasoning with knowledge retrieval. Unlike prior methods, IRCoT leverages each generated CoT sentence as a precise query for subsequent retrieval, allowing reasoning to guide information seeking and newly retrieved facts to ground subsequent CoT steps. This adaptive mechanism significantly enhances factual accuracy and reduces hallucination.\n\nEvaluated on four challenging multi-step open-domain QA datasets, IRCoT dramatically improves retrieval recall by 11-21 points and boosts QA performance by up to 15 F1 points. Crucially, it slashes factual errors in CoT by 50%. Our findings demonstrate that even smaller LLMs (e.g., Flan-T5-XL) augmented with IRCoT can outperform much larger models (e.g., GPT3) relying on one-step retrieval. IRCoT sets a new standard for robust, verifiable multi-step reasoning, democratizing advanced QA capabilities and paving the way for more reliable, knowledge-grounded LLM applications.",
      "keywords": [
        "Interleaving Retrieval with Chain-of-Thought (IRCoT)",
        "Large Language Models (LLMs)",
        "Chain-of-Thought (CoT) reasoning",
        "Knowledge-intensive multi-step question answering",
        "Dynamic query generation",
        "Few-shot learning",
        "Training-free approach",
        "Reduced hallucination",
        "Factual accuracy",
        "Iterative retrieval",
        "Open-domain QA",
        "Retrieval recall",
        "QA performance gains",
        "Adaptive information seeking"
      ],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/f208ea909fa7f54fea82def9a92fd81dfc758c39.pdf",
      "citation_key": "trivedi2022qsf",
      "metadata": {
        "title": "Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions",
        "authors": [
          "H. Trivedi",
          "Niranjan Balasubramanian",
          "Tushar Khot",
          "Ashish Sabharwal"
        ],
        "published_date": "2022",
        "abstract": "Prompting-based large language models (LLMs) are surprisingly powerful at generating natural language reasoning steps or Chains-of-Thoughts (CoT) for multi-step question answering (QA). They struggle, however, when the necessary knowledge is either unavailable to the LLM or not up-to-date within its parameters. While using the question to retrieve relevant text from an external knowledge source helps LLMs, we observe that this one-step retrieve-and-read approach is insufficient for multi-step QA. Here, what to retrieve depends on what has already been derived, which in turn may depend on what was previously retrieved. To address this, we propose IRCoT, a new approach for multi-step QA that interleaves retrieval with steps (sentences) in a CoT, guiding the retrieval with CoT and in turn using retrieved results to improve CoT. Using IRCoT with GPT3 substantially improves retrieval (up to 21 points) as well as downstream QA (up to 15 points) on four datasets: HotpotQA, 2WikiMultihopQA, MuSiQue, and IIRC. We observe similar substantial gains in out-of-distribution (OOD) settings as well as with much smaller models such as Flan-T5-large without additional training. IRCoT reduces model hallucination, resulting in factually more accurate CoT reasoning.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/f208ea909fa7f54fea82def9a92fd81dfc758c39.pdf",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the paper \"Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions\" \\cite{trivedi2022qsf} for a literature review:\n\n---\n\n### Focused Summary for Literature Review: Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions \\cite{trivedi2022qsf}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Large Language Models (LLMs) using Chain-of-Thought (CoT) reasoning struggle with knowledge-intensive, multi-step question answering (QA) when the necessary information is not available or up-to-date within their parameters \\cite{trivedi2022qsf}.\n    *   **Importance and Challenge**: A simple one-step retrieve-and-read approach (retrieving based solely on the initial question) is insufficient for multi-step QA. For complex questions, what needs to be retrieved often depends on intermediate reasoning steps already derived, which in turn might depend on previously retrieved information. This creates a circular dependency where reasoning needs facts, and facts need reasoning to be identified, leading to potential hallucination without proper grounding \\cite{trivedi2022qsf}.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**:\n        *   **Prompting-based Open-Domain QA**: Approaches like SelfAsk \\cite{trivedi2022qsf}, DecomP \\cite{trivedi2022qsf}, and ReAct \\cite{trivedi2022qsf} decompose questions or generate action sequences.\n        *   **Supervised Multi-Step Open-Domain QA**: Prior work (e.g., Das et al., 2019; Xiong et al., 2021; Qi et al., 2019) explored iterative retrieval, often using neural query reformulation or entity linking.\n    *   **Limitations of Previous Solutions**:\n        *   **Prompting-based**: SelfAsk and DecomP are not designed for CoT reasoning, do not focus on the dynamic retrieval problem during reasoning, and often require single-hop QA models. ReAct relies on much larger models (e.g., PaLM-540B) and often requires fine-tuning \\cite{trivedi2022qsf}. These methods are not shown to be effective for smaller models without training.\n        *   **Supervised Iterative Retrieval**: These methods typically rely on extensive supervised training on large-scale datasets, making them unsuitable for few-shot settings \\cite{trivedi2022qsf}.\n    *   **Positioning**: IRCoT distinguishes itself by proposing a novel, few-shot, training-free approach that *interleaves* CoT generation with retrieval, allowing each to dynamically inform the other, specifically addressing the limitations of one-shot retrieval and the training requirements of supervised iterative methods \\cite{trivedi2022qsf}.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method (IRCoT)**: IRCoT (Interleaved Retrieval guided by Chain-of-Thought) is an iterative process that alternates between generating a CoT step and retrieving knowledge.\n        1.  **Initial Retrieval**: Begins by retrieving a base set of `K` paragraphs using the original question as the query \\cite{trivedi2022qsf}.\n        2.  **Iterative Interleaving**:\n            *   **Reason Step**: A language model generates the next CoT sentence, conditioned on the original question, all paragraphs collected so far, and all CoT sentences generated so far \\cite{trivedi2022qsf}.\n            *   **Retrieve Step**: The *last generated CoT sentence* is then used as a new query to retrieve `K` additional paragraphs, which are added to the cumulative set of retrieved documents \\cite{trivedi2022qsf}.\n        3.  **Termination**: This loop continues until the CoT explicitly states an answer (e.g., \"answer is:\") or a maximum number of reasoning steps is reached \\cite{trivedi2022qsf}.\n        4.  **Final QA**: All collected paragraphs are then passed to a separate QA reader (using either CoT or Direct Prompting) to generate the final answer \\cite{trivedi2022qsf}.\n    *   **Novelty**: The key innovation is the *dynamic, bidirectional interleaving* of CoT reasoning and retrieval. Instead of a static, one-time retrieval or pre-defined sub-questions, IRCoT uses the evolving CoT to generate more precise and context-aware queries for subsequent retrieval, and the newly retrieved information in turn grounds and improves the next CoT step \\cite{trivedi2022qsf}. This allows for adaptive information seeking during complex multi-step reasoning.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithm**: Introduction of IRCoT, a novel retrieval method that dynamically interleaves CoT generation with knowledge retrieval for multi-step QA \\cite{trivedi2022qsf}.\n    *   **Dynamic Query Generation**: Leverages intermediate CoT sentences as queries for subsequent retrieval, enabling more targeted and relevant information gathering as the reasoning process unfolds \\cite{trivedi2022qsf}.\n    *   **Improved Factual Accuracy**: By grounding reasoning steps in retrieved external knowledge, IRCoT significantly reduces model hallucination, leading to more factually accurate CoT reasoning \\cite{trivedi2022qsf}.\n    *   **Few-shot and Training-free**: Achieves substantial performance gains in a few-shot setting without requiring any additional training or fine-tuning of the underlying LLMs \\cite{trivedi2022qsf}.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**: Evaluated IRCoT on four multi-step, open-domain QA datasets: HotpotQA, 2WikiMultihopQA, MuSiQue, and IIRC \\cite{trivedi2022qsf}. Comparisons were made against a one-step retriever (OneR) and a retriever-less baseline (NoR QA).\n    *   **Models Used**: OpenAI GPT3 (code-davinci-002, 175B parameters) and various Flan-T5 models (0.7B, 3B, 11B parameters) were used as CoT generators and QA readers. BM25 served as the base retriever \\cite{trivedi2022qsf}.\n    *   **Key Performance Metrics**: Retrieval performance was measured by fixed-budget optimal recall (max 15 paragraphs). QA performance was measured by F1 score. Factual error reduction in CoT was also assessed \\cite{trivedi2022qsf}.\n    *   **Comparison Results**:\n        *   **Retrieval**: IRCoT substantially improved retrieval recall by 11-21 points compared to the one-step retriever \\cite{trivedi2022qsf}.\n        *   **QA Performance**: Led to significant improvements in downstream few-shot QA performance, up to 15 F1 points \\cite{trivedi2022qsf}.\n        *   **Factual Accuracy**: Reduced factual errors in generated CoTs by up to 50% \\cite{trivedi2022qsf}.\n        *   **Model Scalability**: Demonstrated effectiveness with both large (GPT3) and smaller (Flan-T5-*) models. Notably, Flan-T5-XL (3B) with IRCoT outperformed the 58X larger GPT3 using a one-step retriever \\cite{trivedi2022qsf}.\n        *   **Robustness**: Improvements were consistent in out-of-distribution (OOD) settings \\cite{trivedi2022qsf}.\n        *   **State-of-the-Art**: QA scores exceeded those reported by recent few-shot ODQA works like SelfAsk and ReAct \\cite{trivedi2022qsf}.\n\n6.  **Limitations & Scope**\n    *   **Fixed Budget**: The retrieval process is constrained by a maximum number of retrieved paragraphs (e.g., 15) and a maximum number of reasoning steps (e.g., 8) \\cite{trivedi2022qsf}.\n    *   **Base Retriever Dependency**: Performance is somewhat dependent on the quality of the underlying base retriever (BM25 in this work) \\cite{trivedi2022qsf}.\n    *   **Prompt Engineering**: Requires a small set of annotated questions with reasoning steps for in-context demonstrations \\cite{trivedi2022qsf}.\n    *   **Scope of Applicability**: Primarily focused on knowledge-intensive, multi-step QA in open-domain, few-shot settings. It is not designed for tasks where all information is provided in the context or where purely parametric knowledge is sufficient \\cite{trivedi2022qsf}.\n\n7.  **Technical Significance**\n    *   **Advances State-of-the-Art**: Significantly pushes the boundaries of few-shot, open-domain QA for complex, multi-step questions by effectively integrating dynamic knowledge retrieval with LLM reasoning \\cite{trivedi2022qsf}.\n    *   **Reduces LLM Hallucination**: Provides a robust mechanism to ground LLM-generated reasoning in external, verifiable facts, leading to more reliable and accurate outputs \\cite{trivedi2022qsf}.\n    *   **Democratizes Advanced QA**: Demonstrates that sophisticated multi-step QA capabilities can be achieved even with much smaller LLMs (like Flan-T5) when coupled with an intelligent retrieval strategy, potentially reducing the reliance on extremely large, costly models \\cite{trivedi2022qsf}.\n    *   **Impact on Future Research**: Establishes a strong paradigm for future research into more tightly integrated and adaptive reasoning-retrieval systems, moving beyond static information retrieval to truly dynamic knowledge seeking during complex cognitive tasks \\cite{trivedi2022qsf}.",
        "keywords": [
          "Interleaving Retrieval with Chain-of-Thought (IRCoT)",
          "Large Language Models (LLMs)",
          "Chain-of-Thought (CoT) reasoning",
          "Knowledge-intensive multi-step question answering",
          "Dynamic query generation",
          "Few-shot learning",
          "Training-free approach",
          "Reduced hallucination",
          "Factual accuracy",
          "Iterative retrieval",
          "Open-domain QA",
          "Retrieval recall",
          "QA performance gains",
          "Adaptive information seeking"
        ],
        "paper_type": "the paper type is **technical**.\n\n**reasoning:**\n\n1.  **abstract analysis:**\n    *   \"we propose ircot, a new approach for multi-step qa that interleaves retrieval with steps (sentences) in a cot...\" - this explicitly states the development and presentation of a *new method* or *system*.\n    *   the abstract then details the mechanism of ircot and its benefits (\"guiding the retrieval with cot and in turn using retrieved results to improve cot\").\n    *   the mention of \"substantially improves retrieval... as well as downstream qa... on four datasets\" indicates empirical evaluation, which is a common component of technical papers demonstrating the effectiveness of the proposed method.\n\n2.  **introduction analysis:**\n    *   it sets up a problem (\"llms struggle... when the necessary knowledge is either unavailable...\") and then poses a research question (\"how can we augment chain-of-thought prompting for open-domain, knowledge-intensive tasks...?\").\n    *   figure 1 visually represents \"ircot interleaves chain-of-thought (cot) generation and knowledge retrieval steps,\" further emphasizing the proposed *method*.\n\n3.  **matching with criteria:**\n    *   the criteria for **technical** state: \"presents new methods, algorithms, or systems\" and mentions \"propose\", \"develop\", \"present\", \"algorithm\", \"method\" in the abstract. this aligns perfectly with the abstract's \"we propose ircot, a new approach.\"\n    *   while the paper *does* include empirical evaluation, its primary contribution is the *creation and presentation* of the ircot method. empirical studies often evaluate existing methods or phenomena, whereas technical papers introduce and then validate new ones."
      },
      "file_name": "f208ea909fa7f54fea82def9a92fd81dfc758c39.pdf"
    },
    {
      "success": true,
      "doc_id": "2a6b7d4a1a4b400330b03dd5c293cfc8",
      "summary": "Here's a focused summary of the paper by \\cite{park20247cm} for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Problem**: Multimodal Large Language Models (MLLMs) frequently suffer from \"hallucinations,\" where generated responses contain information not present in or inconsistent with the input image.\n    *   **Importance & Challenge**: Hallucinations severely undermine MLLM reliability, hindering their adoption in critical applications like medical diagnostics or autonomous systems, where erroneous interpretations could have severe consequences. Addressing this is challenging because it requires models to accurately ground textual generation in visual input.\n\n*   **Related Work & Positioning**\n    *   **Existing Approaches**: Previous methods for hallucination mitigation include:\n        *   **Post-processing**: Revising generated responses (e.g., WoodPecker, LURE) often relying on external APIs or trained revisors.\n        *   **Fine-tuning**: Instruction tuning with additional datasets (e.g., LRV-Instruction, RLHF-V), which are costly in terms of data collection and computational resources for training large MLLMs.\n        *   **Decoding Strategies**: Intervening solely in the decoding process (e.g., OPERA, VCD, HALC) by penalizing tokens not referencing visual information, using distorted images to create contrastive distributions, or leveraging varying fields of view.\n    *   **Limitations of Previous Solutions**: Post-processing and fine-tuning methods often require external APIs, costly human feedback, or significant additional training. Existing decoding strategies use various visual cues but do not directly visualize the hallucinated content.\n    *   **Positioning of this Work**: `\\cite{park20247cm}` introduces ConVis, a novel *training-free contrastive decoding method* that operates purely within the decoding process, avoiding the need for additional data or model updates, and uniquely leverages a Text-to-Image (T2I) model for hallucination visualization.\n\n*   **Technical Approach & Innovation**\n    *   **Core Method (ConVis)**: ConVis mitigates hallucinations by capturing visual contrastive signals using a T2I generation model.\n        1.  **Initial Caption Generation**: The MLLM first generates an initial caption for the input image.\n        2.  **Hallucination Visualization**: A T2I model (specifically Hyper-SDXL) reconstructs an image (`v'`) based on this MLLM-generated caption. If the caption contains hallucinations, these will be visually represented in `v'`, creating discrepancies with the original image (`v`).\n        3.  **Contrastive Decoding**: During the MLLM's main decoding process, `\\cite{park20247cm}` compares the logit distributions produced by the original image (`v`) and the T2I-reconstructed image (`v'`).\n        4.  **Penalty Application**: The final contrastive logit distribution `Ë†fÎ¸` is derived by averaging the differences between `fÎ¸(Â·|v, x, y<t)` and `fÎ¸(Â·|v', x, y<t)`. This process amplifies the logits of tokens corresponding to visualized hallucinations, allowing ConVis to penalize their generation.\n    *   **Novelty**: The key innovation is the *direct visualization of potential hallucinations* using a T2I model to generate visual contrast signals. This is the first time a T2I model has been employed in a decoding strategy to mitigate MLLM hallucinations. The method is entirely training-free and applicable to existing MLLMs. To enhance robustness, `\\cite{park20247cm}` generates a diverse set of `n` captions (using Nucleus Decoding) and `n` corresponding images from the T2I model.\n\n*   **Key Technical Contributions**\n    *   **Novel Method**: Proposing ConVis, a novel contrastive decoding method that visualizes hallucinations using a T2I model.\n    *   **First-time Application**: The first known instance of employing a T2I model to mitigate MLLM hallucinations through a decoding strategy.\n    *   **Training-Free**: The method requires no additional data or model training, making it highly efficient and adaptable.\n    *   **Insight**: Demonstrating how T2I models can serve as a valuable source of visual contrastive signals for hallucination mitigation in MLLMs.\n\n*   **Experimental Validation**\n    *   **Benchmarks**: Evaluated on five popular benchmarks:\n        *   **Hallucination-specific**: CHAIR (S and I metrics), HallusionBench (Figure Accuracy, All Accuracy), POPE (F1-scores across splits).\n        *   **General-purpose**: MME, LLaVA-Bench (to ensure overall performance is maintained).\n    *   **Backbones**: Tested across three well-known MLLMs: LLaVA-1.5, mPLUG-Owl2, and MiniGPT-4.\n    *   **Key Results**:\n        *   **CHAIR**: ConVis achieved the best performance on CHAIR S (reducing total hallucinations) across all three MLLM backbones and consistently ranked first or second on CHAIR I (minimizing hallucinated objects).\n        *   **HallusionBench**: Outperformed all baselines and state-of-the-art methods in Figure Accuracy (fAcc) and All Accuracy (aAcc) for LLaVA-1.5, indicating better visual grounding.\n        *   **POPE**: Achieved new state-of-the-art performance on MiniGPT-4 and comparable results on LLaVA-1.5 and mPLUG-Owl2, demonstrating strong average performance across backbones.\n        *   **Overall**: Consistently reduced hallucinations across various MLLMs and benchmarks while maintaining overall response generation performance.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations**: The performance of ConVis is somewhat dependent on the T2I model's ability to accurately generate images from captions. `\\cite{park20247cm}` addresses this by generating diverse captions and multiple images to increase robustness against potential T2I model misalignment.\n    *   **Scope of Applicability**: ConVis is a training-free decoding strategy, making it broadly applicable to any existing MLLM without requiring architectural changes or retraining. It focuses specifically on mitigating hallucinations during text generation.\n\n*   **Technical Significance**\n    *   **Advancement**: `\\cite{park20247cm}` significantly advances the state-of-the-art in hallucination mitigation by introducing a novel, training-free approach that leverages the generative capabilities of T2I models.\n    *   **Potential Impact**: This work opens new avenues for research into using generative models (like T2I) as \"critics\" or \"visualizers\" within MLLM decoding processes. It offers a practical and efficient solution for enhancing MLLM reliability, particularly valuable for real-world applications where model training or fine-tuning is impractical or too costly.",
      "intriguing_abstract": "Multimodal Large Language Models (MLLMs) are revolutionizing AI, yet their pervasive \"hallucinations\"â€”generating information inconsistent with visual inputsâ€”severely limit their trustworthiness in critical applications. We introduce ConVis, a groundbreaking *training-free contrastive decoding method* that directly confronts this challenge. ConVis uniquely leverages a *Text-to-Image (T2I) model* to *visualize potential hallucinations*. By reconstructing an image from the MLLM's generated caption and comparing its logit distribution against the original visual input, ConVis identifies and penalizes tokens corresponding to visually ungrounded content. This novel approach is the first to employ a T2I model for *hallucination mitigation* within the decoding process, requiring no additional data or costly model fine-tuning. Extensive experiments across LLaVA-1.5, mPLUG-Owl2, and MiniGPT-4 demonstrate ConVis achieving state-of-the-art performance on CHAIR, HallusionBench, and POPE benchmarks, significantly enhancing *visual grounding* and MLLM reliability. ConVis offers an efficient, broadly applicable solution, paving the way for more dependable MLLMs and opening new research avenues for generative models as critical evaluators.",
      "keywords": [
        "Multimodal Large Language Models (MLLMs)",
        "Hallucinations (MLLM)",
        "ConVis",
        "Training-free contrastive decoding",
        "Text-to-Image (T2I) models",
        "Hallucination visualization",
        "Visual grounding",
        "Logit distributions",
        "MLLM reliability",
        "State-of-the-art performance",
        "Decoding strategies",
        "Generative models as critics"
      ],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/57f0d904629955d16bb2b80a5d427e6b1efa6562.pdf",
      "citation_key": "park20247cm",
      "metadata": {
        "title": "ConVis: Contrastive Decoding with Hallucination Visualization for Mitigating Hallucinations in Multimodal Large Language Models",
        "authors": [
          "Yeji Park",
          "Deokyeong Lee",
          "Junsuk Choe",
          "Buru Chang"
        ],
        "published_date": "2024",
        "abstract": "Hallucinations in Multimodal Large Language Models (MLLMs) where generated responses fail to accurately reflect the given image pose a significant challenge to their reliability. To address this, we introduce ConVis, a novel training-free contrastive decoding method. ConVis leverages a text-to-image (T2I) generation model to semantically reconstruct the given image from hallucinated captions. By comparing the contrasting probability distributions produced by the original and reconstructed images, ConVis enables MLLMs to capture visual contrastive signals that penalize hallucination generation. Notably, this method operates purely within the decoding process, eliminating the need for additional data or model updates. Our extensive experiments on five popular benchmarks demonstrate that ConVis effectively reduces hallucinations across various MLLMs, highlighting its potential to enhance model reliability.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/57f0d904629955d16bb2b80a5d427e6b1efa6562.pdf",
        "venue": "AAAI Conference on Artificial Intelligence",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the paper by \\cite{park20247cm} for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Problem**: Multimodal Large Language Models (MLLMs) frequently suffer from \"hallucinations,\" where generated responses contain information not present in or inconsistent with the input image.\n    *   **Importance & Challenge**: Hallucinations severely undermine MLLM reliability, hindering their adoption in critical applications like medical diagnostics or autonomous systems, where erroneous interpretations could have severe consequences. Addressing this is challenging because it requires models to accurately ground textual generation in visual input.\n\n*   **Related Work & Positioning**\n    *   **Existing Approaches**: Previous methods for hallucination mitigation include:\n        *   **Post-processing**: Revising generated responses (e.g., WoodPecker, LURE) often relying on external APIs or trained revisors.\n        *   **Fine-tuning**: Instruction tuning with additional datasets (e.g., LRV-Instruction, RLHF-V), which are costly in terms of data collection and computational resources for training large MLLMs.\n        *   **Decoding Strategies**: Intervening solely in the decoding process (e.g., OPERA, VCD, HALC) by penalizing tokens not referencing visual information, using distorted images to create contrastive distributions, or leveraging varying fields of view.\n    *   **Limitations of Previous Solutions**: Post-processing and fine-tuning methods often require external APIs, costly human feedback, or significant additional training. Existing decoding strategies use various visual cues but do not directly visualize the hallucinated content.\n    *   **Positioning of this Work**: `\\cite{park20247cm}` introduces ConVis, a novel *training-free contrastive decoding method* that operates purely within the decoding process, avoiding the need for additional data or model updates, and uniquely leverages a Text-to-Image (T2I) model for hallucination visualization.\n\n*   **Technical Approach & Innovation**\n    *   **Core Method (ConVis)**: ConVis mitigates hallucinations by capturing visual contrastive signals using a T2I generation model.\n        1.  **Initial Caption Generation**: The MLLM first generates an initial caption for the input image.\n        2.  **Hallucination Visualization**: A T2I model (specifically Hyper-SDXL) reconstructs an image (`v'`) based on this MLLM-generated caption. If the caption contains hallucinations, these will be visually represented in `v'`, creating discrepancies with the original image (`v`).\n        3.  **Contrastive Decoding**: During the MLLM's main decoding process, `\\cite{park20247cm}` compares the logit distributions produced by the original image (`v`) and the T2I-reconstructed image (`v'`).\n        4.  **Penalty Application**: The final contrastive logit distribution `Ë†fÎ¸` is derived by averaging the differences between `fÎ¸(Â·|v, x, y<t)` and `fÎ¸(Â·|v', x, y<t)`. This process amplifies the logits of tokens corresponding to visualized hallucinations, allowing ConVis to penalize their generation.\n    *   **Novelty**: The key innovation is the *direct visualization of potential hallucinations* using a T2I model to generate visual contrast signals. This is the first time a T2I model has been employed in a decoding strategy to mitigate MLLM hallucinations. The method is entirely training-free and applicable to existing MLLMs. To enhance robustness, `\\cite{park20247cm}` generates a diverse set of `n` captions (using Nucleus Decoding) and `n` corresponding images from the T2I model.\n\n*   **Key Technical Contributions**\n    *   **Novel Method**: Proposing ConVis, a novel contrastive decoding method that visualizes hallucinations using a T2I model.\n    *   **First-time Application**: The first known instance of employing a T2I model to mitigate MLLM hallucinations through a decoding strategy.\n    *   **Training-Free**: The method requires no additional data or model training, making it highly efficient and adaptable.\n    *   **Insight**: Demonstrating how T2I models can serve as a valuable source of visual contrastive signals for hallucination mitigation in MLLMs.\n\n*   **Experimental Validation**\n    *   **Benchmarks**: Evaluated on five popular benchmarks:\n        *   **Hallucination-specific**: CHAIR (S and I metrics), HallusionBench (Figure Accuracy, All Accuracy), POPE (F1-scores across splits).\n        *   **General-purpose**: MME, LLaVA-Bench (to ensure overall performance is maintained).\n    *   **Backbones**: Tested across three well-known MLLMs: LLaVA-1.5, mPLUG-Owl2, and MiniGPT-4.\n    *   **Key Results**:\n        *   **CHAIR**: ConVis achieved the best performance on CHAIR S (reducing total hallucinations) across all three MLLM backbones and consistently ranked first or second on CHAIR I (minimizing hallucinated objects).\n        *   **HallusionBench**: Outperformed all baselines and state-of-the-art methods in Figure Accuracy (fAcc) and All Accuracy (aAcc) for LLaVA-1.5, indicating better visual grounding.\n        *   **POPE**: Achieved new state-of-the-art performance on MiniGPT-4 and comparable results on LLaVA-1.5 and mPLUG-Owl2, demonstrating strong average performance across backbones.\n        *   **Overall**: Consistently reduced hallucinations across various MLLMs and benchmarks while maintaining overall response generation performance.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations**: The performance of ConVis is somewhat dependent on the T2I model's ability to accurately generate images from captions. `\\cite{park20247cm}` addresses this by generating diverse captions and multiple images to increase robustness against potential T2I model misalignment.\n    *   **Scope of Applicability**: ConVis is a training-free decoding strategy, making it broadly applicable to any existing MLLM without requiring architectural changes or retraining. It focuses specifically on mitigating hallucinations during text generation.\n\n*   **Technical Significance**\n    *   **Advancement**: `\\cite{park20247cm}` significantly advances the state-of-the-art in hallucination mitigation by introducing a novel, training-free approach that leverages the generative capabilities of T2I models.\n    *   **Potential Impact**: This work opens new avenues for research into using generative models (like T2I) as \"critics\" or \"visualizers\" within MLLM decoding processes. It offers a practical and efficient solution for enhancing MLLM reliability, particularly valuable for real-world applications where model training or fine-tuning is impractical or too costly.",
        "keywords": [
          "Multimodal Large Language Models (MLLMs)",
          "Hallucinations (MLLM)",
          "ConVis",
          "Training-free contrastive decoding",
          "Text-to-Image (T2I) models",
          "Hallucination visualization",
          "Visual grounding",
          "Logit distributions",
          "MLLM reliability",
          "State-of-the-art performance",
          "Decoding strategies",
          "Generative models as critics"
        ],
        "paper_type": "the paper type is **technical**.\n\n**reasoning:**\n\n*   **abstract:** explicitly states \"we introduce convis, a novel training-free contrastive decoding method.\" it then describes the mechanism of this method (\"leverages a text-to-image (t2i) generation model,\" \"comparing the contrasting probability distributions\"). it also mentions \"extensive experiments\" to demonstrate the effectiveness of this *new method*.\n*   **introduction:** sets up a \"technical problem\" (hallucinations in mllms) and briefly discusses existing approaches, implying that the paper will present a new \"proposed solution\" to address the limitations of current methods.\n*   **keywords from criteria:** the abstract uses \"introduce,\" \"novel method,\" \"leverages,\" \"comparing,\" all of which align with presenting a new method or system. while it includes empirical validation (\"extensive experiments\"), the core contribution is the *development and presentation* of the convis method itself."
      },
      "file_name": "57f0d904629955d16bb2b80a5d427e6b1efa6562.pdf"
    },
    {
      "success": true,
      "doc_id": "b2b97f71565538b9dcbccbb686dfbb71",
      "summary": "Here is a focused summary of the paper for a literature review:\n\n*   **Research Problem & Motivation**\n    *   The paper addresses the problem of **hallucinations** in abstractive summarization generated by large pretrained language models \\cite{sridhar2022l1c}.\n    *   This problem is critical due to the increasing commercial use of text-generative applications, which necessitates factual and reliable responses \\cite{sridhar2022l1c}.\n    *   It is challenging because pretraining on unlabeled data can lead models to learn inaccuracies and generate content not pertinent to the input document \\cite{sridhar2022l1c}. Existing mitigation methods often rely on heuristics, require extensive manual tuning, or add significant model complexity.\n\n*   **Related Work & Positioning**\n    *   Existing approaches to curb hallucination include modifying beam search (e.g., constraining decoding to input-supported tokens \\cite{king2022pinocchio}) or using fact checkers as a post-processing step \\cite{balachandran2022factedit}.\n    *   Limitations of previous solutions:\n        *   Heuristic-based beam search modifications (e.g., PINOCCHIO \\cite{king2022pinocchio}) require intricate knowledge of the dataset, task, and model for hyperparameter initialization, making them impractical for production systems with varying data distributions \\cite{sridhar2022l1c}. Simple word-level heuristics like cosine distance are ineffective for highly abstractive summarization \\cite{sridhar2022l1c}.\n        *   Fact correction models (e.g., FactEdit \\cite{balachandran2022factedit}) need to be fine-tuned for each dataset, increasing overall model complexity \\cite{sridhar2022l1c}.\n        *   Prior work using Natural Language Inference (NLI) models for re-ranking typically compared *complete* summary candidates with the context, finding that NLI entailment probability alone was insufficient to differentiate correct from incorrect beams \\cite{sridhar2022l1c}.\n        *   Architectural modifications are often not generalizable and require retraining existing summarizer models from scratch \\cite{sridhar2022l1c}.\n\n*   **Technical Approach & Innovation**\n    *   The core technical method is an **NLI-assisted beam re-ranking mechanism** applied *during* the decoding phase of abstractive summarization \\cite{sridhar2022l1c}.\n    *   The approach involves:\n        1.  **Saliency-enhanced Greedy Rollout (SGR)**: Partial hypotheses (intermediate beams) are completed into full candidate beams using a greedy search. To address the issue of models not effectively attending to relevant context, SGR incorporates saliency on the context relative to the intermediate beam using attention head masking. Two versions are proposed: Saliency v1 (hard masking for extractive summaries) and Saliency v2 (variable soft attention for abstractive summaries) \\cite{sridhar2022l1c}.\n        2.  **NLI Scorer**: The completed candidate beams are then passed to an NLI model, which computes an entailment probability between the input context (premise) and the rolled-out beam (hypothesis). The paper hypothesizes that this entailment probability is inversely proportional to the hallucination content \\cite{sridhar2022l1c}.\n        3.  **Weighted Beam Re-ranker**: The NLI entailment score is incorporated into the overall cumulative beam probability by taking a weighted average with the model's original probability for each decoding step. Beams are then re-ranked based on this modified cumulative probability \\cite{sridhar2022l1c}.\n    *   This approach is novel because it introduces NLI-based semantic matching *at the token level during the decoding phase* to actively steer beam generation away from hallucinated regions, rather than as a post-processing step on complete summaries \\cite{sridhar2022l1c}.\n\n*   **Key Technical Contributions**\n    *   **Novel Algorithm**: Development of a hallucination mitigation component for beam search that modifies the cumulative beam probability at the token level using the NLI metric \\cite{sridhar2022l1c}.\n    *   **System Design/Architectural Innovations**: Introduction of \"Saliency-enhanced Greedy Rollout\" with two variations (v1 for extractive, v2 for abstractive) to effectively complete partial hypotheses and ensure context relevance during NLI scoring \\cite{sridhar2022l1c}.\n    *   **Theoretical Insights/Analysis**: Investigation into the role of the `Î±` hyperparameter in guiding factual generation and its impact on beam diversity, demonstrating that non-zero `Î±` leads to more diverse and factually consistent beams \\cite{sridhar2022l1c}. The paper also highlights the limitations and inconsistencies of existing state-of-the-art factuality metrics (FactCC, SummaC, QGQA) across different datasets \\cite{sridhar2022l1c}.\n\n*   **Experimental Validation**\n    *   **Datasets**: Experiments were conducted on two publicly available datasets: CNN/DM (multi-line, less abstractive) and XSum (one-line, highly abstractive) \\cite{sridhar2022l1c}.\n    *   **Metrics**: Performance was evaluated using multiple automatic factuality metrics (SummaC-Conv, SummaC-CZS, QGQA, FactCC) and, crucially, **human evaluation** \\cite{sridhar2022l1c}.\n    *   **Comparison Results**:\n        *   Automatic metrics showed inconsistent trends across datasets, with the proposed approach outperforming baselines on XSum (except QGQA) but FactEdit sometimes favored on CNN/DM \\cite{sridhar2022l1c}. This reinforced the paper's claim about the unreliability of current automatic metrics.\n        *   **Human evaluation** (50 instances, 4 annotators, 1-5 faithfulness score) consistently demonstrated that the proposed NLI-based re-ranker achieved a **higher faithfulness score** compared to baselines (vanilla beam search, PINNOCHIO \\cite{king2022pinocchio}, FactEdit \\cite{balachandran2022factedit}), bolstering the efficacy of the algorithm \\cite{sridhar2022l1c}.\n        *   Analysis showed that varying the `Î±` hyperparameter significantly impacted factual consistency and beam diversity, with non-zero `Î±` values leading to factually consistent and more diverse generations \\cite{sridhar2022l1c}.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations**: The NLI task is difficult to perform on partial hypotheses, necessitating the greedy rollout mechanism \\cite{sridhar2022l1c}. The re-ranking is only performed if the hypothesis has a predefined minimum number of words, as very short beams might lack necessary entities for hallucination measurement \\cite{sridhar2022l1c}.\n    *   **Scope of Applicability**: The method is demonstrated for abstractive summarization using BART-Base models but is designed to be easily generalizable and inference-time applicable \\cite{sridhar2022l1c}. The two saliency variations (v1, v2) aim to make it suitable for both extractive and abstractive summarization styles \\cite{sridhar2022l1c}.\n\n*   **Technical Significance**\n    *   This work significantly advances the technical state-of-the-art by providing an **inference-time and easily generalizable** method for hallucination mitigation in abstractive summarization \\cite{sridhar2022l1c}.\n    *   By integrating NLI directly into the beam decoding process, it offers a more robust and semantically aware mechanism to guide generation towards factual consistency compared to heuristic-based or post-processing approaches \\cite{sridhar2022l1c}.\n    *   The explicit demonstration of the inconsistencies of current automatic factuality metrics, coupled with strong human evaluation results, highlights the need for more reliable benchmarks and validates the proposed approach's practical effectiveness \\cite{sridhar2022l1c}.\n    *   Potential impact on future research includes exploring optimal time steps for re-ranking hypotheses and investigating the NLI-based re-ranker's performance on other conditional language generation tasks \\cite{sridhar2022l1c}.",
      "intriguing_abstract": "Hallucinations remain a critical impediment to the trustworthiness of abstractive summaries generated by large language models. Existing mitigation strategies often rely on brittle heuristics or complex post-processing, failing to intervene where factual inconsistencies truly emerge. We fundamentally re-think beam search by introducing a novel, inference-time **NLI-assisted beam re-ranking mechanism** that actively steers decoding *during* generation. Our approach leverages **Saliency-enhanced Greedy Rollout (SGR)** to complete partial hypotheses, enabling fine-grained, **token-level Natural Language Inference (NLI)** scoring. This NLI entailment probability is then weighted and integrated into the cumulative beam probability, guiding the model towards factually consistent outputs. Extensive human evaluations on CNN/DM and XSum datasets demonstrate consistently superior faithfulness scores compared to state-of-the-art baselines, robustly validating its efficacy and critically exposing the inconsistencies of current automatic factuality metrics. This generalizable framework offers a robust paradigm for enhancing the reliability and trustworthiness of conditional language generation.",
      "keywords": [
        "Hallucinations",
        "abstractive summarization",
        "NLI-assisted beam re-ranking",
        "Saliency-enhanced Greedy Rollout",
        "decoding phase",
        "token-level semantic matching",
        "factual consistency",
        "human evaluation",
        "inference-time generalizable",
        "automatic factuality metrics limitations",
        "entailment probability",
        "beam search"
      ],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/6d26836a4cee8f90c6fa4d5751d5f10e0f720301.pdf",
      "citation_key": "sridhar2022l1c",
      "metadata": {
        "title": "Improved Beam Search for Hallucination Mitigation in Abstractive Summarization",
        "authors": [
          "A. Sridhar",
          "Erik M. Visser"
        ],
        "published_date": "2022",
        "abstract": "Advancement in large pretrained language models has significantly improved their performance for conditional language generation tasks including summarization albeit with hallucinations. To reduce hallucinations, conventional methods proposed improving beam search or using a fact checker as a postprocessing step. In this paper, we investigate the use of the Natural Language Inference (NLI) entailment metric to detect and prevent hallucinations in summary generation. We propose an NLI-assisted beam re-ranking mechanism by computing entailment probability scores between the input context and summarization model-generated beams during saliency-enhanced greedy decoding. Moreover, a diversity metric is introduced to compare its effectiveness against vanilla beam search. Our proposed algorithm significantly outperforms vanilla beam decoding on XSum and CNN/DM datasets.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/6d26836a4cee8f90c6fa4d5751d5f10e0f720301.pdf",
        "venue": "arXiv.org",
        "citationCount": 0,
        "score": 0,
        "summary": "Here is a focused summary of the paper for a literature review:\n\n*   **Research Problem & Motivation**\n    *   The paper addresses the problem of **hallucinations** in abstractive summarization generated by large pretrained language models \\cite{sridhar2022l1c}.\n    *   This problem is critical due to the increasing commercial use of text-generative applications, which necessitates factual and reliable responses \\cite{sridhar2022l1c}.\n    *   It is challenging because pretraining on unlabeled data can lead models to learn inaccuracies and generate content not pertinent to the input document \\cite{sridhar2022l1c}. Existing mitigation methods often rely on heuristics, require extensive manual tuning, or add significant model complexity.\n\n*   **Related Work & Positioning**\n    *   Existing approaches to curb hallucination include modifying beam search (e.g., constraining decoding to input-supported tokens \\cite{king2022pinocchio}) or using fact checkers as a post-processing step \\cite{balachandran2022factedit}.\n    *   Limitations of previous solutions:\n        *   Heuristic-based beam search modifications (e.g., PINOCCHIO \\cite{king2022pinocchio}) require intricate knowledge of the dataset, task, and model for hyperparameter initialization, making them impractical for production systems with varying data distributions \\cite{sridhar2022l1c}. Simple word-level heuristics like cosine distance are ineffective for highly abstractive summarization \\cite{sridhar2022l1c}.\n        *   Fact correction models (e.g., FactEdit \\cite{balachandran2022factedit}) need to be fine-tuned for each dataset, increasing overall model complexity \\cite{sridhar2022l1c}.\n        *   Prior work using Natural Language Inference (NLI) models for re-ranking typically compared *complete* summary candidates with the context, finding that NLI entailment probability alone was insufficient to differentiate correct from incorrect beams \\cite{sridhar2022l1c}.\n        *   Architectural modifications are often not generalizable and require retraining existing summarizer models from scratch \\cite{sridhar2022l1c}.\n\n*   **Technical Approach & Innovation**\n    *   The core technical method is an **NLI-assisted beam re-ranking mechanism** applied *during* the decoding phase of abstractive summarization \\cite{sridhar2022l1c}.\n    *   The approach involves:\n        1.  **Saliency-enhanced Greedy Rollout (SGR)**: Partial hypotheses (intermediate beams) are completed into full candidate beams using a greedy search. To address the issue of models not effectively attending to relevant context, SGR incorporates saliency on the context relative to the intermediate beam using attention head masking. Two versions are proposed: Saliency v1 (hard masking for extractive summaries) and Saliency v2 (variable soft attention for abstractive summaries) \\cite{sridhar2022l1c}.\n        2.  **NLI Scorer**: The completed candidate beams are then passed to an NLI model, which computes an entailment probability between the input context (premise) and the rolled-out beam (hypothesis). The paper hypothesizes that this entailment probability is inversely proportional to the hallucination content \\cite{sridhar2022l1c}.\n        3.  **Weighted Beam Re-ranker**: The NLI entailment score is incorporated into the overall cumulative beam probability by taking a weighted average with the model's original probability for each decoding step. Beams are then re-ranked based on this modified cumulative probability \\cite{sridhar2022l1c}.\n    *   This approach is novel because it introduces NLI-based semantic matching *at the token level during the decoding phase* to actively steer beam generation away from hallucinated regions, rather than as a post-processing step on complete summaries \\cite{sridhar2022l1c}.\n\n*   **Key Technical Contributions**\n    *   **Novel Algorithm**: Development of a hallucination mitigation component for beam search that modifies the cumulative beam probability at the token level using the NLI metric \\cite{sridhar2022l1c}.\n    *   **System Design/Architectural Innovations**: Introduction of \"Saliency-enhanced Greedy Rollout\" with two variations (v1 for extractive, v2 for abstractive) to effectively complete partial hypotheses and ensure context relevance during NLI scoring \\cite{sridhar2022l1c}.\n    *   **Theoretical Insights/Analysis**: Investigation into the role of the `Î±` hyperparameter in guiding factual generation and its impact on beam diversity, demonstrating that non-zero `Î±` leads to more diverse and factually consistent beams \\cite{sridhar2022l1c}. The paper also highlights the limitations and inconsistencies of existing state-of-the-art factuality metrics (FactCC, SummaC, QGQA) across different datasets \\cite{sridhar2022l1c}.\n\n*   **Experimental Validation**\n    *   **Datasets**: Experiments were conducted on two publicly available datasets: CNN/DM (multi-line, less abstractive) and XSum (one-line, highly abstractive) \\cite{sridhar2022l1c}.\n    *   **Metrics**: Performance was evaluated using multiple automatic factuality metrics (SummaC-Conv, SummaC-CZS, QGQA, FactCC) and, crucially, **human evaluation** \\cite{sridhar2022l1c}.\n    *   **Comparison Results**:\n        *   Automatic metrics showed inconsistent trends across datasets, with the proposed approach outperforming baselines on XSum (except QGQA) but FactEdit sometimes favored on CNN/DM \\cite{sridhar2022l1c}. This reinforced the paper's claim about the unreliability of current automatic metrics.\n        *   **Human evaluation** (50 instances, 4 annotators, 1-5 faithfulness score) consistently demonstrated that the proposed NLI-based re-ranker achieved a **higher faithfulness score** compared to baselines (vanilla beam search, PINNOCHIO \\cite{king2022pinocchio}, FactEdit \\cite{balachandran2022factedit}), bolstering the efficacy of the algorithm \\cite{sridhar2022l1c}.\n        *   Analysis showed that varying the `Î±` hyperparameter significantly impacted factual consistency and beam diversity, with non-zero `Î±` values leading to factually consistent and more diverse generations \\cite{sridhar2022l1c}.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations**: The NLI task is difficult to perform on partial hypotheses, necessitating the greedy rollout mechanism \\cite{sridhar2022l1c}. The re-ranking is only performed if the hypothesis has a predefined minimum number of words, as very short beams might lack necessary entities for hallucination measurement \\cite{sridhar2022l1c}.\n    *   **Scope of Applicability**: The method is demonstrated for abstractive summarization using BART-Base models but is designed to be easily generalizable and inference-time applicable \\cite{sridhar2022l1c}. The two saliency variations (v1, v2) aim to make it suitable for both extractive and abstractive summarization styles \\cite{sridhar2022l1c}.\n\n*   **Technical Significance**\n    *   This work significantly advances the technical state-of-the-art by providing an **inference-time and easily generalizable** method for hallucination mitigation in abstractive summarization \\cite{sridhar2022l1c}.\n    *   By integrating NLI directly into the beam decoding process, it offers a more robust and semantically aware mechanism to guide generation towards factual consistency compared to heuristic-based or post-processing approaches \\cite{sridhar2022l1c}.\n    *   The explicit demonstration of the inconsistencies of current automatic factuality metrics, coupled with strong human evaluation results, highlights the need for more reliable benchmarks and validates the proposed approach's practical effectiveness \\cite{sridhar2022l1c}.\n    *   Potential impact on future research includes exploring optimal time steps for re-ranking hypotheses and investigating the NLI-based re-ranker's performance on other conditional language generation tasks \\cite{sridhar2022l1c}.",
        "keywords": [
          "Hallucinations",
          "abstractive summarization",
          "NLI-assisted beam re-ranking",
          "Saliency-enhanced Greedy Rollout",
          "decoding phase",
          "token-level semantic matching",
          "factual consistency",
          "human evaluation",
          "inference-time generalizable",
          "automatic factuality metrics limitations",
          "entailment probability",
          "beam search"
        ],
        "paper_type": "based on the abstract and introduction:\n\n*   the abstract explicitly states: \"we **propose** an inference time and easily generalizable nli-assisted beam re-ranking mechanism...\" and refers to it as \"our **proposed algorithm**\". this directly matches the criteria for \"technical\" papers: \"abstract mentions: 'propose', 'develop', 'present', 'algorithm', 'method'\".\n*   the introduction discusses a \"technical problem\" (hallucinations in summarization) and reviews existing \"methods\" (modifications to beam search), setting the stage for their \"proposed solution\".\n*   while the paper also includes an empirical component (\"demonstrate that our proposed algorithm consistently outperforms the baselines in human evaluation on publicly available xsum and cnn/dm datasets\"), this empirical evaluation serves to validate the *new method* being proposed. the primary contribution is the development of this new algorithm.\n\ntherefore, the most appropriate classification is **technical**."
      },
      "file_name": "6d26836a4cee8f90c6fa4d5751d5f10e0f720301.pdf"
    },
    {
      "success": true,
      "doc_id": "b9163e54a4c870300153a965562479ab",
      "summary": "Here's a focused summary of the paper for a literature review, adhering to your requirements:\n\n---\n\n### Analysis of \"Unsupervised Real-Time Hallucination Detection based on the Internal States of Large Language Models\" \\cite{su2024lem}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: The paper addresses the critical issue of \"hallucinations\" in Large Language Models (LLMs), where models generate factually inaccurate or misleading yet coherent responses.\n    *   **Importance and Challenge**: Hallucinations severely undermine the effectiveness and trustworthiness of LLMs in practical applications. Existing detection methods are primarily post-processing, suffering from high computational costs, significant latency, and limited effectiveness due to their separation from the LLM's inference process. They often require extensive and expensive manual annotations for training, which is unsustainable given the rapid evolution of LLMs.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: Previous studies mainly focus on post-processing techniques that analyze LLM outputs *after* generation to identify factual errors \\cite{su2024lem}. These include methods based on multi-response consistency, LLM confidence, or proxy models.\n    *   **Limitations of Previous Solutions**:\n        *   **Computational Intensity & Latency**: Post-processing methods often employ other powerful LLMs for detection, making the cost and latency comparable to or even greater than the original LLM inference.\n        *   **Limited Model Capacity**: By operating independently of the LLM's generation process, these methods cannot analyze how hallucinations are formed internally.\n        *   **Reliance on Manual Annotations**: Proxy models or other supervised approaches require extensive human-annotated data, which is costly to collect and difficult to scale.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper introduces **MIND** (Modeling of INternal states for hallucination Detection), an unsupervised training framework for real-time hallucination detection that leverages the internal states of LLMs. It consists of two main steps:\n        1.  **Unsupervised Training Data Generation**: Automatically annotates hallucinations by taking high-quality Wikipedia articles, truncating them at a random entity, and having the target LLM continue the text. If the LLM's continuation does not start with or contain the correct entity, it's labeled as a hallucination; otherwise, it's a non-hallucination. This process records the LLM's internal states (contextualized embeddings, self-attentions, hidden-layer activations) during generation.\n        2.  **Hallucination Classifier Training**: A lightweight Multilayer Perceptron (MLP) is trained to classify hallucinations. The input to the MLP is the contextualized embedding of the *last token of the final Transformer layer* of the LLM, which is found to be highly effective. Binary Cross-Entropy (BCE) Loss is used for optimization.\n    *   **Novelty/Differentiation**:\n        *   **Unsupervised**: Eliminates the need for manual annotations by automatically generating pseudo-training data from Wikipedia.\n        *   **Real-time & Internal State-based**: Detects hallucinations *during* the LLM's inference process by analyzing its internal states (specifically contextualized embeddings), significantly reducing computational overhead and latency compared to post-processing methods.\n        *   **Lightweight & Compatible**: Uses a simple MLP, making it efficient and easily integrable into any Transformer-based LLM.\n\n4.  **Key Technical Contributions**\n    *   **Novel Framework (MIND)**: Proposes an unsupervised, real-time hallucination detection framework that directly utilizes the internal states (contextualized embeddings) of LLMs during generation.\n    *   **Unsupervised Data Generation Mechanism**: Develops a novel method to automatically create hallucination/non-hallucination training data from Wikipedia by comparing LLM-generated continuations with original text entities.\n    *   **Feature Selection Insight**: Demonstrates that the contextualized embedding of the last token in the final Transformer layer is a highly effective and sufficient feature for hallucination detection.\n    *   **New Benchmark (HELM)**: Introduces **HELM** (Hallucination detection Evaluation for multiple LLMs), a comprehensive benchmark dataset featuring:\n        *   Text outputs from six diverse open-source LLMs (base and chat models, 6B-40B parameters).\n        *   Human-annotated hallucination labels at both sentence and passage levels.\n        *   Crucially, the *internal states* (contextualized embeddings, self-attentions, hidden-layer activations) of each LLM recorded during text generation, providing snapshots of their inference process.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**: Evaluated MIND and several state-of-the-art reference-free hallucination detection baselines on the newly introduced HELM benchmark. Experiments were conducted at both sentence-level and passage-level hallucination detection.\n    *   **Key Performance Metrics**: Area Under the Curve (AUC) and Pearson correlation coefficient (corr) with human-annotated relevance.\n    *   **Comparison Results**: MIND consistently outperformed existing state-of-the-art baselines, including Predictive Probability (PP), Predictive Entropy (PE), and various SelfCheckGPT (SCG) variants (e.g., SCG_BERTScore, SCG_QA, SCG_NLI, SCG_n-gram). The results demonstrate the effectiveness of using internal states for detection.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**:\n        *   The unsupervised data generation relies on the assumption that an LLM failing to continue a Wikipedia article with the correct entity indicates a hallucination.\n        *   The hallucination classifier is a relatively simple Multilayer Perceptron (MLP), which might have limitations in capturing highly complex or subtle hallucination patterns compared to more sophisticated models.\n        *   The method is specifically designed for Transformer-based LLMs, as it leverages their internal contextualized embeddings.\n    *   **Scope of Applicability**: Primarily applicable to detecting factual hallucinations in generative LLMs during their inference process. The current data generation method is tied to factual continuation tasks from Wikipedia.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: `MIND` significantly advances the technical state-of-the-art by shifting hallucination detection from computationally expensive, post-processing, and annotation-heavy methods to an unsupervised, real-time, and internal-state-based approach. This makes detection more efficient, scalable, and directly integrated with the LLM's generation.\n    *   **Potential Impact on Future Research**:\n        *   **Enables Real-time Applications**: The real-time nature of `MIND` opens possibilities for integrating hallucination detection directly into LLM applications, allowing for immediate feedback or correction.\n        *   **Reduces Annotation Burden**: The unsupervised training framework alleviates the significant bottleneck of manual data annotation, accelerating research and development in hallucination detection.\n        *   **New Research Direction**: Highlights the potential of leveraging LLM internal states for deeper insights into their behavior and for developing more intrinsic detection mechanisms.\n        *   **Standardized Evaluation**: The `HELM` benchmark provides a valuable, comprehensive resource for future research, offering diverse LLM outputs with human annotations and, critically, the corresponding internal states, which can foster more robust and comparable evaluations of new detection methods.",
      "intriguing_abstract": "The pervasive challenge of factual hallucinations in Large Language Models (LLMs) severely erodes their reliability, yet current detection methods are often slow, costly, and annotation-intensive. We introduce **MIND** (Modeling of INternal states for hallucination Detection), a novel, unsupervised framework that revolutionizes real-time hallucination detection. Unlike computationally expensive post-processing methods, MIND directly leverages the LLM's internal statesâ€”specifically, the contextualized embedding of the final Transformer layer's last tokenâ€”*during* generation. This allows for unprecedented efficiency and low latency. Our innovative unsupervised data generation mechanism, utilizing Wikipedia, eliminates the prohibitive cost of manual annotations, making detection scalable and adaptable to rapidly evolving LLMs. To facilitate robust evaluation, we present **HELM**, a comprehensive benchmark featuring human-annotated outputs and, crucially, the corresponding internal states of diverse LLMs. Experiments on HELM demonstrate that MIND consistently outperforms state-of-the-art baselines. MIND not only significantly enhances LLM trustworthiness by enabling real-time feedback but also pioneers a new research direction, unlocking the potential of intrinsic model understanding for more reliable and scalable AI systems.",
      "keywords": [
        "Hallucination detection",
        "Large Language Models (LLMs)",
        "LLM internal states",
        "Unsupervised learning",
        "Real-time detection",
        "MIND framework",
        "Contextualized embeddings",
        "Transformer-based LLMs",
        "HELM benchmark",
        "Unsupervised data generation",
        "Computational efficiency",
        "Factual hallucinations",
        "Multilayer Perceptron (MLP)",
        "Annotation burden reduction"
      ],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/411b725522e2747e890ba5acfbf43d22f759c00a.pdf",
      "citation_key": "su2024lem",
      "metadata": {
        "title": "Unsupervised Real-Time Hallucination Detection based on the Internal States of Large Language Models",
        "authors": [
          "Weihang Su",
          "Changyue Wang",
          "Qingyao Ai",
          "Hu Yiran",
          "Zhijing Wu",
          "Yujia Zhou",
          "Yiqun Liu"
        ],
        "published_date": "2024",
        "abstract": "Hallucinations in large language models (LLMs) refer to the phenomenon of LLMs producing responses that are coherent yet factually inaccurate. This issue undermines the effectiveness of LLMs in practical applications, necessitating research into detecting and mitigating hallucinations of LLMs. Previous studies have mainly concentrated on post-processing techniques for hallucination detection, which tend to be computationally intensive and limited in effectiveness due to their separation from the LLM's inference process. To overcome these limitations, we introduce MIND, an unsupervised training framework that leverages the internal states of LLMs for real-time hallucination detection without requiring manual annotations. Additionally, we present HELM, a new benchmark for evaluating hallucination detection across multiple LLMs, featuring diverse LLM outputs and the internal states of LLMs during their inference process. Our experiments demonstrate that MIND outperforms existing state-of-the-art methods in hallucination detection.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/411b725522e2747e890ba5acfbf43d22f759c00a.pdf",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the paper for a literature review, adhering to your requirements:\n\n---\n\n### Analysis of \"Unsupervised Real-Time Hallucination Detection based on the Internal States of Large Language Models\" \\cite{su2024lem}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: The paper addresses the critical issue of \"hallucinations\" in Large Language Models (LLMs), where models generate factually inaccurate or misleading yet coherent responses.\n    *   **Importance and Challenge**: Hallucinations severely undermine the effectiveness and trustworthiness of LLMs in practical applications. Existing detection methods are primarily post-processing, suffering from high computational costs, significant latency, and limited effectiveness due to their separation from the LLM's inference process. They often require extensive and expensive manual annotations for training, which is unsustainable given the rapid evolution of LLMs.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: Previous studies mainly focus on post-processing techniques that analyze LLM outputs *after* generation to identify factual errors \\cite{su2024lem}. These include methods based on multi-response consistency, LLM confidence, or proxy models.\n    *   **Limitations of Previous Solutions**:\n        *   **Computational Intensity & Latency**: Post-processing methods often employ other powerful LLMs for detection, making the cost and latency comparable to or even greater than the original LLM inference.\n        *   **Limited Model Capacity**: By operating independently of the LLM's generation process, these methods cannot analyze how hallucinations are formed internally.\n        *   **Reliance on Manual Annotations**: Proxy models or other supervised approaches require extensive human-annotated data, which is costly to collect and difficult to scale.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper introduces **MIND** (Modeling of INternal states for hallucination Detection), an unsupervised training framework for real-time hallucination detection that leverages the internal states of LLMs. It consists of two main steps:\n        1.  **Unsupervised Training Data Generation**: Automatically annotates hallucinations by taking high-quality Wikipedia articles, truncating them at a random entity, and having the target LLM continue the text. If the LLM's continuation does not start with or contain the correct entity, it's labeled as a hallucination; otherwise, it's a non-hallucination. This process records the LLM's internal states (contextualized embeddings, self-attentions, hidden-layer activations) during generation.\n        2.  **Hallucination Classifier Training**: A lightweight Multilayer Perceptron (MLP) is trained to classify hallucinations. The input to the MLP is the contextualized embedding of the *last token of the final Transformer layer* of the LLM, which is found to be highly effective. Binary Cross-Entropy (BCE) Loss is used for optimization.\n    *   **Novelty/Differentiation**:\n        *   **Unsupervised**: Eliminates the need for manual annotations by automatically generating pseudo-training data from Wikipedia.\n        *   **Real-time & Internal State-based**: Detects hallucinations *during* the LLM's inference process by analyzing its internal states (specifically contextualized embeddings), significantly reducing computational overhead and latency compared to post-processing methods.\n        *   **Lightweight & Compatible**: Uses a simple MLP, making it efficient and easily integrable into any Transformer-based LLM.\n\n4.  **Key Technical Contributions**\n    *   **Novel Framework (MIND)**: Proposes an unsupervised, real-time hallucination detection framework that directly utilizes the internal states (contextualized embeddings) of LLMs during generation.\n    *   **Unsupervised Data Generation Mechanism**: Develops a novel method to automatically create hallucination/non-hallucination training data from Wikipedia by comparing LLM-generated continuations with original text entities.\n    *   **Feature Selection Insight**: Demonstrates that the contextualized embedding of the last token in the final Transformer layer is a highly effective and sufficient feature for hallucination detection.\n    *   **New Benchmark (HELM)**: Introduces **HELM** (Hallucination detection Evaluation for multiple LLMs), a comprehensive benchmark dataset featuring:\n        *   Text outputs from six diverse open-source LLMs (base and chat models, 6B-40B parameters).\n        *   Human-annotated hallucination labels at both sentence and passage levels.\n        *   Crucially, the *internal states* (contextualized embeddings, self-attentions, hidden-layer activations) of each LLM recorded during text generation, providing snapshots of their inference process.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**: Evaluated MIND and several state-of-the-art reference-free hallucination detection baselines on the newly introduced HELM benchmark. Experiments were conducted at both sentence-level and passage-level hallucination detection.\n    *   **Key Performance Metrics**: Area Under the Curve (AUC) and Pearson correlation coefficient (corr) with human-annotated relevance.\n    *   **Comparison Results**: MIND consistently outperformed existing state-of-the-art baselines, including Predictive Probability (PP), Predictive Entropy (PE), and various SelfCheckGPT (SCG) variants (e.g., SCG_BERTScore, SCG_QA, SCG_NLI, SCG_n-gram). The results demonstrate the effectiveness of using internal states for detection.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**:\n        *   The unsupervised data generation relies on the assumption that an LLM failing to continue a Wikipedia article with the correct entity indicates a hallucination.\n        *   The hallucination classifier is a relatively simple Multilayer Perceptron (MLP), which might have limitations in capturing highly complex or subtle hallucination patterns compared to more sophisticated models.\n        *   The method is specifically designed for Transformer-based LLMs, as it leverages their internal contextualized embeddings.\n    *   **Scope of Applicability**: Primarily applicable to detecting factual hallucinations in generative LLMs during their inference process. The current data generation method is tied to factual continuation tasks from Wikipedia.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: `MIND` significantly advances the technical state-of-the-art by shifting hallucination detection from computationally expensive, post-processing, and annotation-heavy methods to an unsupervised, real-time, and internal-state-based approach. This makes detection more efficient, scalable, and directly integrated with the LLM's generation.\n    *   **Potential Impact on Future Research**:\n        *   **Enables Real-time Applications**: The real-time nature of `MIND` opens possibilities for integrating hallucination detection directly into LLM applications, allowing for immediate feedback or correction.\n        *   **Reduces Annotation Burden**: The unsupervised training framework alleviates the significant bottleneck of manual data annotation, accelerating research and development in hallucination detection.\n        *   **New Research Direction**: Highlights the potential of leveraging LLM internal states for deeper insights into their behavior and for developing more intrinsic detection mechanisms.\n        *   **Standardized Evaluation**: The `HELM` benchmark provides a valuable, comprehensive resource for future research, offering diverse LLM outputs with human annotations and, critically, the corresponding internal states, which can foster more robust and comparable evaluations of new detection methods.",
        "keywords": [
          "Hallucination detection",
          "Large Language Models (LLMs)",
          "LLM internal states",
          "Unsupervised learning",
          "Real-time detection",
          "MIND framework",
          "Contextualized embeddings",
          "Transformer-based LLMs",
          "HELM benchmark",
          "Unsupervised data generation",
          "Computational efficiency",
          "Factual hallucinations",
          "Multilayer Perceptron (MLP)",
          "Annotation burden reduction"
        ],
        "paper_type": "based on the abstract and introduction:\n\nthe paper introduces **mind**, an \"unsupervised training framework\" for real-time hallucination detection, and **helm**, a \"new benchmark\" for evaluation. it discusses the limitations of previous studies and proposes a novel solution (\"to overcome these limitations, we introduce mind\"). the abstract also mentions \"our experiments demonstrate that mind outperforms existing state-of-the-art methods.\"\n\nthese elements strongly align with the criteria for a **technical** paper:\n*   abstract mentions: \"introduce\", \"present\", \"framework\", \"benchmark\" (which can be considered a system/resource).\n*   introduction discusses: a \"critical problem\" (hallucination) and the \"pressing necessity for research on detecting and mitigating hallucinations,\" setting the stage for their proposed solution.\n\ntherefore, the paper is a **technical** paper."
      },
      "file_name": "411b725522e2747e890ba5acfbf43d22f759c00a.pdf"
    },
    {
      "success": true,
      "doc_id": "b3c4b039d063da45a38347ce82e0b002",
      "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **Research Problem & Motivation**\n    *   The paper addresses the critical issue of \"hallucination\" in Large Language Models (LLMs), where models generate factually inaccurate or ungrounded information \\cite{luo2023xyc}.\n    *   This problem is important because it significantly hinders LLMs' reliability and trustworthiness, especially in sensitive domains like healthcare.\n    *   It is challenging because existing methods primarily focus on *post-detection* of hallucinations, lacking the ability to *prevent* their occurrence. These methods also suffer from interpretability issues, inconsistent performance due to instruction format and model style, and often rely on external knowledge bases or strong inference abilities from the LLM itself \\cite{luo2023xyc}. A proactive, preventative, and zero-resource strategy is needed.\n\n*   **Related Work & Positioning**\n    *   Existing approaches for hallucination detection and correction in open conversations fall into two main categories:\n        *   **Chain-of-Thought (CoT) or prompt programming methods**: These evaluate and amend responses, sometimes with external knowledge bases. Limitations include being engineered for specific responses, high dependence on the model's inner inference ability, and ambiguous classification thresholds due to free-text outputs \\cite{luo2023xyc}.\n        *   **Parameter-based methods**: These use metrics like token probability or perplexity to determine hallucination levels. While offering better generalization and precise scores, they often reduce interpretability and may be limited to specific domains (e.g., biography-related issues for Self-check GPT) \\cite{luo2023xyc}.\n    *   All these methods are primarily *post-detection*, meaning they identify hallucinations after generation, rather than preventing them. They also often rely on external knowledge or are sensitive to instruction/model styles.\n    *   This work positions `SELF-FAMILIARITY` \\cite{luo2023xyc} as a novel *pre-detection*, *zero-resource* method that analyzes the input instruction itself to prevent hallucinations, distinguishing it from prior art.\n\n*   **Technical Approach & Innovation**\n    *   The core technical method is `SELF-FAMILIARITY` \\cite{luo2023xyc}, a pre-detection self-evaluation technique that mimics human self-assessment by evaluating the LLM's familiarity with concepts in the input instruction. If unfamiliar concepts are detected, the model refrains from generating a response.\n    *   The approach consists of three main stages:\n        1.  **Concept Extraction**: Uses a Named Entity Recognition (NER) model to extract key concept entities from the input instruction. It refines these by \"Concept Grouping\" (fusing adjacent concepts) and \"Concept Filtering\" (removing common concepts using Wiktionary) to reduce noise and improve efficiency \\cite{luo2023xyc}.\n        2.  **Concept Guessing**: For each extracted concept, the model performs a self-evaluation to determine its familiarity. This involves:\n            *   **Concept Explanation**: The LLM generates an explanation for the concept using greedy search. To prevent \"cheating,\" the original concept's words are masked within the generated explanation \\cite{luo2023xyc}.\n            *   **Concept Inference**: Using the masked explanation, the LLM is prompted to regenerate the original concept. Constrained beam search is employed to ensure the original concept is sought, and the highest probability score of the successful regeneration is taken as the \"familiarity score\" for that concept \\cite{luo2023xyc}.\n        3.  **Aggregation**: Combines individual concept familiarity scores into a single instruction-level familiarity score. This involves calculating a \"Concept Frequency Score\" (based on word frequency ranks from Wiktionary) to weigh the importance of each concept, followed by a \"Weighted Aggregation\" using a geometrically decreasing scheme. If the final score falls below a predetermined threshold, the response generation is withheld \\cite{luo2023xyc}.\n    *   This approach is novel because it is proactive (pre-detection), zero-resource (no external knowledge base needed), and uses prompt engineering to derive a quantitative familiarity score without requiring strong inference abilities or producing ambiguous free-text outputs \\cite{luo2023xyc}.\n\n*   **Key Technical Contributions**\n    *   **Novel Algorithm/Method**: Introduction of `SELF-FAMILIARITY` \\cite{luo2023xyc}, a pre-detection self-evaluation framework for hallucination prevention in LLMs.\n    *   **Innovative Techniques**:\n        *   A multi-stage concept processing pipeline (NER, grouping, filtering) for robust concept extraction from free-text instructions.\n        *   The \"Concept Guessing\" mechanism, which uses a two-step prompt engineering process (explanation generation with masking, followed by constrained beam search for inference) to quantitatively assess an LLM's familiarity with a concept.\n        *   A weighted aggregation scheme for instruction-level familiarity, incorporating concept importance via frequency scores.\n    *   **System Design**: A zero-resource approach that operates solely on the LLM's intrinsic knowledge, enhancing universality and applicability \\cite{luo2023xyc}.\n    *   **Theoretical Insight**: Emulation of human self-assessment to refrain from responding to unfamiliar topics, providing a more intuitive and interpretable mechanism for hallucination mitigation.\n    *   **Dataset Contribution**: Introduction of the Concept-7 dataset, specifically designed for validating pre-detection hallucinatory instruction classification, addressing a gap in existing post-detection datasets \\cite{luo2023xyc}.\n\n*   **Experimental Validation**\n    *   The `SELF-FAMILIARITY` method \\cite{luo2023xyc} was validated across four different large language models.\n    *   Experiments were conducted using a newly proposed dataset, Concept-7, which focuses on the classification of potential hallucinatory instructions.\n    *   The results consistently demonstrated superior performance of `SELF-FAMILIARITY` \\cite{luo2023xyc} compared to existing techniques across all tested models.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations**: While the paper highlights its strengths, the effectiveness of `SELF-FAMILIARITY` \\cite{luo2023xyc} relies on the accuracy of the initial NER model for concept extraction and the robustness of the prompt engineering for concept explanation and inference. The paper addresses some of these (grouping, filtering, masking), but these remain dependencies. The final threshold `h` for withholding responses needs to be predetermined, which might require careful tuning.\n    *   **Scope of Applicability**: The method is designed for a *zero-resource* environment and targets *pre-detection* of hallucinations in *open conversation* settings, making it broadly applicable where external knowledge bases are unavailable or real-time prevention is crucial \\cite{luo2023xyc}.\n\n*   **Technical Significance**\n    *   This work represents a significant advancement by shifting the paradigm from post-detection to *preemptive prevention* of LLM hallucinations \\cite{luo2023xyc}.\n    *   It enhances the reliability, applicability, and interpretability of LLM assistants by enabling them to \"know what they don't know\" and refrain from generating potentially inaccurate information.\n    *   The zero-resource nature and robustness against instruction/model styles make it highly practical for diverse real-world applications. This approach has the potential to significantly improve the trustworthiness and utility of LLMs in critical domains \\cite{luo2023xyc}.",
      "intriguing_abstract": "Large Language Models (LLMs) are transforming AI, yet their pervasive \"hallucination\"â€”generating factually inaccurate or ungrounded informationâ€”severely undermines their reliability and trustworthiness, particularly in critical domains. Existing mitigation strategies are predominantly *post-detection*, reacting to errors rather than proactively preventing them, often relying on external knowledge or suffering from interpretability issues.\n\nWe introduce `SELF-FAMILIARITY`, a groundbreaking *pre-detection*, *zero-resource* framework that fundamentally shifts the paradigm of hallucination prevention. Emulating human self-assessment, `SELF-FAMILIARITY` empowers LLMs to evaluate their intrinsic familiarity with concepts in an input instruction *before* generating a response. Our novel approach leverages a multi-stage pipeline involving *Named Entity Recognition (NER)* for robust concept extraction, followed by a unique *Concept Guessing* mechanism using *masked explanation* and *constrained beam search* to derive a quantitative familiarity score. This score, aggregated using a weighted scheme, determines whether to withhold a potentially hallucinatory output.\n\nValidated across multiple LLMs using the novel *Concept-7 dataset*, `SELF-FAMILIARITY` consistently demonstrates superior performance. This work represents a significant leap towards building truly reliable and interpretable LLM assistants, enabling them to \"know what they don't know\" and fostering unprecedented trustworthiness in diverse, sensitive applications.",
      "keywords": [
        "LLM hallucination",
        "SELF-FAMILIARITY",
        "pre-detection hallucination prevention",
        "zero-resource approach",
        "LLM self-evaluation",
        "Concept Guessing",
        "prompt engineering",
        "Named Entity Recognition (NER)",
        "constrained beam search",
        "familiarity score",
        "Concept-7 dataset",
        "LLM reliability and trustworthiness"
      ],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/705ffeccfde95c3b0723f197c4565f7d3f0451a1.pdf",
      "citation_key": "luo2023xyc",
      "metadata": {
        "title": "Zero-Resource Hallucination Prevention for Large Language Models",
        "authors": [
          "Junyu Luo",
          "Cao Xiao",
          "Fenglong Ma"
        ],
        "published_date": "2023",
        "abstract": "The prevalent use of large language models (LLMs) in various domains has drawn attention to the issue of\"hallucination,\"which refers to instances where LLMs generate factually inaccurate or ungrounded information. Existing techniques for hallucination detection in language assistants rely on intricate fuzzy, specific free-language-based chain of thought (CoT) techniques or parameter-based methods that suffer from interpretability issues. Additionally, the methods that identify hallucinations post-generation could not prevent their occurrence and suffer from inconsistent performance due to the influence of the instruction format and model style. In this paper, we introduce a novel pre-detection self-evaluation technique, referred to as SELF-FAMILIARITY, which focuses on evaluating the model's familiarity with the concepts present in the input instruction and withholding the generation of response in case of unfamiliar concepts. This approach emulates the human ability to refrain from responding to unfamiliar topics, thus reducing hallucinations. We validate SELF-FAMILIARITY across four different large language models, demonstrating consistently superior performance compared to existing techniques. Our findings propose a significant shift towards preemptive strategies for hallucination mitigation in LLM assistants, promising improvements in reliability, applicability, and interpretability.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/705ffeccfde95c3b0723f197c4565f7d3f0451a1.pdf",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **Research Problem & Motivation**\n    *   The paper addresses the critical issue of \"hallucination\" in Large Language Models (LLMs), where models generate factually inaccurate or ungrounded information \\cite{luo2023xyc}.\n    *   This problem is important because it significantly hinders LLMs' reliability and trustworthiness, especially in sensitive domains like healthcare.\n    *   It is challenging because existing methods primarily focus on *post-detection* of hallucinations, lacking the ability to *prevent* their occurrence. These methods also suffer from interpretability issues, inconsistent performance due to instruction format and model style, and often rely on external knowledge bases or strong inference abilities from the LLM itself \\cite{luo2023xyc}. A proactive, preventative, and zero-resource strategy is needed.\n\n*   **Related Work & Positioning**\n    *   Existing approaches for hallucination detection and correction in open conversations fall into two main categories:\n        *   **Chain-of-Thought (CoT) or prompt programming methods**: These evaluate and amend responses, sometimes with external knowledge bases. Limitations include being engineered for specific responses, high dependence on the model's inner inference ability, and ambiguous classification thresholds due to free-text outputs \\cite{luo2023xyc}.\n        *   **Parameter-based methods**: These use metrics like token probability or perplexity to determine hallucination levels. While offering better generalization and precise scores, they often reduce interpretability and may be limited to specific domains (e.g., biography-related issues for Self-check GPT) \\cite{luo2023xyc}.\n    *   All these methods are primarily *post-detection*, meaning they identify hallucinations after generation, rather than preventing them. They also often rely on external knowledge or are sensitive to instruction/model styles.\n    *   This work positions `SELF-FAMILIARITY` \\cite{luo2023xyc} as a novel *pre-detection*, *zero-resource* method that analyzes the input instruction itself to prevent hallucinations, distinguishing it from prior art.\n\n*   **Technical Approach & Innovation**\n    *   The core technical method is `SELF-FAMILIARITY` \\cite{luo2023xyc}, a pre-detection self-evaluation technique that mimics human self-assessment by evaluating the LLM's familiarity with concepts in the input instruction. If unfamiliar concepts are detected, the model refrains from generating a response.\n    *   The approach consists of three main stages:\n        1.  **Concept Extraction**: Uses a Named Entity Recognition (NER) model to extract key concept entities from the input instruction. It refines these by \"Concept Grouping\" (fusing adjacent concepts) and \"Concept Filtering\" (removing common concepts using Wiktionary) to reduce noise and improve efficiency \\cite{luo2023xyc}.\n        2.  **Concept Guessing**: For each extracted concept, the model performs a self-evaluation to determine its familiarity. This involves:\n            *   **Concept Explanation**: The LLM generates an explanation for the concept using greedy search. To prevent \"cheating,\" the original concept's words are masked within the generated explanation \\cite{luo2023xyc}.\n            *   **Concept Inference**: Using the masked explanation, the LLM is prompted to regenerate the original concept. Constrained beam search is employed to ensure the original concept is sought, and the highest probability score of the successful regeneration is taken as the \"familiarity score\" for that concept \\cite{luo2023xyc}.\n        3.  **Aggregation**: Combines individual concept familiarity scores into a single instruction-level familiarity score. This involves calculating a \"Concept Frequency Score\" (based on word frequency ranks from Wiktionary) to weigh the importance of each concept, followed by a \"Weighted Aggregation\" using a geometrically decreasing scheme. If the final score falls below a predetermined threshold, the response generation is withheld \\cite{luo2023xyc}.\n    *   This approach is novel because it is proactive (pre-detection), zero-resource (no external knowledge base needed), and uses prompt engineering to derive a quantitative familiarity score without requiring strong inference abilities or producing ambiguous free-text outputs \\cite{luo2023xyc}.\n\n*   **Key Technical Contributions**\n    *   **Novel Algorithm/Method**: Introduction of `SELF-FAMILIARITY` \\cite{luo2023xyc}, a pre-detection self-evaluation framework for hallucination prevention in LLMs.\n    *   **Innovative Techniques**:\n        *   A multi-stage concept processing pipeline (NER, grouping, filtering) for robust concept extraction from free-text instructions.\n        *   The \"Concept Guessing\" mechanism, which uses a two-step prompt engineering process (explanation generation with masking, followed by constrained beam search for inference) to quantitatively assess an LLM's familiarity with a concept.\n        *   A weighted aggregation scheme for instruction-level familiarity, incorporating concept importance via frequency scores.\n    *   **System Design**: A zero-resource approach that operates solely on the LLM's intrinsic knowledge, enhancing universality and applicability \\cite{luo2023xyc}.\n    *   **Theoretical Insight**: Emulation of human self-assessment to refrain from responding to unfamiliar topics, providing a more intuitive and interpretable mechanism for hallucination mitigation.\n    *   **Dataset Contribution**: Introduction of the Concept-7 dataset, specifically designed for validating pre-detection hallucinatory instruction classification, addressing a gap in existing post-detection datasets \\cite{luo2023xyc}.\n\n*   **Experimental Validation**\n    *   The `SELF-FAMILIARITY` method \\cite{luo2023xyc} was validated across four different large language models.\n    *   Experiments were conducted using a newly proposed dataset, Concept-7, which focuses on the classification of potential hallucinatory instructions.\n    *   The results consistently demonstrated superior performance of `SELF-FAMILIARITY` \\cite{luo2023xyc} compared to existing techniques across all tested models.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations**: While the paper highlights its strengths, the effectiveness of `SELF-FAMILIARITY` \\cite{luo2023xyc} relies on the accuracy of the initial NER model for concept extraction and the robustness of the prompt engineering for concept explanation and inference. The paper addresses some of these (grouping, filtering, masking), but these remain dependencies. The final threshold `h` for withholding responses needs to be predetermined, which might require careful tuning.\n    *   **Scope of Applicability**: The method is designed for a *zero-resource* environment and targets *pre-detection* of hallucinations in *open conversation* settings, making it broadly applicable where external knowledge bases are unavailable or real-time prevention is crucial \\cite{luo2023xyc}.\n\n*   **Technical Significance**\n    *   This work represents a significant advancement by shifting the paradigm from post-detection to *preemptive prevention* of LLM hallucinations \\cite{luo2023xyc}.\n    *   It enhances the reliability, applicability, and interpretability of LLM assistants by enabling them to \"know what they don't know\" and refrain from generating potentially inaccurate information.\n    *   The zero-resource nature and robustness against instruction/model styles make it highly practical for diverse real-world applications. This approach has the potential to significantly improve the trustworthiness and utility of LLMs in critical domains \\cite{luo2023xyc}.",
        "keywords": [
          "LLM hallucination",
          "SELF-FAMILIARITY",
          "pre-detection hallucination prevention",
          "zero-resource approach",
          "LLM self-evaluation",
          "Concept Guessing",
          "prompt engineering",
          "Named Entity Recognition (NER)",
          "constrained beam search",
          "familiarity score",
          "Concept-7 dataset",
          "LLM reliability and trustworthiness"
        ],
        "paper_type": "based on the abstract and introduction, this paper is best classified as **empirical**.\n\nhere's why:\n\n*   **abstract mentions:** \"we validate self-familiarity across four different large language models, demonstrating consistently superior performance compared to existing techniques. our findings propose a significant shift...\" (strong indicators of experimentation and results).\n*   **introduction discusses:** \"we assessed our method across four large language models using a newly proposed pre-detection hallucinatory instruction classification dataset, concept-7. experimental results show that the proposed self-familiarity consistently outperforms other methods across all models...\" (explicitly mentions assessment, experiments, a new dataset, and comparative performance).\n*   **content structure:** the paper clearly outlines a \"methodology\" (section 3) for its novel technique, but then dedicates a significant portion to \"experiments\" (section 4), including \"dataset\" creation, \"baseline methods,\" \"evaluation metrics,\" and \"results\" (table 2). this structure is characteristic of an empirical study.\n*   **venue:** \"conference on empirical methods in natural language processing\" directly supports an empirical classification.\n\nwhile the paper *does* present a new method (\"self-familiarity\"), its primary contribution, as highlighted in the abstract and introduction, is the *validation* and *demonstration of superior performance* of this method through data-driven experiments and statistical analysis against baselines. the development of the method serves the purpose of the empirical study."
      },
      "file_name": "705ffeccfde95c3b0723f197c4565f7d3f0451a1.pdf"
    },
    {
      "success": true,
      "doc_id": "efc9d69eed1e8850ce0ed27bb380c2b9",
      "summary": "Here's a focused summary of the paper for a literature review:\n\n---\n\n*   **CITATION**: Wu, J., Liu, Q., Wang, D., Zhang, J., Wu, S., Wang, L., & Tan, T. (2024). Logical Closed Loop: Uncovering Object Hallucinations in Large Vision-Language Models. *arXiv preprint arXiv:2402.11622*. \\cite{wu2024h81}\n\n---\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Large Vision-Language Models (LVLMs) frequently suffer from \"object hallucination,\" where they generate descriptions claiming the existence of objects not present in the given image \\cite{wu2024h81}.\n    *   **Importance and Challenge**: This hallucination issue is a significant barrier to the widespread and reliable application of LVLMs, particularly in safety-critical domains. Current mitigation strategies are often computationally expensive (instruction tuning), rely on external models, or require access to the LVLM's internal parameters, leaving a gap for methods that leverage the LVLM's inherent capabilities for self-correction \\cite{wu2024h81}.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: Previous efforts to mitigate LVLM hallucinations include instruction tuning/retraining, integrating external detection models, and devising novel decoding strategies \\cite{wu2024h81}. The paper also distinguishes itself from consistency-checking methods in Large Language Models (LLMs) (e.g., SelfCheckGPT), which typically assess consistency among responses to the *same* question \\cite{wu2024h81}.\n    *   **Limitations of Previous Solutions**: Instruction tuning demands substantial computational resources and high-quality data. External model-based approaches introduce dependencies and do not explore the base LVLM's intrinsic reasoning. Decoding strategies often require privileged access to internal model states. Existing LLM consistency checks do not deeply probe the logical coherence across *related* questions, which is crucial for identifying object hallucinations \\cite{wu2024h81}.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper introduces `LogicCheckGPT`, a Logical Closed Loop-based framework for detecting and mitigating object hallucinations \\cite{wu2024h81}. It operates on the principle that LVLMs exhibit logical consistency when discussing existent objects but inconsistency for hallucinated ones. The framework comprises five steps:\n        1.  **Object Extraction**: An auxiliary LLM (e.g., GPT-3.5) extracts candidate objects from the LVLM's initial image description \\cite{wu2024h81}.\n        2.  **Object-to-Attribute Inquiring**: The LVLM is prompted to provide detailed attributes for each extracted object \\cite{wu2024h81}.\n        3.  **Attribute-to-Object Inquiring**: An LLM first extracts specific attributes from the descriptions, then formulates follow-up questions (e.g., \"Could you tell me all the objects that {attribute} in the image?\") to query which objects possess these attributes \\cite{wu2024h81}.\n        4.  **Logical Closed Loop Checking**: An LLM verifies if the object identified in the attribute-to-object response is consistent with the original examinee object, thus forming a \"logical closed loop\" \\cite{wu2024h81}.\n        5.  **Hallucination Detection and Mitigation**: A \"logical closed loop rate\" is calculated for each object. Objects with a rate below a predefined threshold are flagged as hallucinated, and the LVLM's original response is then rectified by removing content related to these objects \\cite{wu2024h81}.\n    *   **Novelty/Difference**: `LogicCheckGPT` is novel as it is the first to employ the \"logical closed loop\" concept for object hallucination alleviation in LVLMs \\cite{wu2024h81}. It is a training-free, plug-and-play method that relies solely on language interactions, avoiding the need for external models or internal parameter access, thereby leveraging the LVLM's inherent capabilities \\cite{wu2024h81}. The specific prompt design for attribute-to-object inquiring (asking for \"all objects\") is also a key innovation to ensure comprehensive coverage \\cite{wu2024h81}.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**: Introduction of the \"logical closed loop\" as a novel metric and probing mechanism for object hallucination in LVLMs \\cite{wu2024h81}. Development of the `LogicCheckGPT` framework, which systematically uses a sequence of logically correlated questions to assess and correct LVLM outputs \\cite{wu2024h81}.\n    *   **System Design/Architectural Innovations**: A plug-and-play, training-free framework that seamlessly integrates with existing LVLMs, requiring no architectural modifications or retraining \\cite{wu2024h81}. The strategic use of an auxiliary LLM for sub-tasks like object and attribute extraction, and logical checking, enhances flexibility and robustness \\cite{wu2024h81}.\n    *   **Theoretical Insights/Analysis**: The core insight that the logical consistency of an LVLM's responses to related inquiries can serve as a reliable indicator of object hallucination \\cite{wu2024h81}.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**: Comprehensive experiments were performed across three benchmarks, including the POPE dataset (with random, popular, and adversarial settings), and evaluated on four advanced LVLMs \\cite{wu2024h81}. GPT-4v assisted evaluation was also utilized \\cite{wu2024h81}.\n    *   **Key Performance Metrics and Comparison Results**: `LogicCheckGPT` demonstrated significant improvements in mitigating object hallucinations. For example, it achieved a 31.33% improvement for mPLUG-Owl and a 10.00% improvement for MiniGPT-4 on the POPE dataset, showcasing its effectiveness and generality across state-of-the-art LVLMs \\cite{wu2024h81}.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The method's performance is partly dependent on the capabilities of the auxiliary LLM used for sub-tasks. The effectiveness of hallucination detection relies on a properly tuned \"hallucination threshold\" (Î») \\cite{wu2024h81}. The approach assumes that LVLMs will exhibit differential logical consistency, which might not hold universally for all types of hallucinations or models \\cite{wu2024h81}.\n    *   **Scope of Applicability**: `LogicCheckGPT` is designed as a plug-and-play solution, making it broadly applicable to any existing LVLM that can engage in language-based question-answering, without requiring model-specific training or internal access \\cite{wu2024h81}. Its primary focus is on detecting and mitigating object hallucinations in multimodal contexts \\cite{wu2024h81}.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: `LogicCheckGPT` significantly advances the state-of-the-art by providing a novel, training-free, and interpretable method for object hallucination mitigation that leverages the intrinsic reasoning capabilities of LVLMs themselves \\cite{wu2024h81}. It addresses key limitations of prior methods by avoiding high computational costs, external dependencies, and the need for internal model access \\cite{wu2024h81}.\n    *   **Potential Impact on Future Research**: This work opens new avenues for research into self-correction, intrinsic reasoning, and language-based introspection within LVLMs and other large AI models \\cite{wu2024h81}. The \"logical closed loop\" concept could be extended to detect other forms of inconsistencies or hallucinations in various AI systems, fostering further development of robust and reliable AI \\cite{wu2024h81}.",
      "intriguing_abstract": "Large Vision-Language Models (LVLMs) promise transformative AI, yet their reliability is critically undermined by \"object hallucinations\"â€”generating descriptions for non-existent objects. Existing mitigation strategies are often computationally expensive, rely on external models, or demand internal parameter access, highlighting a crucial need for intrinsic self-correction.\n\nWe present `LogicCheckGPT`, a novel, training-free framework that harnesses LVLMs' inherent reasoning to detect and mitigate object hallucinations via a groundbreaking \"Logical Closed Loop\" mechanism. `LogicCheckGPT` systematically probes an LVLM by extracting candidate objects, inquiring about their attributes, and then querying which objects possess those attributes. By verifying the logical consistency of these interconnected responses, our plug-and-play framework precisely identifies and rectifies hallucinated content, operating purely via language without architectural modification. Evaluated across diverse benchmarks and state-of-the-art LVLMs, `LogicCheckGPT` achieves substantial improvements, boosting performance by up to 31.33% on the POPE dataset. This work significantly advances reliable and trustworthy LVLMs, fostering robust multimodal AI and opening new avenues for self-correction.",
      "keywords": [
        "Large Vision-Language Models (LVLMs)",
        "object hallucination",
        "Logical Closed Loop",
        "LogicCheckGPT framework",
        "hallucination mitigation",
        "training-free method",
        "plug-and-play solution",
        "self-correction",
        "intrinsic reasoning",
        "consistency checking",
        "auxiliary LLM",
        "multimodal contexts",
        "safety-critical domains",
        "prompt design"
      ],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/80248c8c7cbb5bb1d2a508001108f3f15bb60430.pdf",
      "citation_key": "wu2024h81",
      "metadata": {
        "title": "Logical Closed Loop: Uncovering Object Hallucinations in Large Vision-Language Models",
        "authors": [
          "Jun Wu",
          "Q. Liu",
          "Ding Wang",
          "Jinghao Zhang",
          "Shu Wu",
          "Liang Wang",
          "Tien-Ping Tan"
        ],
        "published_date": "2024",
        "abstract": "Object hallucination has been an Achilles' heel which hinders the broader applications of large vision-language models (LVLMs). Object hallucination refers to the phenomenon that the LVLMs claim non-existent objects in the image. To mitigate the object hallucinations, instruction tuning and external model-based detection methods have been proposed, which either require large-scare computational resources or depend on the detection result of external models. However, there remains an under-explored field to utilize the LVLM itself to alleviate object hallucinations. In this work, we adopt the intuition that the LVLM tends to respond logically consistently for existent objects but inconsistently for hallucinated objects. Therefore, we propose a Logical Closed Loop-based framework for Object Hallucination Detection and Mitigation, namely LogicCheckGPT. In specific, we devise logical consistency probing to raise questions with logical correlations, inquiring about attributes from objects and vice versa. Whether their responses can form a logical closed loop serves as an indicator of object hallucination. As a plug-and-play method, it can be seamlessly applied to all existing LVLMs. Comprehensive experiments conducted on three benchmarks across four LVLMs have demonstrated significant improvements brought by our method, indicating its effectiveness and generality.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/80248c8c7cbb5bb1d2a508001108f3f15bb60430.pdf",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the paper for a literature review:\n\n---\n\n*   **CITATION**: Wu, J., Liu, Q., Wang, D., Zhang, J., Wu, S., Wang, L., & Tan, T. (2024). Logical Closed Loop: Uncovering Object Hallucinations in Large Vision-Language Models. *arXiv preprint arXiv:2402.11622*. \\cite{wu2024h81}\n\n---\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Large Vision-Language Models (LVLMs) frequently suffer from \"object hallucination,\" where they generate descriptions claiming the existence of objects not present in the given image \\cite{wu2024h81}.\n    *   **Importance and Challenge**: This hallucination issue is a significant barrier to the widespread and reliable application of LVLMs, particularly in safety-critical domains. Current mitigation strategies are often computationally expensive (instruction tuning), rely on external models, or require access to the LVLM's internal parameters, leaving a gap for methods that leverage the LVLM's inherent capabilities for self-correction \\cite{wu2024h81}.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: Previous efforts to mitigate LVLM hallucinations include instruction tuning/retraining, integrating external detection models, and devising novel decoding strategies \\cite{wu2024h81}. The paper also distinguishes itself from consistency-checking methods in Large Language Models (LLMs) (e.g., SelfCheckGPT), which typically assess consistency among responses to the *same* question \\cite{wu2024h81}.\n    *   **Limitations of Previous Solutions**: Instruction tuning demands substantial computational resources and high-quality data. External model-based approaches introduce dependencies and do not explore the base LVLM's intrinsic reasoning. Decoding strategies often require privileged access to internal model states. Existing LLM consistency checks do not deeply probe the logical coherence across *related* questions, which is crucial for identifying object hallucinations \\cite{wu2024h81}.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper introduces `LogicCheckGPT`, a Logical Closed Loop-based framework for detecting and mitigating object hallucinations \\cite{wu2024h81}. It operates on the principle that LVLMs exhibit logical consistency when discussing existent objects but inconsistency for hallucinated ones. The framework comprises five steps:\n        1.  **Object Extraction**: An auxiliary LLM (e.g., GPT-3.5) extracts candidate objects from the LVLM's initial image description \\cite{wu2024h81}.\n        2.  **Object-to-Attribute Inquiring**: The LVLM is prompted to provide detailed attributes for each extracted object \\cite{wu2024h81}.\n        3.  **Attribute-to-Object Inquiring**: An LLM first extracts specific attributes from the descriptions, then formulates follow-up questions (e.g., \"Could you tell me all the objects that {attribute} in the image?\") to query which objects possess these attributes \\cite{wu2024h81}.\n        4.  **Logical Closed Loop Checking**: An LLM verifies if the object identified in the attribute-to-object response is consistent with the original examinee object, thus forming a \"logical closed loop\" \\cite{wu2024h81}.\n        5.  **Hallucination Detection and Mitigation**: A \"logical closed loop rate\" is calculated for each object. Objects with a rate below a predefined threshold are flagged as hallucinated, and the LVLM's original response is then rectified by removing content related to these objects \\cite{wu2024h81}.\n    *   **Novelty/Difference**: `LogicCheckGPT` is novel as it is the first to employ the \"logical closed loop\" concept for object hallucination alleviation in LVLMs \\cite{wu2024h81}. It is a training-free, plug-and-play method that relies solely on language interactions, avoiding the need for external models or internal parameter access, thereby leveraging the LVLM's inherent capabilities \\cite{wu2024h81}. The specific prompt design for attribute-to-object inquiring (asking for \"all objects\") is also a key innovation to ensure comprehensive coverage \\cite{wu2024h81}.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**: Introduction of the \"logical closed loop\" as a novel metric and probing mechanism for object hallucination in LVLMs \\cite{wu2024h81}. Development of the `LogicCheckGPT` framework, which systematically uses a sequence of logically correlated questions to assess and correct LVLM outputs \\cite{wu2024h81}.\n    *   **System Design/Architectural Innovations**: A plug-and-play, training-free framework that seamlessly integrates with existing LVLMs, requiring no architectural modifications or retraining \\cite{wu2024h81}. The strategic use of an auxiliary LLM for sub-tasks like object and attribute extraction, and logical checking, enhances flexibility and robustness \\cite{wu2024h81}.\n    *   **Theoretical Insights/Analysis**: The core insight that the logical consistency of an LVLM's responses to related inquiries can serve as a reliable indicator of object hallucination \\cite{wu2024h81}.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**: Comprehensive experiments were performed across three benchmarks, including the POPE dataset (with random, popular, and adversarial settings), and evaluated on four advanced LVLMs \\cite{wu2024h81}. GPT-4v assisted evaluation was also utilized \\cite{wu2024h81}.\n    *   **Key Performance Metrics and Comparison Results**: `LogicCheckGPT` demonstrated significant improvements in mitigating object hallucinations. For example, it achieved a 31.33% improvement for mPLUG-Owl and a 10.00% improvement for MiniGPT-4 on the POPE dataset, showcasing its effectiveness and generality across state-of-the-art LVLMs \\cite{wu2024h81}.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The method's performance is partly dependent on the capabilities of the auxiliary LLM used for sub-tasks. The effectiveness of hallucination detection relies on a properly tuned \"hallucination threshold\" (Î») \\cite{wu2024h81}. The approach assumes that LVLMs will exhibit differential logical consistency, which might not hold universally for all types of hallucinations or models \\cite{wu2024h81}.\n    *   **Scope of Applicability**: `LogicCheckGPT` is designed as a plug-and-play solution, making it broadly applicable to any existing LVLM that can engage in language-based question-answering, without requiring model-specific training or internal access \\cite{wu2024h81}. Its primary focus is on detecting and mitigating object hallucinations in multimodal contexts \\cite{wu2024h81}.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: `LogicCheckGPT` significantly advances the state-of-the-art by providing a novel, training-free, and interpretable method for object hallucination mitigation that leverages the intrinsic reasoning capabilities of LVLMs themselves \\cite{wu2024h81}. It addresses key limitations of prior methods by avoiding high computational costs, external dependencies, and the need for internal model access \\cite{wu2024h81}.\n    *   **Potential Impact on Future Research**: This work opens new avenues for research into self-correction, intrinsic reasoning, and language-based introspection within LVLMs and other large AI models \\cite{wu2024h81}. The \"logical closed loop\" concept could be extended to detect other forms of inconsistencies or hallucinations in various AI systems, fostering further development of robust and reliable AI \\cite{wu2024h81}.",
        "keywords": [
          "Large Vision-Language Models (LVLMs)",
          "object hallucination",
          "Logical Closed Loop",
          "LogicCheckGPT framework",
          "hallucination mitigation",
          "training-free method",
          "plug-and-play solution",
          "self-correction",
          "intrinsic reasoning",
          "consistency checking",
          "auxiliary LLM",
          "multimodal contexts",
          "safety-critical domains",
          "prompt design"
        ],
        "paper_type": "based on the abstract and introduction:\n\n*   the abstract explicitly states: \"we propose a logical closed loop-based framework for object hallucination detection and mitigation, namely logiccheckgpt.\" it then describes the \"methodology\" (\"devise logical consistency probing\") and its \"effectiveness and generality\" demonstrated through \"comprehensive experiments.\"\n*   the introduction immediately dives into the problem of object hallucinations in lvlms and uses figure 1 to illustrate how the proposed method (logiccheckgpt) works.\n\nthis clearly indicates the paper is presenting a **new method or system** to solve a technical problem. while it includes empirical evaluation, the core contribution is the proposed framework and its underlying mechanism.\n\ntherefore, the paper type is **technical**."
      },
      "file_name": "80248c8c7cbb5bb1d2a508001108f3f15bb60430.pdf"
    },
    {
      "success": true,
      "doc_id": "8345f8b5a8a8d171c2e54d03e2707819",
      "summary": "Here's a focused summary of the paper \"Multi-Object Hallucination in Vision Language Models\" by Chen et al. \\cite{chen2024vy7} for a literature review:\n\n---\n\n### Analysis of \"Multi-Object Hallucination in Vision Language Models\" \\cite{chen2024vy7}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Large Vision Language Models (LVLMs) frequently suffer from \"object hallucination,\" where they generate descriptions of objects not present in an image. Existing benchmarks primarily focus on single-object hallucination, neglecting the more complex and realistic scenario of *multi-object hallucination* where models must simultaneously recognize and reason about multiple entities.\n    *   **Importance and Challenge**:\n        *   Real-world applications (e.g., embodied AI, autonomous driving) require LVLMs to accurately perceive and interact with multiple objects concurrently.\n        *   Evaluating multiple objects simultaneously is more challenging than single-object queries, as models may invent non-existent objects or become distracted due to common associations (e.g., knife and fork).\n        *   Traditional textual prompts for multiple objects can introduce referential ambiguity, making accurate evaluation difficult.\n        *   Quantifying and mitigating multi-object hallucination is crucial for advancing LVLM reliability and applicability.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: This work builds upon research in Large Vision-Language Models (LVLMs), visual prompting, and object hallucination benchmarks.\n    *   **Limitations of Previous Solutions**:\n        *   **Single-Object Focus**: Most prior object hallucination benchmarks (e.g., POPE \\cite{chen2024vy7}, CHAIR \\cite{chen2024vy7}) concentrate on the presence of a single object class or instance, failing to capture multi-object misperceptions.\n        *   **Referential Ambiguity**: Existing methods often rely on textual descriptions for object reference, which can lead to ambiguity, especially when multiple instances of the same class are present or when distinguishing specific entities.\n        *   **Limited Scope of Distribution Analysis**: While some work considers object class distribution in training data, the impact of object class distribution *within an image at test time* on hallucination remains under-explored.\n        *   **Evaluation Methodology**: Many benchmarks rely on human evaluators or black-box neural models, which can be costly, time-consuming, and lack transparency.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper introduces **Recognition-based Object Probing Evaluation (ROPE)**, an automated protocol for assessing multi-object hallucination in LVLMs.\n        *   **Visual Referring Prompts**: ROPE leverages visual prompts (e.g., marked bounding boxes) to uniquely refer to specific objects, eliminating ambiguity inherent in textual descriptions.\n        *   **Formatted Output Control**: LVLMs are explicitly instructed to generate a structured output (e.g., `obj1: <class1>, obj2: <class2>`), enabling automated parsing and evaluation without human or black-box model intervention.\n        *   **Multi-Object Query Distributions**: ROPE systematically investigates hallucination across four distinct object class distributions within a single image at test time:\n            *   **Homogeneous**: All queried objects belong to the same class.\n            *   **Heterogeneous**: All queried objects belong to different classes.\n            *   **Adversarial**: Most objects are of one class, with one object being different (e.g., AAAAB).\n            *   **In-the-Wild**: Randomly chosen and ordered objects.\n        *   **Task Prompt Variations**: To isolate different aspects of model behavior, ROPE defines three multi-object query types and a single-object baseline:\n            *   **Default**: Standard multi-object recognition in a single turn.\n            *   **Student-Forcing**: Forces the model to adhere to the output format, focusing on recognition accuracy.\n            *   **Teacher-Forcing**: Conditions the model on correct previous context, providing an upper bound for multi-object recognition performance.\n            *   **Single-Object Query**: Probes objects one at a time as a baseline for comparison.\n    *   **Novelty**: ROPE's novelty lies in its systematic approach to multi-object hallucination, combining visual referring prompts, automated evaluation, and a detailed analysis of object class distributions at test time, which is a significant departure from prior single-object, text-based, or human-evaluated benchmarks.\n\n4.  **Key Technical Contributions**\n    *   **Novel Evaluation Protocol**: Introduction of ROPE, the first automated, recognition-based protocol specifically designed for multi-object hallucination in LVLMs.\n    *   **Visual Referring Prompts**: Pioneering the use of visual cues (bounding boxes) to eliminate referential ambiguity in multi-object evaluation.\n    *   **Systematic Distribution Analysis**: Proposing and evaluating LVLMs across novel object class distributions (Homogeneous, Heterogeneous, Adversarial) at test time to uncover shortcut learning and spurious correlations.\n    *   **Controlled Task Prompts**: Designing specific prompt variations (Default, Student-Forcing, Teacher-Forcing) to disentangle different sources of error (e.g., instruction following vs. recognition ability).\n    *   **Automated Evaluation Pipeline**: Enabling fully automated evaluation, reducing reliance on human annotators or complex neural models.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**: Comprehensive empirical studies were performed on various LVLMs of different scales and training data (e.g., LLaVA-7B, LLaVA-34B, GPT-4V, GPT-4O, Gemini 1.0 Pro, Gemini 1.5 Pro, Qwen-VL-Chat, Qwen-VL-Max). The evaluation used a dataset constructed from MSCOCO-Panoptic and ADE20K, split into \"Seen\" and \"Unseen\" images.\n    *   **Key Performance Metrics and Results**: The primary metric is the hallucination rate (incorrect object recognition).\n        *   **(1) Increased Hallucination with Multiple Objects**: LVLMs exhibit significantly more hallucinations when tasked with focusing on multiple objects simultaneously compared to single-object queries.\n        *   **(2) Impact of Object Class Distribution**: The distribution of object classes within an image at test time strongly influences hallucination behaviors, suggesting that LVLMs may exploit shortcuts and spurious correlations learned from training data. For instance, models perform better on Homogeneous distributions but struggle with Heterogeneous and Adversarial ones.\n        *   **(3) Influencing Factors**: Hallucinatory behaviors are influenced by data-specific factors (e.g., salience, frequency of objects in training data) and intrinsic model behaviors (e.g., tendency to repeat classes or follow common associations like \"knife and fork\"). Teacher-forcing and Student-forcing prompts helped reveal these underlying tendencies.\n\n6.  **Limitations & Scope**\n    *   **Dataset Specificity**: The empirical validation in \\cite{chen2024vy7} uses a dataset with a fixed number of objects (n=5) to be recognized and a predefined set of candidate object classes (N=50). While the ROPE protocol is general, the specific findings are tied to this experimental setup.\n    *   **Focus on Recognition**: ROPE primarily evaluates object *recognition* and classification, rather than complex reasoning or relational understanding between multiple objects, though it provides a foundation for such extensions.\n    *   **Visual Prompt Format**: The current implementation uses bounding boxes as visual prompts; other forms of visual cues (e.g., segmentation masks, points) could be explored.\n\n7.  **Technical Significance**\n    *   **Advances State-of-the-Art**: \\cite{chen2024vy7} significantly advances the understanding and evaluation of object hallucination by shifting focus to the more challenging and realistic multi-object scenario. It provides a robust, automated, and unambiguous protocol (ROPE) for this purpose.\n    *   **Insights into LVLM Behavior**: The findings offer critical insights into why LVLMs hallucinate in multi-object contexts, highlighting the role of object class distributions, data biases, and model intrinsic behaviors (e.g., shortcut learning, spurious correlations).\n    *   **Impact on Future Research**: This work lays the groundwork for:\n        *   Developing more robust LVLMs capable of accurate multi-object recognition and reasoning.\n        *   Guiding the creation of more balanced and diverse training datasets.\n        *   Informing the design of enhanced multi-object instructions and grounding mechanisms in LVLMs.\n        *   Quantifying progress in mitigating a critical failure mode of LVLMs, moving towards more reliable and trustworthy AI systems.",
      "intriguing_abstract": "Large Vision Language Models (LVLMs) are increasingly deployed in complex real-world scenarios, yet their susceptibility to \"object hallucination\" remains a critical challenge. While prior work largely focuses on single-object misperceptions, we uncover a more pervasive and intricate problem: *multi-object hallucination*, where LVLMs invent or misidentify multiple entities simultaneously. To address this, we introduce **Recognition-based Object Probing Evaluation (ROPE)**, a novel, automated protocol that leverages visual referring prompts to eliminate referential ambiguity and systematically evaluate LVLMs across diverse object class distributions within an image. Our comprehensive experiments reveal that multi-object hallucination is significantly higher than single-object errors, and critically, is profoundly influenced by the *distribution of objects at test time*, exposing models' reliance on shortcut learning and spurious correlations. ROPE provides unprecedented insights into the underlying mechanisms of LVLM hallucination, offering a robust, scalable benchmark to drive the development of more reliable and trustworthy multi-object perception in next-generation AI systems.",
      "keywords": [
        "Multi-object hallucination",
        "Large Vision Language Models (LVLMs)",
        "Recognition-based Object Probing Evaluation (ROPE)",
        "Automated evaluation protocol",
        "Visual referring prompts",
        "Object class distribution",
        "Increased hallucination rate",
        "Shortcut learning",
        "Spurious correlations",
        "Embodied AI",
        "Autonomous driving",
        "Task prompt variations"
      ],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/39d8486475173357619647061dda377f4c38853e.pdf",
      "citation_key": "chen2024vy7",
      "metadata": {
        "title": "Multi-Object Hallucination in Vision-Language Models",
        "authors": [
          "Xuweiyi Chen",
          "Ziqiao Ma",
          "Xuejun Zhang",
          "Sihan Xu",
          "Shengyi Qian",
          "Jianing Yang",
          "D. Fouhey",
          "Joyce Chai"
        ],
        "published_date": "2024",
        "abstract": "Large vision language models (LVLMs) often suffer from object hallucination, producing objects not present in the given images. While current benchmarks for object hallucination primarily concentrate on the presence of a single object class rather than individual entities, this work systematically investigates multi-object hallucination, examining how models misperceive (e.g., invent nonexistent objects or become distracted) when tasked with focusing on multiple objects simultaneously. We introduce Recognition-based Object Probing Evaluation (ROPE), an automated evaluation protocol that considers the distribution of object classes within a single image during testing and uses visual referring prompts to eliminate ambiguity. With comprehensive empirical studies and analysis of potential factors leading to multi-object hallucination, we found that (1). LVLMs suffer more hallucinations when focusing on multiple objects compared to a single object. (2). The tested object class distribution affects hallucination behaviors, indicating that LVLMs may follow shortcuts and spurious correlations. (3). Hallucinatory behaviors are influenced by data-specific factors, salience and frequency, and model intrinsic behaviors. We hope to enable LVLMs to recognize and reason about multiple objects that often occur in realistic visual scenes, provide insights, and quantify our progress towards mitigating the issues.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/39d8486475173357619647061dda377f4c38853e.pdf",
        "venue": "Neural Information Processing Systems",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the paper \"Multi-Object Hallucination in Vision Language Models\" by Chen et al. \\cite{chen2024vy7} for a literature review:\n\n---\n\n### Analysis of \"Multi-Object Hallucination in Vision Language Models\" \\cite{chen2024vy7}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Large Vision Language Models (LVLMs) frequently suffer from \"object hallucination,\" where they generate descriptions of objects not present in an image. Existing benchmarks primarily focus on single-object hallucination, neglecting the more complex and realistic scenario of *multi-object hallucination* where models must simultaneously recognize and reason about multiple entities.\n    *   **Importance and Challenge**:\n        *   Real-world applications (e.g., embodied AI, autonomous driving) require LVLMs to accurately perceive and interact with multiple objects concurrently.\n        *   Evaluating multiple objects simultaneously is more challenging than single-object queries, as models may invent non-existent objects or become distracted due to common associations (e.g., knife and fork).\n        *   Traditional textual prompts for multiple objects can introduce referential ambiguity, making accurate evaluation difficult.\n        *   Quantifying and mitigating multi-object hallucination is crucial for advancing LVLM reliability and applicability.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: This work builds upon research in Large Vision-Language Models (LVLMs), visual prompting, and object hallucination benchmarks.\n    *   **Limitations of Previous Solutions**:\n        *   **Single-Object Focus**: Most prior object hallucination benchmarks (e.g., POPE \\cite{chen2024vy7}, CHAIR \\cite{chen2024vy7}) concentrate on the presence of a single object class or instance, failing to capture multi-object misperceptions.\n        *   **Referential Ambiguity**: Existing methods often rely on textual descriptions for object reference, which can lead to ambiguity, especially when multiple instances of the same class are present or when distinguishing specific entities.\n        *   **Limited Scope of Distribution Analysis**: While some work considers object class distribution in training data, the impact of object class distribution *within an image at test time* on hallucination remains under-explored.\n        *   **Evaluation Methodology**: Many benchmarks rely on human evaluators or black-box neural models, which can be costly, time-consuming, and lack transparency.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper introduces **Recognition-based Object Probing Evaluation (ROPE)**, an automated protocol for assessing multi-object hallucination in LVLMs.\n        *   **Visual Referring Prompts**: ROPE leverages visual prompts (e.g., marked bounding boxes) to uniquely refer to specific objects, eliminating ambiguity inherent in textual descriptions.\n        *   **Formatted Output Control**: LVLMs are explicitly instructed to generate a structured output (e.g., `obj1: <class1>, obj2: <class2>`), enabling automated parsing and evaluation without human or black-box model intervention.\n        *   **Multi-Object Query Distributions**: ROPE systematically investigates hallucination across four distinct object class distributions within a single image at test time:\n            *   **Homogeneous**: All queried objects belong to the same class.\n            *   **Heterogeneous**: All queried objects belong to different classes.\n            *   **Adversarial**: Most objects are of one class, with one object being different (e.g., AAAAB).\n            *   **In-the-Wild**: Randomly chosen and ordered objects.\n        *   **Task Prompt Variations**: To isolate different aspects of model behavior, ROPE defines three multi-object query types and a single-object baseline:\n            *   **Default**: Standard multi-object recognition in a single turn.\n            *   **Student-Forcing**: Forces the model to adhere to the output format, focusing on recognition accuracy.\n            *   **Teacher-Forcing**: Conditions the model on correct previous context, providing an upper bound for multi-object recognition performance.\n            *   **Single-Object Query**: Probes objects one at a time as a baseline for comparison.\n    *   **Novelty**: ROPE's novelty lies in its systematic approach to multi-object hallucination, combining visual referring prompts, automated evaluation, and a detailed analysis of object class distributions at test time, which is a significant departure from prior single-object, text-based, or human-evaluated benchmarks.\n\n4.  **Key Technical Contributions**\n    *   **Novel Evaluation Protocol**: Introduction of ROPE, the first automated, recognition-based protocol specifically designed for multi-object hallucination in LVLMs.\n    *   **Visual Referring Prompts**: Pioneering the use of visual cues (bounding boxes) to eliminate referential ambiguity in multi-object evaluation.\n    *   **Systematic Distribution Analysis**: Proposing and evaluating LVLMs across novel object class distributions (Homogeneous, Heterogeneous, Adversarial) at test time to uncover shortcut learning and spurious correlations.\n    *   **Controlled Task Prompts**: Designing specific prompt variations (Default, Student-Forcing, Teacher-Forcing) to disentangle different sources of error (e.g., instruction following vs. recognition ability).\n    *   **Automated Evaluation Pipeline**: Enabling fully automated evaluation, reducing reliance on human annotators or complex neural models.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**: Comprehensive empirical studies were performed on various LVLMs of different scales and training data (e.g., LLaVA-7B, LLaVA-34B, GPT-4V, GPT-4O, Gemini 1.0 Pro, Gemini 1.5 Pro, Qwen-VL-Chat, Qwen-VL-Max). The evaluation used a dataset constructed from MSCOCO-Panoptic and ADE20K, split into \"Seen\" and \"Unseen\" images.\n    *   **Key Performance Metrics and Results**: The primary metric is the hallucination rate (incorrect object recognition).\n        *   **(1) Increased Hallucination with Multiple Objects**: LVLMs exhibit significantly more hallucinations when tasked with focusing on multiple objects simultaneously compared to single-object queries.\n        *   **(2) Impact of Object Class Distribution**: The distribution of object classes within an image at test time strongly influences hallucination behaviors, suggesting that LVLMs may exploit shortcuts and spurious correlations learned from training data. For instance, models perform better on Homogeneous distributions but struggle with Heterogeneous and Adversarial ones.\n        *   **(3) Influencing Factors**: Hallucinatory behaviors are influenced by data-specific factors (e.g., salience, frequency of objects in training data) and intrinsic model behaviors (e.g., tendency to repeat classes or follow common associations like \"knife and fork\"). Teacher-forcing and Student-forcing prompts helped reveal these underlying tendencies.\n\n6.  **Limitations & Scope**\n    *   **Dataset Specificity**: The empirical validation in \\cite{chen2024vy7} uses a dataset with a fixed number of objects (n=5) to be recognized and a predefined set of candidate object classes (N=50). While the ROPE protocol is general, the specific findings are tied to this experimental setup.\n    *   **Focus on Recognition**: ROPE primarily evaluates object *recognition* and classification, rather than complex reasoning or relational understanding between multiple objects, though it provides a foundation for such extensions.\n    *   **Visual Prompt Format**: The current implementation uses bounding boxes as visual prompts; other forms of visual cues (e.g., segmentation masks, points) could be explored.\n\n7.  **Technical Significance**\n    *   **Advances State-of-the-Art**: \\cite{chen2024vy7} significantly advances the understanding and evaluation of object hallucination by shifting focus to the more challenging and realistic multi-object scenario. It provides a robust, automated, and unambiguous protocol (ROPE) for this purpose.\n    *   **Insights into LVLM Behavior**: The findings offer critical insights into why LVLMs hallucinate in multi-object contexts, highlighting the role of object class distributions, data biases, and model intrinsic behaviors (e.g., shortcut learning, spurious correlations).\n    *   **Impact on Future Research**: This work lays the groundwork for:\n        *   Developing more robust LVLMs capable of accurate multi-object recognition and reasoning.\n        *   Guiding the creation of more balanced and diverse training datasets.\n        *   Informing the design of enhanced multi-object instructions and grounding mechanisms in LVLMs.\n        *   Quantifying progress in mitigating a critical failure mode of LVLMs, moving towards more reliable and trustworthy AI systems.",
        "keywords": [
          "Multi-object hallucination",
          "Large Vision Language Models (LVLMs)",
          "Recognition-based Object Probing Evaluation (ROPE)",
          "Automated evaluation protocol",
          "Visual referring prompts",
          "Object class distribution",
          "Increased hallucination rate",
          "Shortcut learning",
          "Spurious correlations",
          "Embodied AI",
          "Autonomous driving",
          "Task prompt variations"
        ],
        "paper_type": "based on the abstract and introduction:\n\n1.  **problem identification:** the paper identifies a problem with large vision language models (lvlms) â€“ multi-object hallucination â€“ and points out limitations in current benchmarks for addressing this.\n2.  **new methodology:** it introduces a new evaluation protocol called \"recognition-based object probing evaluation (rope)\". this is a technical contribution.\n3.  **core contribution:** the abstract explicitly states: \"with comprehensive empirical studies and analysis of potential factors leading to multi-object hallucination, we found that (1) lvlms suffer more hallucinations... (2) the tested object class distribution affects hallucination behaviors... (3) hallucinatory behaviors are influenced by data-specific factors...\"\n\nwhile the paper introduces a new method (rope), its primary focus, as highlighted by the abstract, is the \"comprehensive empirical studies\" and the \"findings\" derived from these studies regarding multi-object hallucination. the new protocol serves as the methodology for these data-driven investigations. the emphasis is on understanding and quantifying the phenomenon through experimentation and analysis.\n\ntherefore, the paper is best classified as **empirical**."
      },
      "file_name": "39d8486475173357619647061dda377f4c38853e.pdf"
    },
    {
      "success": true,
      "doc_id": "f9da0f5b630ffc47d2f1eb08249ee021",
      "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **1. Research Problem & Motivation**\n    *   **Specific Technical Problem:** Multimodal Large Language Models (MLLMs) suffer from \"relation hallucinations,\" where they misinterpret logical relationships between two or more objects in an image \\cite{zheng20246fk}. Existing research primarily addresses simpler object-level or attribute-level hallucinations.\n    *   **Importance & Challenge:** This problem is crucial because relation hallucinations demand advanced reasoning capabilities from MLLMs. Current benchmarks for relation hallucinations are limited, lacking detailed evaluation and effective mitigation strategies, and their datasets often suffer from biases due to systematic annotation processes \\cite{zheng20246fk}. The paper demonstrates that relation hallucination can be more severe than object hallucination in current MLLMs.\n\n*   **2. Related Work & Positioning**\n    *   **Relation to Existing Approaches:** Previous work on MLLM hallucinations largely focuses on object-level (e.g., POPE) or attribute-level issues (e.g., MME) \\cite{zheng20246fk}.\n    *   **Limitations of Previous Solutions:**\n        *   Existing relation hallucination benchmarks (e.g., MMRel, R-Bench) often use simple Yes/No (Y/N) tasks or LLM-based scoring, failing to provide comprehensive evaluation from both discriminative (Y/N, MCQ) and generative (VQA) perspectives \\cite{zheng20246fk}.\n        *   Many benchmarks rely on post-processed, manually annotated, or synthetic data, which can introduce biases.\n        *   Few previous benchmarks propose effective mitigation methods, with most focusing on co-occurrence or attention mechanisms.\n\n*   **3. Technical Approach & Innovation**\n    *   **Core Technical Method:**\n        *   Introduces **Reefknot**, a comprehensive benchmark for evaluating, analyzing, and mitigating relation hallucinations in MLLMs \\cite{zheng20246fk}.\n        *   Provides a systematic definition of relation hallucinations, categorizing them into *perceptive* (concrete, e.g., \"on,\" \"behind\") and *cognitive* (abstract, e.g., \"eating,\" \"watching\") \\cite{zheng20246fk}.\n        *   Constructs the Reefknot dataset using real-world semantic triplets from the Visual Genome scene graph dataset, avoiding post-processing, manual annotation, or synthetic methods \\cite{zheng20246fk}.\n        *   Develops three diverse evaluation tasks: Yes/No (Y/N), Multiple Choice Questions (MCQ), and Visual Question Answering (VQA) \\cite{zheng20246fk}.\n        *   Proposes a novel **\"Detect-then-Calibrate\"** mitigation strategy based on analyzing token-level confidence and entropy to identify and reduce hallucinations \\cite{zheng20246fk}.\n    *   **Novelty/Difference:**\n        *   Reefknot is the *first comprehensive benchmark* specifically designed for relation hallucinations, offering a systematic definition and multi-faceted evaluation tasks.\n        *   The dataset construction method ensures real-world relevance and avoids common biases found in other benchmarks.\n        *   The \"Detect-then-Calibrate\" strategy is novel in its use of token-level confidence and entropy for hallucination detection and mitigation, differing from prior co-occurrence or attention-based methods \\cite{zheng20246fk}.\n\n*   **4. Key Technical Contributions**\n    *   **Novel Algorithms, Methods, or Techniques:**\n        *   Systematic definition and categorization of relation hallucinations into perceptive and cognitive types \\cite{zheng20246fk}.\n        *   The \"Detect-then-Calibrate\" method for hallucination mitigation, which leverages token-level confidence scores and entropy analysis to establish detection thresholds and apply calibration \\cite{zheng20246fk}.\n    *   **System Design or Architectural Innovations:**\n        *   The Reefknot benchmark itself, comprising over 20,000 real-world samples, two types of relationships, and three distinct evaluation tasks (Y/N, MCQ, VQA) \\cite{zheng20246fk}.\n        *   A robust data construction pipeline involving triplet identification, filtering, extraction, categorization, question construction, and multi-turn expert-based manual verification \\cite{zheng20246fk}.\n    *   **Theoretical Insights or Analysis:**\n        *   Empirical observation that MLLMs' response probability significantly drops (to ~70%) when hallucinations occur, compared to normal truthful answers (~95%) \\cite{zheng20246fk}.\n        *   Identification of a strong correlation between relation hallucination and high uncertainty (entropy E(X) > 0.6) at token levels \\cite{zheng20246fk}.\n        *   Discovery that MLLMs are disproportionately susceptible to *perceptive* hallucinations (consistently 10% higher incidence) compared to *cognitive* ones, which is counter-intuitive \\cite{zheng20246fk}.\n\n*   **5. Experimental Validation**\n    *   **Experiments Conducted:**\n        *   Comprehensive evaluation of mainstream MLLMs (e.g., LLaVA, MiniGPT4-v2, Qwen-VL, GPT-4o, MiniCPM, Yi-VL, GLM4V, CogVLM, Deepseek-VL, Phi-3) on the Reefknot benchmark across Y/N, MCQ, and VQA tasks \\cite{zheng20246fk}.\n        *   Analysis of hallucination rates for perceptive vs. cognitive relationships.\n        *   Quantitative comparison of decision probability distributions and entropy values for hallucination vs. non-hallucination cases.\n        *   Evaluation of the \"Detect-then-Calibrate\" mitigation strategy on Reefknot and two other relation hallucination benchmarks (MMRel, Rbench) \\cite{zheng20246fk}.\n    *   **Key Performance Metrics & Comparison Results:**\n        *   **Hallucination Rate (Halr)** and **Rscore** (a comprehensive metric for overall performance) were used \\cite{zheng20246fk}.\n        *   Current MLLMs exhibit significant limitations in handling relation hallucinations, with GPT-4o generally outperforming open-sourced models \\cite{zheng20246fk}.\n        *   MLLMs show a consistent 10% higher incidence of *perceptive* hallucinations compared to *cognitive* ones across models and settings \\cite{zheng20246fk}.\n        *   The \"Detect-then-Calibrate\" mitigation strategy successfully reduced the hallucination rate by an average of **9.75%** across the three evaluated datasets \\cite{zheng20246fk}.\n        *   Confusion matrices revealed biases in MLLM responses, such as a tendency to favor \"Yes\" in Y/N questions and specific options in MCQs.\n\n*   **6. Limitations & Scope**\n    *   **Technical Limitations/Assumptions:**\n        *   The paper suggests that MLLMs' struggles with perceptive relationships might stem from these common-sense relations being overlooked in the annotation process of original datasets like Visual Genome \\cite{zheng20246fk}.\n        *   The entropy analysis for hallucination detection is restricted to potential answers due to the extensive vocabulary of MLLMs.\n    *   **Scope of Applicability:** The work is specifically focused on *relation hallucinations* in Multimodal Large Language Models. The benchmark and mitigation strategy are designed for evaluating and improving relational understanding in MLLMs.\n\n*   **7. Technical Significance**\n    *   **Advancement of Technical State-of-the-Art:**\n        *   Reefknot establishes a new, rigorous standard for evaluating relation hallucinations, filling a critical gap in MLLM trustworthiness assessment \\cite{zheng20246fk}.\n        *   The proposed confidence-based mitigation strategy offers a practical and effective method to reduce hallucination rates, advancing the state-of-the-art in MLLM reliability.\n        *   The detailed analysis provides novel insights into the underlying mechanisms and biases of MLLMs regarding relational understanding, particularly the counter-intuitive finding about perceptive vs. cognitive hallucinations \\cite{zheng20246fk}.\n    *   **Potential Impact on Future Research:**\n        *   Reefknot will serve as a crucial tool for future research in developing and benchmarking more robust and trustworthy MLLMs.\n        *   The findings on perceptive vs. cognitive hallucinations highlight specific areas for improvement in MLLM training data and architectural design, particularly concerning common-sense reasoning and fine-grained relational understanding.\n        *   The \"Detect-then-Calibrate\" method opens avenues for further research into confidence-aware and uncertainty-driven mitigation strategies for MLLM hallucinations.",
      "intriguing_abstract": "Multimodal Large Language Models (MLLMs) are increasingly powerful, yet remain critically vulnerable to \"relation hallucinations\"â€”misinterpreting logical relationships between objects, a problem often more severe than object-level errors. Existing benchmarks are limited, failing to offer comprehensive evaluation or effective mitigation. We introduce **Reefknot**, the first comprehensive benchmark meticulously designed to systematically evaluate, analyze, and mitigate these complex hallucinations. Reefknot precisely categorizes relation hallucinations into *perceptive* and *cognitive* types, leveraging an unbiased dataset of real-world semantic triplets and diverse evaluation tasks (Yes/No, MCQ, VQA). Our analysis reveals a surprising susceptibility: MLLMs are consistently 10% more prone to *perceptive* hallucinations and that these errors strongly correlate with high token-level entropy. To combat this, we propose \"Detect-then-Calibrate,\" a novel mitigation strategy leveraging token-level confidence, which successfully reduces hallucination rates by an average of 9.75%. Reefknot establishes a new standard for MLLM trustworthiness, offering crucial insights and a practical solution to enhance relational understanding, paving the way for more reliable and robust MLLM development.",
      "keywords": [
        "Multimodal Large Language Models (MLLMs)",
        "relation hallucinations",
        "Reefknot benchmark",
        "\"Detect-then-Calibrate\" mitigation strategy",
        "perceptive hallucinations",
        "cognitive hallucinations",
        "token-level confidence and entropy",
        "Visual Question Answering (VQA)",
        "dataset biases",
        "MLLM reliability and trustworthiness",
        "disproportionate perceptive hallucinations",
        "hallucination rate reduction"
      ],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/4f83d9c391d782d358c2bf0d7ffc6150924dae01.pdf",
      "citation_key": "zheng20246fk",
      "metadata": {
        "title": "Reefknot: A Comprehensive Benchmark for Relation Hallucination Evaluation, Analysis and Mitigation in Multimodal Large Language Models",
        "authors": [
          "Kening Zheng",
          "Junkai Chen",
          "Yibo Yan",
          "Xin Zou",
          "Xuming Hu"
        ],
        "published_date": "2024",
        "abstract": "Hallucination issues continue to affect multimodal large language models (MLLMs), with existing research mainly addressing object-level or attribute-level hallucinations, neglecting the more complex relation hallucinations that require advanced reasoning. Current benchmarks for relation hallucinations lack detailed evaluation and effective mitigation, and their datasets often suffer from biases due to systematic annotation processes. To address these challenges, we introduce Reefknot, a comprehensive benchmark targeting relation hallucinations, comprising over 20,000 real-world samples. We provide a systematic definition of relation hallucinations, integrating perceptive and cognitive perspectives, and construct a relation-based corpus using the Visual Genome scene graph dataset. Our comparative evaluation reveals significant limitations in current MLLMs' ability to handle relation hallucinations. Additionally, we propose a novel confidence-based mitigation strategy, which reduces the hallucination rate by an average of 9.75% across three datasets, including Reefknot. Our work offers valuable insights for achieving trustworthy multimodal intelligence.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/4f83d9c391d782d358c2bf0d7ffc6150924dae01.pdf",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **1. Research Problem & Motivation**\n    *   **Specific Technical Problem:** Multimodal Large Language Models (MLLMs) suffer from \"relation hallucinations,\" where they misinterpret logical relationships between two or more objects in an image \\cite{zheng20246fk}. Existing research primarily addresses simpler object-level or attribute-level hallucinations.\n    *   **Importance & Challenge:** This problem is crucial because relation hallucinations demand advanced reasoning capabilities from MLLMs. Current benchmarks for relation hallucinations are limited, lacking detailed evaluation and effective mitigation strategies, and their datasets often suffer from biases due to systematic annotation processes \\cite{zheng20246fk}. The paper demonstrates that relation hallucination can be more severe than object hallucination in current MLLMs.\n\n*   **2. Related Work & Positioning**\n    *   **Relation to Existing Approaches:** Previous work on MLLM hallucinations largely focuses on object-level (e.g., POPE) or attribute-level issues (e.g., MME) \\cite{zheng20246fk}.\n    *   **Limitations of Previous Solutions:**\n        *   Existing relation hallucination benchmarks (e.g., MMRel, R-Bench) often use simple Yes/No (Y/N) tasks or LLM-based scoring, failing to provide comprehensive evaluation from both discriminative (Y/N, MCQ) and generative (VQA) perspectives \\cite{zheng20246fk}.\n        *   Many benchmarks rely on post-processed, manually annotated, or synthetic data, which can introduce biases.\n        *   Few previous benchmarks propose effective mitigation methods, with most focusing on co-occurrence or attention mechanisms.\n\n*   **3. Technical Approach & Innovation**\n    *   **Core Technical Method:**\n        *   Introduces **Reefknot**, a comprehensive benchmark for evaluating, analyzing, and mitigating relation hallucinations in MLLMs \\cite{zheng20246fk}.\n        *   Provides a systematic definition of relation hallucinations, categorizing them into *perceptive* (concrete, e.g., \"on,\" \"behind\") and *cognitive* (abstract, e.g., \"eating,\" \"watching\") \\cite{zheng20246fk}.\n        *   Constructs the Reefknot dataset using real-world semantic triplets from the Visual Genome scene graph dataset, avoiding post-processing, manual annotation, or synthetic methods \\cite{zheng20246fk}.\n        *   Develops three diverse evaluation tasks: Yes/No (Y/N), Multiple Choice Questions (MCQ), and Visual Question Answering (VQA) \\cite{zheng20246fk}.\n        *   Proposes a novel **\"Detect-then-Calibrate\"** mitigation strategy based on analyzing token-level confidence and entropy to identify and reduce hallucinations \\cite{zheng20246fk}.\n    *   **Novelty/Difference:**\n        *   Reefknot is the *first comprehensive benchmark* specifically designed for relation hallucinations, offering a systematic definition and multi-faceted evaluation tasks.\n        *   The dataset construction method ensures real-world relevance and avoids common biases found in other benchmarks.\n        *   The \"Detect-then-Calibrate\" strategy is novel in its use of token-level confidence and entropy for hallucination detection and mitigation, differing from prior co-occurrence or attention-based methods \\cite{zheng20246fk}.\n\n*   **4. Key Technical Contributions**\n    *   **Novel Algorithms, Methods, or Techniques:**\n        *   Systematic definition and categorization of relation hallucinations into perceptive and cognitive types \\cite{zheng20246fk}.\n        *   The \"Detect-then-Calibrate\" method for hallucination mitigation, which leverages token-level confidence scores and entropy analysis to establish detection thresholds and apply calibration \\cite{zheng20246fk}.\n    *   **System Design or Architectural Innovations:**\n        *   The Reefknot benchmark itself, comprising over 20,000 real-world samples, two types of relationships, and three distinct evaluation tasks (Y/N, MCQ, VQA) \\cite{zheng20246fk}.\n        *   A robust data construction pipeline involving triplet identification, filtering, extraction, categorization, question construction, and multi-turn expert-based manual verification \\cite{zheng20246fk}.\n    *   **Theoretical Insights or Analysis:**\n        *   Empirical observation that MLLMs' response probability significantly drops (to ~70%) when hallucinations occur, compared to normal truthful answers (~95%) \\cite{zheng20246fk}.\n        *   Identification of a strong correlation between relation hallucination and high uncertainty (entropy E(X) > 0.6) at token levels \\cite{zheng20246fk}.\n        *   Discovery that MLLMs are disproportionately susceptible to *perceptive* hallucinations (consistently 10% higher incidence) compared to *cognitive* ones, which is counter-intuitive \\cite{zheng20246fk}.\n\n*   **5. Experimental Validation**\n    *   **Experiments Conducted:**\n        *   Comprehensive evaluation of mainstream MLLMs (e.g., LLaVA, MiniGPT4-v2, Qwen-VL, GPT-4o, MiniCPM, Yi-VL, GLM4V, CogVLM, Deepseek-VL, Phi-3) on the Reefknot benchmark across Y/N, MCQ, and VQA tasks \\cite{zheng20246fk}.\n        *   Analysis of hallucination rates for perceptive vs. cognitive relationships.\n        *   Quantitative comparison of decision probability distributions and entropy values for hallucination vs. non-hallucination cases.\n        *   Evaluation of the \"Detect-then-Calibrate\" mitigation strategy on Reefknot and two other relation hallucination benchmarks (MMRel, Rbench) \\cite{zheng20246fk}.\n    *   **Key Performance Metrics & Comparison Results:**\n        *   **Hallucination Rate (Halr)** and **Rscore** (a comprehensive metric for overall performance) were used \\cite{zheng20246fk}.\n        *   Current MLLMs exhibit significant limitations in handling relation hallucinations, with GPT-4o generally outperforming open-sourced models \\cite{zheng20246fk}.\n        *   MLLMs show a consistent 10% higher incidence of *perceptive* hallucinations compared to *cognitive* ones across models and settings \\cite{zheng20246fk}.\n        *   The \"Detect-then-Calibrate\" mitigation strategy successfully reduced the hallucination rate by an average of **9.75%** across the three evaluated datasets \\cite{zheng20246fk}.\n        *   Confusion matrices revealed biases in MLLM responses, such as a tendency to favor \"Yes\" in Y/N questions and specific options in MCQs.\n\n*   **6. Limitations & Scope**\n    *   **Technical Limitations/Assumptions:**\n        *   The paper suggests that MLLMs' struggles with perceptive relationships might stem from these common-sense relations being overlooked in the annotation process of original datasets like Visual Genome \\cite{zheng20246fk}.\n        *   The entropy analysis for hallucination detection is restricted to potential answers due to the extensive vocabulary of MLLMs.\n    *   **Scope of Applicability:** The work is specifically focused on *relation hallucinations* in Multimodal Large Language Models. The benchmark and mitigation strategy are designed for evaluating and improving relational understanding in MLLMs.\n\n*   **7. Technical Significance**\n    *   **Advancement of Technical State-of-the-Art:**\n        *   Reefknot establishes a new, rigorous standard for evaluating relation hallucinations, filling a critical gap in MLLM trustworthiness assessment \\cite{zheng20246fk}.\n        *   The proposed confidence-based mitigation strategy offers a practical and effective method to reduce hallucination rates, advancing the state-of-the-art in MLLM reliability.\n        *   The detailed analysis provides novel insights into the underlying mechanisms and biases of MLLMs regarding relational understanding, particularly the counter-intuitive finding about perceptive vs. cognitive hallucinations \\cite{zheng20246fk}.\n    *   **Potential Impact on Future Research:**\n        *   Reefknot will serve as a crucial tool for future research in developing and benchmarking more robust and trustworthy MLLMs.\n        *   The findings on perceptive vs. cognitive hallucinations highlight specific areas for improvement in MLLM training data and architectural design, particularly concerning common-sense reasoning and fine-grained relational understanding.\n        *   The \"Detect-then-Calibrate\" method opens avenues for further research into confidence-aware and uncertainty-driven mitigation strategies for MLLM hallucinations.",
        "keywords": [
          "Multimodal Large Language Models (MLLMs)",
          "relation hallucinations",
          "Reefknot benchmark",
          "\"Detect-then-Calibrate\" mitigation strategy",
          "perceptive hallucinations",
          "cognitive hallucinations",
          "token-level confidence and entropy",
          "Visual Question Answering (VQA)",
          "dataset biases",
          "MLLM reliability and trustworthiness",
          "disproportionate perceptive hallucinations",
          "hallucination rate reduction"
        ],
        "paper_type": "based on the abstract and introduction, this paper is best classified as **technical**.\n\nhere's why:\n\n*   **abstract mentions:**\n    *   \"to address these challenges, we **introduce reefknot, a comprehensive benchmark**...\" (presents a new system/resource)\n    *   \"we provide a systematic definition... and **construct a relation-based corpus**...\" (describes the methodology for building a new resource)\n    *   \"additionally, we **propose a novel confidence-based mitigation strategy**...\" (presents a new method/algorithm)\n*   **introduction discusses:**\n    *   a specific technical problem: \"hallucination issues continue to affect multimodal large language models (mllms), with existing research mainly addressing object-level or attribute-level hallucinations, neglecting the more complex relation hallucinations...\"\n    *   the proposed solutions: the reefknot benchmark and the mitigation strategy.\n\nwhile the paper also includes empirical evaluation (\"our comparative evaluation reveals significant limitations...\", \"reduces the hallucination rate by an average of 9.75%...\"), these empirical findings are presented as results of applying and validating the *new benchmark* and *new mitigation strategy* that the paper proposes. the core contribution is the development and presentation of these novel methods and systems."
      },
      "file_name": "4f83d9c391d782d358c2bf0d7ffc6150924dae01.pdf"
    },
    {
      "success": true,
      "doc_id": "170e50f26bceca9699535f2f4d6cd4d7",
      "summary": "Here's a focused summary of the technical paper for a literature review, incorporating the requested citation:\n\n### Analysis of \"Unified Hallucination Detection for Multimodal Large Language Models\" \\cite{chen2024lc5}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: The paper addresses the critical issue of hallucination in Multimodal Large Language Models (MLLMs), where models generate seemingly credible content that contradicts input data or established world knowledge.\n    *   **Importance and Challenge**: Hallucinations hinder the practical deployment of MLLMs and contribute to misinformation. Existing detection methods are limited by:\n        *   **Task Singularity**: Focusing only on specific tasks (e.g., image captioning) and neglecting others like text-to-image generation.\n        *   **Limited Hallucination Categories**: Primarily identifying object-level hallucinations, overlooking scene-text or factual inconsistencies.\n        *   **Incomplete Granularity**: Evaluating entire responses holistically instead of fine-grained, claim-level assessment.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: Prior research has focused on detecting or alleviating hallucinations in MLLMs (e.g., Zhou et al., 2023; Zhai et al., 2023; Li et al., 2023b; Wang et al., 2023c; Xing et al., 2024; Wu et al., 2024).\n    *   **Limitations of Previous Solutions**: These efforts operate in isolation and suffer from the aforementioned task singularity, limited hallucination categories, and incomplete granularity, preventing a unified understanding and detection of multimodal hallucinations.\n    *   **Positioning**: \\cite{chen2024lc5} aims to overcome these limitations by proposing a unified problem setting and a comprehensive framework for hallucination detection across various tasks and hallucination types, with fine-grained analysis.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method (UNIHD)**: The paper introduces UNIHD (Unified Multimodal Hallucination Detection), a task-agnostic, tool-enhanced framework that integrates evidence from multiple auxiliary tools through a four-stage procedure:\n        1.  **Essential Claim Extraction**: Uses MLLMs (e.g., GPT-4V/Gemini) to extract core claims from generated responses (image-to-text) or user queries (text-to-image).\n        2.  **Autonomous Tool Selection via Query Formulation**: Prompts MLLMs to autonomously generate pertinent questions for each claim, determining the specific tool needed and its input.\n        3.  **Parallel Tool Execution**: Deploys a suite of specialized tools concurrently (e.g., Grounding DINO for object detection, attribute detection, scene text recognition, search engines for factual checks) to gather evidence.\n        4.  **Hallucination Verification with Rationales**: Aggregates the collected evidence to instruct the underlying MLLM to judge whether a claim is hallucinatory, providing rationales.\n    *   **Novelty/Differentiation**:\n        *   **Unified Problem Setting**: Expands hallucination detection to cover both image-to-text (IC, VQA) and text-to-image generation, and integrates modality-conflicting (object, attribute, scene-text) and fact-conflicting hallucinations.\n        *   **Meta-Evaluation Benchmark (MHaluBench)**: A meticulously crafted benchmark designed for evaluating advancements in unified hallucination detection, featuring fine-grained claim-level annotations.\n        *   **Tool-Augmented Framework**: Leverages external, specialized tools orchestrated by an MLLM to robustly validate claims, moving beyond self-detection mechanisms of MLLMs alone.\n        *   **Autonomous Query Formulation**: MLLMs intelligently generate queries to guide tool selection and execution, enhancing adaptability.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**: The UNIHD framework, with its structured approach to claim extraction, autonomous tool selection, parallel execution, and evidence-based verification, represents a novel methodology for comprehensive hallucination detection.\n    *   **System Design/Architectural Innovations**: The tool-augmented architecture, where an MLLM acts as an orchestrator for specialized external tools, provides a robust and extensible design for multimodal hallucination detection.\n    *   **Benchmark Innovation**: Introduction of MHaluBench, a meta-evaluation benchmark that unifies various hallucination categories and multimodal tasks, offering fine-grained analytical features (segment and claim levels) for rigorous detector assessment.\n    *   **Theoretical Insights**: Proposes a unified view of multimodal hallucination taxonomy, categorizing them into Modality-Conflicting (object, attribute, scene-text) and Fact-Conflicting types.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**: A thorough evaluation of the UNIHD framework was conducted using an underlying MLLM against the newly introduced MHaluBench benchmark.\n    *   **Key Performance Metrics and Comparison Results**: The paper states that findings underscore the effectiveness of their approach. While specific quantitative results are not detailed in the abstract, the \"extensive experiments demonstrate the efficacy of this method,\" and confirm that multimodal hallucination detection remains a formidable challenge.\n    *   **MHaluBench Details**: The benchmark includes 200 Image Captioning (IC), 200 Visual Question Answering (VQA), and 220 Text-to-Image (T2I) exemplars. Data was collected from leading MLLMs (mPLUG, LLaVA, MiniGPT-4 for I2T; DALL-E 2/3 for T2I). Human annotation achieved significant inter-annotator reliability (Fleissâ€™s Kappa Îº=0.822).\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The paper acknowledges that multimodal hallucination detection remains a formidable challenge, implying that UNIHD, while effective, does not fully solve the problem. The reliance on powerful MLLMs (GPT-4V/Gemini) for claim extraction and query formulation might introduce dependencies or computational costs.\n    *   **Scope of Applicability**: The framework is designed for unified detection across both image-to-text (Image Captioning, VQA) and text-to-image generation tasks, covering a broad spectrum of hallucination categories including object, attribute, scene-text, and factual inconsistencies.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: \\cite{chen2024lc5} significantly advances the technical state-of-the-art by proposing a unified and comprehensive approach to multimodal hallucination detection, moving beyond the fragmented efforts of prior research. The tool-augmented framework offers a robust and adaptable solution.\n    *   **Potential Impact on Future Research**: The introduction of MHaluBench provides a crucial meta-evaluation benchmark for future research, enabling standardized and fine-grained assessment of new hallucination detection methods. The insights gained from UNIHD's performance and the strategic guidance on tool application can drive the development of more reliable and trustworthy MLLMs.",
      "intriguing_abstract": "The pervasive challenge of **hallucination** in **Multimodal Large Language Models (MLLMs)** severely impedes their trustworthiness and practical deployment, contributing to misinformation. Existing detection methods are fragmented, suffering from task singularity, limited hallucination categories, and coarse granularity. We introduce **UNIHD (Unified Multimodal Hallucination Detection)**, a novel, **tool-augmented** framework designed to overcome these limitations.\n\nUNIHD orchestrates a suite of specialized external tools through an MLLM-driven four-stage process, enabling robust, **fine-grained, claim-level verification**. It uniquely unifies detection across both **image-to-text** (e.g., VQA, captioning) and **text-to-image generation** tasks, addressing diverse **modality-conflicting** (object, attribute, scene-text) and **fact-conflicting hallucinations**. To facilitate rigorous evaluation, we present **MHaluBench**, a meticulously annotated **meta-evaluation benchmark**. Our extensive experiments demonstrate UNIHD's efficacy, offering a significant advancement towards building more reliable and trustworthy MLLMs. This work provides a critical foundation for future research in robust multimodal AI.",
      "keywords": [
        "Multimodal Large Language Models (MLLMs)",
        "Hallucination detection",
        "Unified problem setting",
        "UNIHD framework",
        "Tool-augmented framework",
        "Autonomous tool selection",
        "MHaluBench",
        "Fine-grained claim-level assessment",
        "Unified hallucination taxonomy",
        "Modality-conflicting hallucinations",
        "Fact-conflicting hallucinations",
        "Image-to-text generation",
        "Text-to-image generation",
        "Evidence-based verification"
      ],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/19e909f88b8b9b0635bd6e441094e1738c3bba9a.pdf",
      "citation_key": "chen2024lc5",
      "metadata": {
        "title": "Unified Hallucination Detection for Multimodal Large Language Models",
        "authors": [
          "Xiang Chen",
          "Chenxi Wang",
          "Yida Xue",
          "Ningyu Zhang",
          "Xiaoyan Yang",
          "Qian Li",
          "Yue Shen",
          "Lei Liang",
          "Jinjie Gu",
          "Huajun Chen"
        ],
        "published_date": "2024",
        "abstract": "Despite significant strides in multimodal tasks, Multimodal Large Language Models (MLLMs) are plagued by the critical issue of hallucination. The reliable detection of such hallucinations in MLLMs has, therefore, become a vital aspect of model evaluation and the safeguarding of practical application deployment. Prior research in this domain has been constrained by a narrow focus on singular tasks, an inadequate range of hallucination categories addressed, and a lack of detailed granularity. In response to these challenges, our work expands the investigative horizons of hallucination detection. We present a novel meta-evaluation benchmark, MHaluBench, meticulously crafted to facilitate the evaluation of advancements in hallucination detection methods. Additionally, we unveil a novel unified multimodal hallucination detection framework, UNIHD, which leverages a suite of auxiliary tools to validate the occurrence of hallucinations robustly. We demonstrate the effectiveness of UNIHD through meticulous evaluation and comprehensive analysis. We also provide strategic insights on the application of specific tools for addressing various categories of hallucinations.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/19e909f88b8b9b0635bd6e441094e1738c3bba9a.pdf",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the technical paper for a literature review, incorporating the requested citation:\n\n### Analysis of \"Unified Hallucination Detection for Multimodal Large Language Models\" \\cite{chen2024lc5}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: The paper addresses the critical issue of hallucination in Multimodal Large Language Models (MLLMs), where models generate seemingly credible content that contradicts input data or established world knowledge.\n    *   **Importance and Challenge**: Hallucinations hinder the practical deployment of MLLMs and contribute to misinformation. Existing detection methods are limited by:\n        *   **Task Singularity**: Focusing only on specific tasks (e.g., image captioning) and neglecting others like text-to-image generation.\n        *   **Limited Hallucination Categories**: Primarily identifying object-level hallucinations, overlooking scene-text or factual inconsistencies.\n        *   **Incomplete Granularity**: Evaluating entire responses holistically instead of fine-grained, claim-level assessment.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: Prior research has focused on detecting or alleviating hallucinations in MLLMs (e.g., Zhou et al., 2023; Zhai et al., 2023; Li et al., 2023b; Wang et al., 2023c; Xing et al., 2024; Wu et al., 2024).\n    *   **Limitations of Previous Solutions**: These efforts operate in isolation and suffer from the aforementioned task singularity, limited hallucination categories, and incomplete granularity, preventing a unified understanding and detection of multimodal hallucinations.\n    *   **Positioning**: \\cite{chen2024lc5} aims to overcome these limitations by proposing a unified problem setting and a comprehensive framework for hallucination detection across various tasks and hallucination types, with fine-grained analysis.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method (UNIHD)**: The paper introduces UNIHD (Unified Multimodal Hallucination Detection), a task-agnostic, tool-enhanced framework that integrates evidence from multiple auxiliary tools through a four-stage procedure:\n        1.  **Essential Claim Extraction**: Uses MLLMs (e.g., GPT-4V/Gemini) to extract core claims from generated responses (image-to-text) or user queries (text-to-image).\n        2.  **Autonomous Tool Selection via Query Formulation**: Prompts MLLMs to autonomously generate pertinent questions for each claim, determining the specific tool needed and its input.\n        3.  **Parallel Tool Execution**: Deploys a suite of specialized tools concurrently (e.g., Grounding DINO for object detection, attribute detection, scene text recognition, search engines for factual checks) to gather evidence.\n        4.  **Hallucination Verification with Rationales**: Aggregates the collected evidence to instruct the underlying MLLM to judge whether a claim is hallucinatory, providing rationales.\n    *   **Novelty/Differentiation**:\n        *   **Unified Problem Setting**: Expands hallucination detection to cover both image-to-text (IC, VQA) and text-to-image generation, and integrates modality-conflicting (object, attribute, scene-text) and fact-conflicting hallucinations.\n        *   **Meta-Evaluation Benchmark (MHaluBench)**: A meticulously crafted benchmark designed for evaluating advancements in unified hallucination detection, featuring fine-grained claim-level annotations.\n        *   **Tool-Augmented Framework**: Leverages external, specialized tools orchestrated by an MLLM to robustly validate claims, moving beyond self-detection mechanisms of MLLMs alone.\n        *   **Autonomous Query Formulation**: MLLMs intelligently generate queries to guide tool selection and execution, enhancing adaptability.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithms/Methods**: The UNIHD framework, with its structured approach to claim extraction, autonomous tool selection, parallel execution, and evidence-based verification, represents a novel methodology for comprehensive hallucination detection.\n    *   **System Design/Architectural Innovations**: The tool-augmented architecture, where an MLLM acts as an orchestrator for specialized external tools, provides a robust and extensible design for multimodal hallucination detection.\n    *   **Benchmark Innovation**: Introduction of MHaluBench, a meta-evaluation benchmark that unifies various hallucination categories and multimodal tasks, offering fine-grained analytical features (segment and claim levels) for rigorous detector assessment.\n    *   **Theoretical Insights**: Proposes a unified view of multimodal hallucination taxonomy, categorizing them into Modality-Conflicting (object, attribute, scene-text) and Fact-Conflicting types.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**: A thorough evaluation of the UNIHD framework was conducted using an underlying MLLM against the newly introduced MHaluBench benchmark.\n    *   **Key Performance Metrics and Comparison Results**: The paper states that findings underscore the effectiveness of their approach. While specific quantitative results are not detailed in the abstract, the \"extensive experiments demonstrate the efficacy of this method,\" and confirm that multimodal hallucination detection remains a formidable challenge.\n    *   **MHaluBench Details**: The benchmark includes 200 Image Captioning (IC), 200 Visual Question Answering (VQA), and 220 Text-to-Image (T2I) exemplars. Data was collected from leading MLLMs (mPLUG, LLaVA, MiniGPT-4 for I2T; DALL-E 2/3 for T2I). Human annotation achieved significant inter-annotator reliability (Fleissâ€™s Kappa Îº=0.822).\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The paper acknowledges that multimodal hallucination detection remains a formidable challenge, implying that UNIHD, while effective, does not fully solve the problem. The reliance on powerful MLLMs (GPT-4V/Gemini) for claim extraction and query formulation might introduce dependencies or computational costs.\n    *   **Scope of Applicability**: The framework is designed for unified detection across both image-to-text (Image Captioning, VQA) and text-to-image generation tasks, covering a broad spectrum of hallucination categories including object, attribute, scene-text, and factual inconsistencies.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: \\cite{chen2024lc5} significantly advances the technical state-of-the-art by proposing a unified and comprehensive approach to multimodal hallucination detection, moving beyond the fragmented efforts of prior research. The tool-augmented framework offers a robust and adaptable solution.\n    *   **Potential Impact on Future Research**: The introduction of MHaluBench provides a crucial meta-evaluation benchmark for future research, enabling standardized and fine-grained assessment of new hallucination detection methods. The insights gained from UNIHD's performance and the strategic guidance on tool application can drive the development of more reliable and trustworthy MLLMs.",
        "keywords": [
          "Multimodal Large Language Models (MLLMs)",
          "Hallucination detection",
          "Unified problem setting",
          "UNIHD framework",
          "Tool-augmented framework",
          "Autonomous tool selection",
          "MHaluBench",
          "Fine-grained claim-level assessment",
          "Unified hallucination taxonomy",
          "Modality-conflicting hallucinations",
          "Fact-conflicting hallucinations",
          "Image-to-text generation",
          "Text-to-image generation",
          "Evidence-based verification"
        ],
        "paper_type": "based on the abstract and introduction:\n\n*   the abstract explicitly states: \"we present a novel meta-evaluation benchmark, mhalubench\" and \"we unveil a novel unified multimodal hallucination detection framework, unihd\".\n*   it discusses leveraging \"a suite of auxiliary tools\" and demonstrating \"the effectiveness of unihd through meticulous evaluation and comprehensive analysis.\"\n*   the introduction further elaborates on the problem (hallucination in mllms) and the need for solutions.\n*   the presence of a github link and a demonstration link (for \"easydetect\") strongly indicates the development and release of a system or tool.\n\nthese points align directly with the criteria for a **technical** paper, which \"presents new methods, algorithms, or systems\" and where the abstract mentions \"propose\", \"develop\", \"present\", \"algorithm\", \"method\" and the introduction discusses a \"technical problem, proposed solution\".\n\n**classification: technical**"
      },
      "file_name": "19e909f88b8b9b0635bd6e441094e1738c3bba9a.pdf"
    },
    {
      "success": true,
      "doc_id": "da934a6c0ebc0abbe5bd13b105000839",
      "summary": "Here's a focused summary of the paper \\cite{sahoo2024hcb} for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem**: The paper addresses the critical challenge of \"hallucination\" in rapidly advancing foundation models (FMs) across various modalities, including language, image, video, and audio. Hallucination refers to FMs generating plausible but factually incoherent or absurd content lacking proper context understanding \\cite{sahoo2024hcb}.\n    *   **Importance and Challenge**: Hallucination is identified as the biggest hindrance to the widespread adoption of FMs in real-world, high-stakes applications where reliability and accuracy are paramount. Root causes include biases in training data, limited access to up-to-date information, and inherent model constraints in contextual comprehension \\cite{sahoo2024hcb}.\n\n*   **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: Most existing survey papers have explored hallucination primarily in the context of Large Language Models (LLMs) \\cite{sahoo2024hcb}.\n    *   **Limitations of Previous Solutions**: Previous work lacks a comprehensive, multimodal perspective on hallucination. Recent studies indicate that hallucination also occurs in vision, audio, and video foundation models, highlighting a significant gap in understanding and addressing this challenge across multiple modalities \\cite{sahoo2024hcb}.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method**: As a survey paper, its core approach is to synthesize and categorize recent advancements in identifying and mitigating hallucination across diverse modalities (text, image, video, audio). It establishes a clear framework encompassing definition, taxonomy, and detection/mitigation strategies \\cite{sahoo2024hcb}.\n    *   **Novelty**: The primary innovation is its *holistic and multimodal perspective*. It extends beyond the typical focus solely on language models to provide a comprehensive overview of hallucination detection and mitigation techniques specifically designed for multimodal foundation models \\cite{sahoo2024hcb}.\n\n*   **Key Technical Contributions**\n    *   **Novel Algorithms, Methods, or Techniques (as discussed in the survey)**: The paper reviews various detection methods (e.g., SelfCheckGPT, FACTSCORE, POPE, NOPE, EMScore, PAM) and mitigation strategies (e.g., data augmentation, chain-of-thought, alignment, prompt engineering for LLMs; context understanding, fine-tuning, GANs, RL for LVLMs; temporal dependency modeling for video; latent diffusion models, retrieval-based for audio) proposed by other researchers \\cite{sahoo2024hcb}.\n    *   **Theoretical Insights or Analysis**: It establishes a precise definition and structured taxonomy of hallucination in large-scale foundation models, organizing detection and mitigation techniques across modalities (Figure 2) \\cite{sahoo2024hcb}.\n    *   **System Design or Architectural Innovations**: Not applicable as the paper is a survey, but it discusses architectural considerations within the reviewed mitigation techniques (e.g., integrating open-source vision models in MARINE for LVLMs) \\cite{sahoo2024hcb}.\n\n*   **Experimental Validation (as reviewed in the survey)**\n    *   **Experiments Conducted**: The survey discusses various benchmarks and evaluation datasets used by other researchers to validate hallucination detection and mitigation techniques. Examples include PHD, FACTOID, FactCHD, HaluEval for LLMs; POPE, NOPE, VHTest, ChartBench, HallusionBench for LVLMs; and EMScore for video models \\cite{sahoo2024hcb}.\n    *   **Key Performance Metrics and Comparison Results**: The paper highlights reported improvements, such as a multi-task learning approach achieving a 40% average accuracy improvement on the FACTOID benchmark for LLMs, and methods like MARINE demonstrating effectiveness in reducing hallucinations and enhancing output detail in LVLMs, validated through assessments using GPT-4V \\cite{sahoo2024hcb}. It also notes varying performance of SOTA MLLMs on benchmarks like VHTest, with GPT-4V exhibiting lower hallucination than MiniGPT-v2 \\cite{sahoo2024hcb}.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations or Assumptions**: The survey identifies that hallucination mitigation efforts have predominantly relied on empirical methods, leaving uncertainty regarding the possibility of complete elimination \\cite{sahoo2024hcb}. It also points out challenges like the absence of a standardized metric for assessing object hallucination in LVLMs \\cite{sahoo2024hcb}.\n    *   **Scope of Applicability**: The survey comprehensively covers hallucination in Large Language, Image, Video, and Audio Foundation Models, providing a broad overview of the problem across these diverse modalities \\cite{sahoo2024hcb}.\n\n*   **Technical Significance**\n    *   **Advance the Technical State-of-the-Art**: This paper significantly advances the technical state-of-the-art by providing the first comprehensive, multimodal survey of hallucination in FMs, thereby filling a critical gap in the existing literature. It synthesizes diverse advancements, offering a unified perspective on a pervasive problem \\cite{sahoo2024hcb}.\n    *   **Potential Impact on Future Research**: By establishing a clear framework, taxonomy, and highlighting open challenges, the paper serves as a vital resource for researchers and practitioners. It is expected to aid in the development of more robust AI solutions and lay a foundational groundwork for future research and development in this pivotal area \\cite{sahoo2024hcb}.",
      "intriguing_abstract": "The pervasive challenge of \"hallucination\"â€”the generation of plausible but factually incoherent contentâ€”remains the most significant barrier to the widespread adoption of advanced Foundation Models (FMs) in high-stakes applications. While existing literature predominantly focuses on Large Language Models (LLMs), a critical gap persists in comprehensively addressing this issue across diverse modalities. This paper presents the first holistic, *multimodal* survey of hallucination in FMs, spanning language, image, video, and audio.\n\nWe establish a novel framework, offering a precise definition and structured taxonomy of hallucination, alongside an exhaustive review of cutting-edge detection and mitigation strategies. From prompt engineering and chain-of-thought for LLMs to context understanding, GANs, RL, and latent diffusion models for Large Vision-Language Models (LVLMs) and audio, we synthesize diverse advancements. Techniques like SelfCheckGPT, FACTSCORE, POPE, NOPE, EMScore, and MARINE are discussed, highlighting their efficacy on benchmarks such as PHD, VHTest, and HallusionBench. By unifying disparate research, this survey provides an indispensable resource, charting a clear path for developing more reliable and trustworthy AI systems and accelerating their integration into real-world applications.",
      "keywords": [
        "Hallucination",
        "Foundation Models",
        "Multimodal Hallucination",
        "Hallucination Detection",
        "Hallucination Mitigation",
        "Large Language Models (LLMs)",
        "Multimodal Perspective",
        "Taxonomy of Hallucination",
        "High-stakes AI applications",
        "Reliability and Accuracy",
        "Survey Paper",
        "Empirical Mitigation Methods",
        "Standardized Metrics",
        "Context Understanding"
      ],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/c14010990c9d75a6e836e1c86d42f405a5d3d0a6.pdf",
      "citation_key": "sahoo2024hcb",
      "metadata": {
        "title": "A Comprehensive Survey of Hallucination in Large Language, Image, Video and Audio Foundation Models",
        "authors": [
          "Pranab Sahoo",
          "Prabhash Meharia",
          "Akash Ghosh",
          "Sriparna Saha",
          "Vinija Jain",
          "Aman Chadha"
        ],
        "published_date": "2024",
        "abstract": "The rapid advancement of foundation models (FMs) across language, image, audio, and video domains has shown remarkable capabilities in diverse tasks. However, the proliferation of FMs brings forth a critical challenge: the potential to generate hallucinated outputs, particularly in high-stakes applications. The tendency of foundation models to produce hallucinated content arguably represents the biggest hindrance to their widespread adoption in real-world scenarios, especially in domains where reliability and accuracy are paramount. This survey paper presents a comprehensive overview of recent developments that aim to identify and mitigate the problem of hallucination in FMs, spanning text, image, video, and audio modalities. By synthesizing recent advancements in detecting and mitigating hallucination across various modalities, the paper aims to provide valuable insights for researchers, developers, and practitioners. Essentially, it establishes a clear framework encompassing definition, taxonomy, and detection strategies for addressing hallucination in multimodal foundation models, laying the foundation for future research in this pivotal area.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/c14010990c9d75a6e836e1c86d42f405a5d3d0a6.pdf",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the paper \\cite{sahoo2024hcb} for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem**: The paper addresses the critical challenge of \"hallucination\" in rapidly advancing foundation models (FMs) across various modalities, including language, image, video, and audio. Hallucination refers to FMs generating plausible but factually incoherent or absurd content lacking proper context understanding \\cite{sahoo2024hcb}.\n    *   **Importance and Challenge**: Hallucination is identified as the biggest hindrance to the widespread adoption of FMs in real-world, high-stakes applications where reliability and accuracy are paramount. Root causes include biases in training data, limited access to up-to-date information, and inherent model constraints in contextual comprehension \\cite{sahoo2024hcb}.\n\n*   **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: Most existing survey papers have explored hallucination primarily in the context of Large Language Models (LLMs) \\cite{sahoo2024hcb}.\n    *   **Limitations of Previous Solutions**: Previous work lacks a comprehensive, multimodal perspective on hallucination. Recent studies indicate that hallucination also occurs in vision, audio, and video foundation models, highlighting a significant gap in understanding and addressing this challenge across multiple modalities \\cite{sahoo2024hcb}.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method**: As a survey paper, its core approach is to synthesize and categorize recent advancements in identifying and mitigating hallucination across diverse modalities (text, image, video, audio). It establishes a clear framework encompassing definition, taxonomy, and detection/mitigation strategies \\cite{sahoo2024hcb}.\n    *   **Novelty**: The primary innovation is its *holistic and multimodal perspective*. It extends beyond the typical focus solely on language models to provide a comprehensive overview of hallucination detection and mitigation techniques specifically designed for multimodal foundation models \\cite{sahoo2024hcb}.\n\n*   **Key Technical Contributions**\n    *   **Novel Algorithms, Methods, or Techniques (as discussed in the survey)**: The paper reviews various detection methods (e.g., SelfCheckGPT, FACTSCORE, POPE, NOPE, EMScore, PAM) and mitigation strategies (e.g., data augmentation, chain-of-thought, alignment, prompt engineering for LLMs; context understanding, fine-tuning, GANs, RL for LVLMs; temporal dependency modeling for video; latent diffusion models, retrieval-based for audio) proposed by other researchers \\cite{sahoo2024hcb}.\n    *   **Theoretical Insights or Analysis**: It establishes a precise definition and structured taxonomy of hallucination in large-scale foundation models, organizing detection and mitigation techniques across modalities (Figure 2) \\cite{sahoo2024hcb}.\n    *   **System Design or Architectural Innovations**: Not applicable as the paper is a survey, but it discusses architectural considerations within the reviewed mitigation techniques (e.g., integrating open-source vision models in MARINE for LVLMs) \\cite{sahoo2024hcb}.\n\n*   **Experimental Validation (as reviewed in the survey)**\n    *   **Experiments Conducted**: The survey discusses various benchmarks and evaluation datasets used by other researchers to validate hallucination detection and mitigation techniques. Examples include PHD, FACTOID, FactCHD, HaluEval for LLMs; POPE, NOPE, VHTest, ChartBench, HallusionBench for LVLMs; and EMScore for video models \\cite{sahoo2024hcb}.\n    *   **Key Performance Metrics and Comparison Results**: The paper highlights reported improvements, such as a multi-task learning approach achieving a 40% average accuracy improvement on the FACTOID benchmark for LLMs, and methods like MARINE demonstrating effectiveness in reducing hallucinations and enhancing output detail in LVLMs, validated through assessments using GPT-4V \\cite{sahoo2024hcb}. It also notes varying performance of SOTA MLLMs on benchmarks like VHTest, with GPT-4V exhibiting lower hallucination than MiniGPT-v2 \\cite{sahoo2024hcb}.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations or Assumptions**: The survey identifies that hallucination mitigation efforts have predominantly relied on empirical methods, leaving uncertainty regarding the possibility of complete elimination \\cite{sahoo2024hcb}. It also points out challenges like the absence of a standardized metric for assessing object hallucination in LVLMs \\cite{sahoo2024hcb}.\n    *   **Scope of Applicability**: The survey comprehensively covers hallucination in Large Language, Image, Video, and Audio Foundation Models, providing a broad overview of the problem across these diverse modalities \\cite{sahoo2024hcb}.\n\n*   **Technical Significance**\n    *   **Advance the Technical State-of-the-Art**: This paper significantly advances the technical state-of-the-art by providing the first comprehensive, multimodal survey of hallucination in FMs, thereby filling a critical gap in the existing literature. It synthesizes diverse advancements, offering a unified perspective on a pervasive problem \\cite{sahoo2024hcb}.\n    *   **Potential Impact on Future Research**: By establishing a clear framework, taxonomy, and highlighting open challenges, the paper serves as a vital resource for researchers and practitioners. It is expected to aid in the development of more robust AI solutions and lay a foundational groundwork for future research and development in this pivotal area \\cite{sahoo2024hcb}.",
        "keywords": [
          "Hallucination",
          "Foundation Models",
          "Multimodal Hallucination",
          "Hallucination Detection",
          "Hallucination Mitigation",
          "Large Language Models (LLMs)",
          "Multimodal Perspective",
          "Taxonomy of Hallucination",
          "High-stakes AI applications",
          "Reliability and Accuracy",
          "Survey Paper",
          "Empirical Mitigation Methods",
          "Standardized Metrics",
          "Context Understanding"
        ],
        "paper_type": "**survey**"
      },
      "file_name": "c14010990c9d75a6e836e1c86d42f405a5d3d0a6.pdf"
    },
    {
      "success": true,
      "doc_id": "01e283fb37f537c414da93c7f34a9974",
      "summary": "Here's a focused summary of the paper for a literature review, emphasizing technical innovations and empirical validation:\n\n### Focused Summary for Literature Review: VL-Uncertainty\n\n1.  **Research Problem & Motivation**\n    *   Large Vision-Language Models (LVLMs) are prone to generating hallucinations with overconfidence, posing significant safety risks, especially given their higher information load compared to single-modal Large Language Models (LLMs).\n    *   Detecting these hallucinations is challenging because it requires a deep understanding of multiple modalities and verifying the authenticity of answers, which is costly in human and time expense.\n    *   Existing hallucination detection methods, which rely on ground-truth annotations or external \"teacher\" models, often fail in novel domains or when external knowledge is outdated, as they cannot intrinsically assess the model's confidence.\n\n2.  **Related Work & Positioning**\n    *   **Limitations of previous solutions**:\n        *   **External-model-based evaluation** (e.g., GAVIE, HaELM, CCEval): These methods depend on strong external LVLMs/LLMs as teachers or APIs to score responses, requiring pseudo annotations or external knowledge. This limits their applicability in unknown domains, introduces additional computational cost, and can be unreliable if the teacher model is outdated.\n        *   **Discrete rule-based checking** (e.g., CHAIR, POPE): These approaches are often restricted to specific object classes (e.g., COCO) and treat hallucination as a binary classification (Yes/No). This fails to capture the continuous spectrum and varying degrees of hallucination severity.\n        *   **General Uncertainty Learning**: Existing methods (single deterministic, Bayesian, ensemble) face challenges such as computational inefficiency, potential for overfitting, or difficulties in designing effective augmentations for meaningful uncertainty.\n    *   **Positioning**: `VL-Uncertainty` \\cite{zhang2024mmj} distinguishes itself as the first uncertainty-based framework for LVLM hallucination detection that is entirely self-contained, intrinsic, and free of external knowledge or annotations. It provides a continuous measure of uncertainty, directly addressing the limitations of discrete classification and reliance on external evaluators.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Method**: `VL-Uncertainty` \\cite{zhang2024mmj} quantifies the intrinsic uncertainty of an LVLM by analyzing the prediction variance across semantically equivalent but perturbed prompts. The core premise is that highly confident LVLMs provide consistent responses to semantically equivalent queries, whereas uncertain ones yield more random or divergent responses.\n    *   **Semantic-equivalent Perturbation**:\n        *   **Visual Prompts**: The original input image is perturbed multiple times by applying varying degrees of 2D Gaussian blurring. This technique is chosen because it preserves the full content, structure, spatial information, and relationships of the original image, mimicking human visual perception (e.g., varying distances or clarity).\n        *   **Textual Prompts**: A pre-trained text-only LLM is utilized to rephrase the original question. This rephrasing varies the wording, grammatical structure, and narrative style while strictly preserving the underlying semantics. The degree of textual perturbation is controlled by adjusting the LLM's temperature.\n        *   **Synchronization**: Visual and textual perturbations are synchronized according to their respective degrees (e.g., a larger blurring radius is paired with a higher LLM temperature for textual rephrasing).\n    *   **Uncertainty Estimation**:\n        *   The LVLM generates a set of answers in response to these perturbed prompt pairs.\n        *   A separate pre-trained LLM is then used to evaluate mutual semantic entailment between pairs of these generated answers, grouping them into distinct semantic clusters.\n        *   The `ULVLM` uncertainty is quantified as the entropy of this cluster distribution, effectively measuring semantic variance rather than mere lexical differences.\n\n4.  **Key Technical Contributions**\n    *   **Novel Uncertainty-Based Framework**: Introduction of `VL-Uncertainty` \\cite{zhang2024mmj}, the first framework to intrinsically detect LVLM hallucinations by estimating model uncertainty, moving beyond external evaluators or ground-truth reliance.\n    *   **Semantic-Equivalent Perturbation Strategy**: A novel and effective method for generating diverse yet semantically identical prompts across both visual (Gaussian blurring) and textual (LLM rephrasing with temperature control) modalities. This is critical for eliciting genuine intrinsic uncertainty.\n    *   **Continuous Uncertainty Metric**: Development of a continuous scalar metric for LVLM uncertainty based on the entropy of semantically clustered responses, which can indicate varying levels of hallucination, offering a more nuanced assessment than binary classification.\n    *   **Scalability and Robustness**: As an intrinsic metric, `VL-Uncertainty` \\cite{zhang2024mmj} is easily scalable to new domains and problems without requiring new ground-truth annotations or external models, enhancing its practical applicability.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**: Extensive experiments were performed to validate `VL-Uncertainty` \\cite{zhang2024mmj} on a wide range of 10 different Large Vision-Language Models (LVLMs).\n    *   **Task Coverage**: The evaluation spanned four diverse benchmarks, encompassing both free-form and multi-choice tasks, demonstrating the method's versatility and robustness across different LVLM application scenarios.\n    *   **Key Performance Metrics & Comparison Results**: `VL-Uncertainty` \\cite{zhang2024mmj} consistently and significantly outperformed strong baseline methods in LVLM hallucination detection. Further qualitative analysis provided additional evidence of its superiority in effectively capturing LVLM uncertainty.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations**: While `VL-Uncertainty` \\cite{zhang2024mmj} provides a continuous measure of uncertainty, existing hallucination benchmarks typically lack such continuous, fine-grained ground truth. For quantitative comparison with these benchmarks, a decision threshold must be established to convert the continuous uncertainty score into a binary (hallucinatory/non-hallucinatory) prediction.\n    *   **Scope of Applicability**: The method is specifically designed for LVLMs and relies on the ability to generate semantically equivalent perturbations for both visual and textual inputs. Its effectiveness is also dependent on the quality and capabilities of the LLM used for textual rephrasing and semantic clustering of responses.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: `VL-Uncertainty` \\cite{zhang2024mmj} significantly advances the technical state-of-the-art in LVLM hallucination detection by introducing an intrinsic, self-contained, and scalable framework. It shifts the paradigm from external validation to internal uncertainty quantification, offering a more robust and flexible solution.\n    *   **Potential Impact**: By enabling continuous and reliable detection of varying levels of hallucination, this work substantially enhances the safety and reliability of LVLM applications, particularly in safety-critical domains. It lays a foundation for future research into more nuanced and intrinsic evaluation of multi-modal model reliability and trustworthiness.",
      "intriguing_abstract": "Large Vision-Language Models (LVLMs) are transforming AI, yet their alarming propensity for generating overconfident hallucinations presents critical safety and reliability challenges. Existing detection methods, often dependent on costly ground-truth annotations or external \"teacher\" models, are inherently limited in novel domains and fail to intrinsically gauge model confidence. We introduce `VL-Uncertainty`, a pioneering, self-contained framework that quantifies the *intrinsic uncertainty* of LVLMs, entirely free from external knowledge or labels.\n\nOur novel approach leverages *semantic-equivalent perturbation* across both visual (via controlled Gaussian blurring) and textual (using LLM-based rephrasing) modalities to elicit genuine model uncertainty. By analyzing the *prediction variance* of responses to these subtly altered inputs and quantifying the *entropy* of *semantically clustered answers*, `VL-Uncertainty` provides a continuous, nuanced metric for hallucination severity. Extensive experiments across 10 diverse LVLMs and four benchmarks demonstrate its consistent and superior performance over existing baselines. This work significantly enhances the trustworthiness and safety of LVLM applications, paving the way for more robust and reliable multi-modal AI systems.",
      "keywords": [
        "Large Vision-Language Models (LVLMs)",
        "hallucination detection",
        "intrinsic uncertainty quantification",
        "VL-Uncertainty framework",
        "semantic-equivalent perturbation",
        "Gaussian blurring",
        "LLM rephrasing",
        "entropy of cluster distribution",
        "continuous uncertainty metric",
        "multi-modal model reliability",
        "safety-critical applications",
        "state-of-the-art advancement",
        "outperformed baseline methods"
      ],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/431a4e7e89863b038069335baa80c3e489538214.pdf",
      "citation_key": "zhang2024mmj",
      "metadata": {
        "title": "VL-Uncertainty: Detecting Hallucination in Large Vision-Language Model via Uncertainty Estimation",
        "authors": [
          "Ruiyang Zhang",
          "Hu Zhang",
          "Zhedong Zheng"
        ],
        "published_date": "2024",
        "abstract": "Given the higher information load processed by large vision-language models (LVLMs) compared to single-modal LLMs, detecting LVLM hallucinations requires more human and time expense, and thus rise a wider safety concerns. In this paper, we introduce VL-Uncertainty, the first uncertainty-based framework for detecting hallucinations in LVLMs. Different from most existing methods that require ground-truth or pseudo annotations, VL-Uncertainty utilizes uncertainty as an intrinsic metric. We measure uncertainty by analyzing the prediction variance across semantically equivalent but perturbed prompts, including visual and textual data. When LVLMs are highly confident, they provide consistent responses to semantically equivalent queries. However, when uncertain, the responses of the target LVLM become more random. Considering semantically similar answers with different wordings, we cluster LVLM responses based on their semantic content and then calculate the cluster distribution entropy as the uncertainty measure to detect hallucination. Our extensive experiments on 10 LVLMs across four benchmarks, covering both free-form and multi-choice tasks, show that VL-Uncertainty significantly outperforms strong baseline methods in hallucination detection.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/431a4e7e89863b038069335baa80c3e489538214.pdf",
        "venue": "arXiv.org",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the paper for a literature review, emphasizing technical innovations and empirical validation:\n\n### Focused Summary for Literature Review: VL-Uncertainty\n\n1.  **Research Problem & Motivation**\n    *   Large Vision-Language Models (LVLMs) are prone to generating hallucinations with overconfidence, posing significant safety risks, especially given their higher information load compared to single-modal Large Language Models (LLMs).\n    *   Detecting these hallucinations is challenging because it requires a deep understanding of multiple modalities and verifying the authenticity of answers, which is costly in human and time expense.\n    *   Existing hallucination detection methods, which rely on ground-truth annotations or external \"teacher\" models, often fail in novel domains or when external knowledge is outdated, as they cannot intrinsically assess the model's confidence.\n\n2.  **Related Work & Positioning**\n    *   **Limitations of previous solutions**:\n        *   **External-model-based evaluation** (e.g., GAVIE, HaELM, CCEval): These methods depend on strong external LVLMs/LLMs as teachers or APIs to score responses, requiring pseudo annotations or external knowledge. This limits their applicability in unknown domains, introduces additional computational cost, and can be unreliable if the teacher model is outdated.\n        *   **Discrete rule-based checking** (e.g., CHAIR, POPE): These approaches are often restricted to specific object classes (e.g., COCO) and treat hallucination as a binary classification (Yes/No). This fails to capture the continuous spectrum and varying degrees of hallucination severity.\n        *   **General Uncertainty Learning**: Existing methods (single deterministic, Bayesian, ensemble) face challenges such as computational inefficiency, potential for overfitting, or difficulties in designing effective augmentations for meaningful uncertainty.\n    *   **Positioning**: `VL-Uncertainty` \\cite{zhang2024mmj} distinguishes itself as the first uncertainty-based framework for LVLM hallucination detection that is entirely self-contained, intrinsic, and free of external knowledge or annotations. It provides a continuous measure of uncertainty, directly addressing the limitations of discrete classification and reliance on external evaluators.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Method**: `VL-Uncertainty` \\cite{zhang2024mmj} quantifies the intrinsic uncertainty of an LVLM by analyzing the prediction variance across semantically equivalent but perturbed prompts. The core premise is that highly confident LVLMs provide consistent responses to semantically equivalent queries, whereas uncertain ones yield more random or divergent responses.\n    *   **Semantic-equivalent Perturbation**:\n        *   **Visual Prompts**: The original input image is perturbed multiple times by applying varying degrees of 2D Gaussian blurring. This technique is chosen because it preserves the full content, structure, spatial information, and relationships of the original image, mimicking human visual perception (e.g., varying distances or clarity).\n        *   **Textual Prompts**: A pre-trained text-only LLM is utilized to rephrase the original question. This rephrasing varies the wording, grammatical structure, and narrative style while strictly preserving the underlying semantics. The degree of textual perturbation is controlled by adjusting the LLM's temperature.\n        *   **Synchronization**: Visual and textual perturbations are synchronized according to their respective degrees (e.g., a larger blurring radius is paired with a higher LLM temperature for textual rephrasing).\n    *   **Uncertainty Estimation**:\n        *   The LVLM generates a set of answers in response to these perturbed prompt pairs.\n        *   A separate pre-trained LLM is then used to evaluate mutual semantic entailment between pairs of these generated answers, grouping them into distinct semantic clusters.\n        *   The `ULVLM` uncertainty is quantified as the entropy of this cluster distribution, effectively measuring semantic variance rather than mere lexical differences.\n\n4.  **Key Technical Contributions**\n    *   **Novel Uncertainty-Based Framework**: Introduction of `VL-Uncertainty` \\cite{zhang2024mmj}, the first framework to intrinsically detect LVLM hallucinations by estimating model uncertainty, moving beyond external evaluators or ground-truth reliance.\n    *   **Semantic-Equivalent Perturbation Strategy**: A novel and effective method for generating diverse yet semantically identical prompts across both visual (Gaussian blurring) and textual (LLM rephrasing with temperature control) modalities. This is critical for eliciting genuine intrinsic uncertainty.\n    *   **Continuous Uncertainty Metric**: Development of a continuous scalar metric for LVLM uncertainty based on the entropy of semantically clustered responses, which can indicate varying levels of hallucination, offering a more nuanced assessment than binary classification.\n    *   **Scalability and Robustness**: As an intrinsic metric, `VL-Uncertainty` \\cite{zhang2024mmj} is easily scalable to new domains and problems without requiring new ground-truth annotations or external models, enhancing its practical applicability.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**: Extensive experiments were performed to validate `VL-Uncertainty` \\cite{zhang2024mmj} on a wide range of 10 different Large Vision-Language Models (LVLMs).\n    *   **Task Coverage**: The evaluation spanned four diverse benchmarks, encompassing both free-form and multi-choice tasks, demonstrating the method's versatility and robustness across different LVLM application scenarios.\n    *   **Key Performance Metrics & Comparison Results**: `VL-Uncertainty` \\cite{zhang2024mmj} consistently and significantly outperformed strong baseline methods in LVLM hallucination detection. Further qualitative analysis provided additional evidence of its superiority in effectively capturing LVLM uncertainty.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations**: While `VL-Uncertainty` \\cite{zhang2024mmj} provides a continuous measure of uncertainty, existing hallucination benchmarks typically lack such continuous, fine-grained ground truth. For quantitative comparison with these benchmarks, a decision threshold must be established to convert the continuous uncertainty score into a binary (hallucinatory/non-hallucinatory) prediction.\n    *   **Scope of Applicability**: The method is specifically designed for LVLMs and relies on the ability to generate semantically equivalent perturbations for both visual and textual inputs. Its effectiveness is also dependent on the quality and capabilities of the LLM used for textual rephrasing and semantic clustering of responses.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: `VL-Uncertainty` \\cite{zhang2024mmj} significantly advances the technical state-of-the-art in LVLM hallucination detection by introducing an intrinsic, self-contained, and scalable framework. It shifts the paradigm from external validation to internal uncertainty quantification, offering a more robust and flexible solution.\n    *   **Potential Impact**: By enabling continuous and reliable detection of varying levels of hallucination, this work substantially enhances the safety and reliability of LVLM applications, particularly in safety-critical domains. It lays a foundation for future research into more nuanced and intrinsic evaluation of multi-modal model reliability and trustworthiness.",
        "keywords": [
          "Large Vision-Language Models (LVLMs)",
          "hallucination detection",
          "intrinsic uncertainty quantification",
          "VL-Uncertainty framework",
          "semantic-equivalent perturbation",
          "Gaussian blurring",
          "LLM rephrasing",
          "entropy of cluster distribution",
          "continuous uncertainty metric",
          "multi-modal model reliability",
          "safety-critical applications",
          "state-of-the-art advancement",
          "outperformed baseline methods"
        ],
        "paper_type": "based on the abstract and introduction:\n\n*   the abstract explicitly states: \"we introduce vl-uncertainty, the first uncertainty-based framework for detecting hallucinations in lvlms.\" it then describes the *methodology*: \"we measure uncertainty by analyzing the prediction variance across semantically equivalent but perturbed prompts,\" \"cluster lvlm responses based on their semantic content and then calculate the cluster distribution entropy as the uncertainty measure.\"\n*   the introduction identifies a \"technical problem\" (hallucination in lvlms) and immediately points to their \"proposed solution\" (vl-uncertainty, as shown in figure 1 and discussed in the text).\n*   the abstract also mentions \"extensive experiments\" and \"significantly outperforms strong baseline methods,\" indicating empirical validation of the *proposed method*.\n\nthis strongly aligns with the criteria for a **technical** paper, which presents new methods, algorithms, or systems. the empirical evaluation is a crucial part of demonstrating the effectiveness of this new technical contribution.\n\n**classification: technical**"
      },
      "file_name": "431a4e7e89863b038069335baa80c3e489538214.pdf"
    },
    {
      "success": true,
      "doc_id": "dbabb0187ad702faed3a40dc0af20de8",
      "summary": "Here's a focused summary of the paper for a literature review, emphasizing technical innovations and empirical validation:\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Multimodal Large Language Models (MLLMs) frequently generate \"hallucinations\"â€”textual responses that are plausible but misaligned with the input image. This significantly compromises their reliability and applicability.\n    *   **Importance and Challenge**: Effective and efficient measurement of MLLM hallucination levels is crucial for diagnosis and mitigation. Existing benchmarks suffer from several limitations:\n        *   **Oversimplification**: Discriminative benchmarks use simple, short questions (e.g., \"Is there an {object}?\") that are not representative of real-world scenarios and fail to capture complex hallucination causes.\n        *   **Limited Variability**: Many benchmarks rely on off-the-shelf object annotations (e.g., COCO's 80 categories), leading to biased evaluations and limited scope of hallucination types.\n        *   **Computational Cost & Instability**: Generative benchmarks often employ LLM evaluators, which are computationally intensive, slow, and prone to instability due to their inherent randomness.\n        *   **Lack of Long-Context Evaluation**: Previous benchmarks largely ignore hallucinations in long, complex textual contexts, which are common in real-world MLLM applications like detailed descriptions or multi-round conversations.\n\n*   **Related Work & Positioning**\n    *   **Existing Approaches**: Previous work includes discriminative benchmarks (e.g., POPE, CIEM, AMBER) focusing on object existence/attributes with simple \"yes/no\" answers, and generative benchmarks (e.g., Hal-Eval, Kaul et al.) using LLM evaluators for free-form text analysis.\n    *   **Limitations of Previous Solutions**:\n        *   Most discriminative benchmarks use short, simple questions and binary answers, failing to capture the nuances and causes of hallucinations.\n        *   Generative benchmarks are slow and unstable due to reliance on LLM evaluators.\n        *   Both types often have limited scope regarding hallucination types and object variability.\n        *   Crucially, they lack evaluation for long-context hallucinations, which are prevalent in advanced MLLM applications.\n    *   **Positioning of LongHalQA**: \\cite{qiu2024zyc} addresses these limitations by introducing an LLM-free, long-context hallucination benchmark that unifies discriminative and generative evaluations into an efficient multiple-choice question (MCQ) format, covering a broader range of complex hallucination types and real-world scenarios.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method**: \\cite{qiu2024zyc} proposes **LongHalQA**, an LLM-free hallucination benchmark with 6K long and complex hallucination texts, and **LongHallGen**, an automated pipeline for generating such data.\n    *   **Novelty**:\n        *   **Unified MCQ Format**: Introduces two novel tasks, **Hallucination Discrimination** and **Hallucination Completion**, both framed as multiple-choice questions. This unifies discriminative and generative evaluations, making them more reliable and efficient by eliminating the need for costly and unstable LLM evaluators.\n        *   **Long-Context Data**: Features three types of long-context data: Object-level Description (avg. 14 words), Image-level Description (avg. 130 words), and Multi-round Conversation (avg. 189 words). This is significantly longer and more complex than existing benchmarks, better simulating real-world MLLM usage.\n        *   **Broad Hallucination Coverage**: Categorizes and annotates 12 distinct types of hallucinations, including complex ones involving logic and contextual consistency (e.g., \"four such plates\" vs. \"five plates\"), which are often overlooked.\n        *   **Automated Data Generation Pipeline (LongHallGen)**: Proposes a generic pipeline that leverages GPT-4V with specific prompt templates to automatically generate long-context hallucinatory data, perform hallucination checks (with optional human verification), create hallucination-explanation pairs, and construct MCQs. This greatly facilitates the creation and expansion of future benchmarks.\n        *   **LLM-Free Evaluation**: The MCQ format allows for direct, objective evaluation without relying on the subjective and computationally expensive outputs of LLM evaluators.\n\n*   **Key Technical Contributions**\n    *   **Novel Benchmark (LongHalQA)**: A new, large-scale (6485 MCQs) benchmark specifically designed for evaluating long-context hallucinations in MLLMs, featuring diverse data formats and hallucination types.\n    *   **Innovative Evaluation Tasks**: Introduction of Hallucination Discrimination and Hallucination Completion tasks, both in an MCQ format, enabling efficient and reliable assessment of MLLMs' ability to identify and avoid generating hallucinations in long texts.\n    *   **Automated Data Generation Pipeline (LongHallGen)**: A robust, automated pipeline for constructing long-context hallucination datasets, significantly reducing manual effort and enabling scalable benchmark creation.\n    *   **Empirical Insights**: Reveals new challenges for MLLMs in handling long-context hallucinations and identifies limitations of common mitigation techniques like Chain-Of-Thought (COT) in this context.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted**: \\cite{qiu2024zyc} evaluates ten mainstream MLLMs (e.g., LLaVA-1.5, InstructBLIP, Qwen-VL-Chat, CogVLM) on the LongHalQA benchmark.\n    *   **Key Performance Metrics**:\n        *   For Hallucination Discrimination: Accuracy, Precision, and \"Yes\" ratios (for binary settings), and (mc-)accuracy for multiple-choice settings.\n        *   For Hallucination Completion: (mc-)accuracy.\n    *   **Comparison Results**:\n        *   **MLLMs struggle with long-context hallucinations**: Evaluations reveal significant constraints of MLLMs in discerning, explaining, and avoiding generating hallucinations within long and complex texts.\n        *   **COT limitations**: Chain-Of-Thought (COT), a common hallucination mitigation method, is found to be effective for short queries and generative hallucinations but *degrades* performance for most MLLMs on long-context hallucination discrimination in LongHalQA, especially for smaller models. This suggests that COT's effectiveness is limited by MLLMs' long-context processing capabilities.\n        *   **MCQ vs. Free-form**: Experiments demonstrate that the MCQ hallucination completion task exhibits similar trends to free-form generative evaluation, while offering significantly higher evaluation speed, especially for large models.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations**: The paper highlights that even state-of-the-art MLLMs like GPT-4V, used for data generation, suffer from severe hallucinations when generating long-context data (e.g., 78.8% of image-level descriptions and 82.4% of conversations contained at least one hallucination). This necessitates a comprehensive hallucination check process within LongHallGen.\n    *   **Scope of Applicability**: LongHalQA focuses on long-context textual hallucinations related to object/image descriptions and multi-round conversations. While comprehensive, it is tailored to these specific interaction patterns. The LongHallGen pipeline is generic and can be adapted for other hallucination types or domains by modifying prompt sets.\n\n*   **Technical Significance**\n    *   **Advances State-of-the-Art**: \\cite{qiu2024zyc} significantly advances the evaluation of MLLM hallucinations by moving beyond simple, short queries to realistic, long-context scenarios. It provides a more comprehensive and efficient evaluation framework.\n    *   **Enables Deeper Analysis**: The detailed annotation of 12 hallucination types, including complex logical and contextual inconsistencies, allows for more in-depth studies of MLLM failure modes.\n    *   **Facilitates Future Research**: LongHallGen provides a powerful, automated tool for researchers to create new hallucination benchmarks or expand existing ones, accelerating the development of more robust MLLMs.\n    *   **Impact on Mitigation Strategies**: The findings regarding COT's degradation in long-context discrimination highlight the need for new or improved hallucination mitigation methods specifically designed for complex, extended interactions. LongHalQA serves as a crucial basis for developing and testing such methods.",
      "intriguing_abstract": "The pervasive issue of hallucinations in Multimodal Large Language Models (MLLMs) severely undermines their reliability, particularly in complex, real-world applications involving extended interactions. Current benchmarks, often simplistic and computationally expensive, fail to capture these nuanced failures. We introduce **LongHalQA**, a novel, LLM-free benchmark designed to rigorously evaluate MLLM hallucinations within long, intricate textual contexts. LongHalQA unifies discriminative and generative evaluation into an efficient multiple-choice question (MCQ) format, encompassing 12 distinct hallucination types across object descriptions, image-level narratives, and multi-round conversations. Complementing this, **LongHallGen** provides an automated pipeline for scalable, high-quality data generation. Our comprehensive evaluation of ten mainstream MLLMs reveals their profound struggle with long-context hallucinations. Crucially, we demonstrate that Chain-Of-Thought (COT), a common mitigation strategy, paradoxically *degrades* performance for long-context hallucination discrimination in many models, challenging prevailing assumptions. LongHalQA offers an indispensable tool for diagnosing MLLM limitations, fostering the development of more robust models, and guiding future research towards truly reliable multimodal AI.",
      "keywords": [
        "Multimodal Large Language Models (MLLMs)",
        "MLLM hallucinations",
        "long-context evaluation",
        "LongHalQA benchmark",
        "LongHallGen pipeline",
        "automated data generation",
        "LLM-free evaluation",
        "multiple-choice question (MCQ) format",
        "unified discriminative and generative evaluation",
        "complex hallucination types",
        "Chain-Of-Thought (COT) limitations",
        "MLLM reliability"
      ],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/9e2037d7d2f8222a7be86d2471eda895c8040ff5.pdf",
      "citation_key": "qiu2024zyc",
      "metadata": {
        "title": "LongHalQA: Long-Context Hallucination Evaluation for MultiModal Large Language Models",
        "authors": [
          "Han Qiu",
          "Jiaxing Huang",
          "Peng Gao",
          "Qi Qin",
          "Xiaoqin Zhang",
          "Ling Shao",
          "Shijian Lu"
        ],
        "published_date": "2024",
        "abstract": "Hallucination, a phenomenon where multimodal large language models~(MLLMs) tend to generate textual responses that are plausible but unaligned with the image, has become one major hurdle in various MLLM-related applications. Several benchmarks have been created to gauge the hallucination levels of MLLMs, by either raising discriminative questions about the existence of objects or introducing LLM evaluators to score the generated text from MLLMs. However, the discriminative data largely involve simple questions that are not aligned with real-world text, while the generative data involve LLM evaluators that are computationally intensive and unstable due to their inherent randomness. We propose LongHalQA, an LLM-free hallucination benchmark that comprises 6K long and complex hallucination text. LongHalQA is featured by GPT4V-generated hallucinatory data that are well aligned with real-world scenarios, including object/image descriptions and multi-round conversations with 14/130 words and 189 words, respectively, on average. It introduces two new tasks, hallucination discrimination and hallucination completion, unifying both discriminative and generative evaluations in a single multiple-choice-question form and leading to more reliable and efficient evaluations without the need for LLM evaluators. Further, we propose an advanced pipeline that greatly facilitates the construction of future hallucination benchmarks with long and complex questions and descriptions. Extensive experiments over multiple recent MLLMs reveal various new challenges when they are handling hallucinations with long and complex textual data. Dataset and evaluation code are available at https://github.com/hanqiu-hq/LongHalQA.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/9e2037d7d2f8222a7be86d2471eda895c8040ff5.pdf",
        "venue": "arXiv.org",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the paper for a literature review, emphasizing technical innovations and empirical validation:\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Multimodal Large Language Models (MLLMs) frequently generate \"hallucinations\"â€”textual responses that are plausible but misaligned with the input image. This significantly compromises their reliability and applicability.\n    *   **Importance and Challenge**: Effective and efficient measurement of MLLM hallucination levels is crucial for diagnosis and mitigation. Existing benchmarks suffer from several limitations:\n        *   **Oversimplification**: Discriminative benchmarks use simple, short questions (e.g., \"Is there an {object}?\") that are not representative of real-world scenarios and fail to capture complex hallucination causes.\n        *   **Limited Variability**: Many benchmarks rely on off-the-shelf object annotations (e.g., COCO's 80 categories), leading to biased evaluations and limited scope of hallucination types.\n        *   **Computational Cost & Instability**: Generative benchmarks often employ LLM evaluators, which are computationally intensive, slow, and prone to instability due to their inherent randomness.\n        *   **Lack of Long-Context Evaluation**: Previous benchmarks largely ignore hallucinations in long, complex textual contexts, which are common in real-world MLLM applications like detailed descriptions or multi-round conversations.\n\n*   **Related Work & Positioning**\n    *   **Existing Approaches**: Previous work includes discriminative benchmarks (e.g., POPE, CIEM, AMBER) focusing on object existence/attributes with simple \"yes/no\" answers, and generative benchmarks (e.g., Hal-Eval, Kaul et al.) using LLM evaluators for free-form text analysis.\n    *   **Limitations of Previous Solutions**:\n        *   Most discriminative benchmarks use short, simple questions and binary answers, failing to capture the nuances and causes of hallucinations.\n        *   Generative benchmarks are slow and unstable due to reliance on LLM evaluators.\n        *   Both types often have limited scope regarding hallucination types and object variability.\n        *   Crucially, they lack evaluation for long-context hallucinations, which are prevalent in advanced MLLM applications.\n    *   **Positioning of LongHalQA**: \\cite{qiu2024zyc} addresses these limitations by introducing an LLM-free, long-context hallucination benchmark that unifies discriminative and generative evaluations into an efficient multiple-choice question (MCQ) format, covering a broader range of complex hallucination types and real-world scenarios.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method**: \\cite{qiu2024zyc} proposes **LongHalQA**, an LLM-free hallucination benchmark with 6K long and complex hallucination texts, and **LongHallGen**, an automated pipeline for generating such data.\n    *   **Novelty**:\n        *   **Unified MCQ Format**: Introduces two novel tasks, **Hallucination Discrimination** and **Hallucination Completion**, both framed as multiple-choice questions. This unifies discriminative and generative evaluations, making them more reliable and efficient by eliminating the need for costly and unstable LLM evaluators.\n        *   **Long-Context Data**: Features three types of long-context data: Object-level Description (avg. 14 words), Image-level Description (avg. 130 words), and Multi-round Conversation (avg. 189 words). This is significantly longer and more complex than existing benchmarks, better simulating real-world MLLM usage.\n        *   **Broad Hallucination Coverage**: Categorizes and annotates 12 distinct types of hallucinations, including complex ones involving logic and contextual consistency (e.g., \"four such plates\" vs. \"five plates\"), which are often overlooked.\n        *   **Automated Data Generation Pipeline (LongHallGen)**: Proposes a generic pipeline that leverages GPT-4V with specific prompt templates to automatically generate long-context hallucinatory data, perform hallucination checks (with optional human verification), create hallucination-explanation pairs, and construct MCQs. This greatly facilitates the creation and expansion of future benchmarks.\n        *   **LLM-Free Evaluation**: The MCQ format allows for direct, objective evaluation without relying on the subjective and computationally expensive outputs of LLM evaluators.\n\n*   **Key Technical Contributions**\n    *   **Novel Benchmark (LongHalQA)**: A new, large-scale (6485 MCQs) benchmark specifically designed for evaluating long-context hallucinations in MLLMs, featuring diverse data formats and hallucination types.\n    *   **Innovative Evaluation Tasks**: Introduction of Hallucination Discrimination and Hallucination Completion tasks, both in an MCQ format, enabling efficient and reliable assessment of MLLMs' ability to identify and avoid generating hallucinations in long texts.\n    *   **Automated Data Generation Pipeline (LongHallGen)**: A robust, automated pipeline for constructing long-context hallucination datasets, significantly reducing manual effort and enabling scalable benchmark creation.\n    *   **Empirical Insights**: Reveals new challenges for MLLMs in handling long-context hallucinations and identifies limitations of common mitigation techniques like Chain-Of-Thought (COT) in this context.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted**: \\cite{qiu2024zyc} evaluates ten mainstream MLLMs (e.g., LLaVA-1.5, InstructBLIP, Qwen-VL-Chat, CogVLM) on the LongHalQA benchmark.\n    *   **Key Performance Metrics**:\n        *   For Hallucination Discrimination: Accuracy, Precision, and \"Yes\" ratios (for binary settings), and (mc-)accuracy for multiple-choice settings.\n        *   For Hallucination Completion: (mc-)accuracy.\n    *   **Comparison Results**:\n        *   **MLLMs struggle with long-context hallucinations**: Evaluations reveal significant constraints of MLLMs in discerning, explaining, and avoiding generating hallucinations within long and complex texts.\n        *   **COT limitations**: Chain-Of-Thought (COT), a common hallucination mitigation method, is found to be effective for short queries and generative hallucinations but *degrades* performance for most MLLMs on long-context hallucination discrimination in LongHalQA, especially for smaller models. This suggests that COT's effectiveness is limited by MLLMs' long-context processing capabilities.\n        *   **MCQ vs. Free-form**: Experiments demonstrate that the MCQ hallucination completion task exhibits similar trends to free-form generative evaluation, while offering significantly higher evaluation speed, especially for large models.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations**: The paper highlights that even state-of-the-art MLLMs like GPT-4V, used for data generation, suffer from severe hallucinations when generating long-context data (e.g., 78.8% of image-level descriptions and 82.4% of conversations contained at least one hallucination). This necessitates a comprehensive hallucination check process within LongHallGen.\n    *   **Scope of Applicability**: LongHalQA focuses on long-context textual hallucinations related to object/image descriptions and multi-round conversations. While comprehensive, it is tailored to these specific interaction patterns. The LongHallGen pipeline is generic and can be adapted for other hallucination types or domains by modifying prompt sets.\n\n*   **Technical Significance**\n    *   **Advances State-of-the-Art**: \\cite{qiu2024zyc} significantly advances the evaluation of MLLM hallucinations by moving beyond simple, short queries to realistic, long-context scenarios. It provides a more comprehensive and efficient evaluation framework.\n    *   **Enables Deeper Analysis**: The detailed annotation of 12 hallucination types, including complex logical and contextual inconsistencies, allows for more in-depth studies of MLLM failure modes.\n    *   **Facilitates Future Research**: LongHallGen provides a powerful, automated tool for researchers to create new hallucination benchmarks or expand existing ones, accelerating the development of more robust MLLMs.\n    *   **Impact on Mitigation Strategies**: The findings regarding COT's degradation in long-context discrimination highlight the need for new or improved hallucination mitigation methods specifically designed for complex, extended interactions. LongHalQA serves as a crucial basis for developing and testing such methods.",
        "keywords": [
          "Multimodal Large Language Models (MLLMs)",
          "MLLM hallucinations",
          "long-context evaluation",
          "LongHalQA benchmark",
          "LongHallGen pipeline",
          "automated data generation",
          "LLM-free evaluation",
          "multiple-choice question (MCQ) format",
          "unified discriminative and generative evaluation",
          "complex hallucination types",
          "Chain-Of-Thought (COT) limitations",
          "MLLM reliability"
        ],
        "paper_type": "based on the abstract and introduction:\n\n*   the abstract explicitly states: \"we **propose longhalqa**, an llm-free hallucination benchmark...\" and \"it **introduces two new tasks**, hallucination discrimination and hallucination completion...\"\n*   the introduction reiterates: \"we **propose longhalqa**...\" and \"it **introduces two new tasks**...\" and \"further, we **propose an advanced pipeline** that greatly facilitates the construction of future hallucination benchmarks...\"\n*   the paper describes the features of this new benchmark and pipeline.\n*   it mentions \"extensive experiments over multiple recent mllms reveal various new challenges,\" indicating the evaluation of the proposed system.\n\nthese phrases directly align with the criteria for a **technical** paper, which \"presents new methods, algorithms, or systems.\" the paper is proposing a new benchmark, new tasks, and a new pipeline.\n\n**classification: technical**"
      },
      "file_name": "9e2037d7d2f8222a7be86d2471eda895c8040ff5.pdf"
    },
    {
      "success": true,
      "doc_id": "fc9c2f9d9a996e7584d9a4786ea6a0ec",
      "summary": "Here's a focused summary of the paper for a literature review:\n\n*   **1. Research Problem & Motivation**\n    *   **Specific Technical Problem**: Evaluating factual hallucination in Large Language Models (LLMs) is challenging, particularly for complex questions and verifying the LLM's thought process. Existing benchmarks are either manually constructed (expensive, not scalable) or automatic but generate simplistic questions, lacking the ability to evaluate intricate reasoning or automatically verify rationales \\cite{oh2024xa3}.\n    *   **Importance and Challenge**: Hallucination is a severe issue for knowledge-related and safety-critical LLM applications. Developing comprehensive, intricate, automatically verifiable, and scalable benchmarks is crucial but difficult due to the complexity of LLM reasoning and the need for robust verification mechanisms \\cite{oh2024xa3}.\n\n*   **2. Related Work & Positioning**\n    *   **Relation to Existing Approaches**: This work positions itself against manual benchmarks (e.g., human-annotated QA datasets) and automatic benchmarks based on knowledge graphs (KGs) \\cite{oh2024xa3}.\n    *   **Limitations of Previous Solutions**:\n        *   **Manual Benchmarks**: Expensive and not scalable \\cite{oh2024xa3}.\n        *   **Knowledge Graph-based Benchmarks**: While scalable and automatically verifiable, questions tend to be simplistic (based on triples) and unmodifiable, thus lacking the ability to evaluate intricate tasks or complex reasoning \\cite{oh2024xa3}. They are also often not designed to evaluate the correctness of an LLM's rationale, only its final answer \\cite{oh2024xa3}.\n\n*   **3. Technical Approach & Innovation**\n    *   **Core Technical Method**: ERBench proposes using existing relational databases (RDBs) based on the Entity-Relationship (ER) model to construct LLM benchmarks. It leverages the database's schema, records, and integrity constraints to generate complex, automatically verifiable questions and rationales \\cite{oh2024xa3}.\n    *   **Novelty/Difference**:\n        *   **Utilizing Integrity Constraints**: ERBench systematically uses Functional Dependencies (FDs) to pinpoint critical keywords for rationale verification and Foreign Key Constraints (FKCs) to construct arbitrarily long multi-hop questions by joining relations \\cite{oh2024xa3}. This allows for evaluating both answer correctness and the underlying thought process (rationale) \\cite{oh2024xa3}.\n        *   **Automatic Verifiability of Rationales**: Unlike prior work, ERBench explicitly checks if the LLM's rationale contains the FD-inferred values, providing a deeper evaluation of factual hallucination \\cite{oh2024xa3}.\n        *   **Extensibility**: Supports continuous evaluation as databases change, multimodal questions (replacing text attributes with images), and various prompt engineering techniques (e.g., Chain-of-Thought, Few-shot, Knowledge Augmentation) \\cite{oh2024xa3}.\n\n*   **4. Key Technical Contributions**\n    *   **Novel Algorithms/Methods**:\n        *   A systematic framework for converting any relational database into an LLM benchmark using its schema, records, and integrity constraints (FDs and FKCs) \\cite{oh2024xa3}.\n        *   Methods for constructing binary and multiple-choice questions based on FDs, and multi-hop questions based on FKCs for increased complexity \\cite{oh2024xa3}.\n        *   An automatic verification mechanism that checks both the correctness of the LLM's answer and the presence of FD-inferred critical keywords in its rationale \\cite{oh2024xa3}.\n    *   **System Design/Architectural Innovations**: ERBench provides a flexible architecture that supports continuous evaluation, multimodal data integration, and compatibility with various prompt engineering strategies \\cite{oh2024xa3}.\n    *   **Theoretical Insights/Analysis**: Demonstrates how the inherent structure and integrity constraints of relational databases (ER model) can be effectively repurposed for robust and fine-grained LLM evaluation, particularly for factual consistency and reasoning verification \\cite{oh2024xa3}.\n\n*   **5. Experimental Validation**\n    *   **Experiments Conducted**:\n        *   Constructed LLM benchmarks using 5 public databases (Movie, Soccer, Airport, Music, Book) from different domains \\cite{oh2024xa3}.\n        *   Evaluated 6 contemporary LLMs (GPT-3.5, GPT-4, Llama2-70B-Chat, Gemini-Pro, Claude-3-Sonnet, Mistral-7B-Instruct) and 2 multimodal LLMs (GPT-4V, Gemini-Pro-Vision) \\cite{oh2024xa3}.\n        *   Tested single-hop, multi-hop, and multimodal questions \\cite{oh2024xa3}.\n        *   Explored various prompt engineering techniques (Chain-of-Thought, Few-shot, Knowledge Augmentation) and fine-tuning \\cite{oh2024xa3}.\n        *   Evaluated LLM performance based on their internal knowledge of entities \\cite{oh2024xa3}.\n    *   **Key Performance Metrics and Comparison Results**:\n        *   **Metrics**: Answer Accuracy (A), Rationale Accuracy (R), Answer-Rationale Accuracy (AR), and Hallucination Rate (H) \\cite{oh2024xa3}. AR and R are novel metrics introduced to specifically evaluate rationale correctness.\n        *   **Results**: Showed that ERBench effectively evaluates LLMs by checking both answer correctness and verifying rationales by looking for critical keywords. The experiments provide comprehensive analyses of LLM performance across different question types and prompt engineering strategies \\cite{oh2024xa3}.\n\n*   **6. Limitations & Scope**\n    *   **Technical Limitations/Assumptions**:\n        *   **Entity Resolution**: Acknowledges potential challenges in entity resolution where LLMs might mention semantically equivalent but syntactically different entities. Heuristics like string matching are used, but using an LLM for matching is considered potentially unfair \\cite{oh2024xa3}.\n        *   **Integrity Constraint Correctness**: Assumes that the integrity constraints (FDs, FKCs) are correctly determined and maintained by the database owner \\cite{oh2024xa3}.\n    *   **Scope of Applicability**: Primarily focuses on evaluating factual hallucination in LLMs using structured knowledge from relational databases. While extensible to multimodal data, the core mechanism relies on the ER model's structured nature \\cite{oh2024xa3}.\n\n*   **7. Technical Significance**\n    *   **Advancement of State-of-the-Art**: ERBench significantly advances LLM evaluation by providing the first benchmark that systematically utilizes relational databases to construct complex, automatically verifiable questions, including the verification of LLM rationales \\cite{oh2024xa3}. This moves beyond simple answer correctness to assess deeper reasoning and factual consistency \\cite{oh2024xa3}.\n    *   **Potential Impact on Future Research**:\n        *   Enables more rigorous and fine-grained evaluation of LLM factual knowledge and reasoning capabilities.\n        *   Facilitates continuous evaluation of LLMs as underlying data evolves.\n        *   Opens avenues for developing LLMs that are not only accurate in their answers but also transparent and factually grounded in their reasoning.\n        *   Provides a scalable and adaptable framework for creating benchmarks across diverse domains, fostering more robust LLM development \\cite{oh2024xa3}.",
      "intriguing_abstract": "The pervasive challenge of factual hallucination in Large Language Models (LLMs) critically undermines their reliability, particularly for complex reasoning tasks where verifying their underlying thought process remains elusive. Current evaluation benchmarks fall short, either lacking scalability or failing to assess intricate reasoning and automatically verify rationales. We introduce ERBench, a novel, scalable framework that revolutionizes LLM evaluation by systematically transforming existing **relational databases (RDBs)**, based on the **Entity-Relationship (ER) model**, into comprehensive benchmarks.\n\nERBench uniquely leverages the inherent structure and **integrity constraints** of RDBs, specifically **Functional Dependencies (FDs)** and **Foreign Key Constraints (FKCs)**, to automatically generate complex, multi-hop questions. Crucially, it pioneers the **automatic verification of LLM rationales**, explicitly checking for FD-inferred critical keywords within the model's explanation, moving beyond mere answer correctness. Our experiments across diverse LLMs and databases demonstrate ERBench's efficacy in providing fine-grained insights into factual consistency and reasoning capabilities, offering novel metrics like Rationale Accuracy. This framework paves the way for developing more transparent, factually grounded, and trustworthy LLMs, enabling continuous, rigorous evaluation essential for real-world applications.",
      "keywords": [
        "Factual hallucination",
        "Large Language Models (LLMs)",
        "ERBench",
        "Relational databases (RDBs)",
        "Integrity constraints",
        "Automatic rationale verification",
        "Complex/multi-hop questions",
        "LLM benchmark construction",
        "Answer-Rationale Accuracy",
        "Factual consistency",
        "Reasoning verification",
        "Scalable evaluation",
        "Prompt engineering",
        "Multimodal questions"
      ],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/b877f5076c617a948081e12e08809e6c6b84b468.pdf",
      "citation_key": "oh2024xa3",
      "metadata": {
        "title": "ERBench: An Entity-Relationship based Automatically Verifiable Hallucination Benchmark for Large Language Models",
        "authors": [
          "Jio Oh",
          "Soyeon Kim",
          "Junseok Seo",
          "Jindong Wang",
          "Ruochen Xu",
          "Xing Xie",
          "Steven Euijong Whang"
        ],
        "published_date": "2024",
        "abstract": "Large language models (LLMs) have achieved unprecedented performances in various applications, yet evaluating them is still challenging. Existing benchmarks are either manually constructed or are automatic, but lack the ability to evaluate the thought process of LLMs with arbitrary complexity. We contend that utilizing existing relational databases based on the entity-relationship (ER) model is a promising approach for constructing benchmarks as they contain structured knowledge that can be used to question LLMs. Unlike knowledge graphs, which are also used to evaluate LLMs, relational databases have integrity constraints that can be used to better construct complex in-depth questions and verify answers: (1) functional dependencies can be used to pinpoint critical keywords that an LLM must know to properly answer a given question containing certain attribute values; and (2) foreign key constraints can be used to join relations and construct multi-hop questions, which can be arbitrarily long and used to debug intermediate answers. We thus propose ERBench, which uses these integrity constraints to convert any database into an LLM benchmark. ERBench supports continuous evaluation as databases change, multimodal questions, and various prompt engineering techniques. In our experiments, we construct LLM benchmarks using databases of multiple domains and make an extensive comparison of contemporary LLMs. We show how ERBench can properly evaluate any LLM by not only checking for answer correctness, but also effectively verifying the rationales by looking for the right keywords.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/b877f5076c617a948081e12e08809e6c6b84b468.pdf",
        "venue": "Neural Information Processing Systems",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the paper for a literature review:\n\n*   **1. Research Problem & Motivation**\n    *   **Specific Technical Problem**: Evaluating factual hallucination in Large Language Models (LLMs) is challenging, particularly for complex questions and verifying the LLM's thought process. Existing benchmarks are either manually constructed (expensive, not scalable) or automatic but generate simplistic questions, lacking the ability to evaluate intricate reasoning or automatically verify rationales \\cite{oh2024xa3}.\n    *   **Importance and Challenge**: Hallucination is a severe issue for knowledge-related and safety-critical LLM applications. Developing comprehensive, intricate, automatically verifiable, and scalable benchmarks is crucial but difficult due to the complexity of LLM reasoning and the need for robust verification mechanisms \\cite{oh2024xa3}.\n\n*   **2. Related Work & Positioning**\n    *   **Relation to Existing Approaches**: This work positions itself against manual benchmarks (e.g., human-annotated QA datasets) and automatic benchmarks based on knowledge graphs (KGs) \\cite{oh2024xa3}.\n    *   **Limitations of Previous Solutions**:\n        *   **Manual Benchmarks**: Expensive and not scalable \\cite{oh2024xa3}.\n        *   **Knowledge Graph-based Benchmarks**: While scalable and automatically verifiable, questions tend to be simplistic (based on triples) and unmodifiable, thus lacking the ability to evaluate intricate tasks or complex reasoning \\cite{oh2024xa3}. They are also often not designed to evaluate the correctness of an LLM's rationale, only its final answer \\cite{oh2024xa3}.\n\n*   **3. Technical Approach & Innovation**\n    *   **Core Technical Method**: ERBench proposes using existing relational databases (RDBs) based on the Entity-Relationship (ER) model to construct LLM benchmarks. It leverages the database's schema, records, and integrity constraints to generate complex, automatically verifiable questions and rationales \\cite{oh2024xa3}.\n    *   **Novelty/Difference**:\n        *   **Utilizing Integrity Constraints**: ERBench systematically uses Functional Dependencies (FDs) to pinpoint critical keywords for rationale verification and Foreign Key Constraints (FKCs) to construct arbitrarily long multi-hop questions by joining relations \\cite{oh2024xa3}. This allows for evaluating both answer correctness and the underlying thought process (rationale) \\cite{oh2024xa3}.\n        *   **Automatic Verifiability of Rationales**: Unlike prior work, ERBench explicitly checks if the LLM's rationale contains the FD-inferred values, providing a deeper evaluation of factual hallucination \\cite{oh2024xa3}.\n        *   **Extensibility**: Supports continuous evaluation as databases change, multimodal questions (replacing text attributes with images), and various prompt engineering techniques (e.g., Chain-of-Thought, Few-shot, Knowledge Augmentation) \\cite{oh2024xa3}.\n\n*   **4. Key Technical Contributions**\n    *   **Novel Algorithms/Methods**:\n        *   A systematic framework for converting any relational database into an LLM benchmark using its schema, records, and integrity constraints (FDs and FKCs) \\cite{oh2024xa3}.\n        *   Methods for constructing binary and multiple-choice questions based on FDs, and multi-hop questions based on FKCs for increased complexity \\cite{oh2024xa3}.\n        *   An automatic verification mechanism that checks both the correctness of the LLM's answer and the presence of FD-inferred critical keywords in its rationale \\cite{oh2024xa3}.\n    *   **System Design/Architectural Innovations**: ERBench provides a flexible architecture that supports continuous evaluation, multimodal data integration, and compatibility with various prompt engineering strategies \\cite{oh2024xa3}.\n    *   **Theoretical Insights/Analysis**: Demonstrates how the inherent structure and integrity constraints of relational databases (ER model) can be effectively repurposed for robust and fine-grained LLM evaluation, particularly for factual consistency and reasoning verification \\cite{oh2024xa3}.\n\n*   **5. Experimental Validation**\n    *   **Experiments Conducted**:\n        *   Constructed LLM benchmarks using 5 public databases (Movie, Soccer, Airport, Music, Book) from different domains \\cite{oh2024xa3}.\n        *   Evaluated 6 contemporary LLMs (GPT-3.5, GPT-4, Llama2-70B-Chat, Gemini-Pro, Claude-3-Sonnet, Mistral-7B-Instruct) and 2 multimodal LLMs (GPT-4V, Gemini-Pro-Vision) \\cite{oh2024xa3}.\n        *   Tested single-hop, multi-hop, and multimodal questions \\cite{oh2024xa3}.\n        *   Explored various prompt engineering techniques (Chain-of-Thought, Few-shot, Knowledge Augmentation) and fine-tuning \\cite{oh2024xa3}.\n        *   Evaluated LLM performance based on their internal knowledge of entities \\cite{oh2024xa3}.\n    *   **Key Performance Metrics and Comparison Results**:\n        *   **Metrics**: Answer Accuracy (A), Rationale Accuracy (R), Answer-Rationale Accuracy (AR), and Hallucination Rate (H) \\cite{oh2024xa3}. AR and R are novel metrics introduced to specifically evaluate rationale correctness.\n        *   **Results**: Showed that ERBench effectively evaluates LLMs by checking both answer correctness and verifying rationales by looking for critical keywords. The experiments provide comprehensive analyses of LLM performance across different question types and prompt engineering strategies \\cite{oh2024xa3}.\n\n*   **6. Limitations & Scope**\n    *   **Technical Limitations/Assumptions**:\n        *   **Entity Resolution**: Acknowledges potential challenges in entity resolution where LLMs might mention semantically equivalent but syntactically different entities. Heuristics like string matching are used, but using an LLM for matching is considered potentially unfair \\cite{oh2024xa3}.\n        *   **Integrity Constraint Correctness**: Assumes that the integrity constraints (FDs, FKCs) are correctly determined and maintained by the database owner \\cite{oh2024xa3}.\n    *   **Scope of Applicability**: Primarily focuses on evaluating factual hallucination in LLMs using structured knowledge from relational databases. While extensible to multimodal data, the core mechanism relies on the ER model's structured nature \\cite{oh2024xa3}.\n\n*   **7. Technical Significance**\n    *   **Advancement of State-of-the-Art**: ERBench significantly advances LLM evaluation by providing the first benchmark that systematically utilizes relational databases to construct complex, automatically verifiable questions, including the verification of LLM rationales \\cite{oh2024xa3}. This moves beyond simple answer correctness to assess deeper reasoning and factual consistency \\cite{oh2024xa3}.\n    *   **Potential Impact on Future Research**:\n        *   Enables more rigorous and fine-grained evaluation of LLM factual knowledge and reasoning capabilities.\n        *   Facilitates continuous evaluation of LLMs as underlying data evolves.\n        *   Opens avenues for developing LLMs that are not only accurate in their answers but also transparent and factually grounded in their reasoning.\n        *   Provides a scalable and adaptable framework for creating benchmarks across diverse domains, fostering more robust LLM development \\cite{oh2024xa3}.",
        "keywords": [
          "Factual hallucination",
          "Large Language Models (LLMs)",
          "ERBench",
          "Relational databases (RDBs)",
          "Integrity constraints",
          "Automatic rationale verification",
          "Complex/multi-hop questions",
          "LLM benchmark construction",
          "Answer-Rationale Accuracy",
          "Factual consistency",
          "Reasoning verification",
          "Scalable evaluation",
          "Prompt engineering",
          "Multimodal questions"
        ],
        "paper_type": "based on the abstract and introduction:\n\n*   the abstract explicitly states: \"we thus **propose erbench**,\" and describes how it \"uses these integrity constraints to convert any database into an llm benchmark.\" it details the mechanisms (\"functional dependencies,\" \"foreign key constraints\") and features of this proposed system.\n*   the introduction identifies a \"fundamental issue of llms is hallucination\" and the need to \"develop benchmarks that are comprehensive, intricate, automatically verifiable, and can be scaled efficiently.\" this sets up the problem that the proposed system aims to solve.\n*   the mention of \"experiments\" and \"extensive comparison of contemporary llms\" in the abstract indicates an empirical evaluation, but this evaluation is performed *on the proposed erbench system* to demonstrate its effectiveness, making the primary contribution the system itself.\n\nthis aligns best with the **technical** classification, as it presents a new system/method (erbench) to address a technical problem.\n\n**classification: technical**"
      },
      "file_name": "b877f5076c617a948081e12e08809e6c6b84b468.pdf"
    },
    {
      "success": true,
      "doc_id": "71ff530c1c2fbb16d8cc78760bcf61d8",
      "summary": "Here's a focused summary of the paper for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem:** Large Language Models (LLMs) frequently generate inaccurate or fabricated information, a phenomenon known as \"hallucination\" \\cite{zhang202396g}. This primarily focuses on *fact-conflicting hallucination*.\n    *   **Importance & Challenge:** Hallucinations hinder the practical application of LLMs in real-world scenarios. Mitigating them is challenging because:\n        *   The maximum-likelihood-based pre-training objective can assign probabilities to non-factual information or over-rely on superficial patterns.\n        *   Directly modifying the pre-training objective is costly.\n        *   Post-hoc supervised fine-tuning (SFT) might inadvertently encourage hallucinations or be computationally infeasible for injecting substantial new factual knowledge.\n\n*   **Related Work & Positioning**\n    *   **Existing Approaches:** Previous work to mitigate hallucinations includes:\n        *   Strategic selection of high-quality training data.\n        *   Reinforcement learning from external feedback.\n        *   Retrieval-augmented generation (RAG).\n        *   Using model uncertainty.\n    *   **Limitations of Previous Solutions:** Most existing methods attempt to *optimize LLMs to generate fewer hallucinations*, which is a challenging direct objective.\n    *   **Positioning:** This work reframes the problem by first creating a *factually weak model* (adept at fabricating information) and then *subtracting its knowledge from the original model's output space* through contrastive decoding \\cite{zhang202396g}. It builds upon Contrastive Decoding (CD) but differs from vanilla CD (contrasting models of different scales) and DoLa (contrasting different layers) by directly inducing and penalizing hallucinations.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method (Induce-then-Contrast Decoding - ICD):** ICD is a two-stage decoding strategy:\n        1.  **Inducing Hallucinations:** A \"factually weak LLM\" is constructed by fine-tuning the original LLM with a dataset of non-factual samples. These non-factual samples are generated by converting factual samples into untruthful ones (e.g., using ChatGPT with few-shot prompting).\n        2.  **Contrastive Decoding:** During generation, the log probabilities of the induced hallucinations from the factually weak model are subtracted from the log probabilities of the original model's predictions. This amplifies truthful predictions and downplays untruthful ones.\n            *   The final next-token prediction is determined by `softmax(Î² * log p(xt|x<t;Î¸) - log p(xt|x<t;Î¸+â–³Î¸))`, where `Î¸` is the original model and `Î¸+â–³Î¸` is the hallucination-induced model, and `Î²` controls contrast strength.\n    *   **Novelty/Difference:**\n        *   Instead of directly training LLMs to be more factual, ICD *induces* hallucinations to create a \"negative\" model and then uses this negative signal to guide the original model towards factuality.\n        *   It introduces an **adaptive plausibility constraint** to prevent penalizing simple aspects like grammar and common sense. This constraint selects a subset of tokens for penalty, considering only tokens with probabilities above a certain proportion (`Î±`) of the maximum probability assigned by the original model.\n\n*   **Key Technical Contributions**\n    *   **Novel Method:** The Induce-then-Contrast Decoding (ICD) strategy, which leverages induced hallucinations as a penalty term during decoding to enhance factuality \\cite{zhang202396g}.\n    *   **Technique for Hallucination Induction:** Demonstrates that hallucinations can be effectively induced from LLMs through slight fine-tuning or zero-shot prompting with non-factual samples.\n    *   **Adaptive Plausibility Constraint:** A mechanism to selectively penalize only potentially untruthful tokens, preserving general generation quality.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted:**\n        *   Evaluated on discrimination-based hallucination benchmarks (TruthfulQA) and generation-based benchmarks (FACTSCORE).\n        *   Tested across various LLM sizes and families (Llama2-7B/13B/70B-Chat, Mistral-7B-Instruct).\n        *   Compared against greedy decoding, ITI, DoLa, and vanilla Contrastive Decoding.\n        *   Assessed impact on original LLM capabilities using MMLU, ARC, and AlpacaEval2.0.\n        *   Conducted ablation studies on different hallucination induction methods (fine-tuning, prompting) and task formats.\n        *   Performed GPT-4 automatic evaluation for factuality, grammaticality, and topicality.\n    *   **Key Performance Metrics & Results:**\n        *   **TruthfulQA (MC1/2/3 scores):** ICD significantly improved truthfulness. Llama2-7B-Chat with ICD achieved +8.70 MC1 score, outperforming its 70B counterpart and matching ChatGPT's performance. Mistral-7B-Instruct with ICD achieved +20 MC1 score, matching GPT-4's performance \\cite{zhang202396g}.\n        *   **FACTSCORE (Factual Precision Score):** ICD increased factual precision by 2.5 points for Llama2-7B-Chat, enabling it to surpass the Llama2-70B-Chat baseline in open-ended biography generation, without affecting response ratio or average fact numbers.\n        *   **Original Capacity (MMLU, ARC, AlpacaEval2.0):** ICD maintained the original capabilities of LLMs, showing no significant compromise in performance on standard benchmarks.\n        *   **GPT-4 Auto-evaluation:** ICD significantly outperformed greedy decoding in factuality while maintaining grammaticality and topicality.\n        *   **Hallucination Induction Methods:** Fine-tuning-based induction was found to be more effective than prompt-based induction.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations/Assumptions:** The method relies on the ability to effectively induce hallucinations to create a \"factually weak\" model. The quality and nature of these induced hallucinations are crucial. The adaptive plausibility constraint requires careful hyperparameter tuning (`Î±`, `Î²`).\n    *   **Scope of Applicability:** Primarily focuses on mitigating *fact-conflicting hallucination*. While effective across various model sizes and families, the induction process (e.g., fine-tuning with non-factual data) adds a pre-processing step.\n\n*   **Technical Significance**\n    *   **Advance State-of-the-Art:** ICD offers a novel and highly effective decoding-time strategy for hallucination mitigation, achieving state-of-the-art factuality performance on benchmarks like TruthfulQA and FACTSCORE, often enabling smaller models to rival or exceed larger, more capable models \\cite{zhang202396g}.\n    *   **Potential Impact:**\n        *   Provides a practical and computationally feasible method to enhance LLM factuality without costly re-training or complex architectural changes.\n        *   Opens new avenues for research into leveraging \"negative\" or \"weak\" models to improve the performance of \"strong\" models, extending the concept of weak-to-strong generalization.\n        *   Could lead to more trustworthy and reliable LLM applications in factual domains.",
      "intriguing_abstract": "Large Language Models (LLMs) are profoundly limited by \"hallucination,\" their tendency to generate factually incorrect information, hindering real-world deployment. Instead of the arduous task of directly training LLMs to avoid these fabrications, we introduce **Induce-then-Contrast Decoding (ICD)**, a novel decoding strategy that ingeniously leverages induced hallucinations to dramatically enhance factuality. ICD first constructs a \"factually weak\" model by fine-tuning an LLM on non-factual data. During inference, it then subtracts the log probabilities of this weak model from the original LLM's predictions, effectively penalizing untruthful tokens. A key innovation, the adaptive plausibility constraint, ensures only genuinely untruthful aspects are penalized, preserving overall generation quality.\n\nOur experiments reveal ICD's remarkable efficacy: it significantly boosts factuality on benchmarks like TruthfulQA and FACTSCORE, enabling smaller models (e.g., Llama2-7B, Mistral-7B) to rival or even surpass the factual performance of much larger models and even GPT-4, all without compromising original capabilities. ICD offers a practical, computationally efficient path to more trustworthy LLMs, opening new research avenues in leveraging \"negative\" models for robust generative AI.",
      "keywords": [
        "Large Language Models (LLMs)",
        "hallucination mitigation",
        "fact-conflicting hallucination",
        "Induce-then-Contrast Decoding (ICD)",
        "hallucination induction",
        "factually weak model",
        "contrastive decoding",
        "adaptive plausibility constraint",
        "decoding-time strategy",
        "TruthfulQA",
        "FACTSCORE",
        "state-of-the-art factuality",
        "maintaining original capabilities",
        "weak-to-strong generalization"
      ],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/3f915aab835cbfe69e7b2ea1c73b74ac8a2d384e.pdf",
      "citation_key": "zhang202396g",
      "metadata": {
        "title": "Alleviating Hallucinations of Large Language Models through Induced Hallucinations",
        "authors": [
          "Yue Zhang",
          "Leyang Cui",
          "Wei Bi",
          "Shuming Shi"
        ],
        "published_date": "2023",
        "abstract": "Despite their impressive capabilities, large language models (LLMs) have been observed to generate responses that include inaccurate or fabricated information, a phenomenon commonly known as ``hallucination''. In this work, we propose a simple \\textit{Induce-then-Contrast} Decoding (ICD) strategy to alleviate hallucinations. We first construct a factually weak LLM by inducing hallucinations from the original LLMs. Then, we penalize these induced hallucinations during decoding to enhance the factuality of the generated content. Concretely, we determine the final next-token predictions by amplifying the predictions from the original model and downplaying the induced untruthful predictions via contrastive decoding. Experimental results on both discrimination-based and generation-based hallucination evaluation benchmarks, such as TruthfulQA and \\textsc{FActScore}, demonstrate that our proposed ICD methods can effectively enhance the factuality of LLMs across various model sizes and families. For example, when equipped with ICD, Llama2-7B-Chat and Mistral-7B-Instruct achieve performance comparable to ChatGPT and GPT4 on TruthfulQA, respectively.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/3f915aab835cbfe69e7b2ea1c73b74ac8a2d384e.pdf",
        "venue": "North American Chapter of the Association for Computational Linguistics",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the paper for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem:** Large Language Models (LLMs) frequently generate inaccurate or fabricated information, a phenomenon known as \"hallucination\" \\cite{zhang202396g}. This primarily focuses on *fact-conflicting hallucination*.\n    *   **Importance & Challenge:** Hallucinations hinder the practical application of LLMs in real-world scenarios. Mitigating them is challenging because:\n        *   The maximum-likelihood-based pre-training objective can assign probabilities to non-factual information or over-rely on superficial patterns.\n        *   Directly modifying the pre-training objective is costly.\n        *   Post-hoc supervised fine-tuning (SFT) might inadvertently encourage hallucinations or be computationally infeasible for injecting substantial new factual knowledge.\n\n*   **Related Work & Positioning**\n    *   **Existing Approaches:** Previous work to mitigate hallucinations includes:\n        *   Strategic selection of high-quality training data.\n        *   Reinforcement learning from external feedback.\n        *   Retrieval-augmented generation (RAG).\n        *   Using model uncertainty.\n    *   **Limitations of Previous Solutions:** Most existing methods attempt to *optimize LLMs to generate fewer hallucinations*, which is a challenging direct objective.\n    *   **Positioning:** This work reframes the problem by first creating a *factually weak model* (adept at fabricating information) and then *subtracting its knowledge from the original model's output space* through contrastive decoding \\cite{zhang202396g}. It builds upon Contrastive Decoding (CD) but differs from vanilla CD (contrasting models of different scales) and DoLa (contrasting different layers) by directly inducing and penalizing hallucinations.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method (Induce-then-Contrast Decoding - ICD):** ICD is a two-stage decoding strategy:\n        1.  **Inducing Hallucinations:** A \"factually weak LLM\" is constructed by fine-tuning the original LLM with a dataset of non-factual samples. These non-factual samples are generated by converting factual samples into untruthful ones (e.g., using ChatGPT with few-shot prompting).\n        2.  **Contrastive Decoding:** During generation, the log probabilities of the induced hallucinations from the factually weak model are subtracted from the log probabilities of the original model's predictions. This amplifies truthful predictions and downplays untruthful ones.\n            *   The final next-token prediction is determined by `softmax(Î² * log p(xt|x<t;Î¸) - log p(xt|x<t;Î¸+â–³Î¸))`, where `Î¸` is the original model and `Î¸+â–³Î¸` is the hallucination-induced model, and `Î²` controls contrast strength.\n    *   **Novelty/Difference:**\n        *   Instead of directly training LLMs to be more factual, ICD *induces* hallucinations to create a \"negative\" model and then uses this negative signal to guide the original model towards factuality.\n        *   It introduces an **adaptive plausibility constraint** to prevent penalizing simple aspects like grammar and common sense. This constraint selects a subset of tokens for penalty, considering only tokens with probabilities above a certain proportion (`Î±`) of the maximum probability assigned by the original model.\n\n*   **Key Technical Contributions**\n    *   **Novel Method:** The Induce-then-Contrast Decoding (ICD) strategy, which leverages induced hallucinations as a penalty term during decoding to enhance factuality \\cite{zhang202396g}.\n    *   **Technique for Hallucination Induction:** Demonstrates that hallucinations can be effectively induced from LLMs through slight fine-tuning or zero-shot prompting with non-factual samples.\n    *   **Adaptive Plausibility Constraint:** A mechanism to selectively penalize only potentially untruthful tokens, preserving general generation quality.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted:**\n        *   Evaluated on discrimination-based hallucination benchmarks (TruthfulQA) and generation-based benchmarks (FACTSCORE).\n        *   Tested across various LLM sizes and families (Llama2-7B/13B/70B-Chat, Mistral-7B-Instruct).\n        *   Compared against greedy decoding, ITI, DoLa, and vanilla Contrastive Decoding.\n        *   Assessed impact on original LLM capabilities using MMLU, ARC, and AlpacaEval2.0.\n        *   Conducted ablation studies on different hallucination induction methods (fine-tuning, prompting) and task formats.\n        *   Performed GPT-4 automatic evaluation for factuality, grammaticality, and topicality.\n    *   **Key Performance Metrics & Results:**\n        *   **TruthfulQA (MC1/2/3 scores):** ICD significantly improved truthfulness. Llama2-7B-Chat with ICD achieved +8.70 MC1 score, outperforming its 70B counterpart and matching ChatGPT's performance. Mistral-7B-Instruct with ICD achieved +20 MC1 score, matching GPT-4's performance \\cite{zhang202396g}.\n        *   **FACTSCORE (Factual Precision Score):** ICD increased factual precision by 2.5 points for Llama2-7B-Chat, enabling it to surpass the Llama2-70B-Chat baseline in open-ended biography generation, without affecting response ratio or average fact numbers.\n        *   **Original Capacity (MMLU, ARC, AlpacaEval2.0):** ICD maintained the original capabilities of LLMs, showing no significant compromise in performance on standard benchmarks.\n        *   **GPT-4 Auto-evaluation:** ICD significantly outperformed greedy decoding in factuality while maintaining grammaticality and topicality.\n        *   **Hallucination Induction Methods:** Fine-tuning-based induction was found to be more effective than prompt-based induction.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations/Assumptions:** The method relies on the ability to effectively induce hallucinations to create a \"factually weak\" model. The quality and nature of these induced hallucinations are crucial. The adaptive plausibility constraint requires careful hyperparameter tuning (`Î±`, `Î²`).\n    *   **Scope of Applicability:** Primarily focuses on mitigating *fact-conflicting hallucination*. While effective across various model sizes and families, the induction process (e.g., fine-tuning with non-factual data) adds a pre-processing step.\n\n*   **Technical Significance**\n    *   **Advance State-of-the-Art:** ICD offers a novel and highly effective decoding-time strategy for hallucination mitigation, achieving state-of-the-art factuality performance on benchmarks like TruthfulQA and FACTSCORE, often enabling smaller models to rival or exceed larger, more capable models \\cite{zhang202396g}.\n    *   **Potential Impact:**\n        *   Provides a practical and computationally feasible method to enhance LLM factuality without costly re-training or complex architectural changes.\n        *   Opens new avenues for research into leveraging \"negative\" or \"weak\" models to improve the performance of \"strong\" models, extending the concept of weak-to-strong generalization.\n        *   Could lead to more trustworthy and reliable LLM applications in factual domains.",
        "keywords": [
          "Large Language Models (LLMs)",
          "hallucination mitigation",
          "fact-conflicting hallucination",
          "Induce-then-Contrast Decoding (ICD)",
          "hallucination induction",
          "factually weak model",
          "contrastive decoding",
          "adaptive plausibility constraint",
          "decoding-time strategy",
          "TruthfulQA",
          "FACTSCORE",
          "state-of-the-art factuality",
          "maintaining original capabilities",
          "weak-to-strong generalization"
        ],
        "paper_type": "based on the abstract and introduction:\n\n1.  **abstract analysis:**\n    *   \"we **propose** a simple induce-then-contrast decoding (icd) strategy to alleviate hallucinations.\" - this is a clear indicator of presenting a new method.\n    *   it describes *how* the method works: \"construct a factually weak llm by inducing hallucinations,\" \"penalize these induced hallucinations during decoding,\" \"amplify the predictions from the original model and downplaying the induced untruthful predictions via contrastive decoding.\"\n    *   \"**experimental results**... demonstrate that our **proposed icd methods** can effectively enhance the factuality of llms...\" - this indicates empirical validation of the proposed method.\n\n2.  **introduction analysis:**\n    *   it sets up a problem: llm hallucinations.\n    *   figure 1 explicitly states: \"illustration of **our induce-then-contrast decoding (icd) method** for reducing hallucinations in llms.\" - this directly points to presenting a new method.\n\n**conclusion:**\nthe paper's primary focus is on proposing and detailing a new method/strategy (induce-then-contrast decoding) to solve a technical problem (llm hallucinations), followed by empirical validation of this method. this aligns perfectly with the \"technical\" classification criteria. while it includes empirical results, these results serve to demonstrate the effectiveness of the *new method*, making \"technical\" the most fitting primary classification.\n\n**classification: technical**"
      },
      "file_name": "3f915aab835cbfe69e7b2ea1c73b74ac8a2d384e.pdf"
    },
    {
      "success": true,
      "doc_id": "cdf3893f5b296ad0610fd4152ff67305",
      "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **1. Research Problem & Motivation**\n    *   **Specific Technical Problem**: Multimodal Large Language Models (MLLMs) frequently suffer from \"object hallucination,\" where generated textual descriptions do not accurately reflect objects present in the visual input \\cite{yin2025s2b}. This stems from an over-reliance on unimodal (especially language) priors during inference.\n    *   **Importance and Challenge**: Object hallucination poses significant risks in high-precision applications like medical diagnosis and autonomous driving. Existing mitigation strategies, particularly contrastive decoding, introduce their own challenges: they can compromise the coherence and accuracy of generated content and significantly increase inference time due to the need for processing additional contrastive inputs \\cite{yin2025s2b}.\n\n*   **2. Related Work & Positioning**\n    *   **Relation to Existing Approaches**: The paper positions itself against existing Contrastive Decoding strategies (e.g., Visual Contrastive Decoding - VCD, Instruction Contrastive Decoding - ICD) which aim to reduce hallucination by contrasting output distributions from original and perturbed inputs \\cite{yin2025s2b}. These methods are attractive due to being training-free and versatile.\n    *   **Limitations of Previous Solutions**: While effective in reducing reliance on language priors, contrastive decoding methods have two main drawbacks:\n        1.  They can degrade the quality, coherence, and accuracy of generated content, especially in complex tasks requiring nuanced natural language generation (e.g., a 19% decrease on NoCaps and 5% on ScienceQA for VCD) \\cite{yin2025s2b}.\n        2.  They considerably increase inference time (nearly doubling it for VCD) because they require separate processing of original and contrastive inputs \\cite{yin2025s2b}.\n\n*   **3. Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes Visual Amplification Fusion (VAF), a plug-and-play technique designed to mitigate object hallucinations \\cite{yin2025s2b}. VAF operates by enhancing attention to visual signals specifically within the MLLM's middle layers, where modality fusion predominantly occurs.\n    *   **Novelty/Difference**:\n        *   VAF \\cite{yin2025s2b} is based on the insight that language bias in MLLMs arises from *insufficient attention to visual information* during modality fusion, rather than an overemphasis on language signals.\n        *   Unlike contrastive decoding, VAF \\cite{yin2025s2b} does not bluntly suppress language priors or require additional forward passes, thus preserving content quality and inference speed.\n        *   It directly targets the attention mechanism in the critical modality fusion layers to amplify visual features, enabling more effective capture of visual information and reducing the model's bias toward the language modality \\cite{yin2025s2b}.\n\n*   **4. Key Technical Contributions**\n    *   **Novel Insights**: Identification of the negative impacts of contrastive decoding on content quality and inference speed \\cite{yin2025s2b}.\n    *   **Theoretical Insights/Analysis**: Analysis of the MLLM modality fusion mechanism, highlighting that visual-textual information flow is critical in middle layers (8th-15th) and that attention to visual features is notably lower than to system prompts and user instructions in these layers \\cite{yin2025s2b}.\n    *   **Novel Method**: Introduction of the Visual Amplification Fusion (VAF) method, a training-free, plug-and-play technique that effectively mitigates object hallucination \\cite{yin2025s2b}.\n\n*   **5. Experimental Validation**\n    *   **Experiments Conducted**: Experiments were conducted using LLaVA-v1.5-7B and LLaVA-v1.5-13B models. Saliency analysis was performed to understand visual information flow and attention distribution across modalities \\cite{yin2025s2b}.\n    *   **Key Performance Metrics and Comparison Results**:\n        *   **Hallucination Mitigation**: VAF \\cite{yin2025s2b} demonstrated notable performance gains, with approximately 3% improvement on the POPE benchmark and 7% on MME.\n        *   **Content Coherence and Accuracy**: While VCD caused a significant decrease (e.g., ~19% on NoCaps and 5% on ScienceQA), VAF \\cite{yin2025s2b} maintained content quality without negative impacts.\n        *   **Inference Speed**: VCD reduced inference speed by 50% (nearly doubling inference time), whereas VAF \\cite{yin2025s2b} had virtually no effect on inference speed.\n\n*   **6. Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The paper primarily focuses on addressing the limitations of *contrastive decoding* methods. It does not explicitly state technical limitations of VAF \\cite{yin2025s2b} itself, but rather presents it as a superior alternative.\n    *   **Scope of Applicability**: VAF \\cite{yin2025s2b} is a plug-and-play technique applicable to various MLLMs and is designed for training-free hallucination mitigation.\n\n*   **7. Technical Significance**\n    *   **Advancement of State-of-the-Art**: ClearSight, with its VAF method, significantly advances the technical state-of-the-art by providing a training-free, plug-and-play solution that effectively mitigates object hallucination in MLLMs without compromising content quality or inference speed \\cite{yin2025s2b}. This overcomes critical drawbacks of widely used contrastive decoding strategies.\n    *   **Potential Impact**: The method's ability to maintain content coherence and inference speed while reducing hallucinations makes MLLMs more reliable and efficient for real-world, high-precision applications. It also offers a novel perspective on the root causes of hallucination, suggesting future research directions in fine-grained control over modality fusion within MLLM architectures \\cite{yin2025s2b}.",
      "intriguing_abstract": "Object hallucination critically undermines the reliability of Multimodal Large Language Models (MLLMs), posing significant risks in high-precision applications like medical diagnosis and autonomous driving. Existing mitigation strategies, particularly contrastive decoding, often degrade generated content quality and drastically increase inference time. We introduce Visual Amplification Fusion (VAF), a novel plug-and-play, training-free technique that fundamentally rethinks hallucination mitigation.\n\nVAF operates on the insight that object hallucination arises from *insufficient attention to visual information* during modality fusion in MLLM middle layers, rather than an overemphasis on language priors. By strategically amplifying visual signals within these critical layers, VAF directly addresses this imbalance without suppressing language or requiring additional forward passes.\n\nOur experiments on LLaVA-v1.5 models demonstrate VAF's superior performance: achieving up to a 7% reduction in object hallucination (e.g., on MME and POPE benchmarks) while crucially preserving content coherence and maintaining inference speed. This stands in stark contrast to contrastive methods, which can degrade quality by 19% and halve speed. VAF offers an efficient and effective solution, paving the way for more reliable MLLMs in critical domains and inspiring new research into fine-grained multimodal fusion.",
      "keywords": [
        "Multimodal Large Language Models (MLLMs)",
        "object hallucination",
        "Visual Amplification Fusion (VAF)",
        "contrastive decoding",
        "modality fusion",
        "attention mechanism",
        "training-free plug-and-play",
        "inference speed preservation",
        "content quality preservation",
        "high-precision applications",
        "unimodal language priors",
        "enhancing visual signals",
        "saliency analysis"
      ],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/ecc51ce52ca524be17616a9c0dc8a051a2996ad7.pdf",
      "citation_key": "yin2025s2b",
      "metadata": {
        "title": "ClearSight: Visual Signal Enhancement for Object Hallucination Mitigation in Multimodal Large Language Models",
        "authors": [
          "Hao Yin",
          "Guangzong Si",
          "Zilei Wang"
        ],
        "published_date": "2025",
        "abstract": "Contrastive decoding strategies are widely used to mitigate object hallucinations in multimodal large language models (MLLMs). By reducing over-reliance on language priors, these strategies ensure that generated content remains closely grounded in visual inputs, producing contextually accurate outputs. Since contrastive decoding requires no additional training or external tools, it offers both computational efficiency and versatility, making it highly attractive. However, these methods present two main limitations: (1) bluntly suppressing language priors can compromise coherence and accuracy of generated content, and (2) processing contrastive inputs adds computational load, significantly slowing inference speed. To address these challenges, we propose Visual Amplification Fusion (VAF), a plug-and-play technique that enhances attention to visual signals within the modelâ€™s middle layers, where modality fusion predominantly occurs. This approach enables more effective capture of visual features, reducing the modelâ€™s bias toward language modality. Experimental results demonstrate that VAF significantly reduces hallucinations across various MLLMs without affecting inference speed, while maintaining coherence and accuracy in generated outputs. The code is available at https://github.com/ustc-hyin/ClearSight.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/ecc51ce52ca524be17616a9c0dc8a051a2996ad7.pdf",
        "venue": "Computer Vision and Pattern Recognition",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **1. Research Problem & Motivation**\n    *   **Specific Technical Problem**: Multimodal Large Language Models (MLLMs) frequently suffer from \"object hallucination,\" where generated textual descriptions do not accurately reflect objects present in the visual input \\cite{yin2025s2b}. This stems from an over-reliance on unimodal (especially language) priors during inference.\n    *   **Importance and Challenge**: Object hallucination poses significant risks in high-precision applications like medical diagnosis and autonomous driving. Existing mitigation strategies, particularly contrastive decoding, introduce their own challenges: they can compromise the coherence and accuracy of generated content and significantly increase inference time due to the need for processing additional contrastive inputs \\cite{yin2025s2b}.\n\n*   **2. Related Work & Positioning**\n    *   **Relation to Existing Approaches**: The paper positions itself against existing Contrastive Decoding strategies (e.g., Visual Contrastive Decoding - VCD, Instruction Contrastive Decoding - ICD) which aim to reduce hallucination by contrasting output distributions from original and perturbed inputs \\cite{yin2025s2b}. These methods are attractive due to being training-free and versatile.\n    *   **Limitations of Previous Solutions**: While effective in reducing reliance on language priors, contrastive decoding methods have two main drawbacks:\n        1.  They can degrade the quality, coherence, and accuracy of generated content, especially in complex tasks requiring nuanced natural language generation (e.g., a 19% decrease on NoCaps and 5% on ScienceQA for VCD) \\cite{yin2025s2b}.\n        2.  They considerably increase inference time (nearly doubling it for VCD) because they require separate processing of original and contrastive inputs \\cite{yin2025s2b}.\n\n*   **3. Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes Visual Amplification Fusion (VAF), a plug-and-play technique designed to mitigate object hallucinations \\cite{yin2025s2b}. VAF operates by enhancing attention to visual signals specifically within the MLLM's middle layers, where modality fusion predominantly occurs.\n    *   **Novelty/Difference**:\n        *   VAF \\cite{yin2025s2b} is based on the insight that language bias in MLLMs arises from *insufficient attention to visual information* during modality fusion, rather than an overemphasis on language signals.\n        *   Unlike contrastive decoding, VAF \\cite{yin2025s2b} does not bluntly suppress language priors or require additional forward passes, thus preserving content quality and inference speed.\n        *   It directly targets the attention mechanism in the critical modality fusion layers to amplify visual features, enabling more effective capture of visual information and reducing the model's bias toward the language modality \\cite{yin2025s2b}.\n\n*   **4. Key Technical Contributions**\n    *   **Novel Insights**: Identification of the negative impacts of contrastive decoding on content quality and inference speed \\cite{yin2025s2b}.\n    *   **Theoretical Insights/Analysis**: Analysis of the MLLM modality fusion mechanism, highlighting that visual-textual information flow is critical in middle layers (8th-15th) and that attention to visual features is notably lower than to system prompts and user instructions in these layers \\cite{yin2025s2b}.\n    *   **Novel Method**: Introduction of the Visual Amplification Fusion (VAF) method, a training-free, plug-and-play technique that effectively mitigates object hallucination \\cite{yin2025s2b}.\n\n*   **5. Experimental Validation**\n    *   **Experiments Conducted**: Experiments were conducted using LLaVA-v1.5-7B and LLaVA-v1.5-13B models. Saliency analysis was performed to understand visual information flow and attention distribution across modalities \\cite{yin2025s2b}.\n    *   **Key Performance Metrics and Comparison Results**:\n        *   **Hallucination Mitigation**: VAF \\cite{yin2025s2b} demonstrated notable performance gains, with approximately 3% improvement on the POPE benchmark and 7% on MME.\n        *   **Content Coherence and Accuracy**: While VCD caused a significant decrease (e.g., ~19% on NoCaps and 5% on ScienceQA), VAF \\cite{yin2025s2b} maintained content quality without negative impacts.\n        *   **Inference Speed**: VCD reduced inference speed by 50% (nearly doubling inference time), whereas VAF \\cite{yin2025s2b} had virtually no effect on inference speed.\n\n*   **6. Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The paper primarily focuses on addressing the limitations of *contrastive decoding* methods. It does not explicitly state technical limitations of VAF \\cite{yin2025s2b} itself, but rather presents it as a superior alternative.\n    *   **Scope of Applicability**: VAF \\cite{yin2025s2b} is a plug-and-play technique applicable to various MLLMs and is designed for training-free hallucination mitigation.\n\n*   **7. Technical Significance**\n    *   **Advancement of State-of-the-Art**: ClearSight, with its VAF method, significantly advances the technical state-of-the-art by providing a training-free, plug-and-play solution that effectively mitigates object hallucination in MLLMs without compromising content quality or inference speed \\cite{yin2025s2b}. This overcomes critical drawbacks of widely used contrastive decoding strategies.\n    *   **Potential Impact**: The method's ability to maintain content coherence and inference speed while reducing hallucinations makes MLLMs more reliable and efficient for real-world, high-precision applications. It also offers a novel perspective on the root causes of hallucination, suggesting future research directions in fine-grained control over modality fusion within MLLM architectures \\cite{yin2025s2b}.",
        "keywords": [
          "Multimodal Large Language Models (MLLMs)",
          "object hallucination",
          "Visual Amplification Fusion (VAF)",
          "contrastive decoding",
          "modality fusion",
          "attention mechanism",
          "training-free plug-and-play",
          "inference speed preservation",
          "content quality preservation",
          "high-precision applications",
          "unimodal language priors",
          "enhancing visual signals",
          "saliency analysis"
        ],
        "paper_type": "based on the abstract and introduction:\n\n*   the abstract explicitly states: \"to address these challenges, **we propose visual amplification fusion (vaf), a plug-and-play technique**\".\n*   it describes the mechanism of vaf: \"enhances attention to visual signals within the modelâ€™s middle layers\".\n*   it mentions \"experimental results demonstrate that vaf significantly reduces hallucinations\". while experiments are empirical, their purpose here is to validate the *newly proposed technique*.\n*   the introduction sets up a technical problem (\"object hallucination\" in mllms) and discusses limitations of existing technical solutions (contrastive decoding strategies).\n\nthese points strongly align with the criteria for a **technical** paper, which presents new methods, algorithms, or systems. the empirical results serve to validate the proposed technical solution.\n\n**classification: technical**"
      },
      "file_name": "ecc51ce52ca524be17616a9c0dc8a051a2996ad7.pdf"
    },
    {
      "success": true,
      "doc_id": "86f9f4c7c4cb051288a8d7b22b5ee360",
      "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **1. Research Problem & Motivation**\n    *   **Specific Technical Problem:** Despite the impressive performance of pre-trained language models (PLMs) like BART when fine-tuned for abstractive summarization, there is a significant lack of understanding regarding *what knowledge* is retained from pre-training and *how content selection and generation strategies* are learned across training iterations \\cite{goyal2021onb}.\n    *   **Importance & Challenge:** Abstractive summarization is a complex task involving implicit content selection and rewriting. Existing work on training dynamics primarily focuses on sequence classification, which differs substantially from text generation due to its sequential nature of predictions and the mismatch between teacher-forced training and inference time. Understanding these dynamics is crucial for developing more robust and controllable summarization models \\cite{goyal2021onb}.\n\n*   **2. Related Work & Positioning**\n    *   **Relation to Existing Approaches:** The work builds upon the success of Transformer-based pre-training for summarization \\cite{goyal2021onb}. It extends the concept of studying training dynamics, which has been applied to sequence classification tasks (e.g., NLI, fact verification) to mitigate dataset biases \\cite{goyal2021onb}.\n    *   **Limitations of Previous Solutions:** Prior analyses of training dynamics are not directly transferable to text generation due due to its unique characteristics. The paper also highlights that simply training for more iterations is often insufficient to achieve desired abstractiveness or factuality, necessitating deeper modifications to the training procedure \\cite{goyal2021onb}.\n\n*   **3. Technical Approach & Innovation**\n    *   **Core Technical Method:** The paper first conducts an in-depth analysis of the fine-tuning dynamics of BART-LARGE and PEGASUS-LARGE models for summarization across three diverse datasets: XSUM (highly abstractive), CNNDM, and MEDIASUM (more extractive) \\cite{goyal2021onb}. This analysis involves:\n        *   Monitoring n-gram overlap (as a proxy for abstractiveness) of generated summaries at different training stages.\n        *   Evaluating Sentence Error Rate (SER) for factual consistency using pre-trained factuality models.\n        *   Analyzing token-level output probabilities for both high/low overlap examples and factual/hallucinated content in reference summaries.\n        *   **Key Findings from Analysis:** Copy behavior is learned very early. Models tend to overfit to \"easier\" (more extractive) examples, especially in CNNDM and MEDIASUM, failing to achieve target abstractiveness. Factual errors (hallucinations) are learned in later stages, particularly on noisy datasets like XSUM, where factual content is predicted with higher confidence than hallucinated content \\cite{goyal2021onb}.\n    *   **Novelty:** Based on these insights, the paper proposes a novel **loss truncation strategy (Algorithm 1: `LOSSTRUNCATION`)** to dynamically modify the loss computation during the later stages of training. This involves **token sub-sampling** based on loss magnitude:\n        *   **`+Abstractive` models:** Exclude low-loss tokens from the final loss calculation, assuming these are easily learned extractive tokens, thereby encouraging more abstractive generation.\n        *   **`+Factuality` models:** Exclude high-loss tokens from the final loss calculation, assuming these correspond to challenging-to-learn or hallucinated content, thereby improving factual consistency.\n        *   A dynamic threshold `q` (controlled by a percentile `p`) is used to identify high/low loss tokens, updated based on recent loss statistics \\cite{goyal2021onb}. This dynamic, property-specific loss modification for generation tasks is a key innovation.\n\n*   **4. Key Technical Contributions**\n    *   **Novel Algorithms, Methods, or Techniques:**\n        *   The first comprehensive analysis of training dynamics for abstractive summarization models, specifically investigating the learning trajectories of abstractiveness and factual consistency \\cite{goyal2021onb}.\n        *   Introduction of a dynamic **loss truncation algorithm (`LOSSTRUNCATION`)** that employs token sub-sampling based on loss magnitude to explicitly steer model learning towards desired summarization properties (abstractiveness or factuality) \\cite{goyal2021onb}.\n    *   **Theoretical Insights or Analysis:**\n        *   Empirical demonstration that \"easy-to-learn\" behaviors like copying are acquired early, while factual errors emerge later and are linked to dataset noise and abstractiveness levels \\cite{goyal2021onb}.\n        *   Observation that models overfit to more extractive examples, hindering the learning of abstractive generation, and that factual tokens are learned with higher confidence than hallucinated tokens \\cite{goyal2021onb}.\n\n*   **5. Experimental Validation**\n    *   **Experiments Conducted:**\n        *   Analysis of n-gram overlap (abstractiveness) and Sentence Error Rate (SER) for factuality across 10 training checkpoints on XSUM, CNNDM, and MEDIASUM datasets using BART-LARGE and PEGASUS-LARGE models \\cite{goyal2021onb}.\n        *   Comparison of token-level output probabilities for high-overlap vs. low-overlap reference summaries and for factual vs. hallucinated content to understand learning patterns \\cite{goyal2021onb}.\n        *   Application of the proposed `LOSSTRUNCATION` method (`+Abstractive` and `+Factuality` variants) to modify training and evaluate its impact on generated summaries \\cite{goyal2021onb}.\n    *   **Key Performance Metrics & Comparison Results:**\n        *   **Abstractiveness:** XSUM models achieved target abstractiveness early, while CNNDM and MEDIASUM models consistently failed to reach target levels, stabilizing at higher n-gram overlap \\cite{goyal2021onb}.\n        *   **Factuality:** SER was roughly proportional to abstractiveness, with XSUM showing higher error rates. For CNNDM/MEDIASUM, SER trajectories mirrored abstractiveness changes \\cite{goyal2021onb}.\n        *   **Impact of Loss Truncation:** The proposed training modifications allowed for configuring models to achieve different goals:\n            *   `+Factuality` models substantially improved factuality for noisy datasets (e.g., XSUM) by downweighting high-loss tokens, while preserving abstractiveness \\cite{goyal2021onb}.\n            *   `+Abstractive` models significantly improved the abstractiveness of generated summaries for relatively extractive datasets (e.g., CNNDM, MEDIASUM) by downweighting low-loss tokens \\cite{goyal2021onb}.\n\n*   **6. Limitations & Scope**\n    *   **Technical Limitations/Assumptions:** The method relies on the assumption that high-loss tokens in later training stages for noisy datasets correlate with hallucinated content, and low-loss tokens for extractive datasets correspond to easily learned extractive content. The effectiveness is tied to the accuracy of these assumptions and the dynamic thresholding mechanism. Factuality assessment relies on external pre-trained factuality models \\cite{goyal2021onb}.\n    *   **Scope of Applicability:** The analysis and proposed method are primarily focused on abstractive text summarization using Transformer-based PLMs (BART, PEGASUS) and news-style datasets. While the insights into training dynamics may generalize, the specific loss truncation strategy is tailored to address the observed abstractiveness and factuality issues in these contexts \\cite{goyal2021onb}.\n\n*   **7. Technical Significance**\n    *   **Advancement of State-of-the-Art:** This work significantly advances the understanding of how complex behaviors like abstractiveness and factual consistency are learned during the fine-tuning of PLMs for text generation. It moves beyond black-box performance metrics to dissect the internal learning process \\cite{goyal2021onb}.\n    *   **Potential Impact on Future Research:**\n        *   Provides a novel methodology for analyzing training dynamics in generation tasks, which can be extended to other complex NLP generation tasks \\cite{goyal2021onb}.\n        *   Introduces an interpretable and effective training modification technique (loss truncation via token sub-sampling) that allows researchers to explicitly control and optimize for specific desired output properties (e.g., factuality, abstractiveness) \\cite{goyal2021onb}.\n        *   Highlights the limitations of simply \"training longer\" and the need for targeted interventions, opening avenues for more sophisticated curriculum learning or adaptive training strategies in generation \\cite{goyal2021onb}.",
      "intriguing_abstract": "Unraveling the black box of pre-trained language models (PLMs) during fine-tuning for abstractive summarization is critical for building robust generation systems. Despite their impressive performance, the intricate learning trajectories of properties like abstractiveness and factual consistency remain poorly understood. We present the first comprehensive analysis of BART-LARGE and PEGASUS-LARGE fine-tuning dynamics across diverse summarization datasets (XSUM, CNNDM, MEDIASUM). Our investigation reveals crucial insights: copy behavior is learned remarkably early, models tend to overfit to \"easier\" extractive examples, and factual errors (hallucinations) emerge in later stages, particularly on noisy datasets.\n\nLeveraging these findings, we introduce `LOSSTRUNCATION`, a novel dynamic loss truncation strategy that employs token sub-sampling to explicitly steer model learning. Our `+Abstractive` variant effectively boosts abstractiveness on extractive datasets by downweighting low-loss tokens, while the `+Factuality` variant significantly enhances factual consistency on noisy datasets by excluding high-loss tokens. This interpretable approach offers unprecedented control over desired summarization properties, moving beyond mere performance metrics. Our work not only deepens the understanding of PLM learning but also provides a powerful, generalizable methodology for developing more controllable and reliable text generation models.",
      "keywords": [
        "Abstractive summarization",
        "pre-trained language models (PLMs)",
        "training dynamics analysis",
        "loss truncation strategy",
        "token sub-sampling",
        "abstractiveness",
        "factuality and hallucinations",
        "content selection",
        "generation strategies",
        "overfitting to extractive examples",
        "dynamic loss modification",
        "n-gram overlap",
        "Sentence Error Rate (SER)"
      ],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/23f1d4b46bc7c8f357a5a89144d5d32af7be13a5.pdf",
      "citation_key": "goyal2021onb",
      "metadata": {
        "title": "Training Dynamics for Text Summarization Models",
        "authors": [
          "Tanya Goyal",
          "Jiacheng Xu",
          "J. Li",
          "Greg Durrett"
        ],
        "published_date": "2021",
        "abstract": "Pre-trained language models (e.g. BART) have shown impressive results when fine-tuned on large summarization datasets. However, little is understood about this fine-tuning process, including what knowledge is retained from pre-training time or how content selection and generation strategies are learnt across iterations. In this work, we analyze the training dynamics for generation models, focusing on summarization. Across different datasets (CNN/DM, XSum, MediaSum) and summary properties, such as abstractiveness and hallucination, we study what the model learns at different stages of its fine-tuning process. We find that a propensity to copy the input is learned early in the training process consistently across all datasets studied. On the other hand, factual errors, such as hallucination of unsupported facts, are learnt in the later stages, though this behavior is more varied across domains. Based on these observations, we explore complementary approaches for modifying training: first, disregarding high-loss tokens that are challenging to learn and second, disregarding low-loss tokens that are learnt very quickly in the latter stages of the training process. We show that these simple training modifications allow us to configure our model to achieve different goals, such as improving factuality or improving abstractiveness.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/23f1d4b46bc7c8f357a5a89144d5d32af7be13a5.pdf",
        "venue": "Findings",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **1. Research Problem & Motivation**\n    *   **Specific Technical Problem:** Despite the impressive performance of pre-trained language models (PLMs) like BART when fine-tuned for abstractive summarization, there is a significant lack of understanding regarding *what knowledge* is retained from pre-training and *how content selection and generation strategies* are learned across training iterations \\cite{goyal2021onb}.\n    *   **Importance & Challenge:** Abstractive summarization is a complex task involving implicit content selection and rewriting. Existing work on training dynamics primarily focuses on sequence classification, which differs substantially from text generation due to its sequential nature of predictions and the mismatch between teacher-forced training and inference time. Understanding these dynamics is crucial for developing more robust and controllable summarization models \\cite{goyal2021onb}.\n\n*   **2. Related Work & Positioning**\n    *   **Relation to Existing Approaches:** The work builds upon the success of Transformer-based pre-training for summarization \\cite{goyal2021onb}. It extends the concept of studying training dynamics, which has been applied to sequence classification tasks (e.g., NLI, fact verification) to mitigate dataset biases \\cite{goyal2021onb}.\n    *   **Limitations of Previous Solutions:** Prior analyses of training dynamics are not directly transferable to text generation due due to its unique characteristics. The paper also highlights that simply training for more iterations is often insufficient to achieve desired abstractiveness or factuality, necessitating deeper modifications to the training procedure \\cite{goyal2021onb}.\n\n*   **3. Technical Approach & Innovation**\n    *   **Core Technical Method:** The paper first conducts an in-depth analysis of the fine-tuning dynamics of BART-LARGE and PEGASUS-LARGE models for summarization across three diverse datasets: XSUM (highly abstractive), CNNDM, and MEDIASUM (more extractive) \\cite{goyal2021onb}. This analysis involves:\n        *   Monitoring n-gram overlap (as a proxy for abstractiveness) of generated summaries at different training stages.\n        *   Evaluating Sentence Error Rate (SER) for factual consistency using pre-trained factuality models.\n        *   Analyzing token-level output probabilities for both high/low overlap examples and factual/hallucinated content in reference summaries.\n        *   **Key Findings from Analysis:** Copy behavior is learned very early. Models tend to overfit to \"easier\" (more extractive) examples, especially in CNNDM and MEDIASUM, failing to achieve target abstractiveness. Factual errors (hallucinations) are learned in later stages, particularly on noisy datasets like XSUM, where factual content is predicted with higher confidence than hallucinated content \\cite{goyal2021onb}.\n    *   **Novelty:** Based on these insights, the paper proposes a novel **loss truncation strategy (Algorithm 1: `LOSSTRUNCATION`)** to dynamically modify the loss computation during the later stages of training. This involves **token sub-sampling** based on loss magnitude:\n        *   **`+Abstractive` models:** Exclude low-loss tokens from the final loss calculation, assuming these are easily learned extractive tokens, thereby encouraging more abstractive generation.\n        *   **`+Factuality` models:** Exclude high-loss tokens from the final loss calculation, assuming these correspond to challenging-to-learn or hallucinated content, thereby improving factual consistency.\n        *   A dynamic threshold `q` (controlled by a percentile `p`) is used to identify high/low loss tokens, updated based on recent loss statistics \\cite{goyal2021onb}. This dynamic, property-specific loss modification for generation tasks is a key innovation.\n\n*   **4. Key Technical Contributions**\n    *   **Novel Algorithms, Methods, or Techniques:**\n        *   The first comprehensive analysis of training dynamics for abstractive summarization models, specifically investigating the learning trajectories of abstractiveness and factual consistency \\cite{goyal2021onb}.\n        *   Introduction of a dynamic **loss truncation algorithm (`LOSSTRUNCATION`)** that employs token sub-sampling based on loss magnitude to explicitly steer model learning towards desired summarization properties (abstractiveness or factuality) \\cite{goyal2021onb}.\n    *   **Theoretical Insights or Analysis:**\n        *   Empirical demonstration that \"easy-to-learn\" behaviors like copying are acquired early, while factual errors emerge later and are linked to dataset noise and abstractiveness levels \\cite{goyal2021onb}.\n        *   Observation that models overfit to more extractive examples, hindering the learning of abstractive generation, and that factual tokens are learned with higher confidence than hallucinated tokens \\cite{goyal2021onb}.\n\n*   **5. Experimental Validation**\n    *   **Experiments Conducted:**\n        *   Analysis of n-gram overlap (abstractiveness) and Sentence Error Rate (SER) for factuality across 10 training checkpoints on XSUM, CNNDM, and MEDIASUM datasets using BART-LARGE and PEGASUS-LARGE models \\cite{goyal2021onb}.\n        *   Comparison of token-level output probabilities for high-overlap vs. low-overlap reference summaries and for factual vs. hallucinated content to understand learning patterns \\cite{goyal2021onb}.\n        *   Application of the proposed `LOSSTRUNCATION` method (`+Abstractive` and `+Factuality` variants) to modify training and evaluate its impact on generated summaries \\cite{goyal2021onb}.\n    *   **Key Performance Metrics & Comparison Results:**\n        *   **Abstractiveness:** XSUM models achieved target abstractiveness early, while CNNDM and MEDIASUM models consistently failed to reach target levels, stabilizing at higher n-gram overlap \\cite{goyal2021onb}.\n        *   **Factuality:** SER was roughly proportional to abstractiveness, with XSUM showing higher error rates. For CNNDM/MEDIASUM, SER trajectories mirrored abstractiveness changes \\cite{goyal2021onb}.\n        *   **Impact of Loss Truncation:** The proposed training modifications allowed for configuring models to achieve different goals:\n            *   `+Factuality` models substantially improved factuality for noisy datasets (e.g., XSUM) by downweighting high-loss tokens, while preserving abstractiveness \\cite{goyal2021onb}.\n            *   `+Abstractive` models significantly improved the abstractiveness of generated summaries for relatively extractive datasets (e.g., CNNDM, MEDIASUM) by downweighting low-loss tokens \\cite{goyal2021onb}.\n\n*   **6. Limitations & Scope**\n    *   **Technical Limitations/Assumptions:** The method relies on the assumption that high-loss tokens in later training stages for noisy datasets correlate with hallucinated content, and low-loss tokens for extractive datasets correspond to easily learned extractive content. The effectiveness is tied to the accuracy of these assumptions and the dynamic thresholding mechanism. Factuality assessment relies on external pre-trained factuality models \\cite{goyal2021onb}.\n    *   **Scope of Applicability:** The analysis and proposed method are primarily focused on abstractive text summarization using Transformer-based PLMs (BART, PEGASUS) and news-style datasets. While the insights into training dynamics may generalize, the specific loss truncation strategy is tailored to address the observed abstractiveness and factuality issues in these contexts \\cite{goyal2021onb}.\n\n*   **7. Technical Significance**\n    *   **Advancement of State-of-the-Art:** This work significantly advances the understanding of how complex behaviors like abstractiveness and factual consistency are learned during the fine-tuning of PLMs for text generation. It moves beyond black-box performance metrics to dissect the internal learning process \\cite{goyal2021onb}.\n    *   **Potential Impact on Future Research:**\n        *   Provides a novel methodology for analyzing training dynamics in generation tasks, which can be extended to other complex NLP generation tasks \\cite{goyal2021onb}.\n        *   Introduces an interpretable and effective training modification technique (loss truncation via token sub-sampling) that allows researchers to explicitly control and optimize for specific desired output properties (e.g., factuality, abstractiveness) \\cite{goyal2021onb}.\n        *   Highlights the limitations of simply \"training longer\" and the need for targeted interventions, opening avenues for more sophisticated curriculum learning or adaptive training strategies in generation \\cite{goyal2021onb}.",
        "keywords": [
          "Abstractive summarization",
          "pre-trained language models (PLMs)",
          "training dynamics analysis",
          "loss truncation strategy",
          "token sub-sampling",
          "abstractiveness",
          "factuality and hallucinations",
          "content selection",
          "generation strategies",
          "overfitting to extractive examples",
          "dynamic loss modification",
          "n-gram overlap",
          "Sentence Error Rate (SER)"
        ],
        "paper_type": "based on the abstract and introduction:\n\n*   the paper explicitly states: \"we **analyze** the training dynamics\", \"we **study** what the model learns at different stages\", and \"we **find** that a propensity to copy the input is learned early... on the other hand, factual errors... are learnt in the later stages\".\n*   it mentions conducting this study \"across different **datasets** (cnndm, xsum, mediasum)\".\n*   the introduction highlights a lack of \"insight into this aspect of the fine-tuning process, namely what â€œskillâ€ or behavior is learnt at which stage of the training process,\" which the paper aims to address through its study.\n*   while it does \"explore complementary approaches for modifying training\" based on these observations, the primary focus and contribution described are the **data-driven study, analysis, and findings** about the training dynamics. the modifications are presented as a consequence and demonstration of these empirical insights.\n\nthis aligns perfectly with the criteria for an **empirical** paper: \"data-driven studies with statistical analysis\", \"abstract mentions: 'study', 'experiment', 'data', 'statistical', 'findings'\", \"introduction discusses: research questions, methodology, participants\".\n\n**classification: empirical**"
      },
      "file_name": "23f1d4b46bc7c8f357a5a89144d5d32af7be13a5.pdf"
    },
    {
      "success": true,
      "doc_id": "ad0c1680d372148d3bef13df1aebf836",
      "summary": "Here's a focused summary of the paper for a literature review:\n\n### Technical Paper Analysis: Understanding Sounds, Missing the Questions: The Challenge of Object Hallucination in Large Audio-Language Models \\cite{kuan20249pm}\n\n1.  **Research Problem & Motivation**\n    *   **Specific technical problem**: The paper addresses the issue of \"object hallucination\" in Large Audio-Language Models (LALMs), where models generate or affirm the presence of objects in audio that are not actually there.\n    *   **Why important and challenging**: LALMs integrate audio perception with Large Language Models (LLMs) to handle diverse audio-related tasks. While previous research has focused on task performance, the reliability of LALMs, particularly concerning hallucination, has been largely overlooked. Hallucination is a known problem in LLMs and Large Vision-Language Models (LVLMs), but there was a lack of discussion and benchmarks for LALMs in the audio domain. This unreliability can lead to incorrect information and reduced trustworthiness in practical applications.\n\n2.  **Related Work & Positioning**\n    *   **Relation to existing approaches**: LALMs build upon traditional LLMs by adding audio perception capabilities. Existing LALM evaluation benchmarks like Dynamic-SUPERB \\cite{kuan20249pm} and AIR-Bench \\cite{kuan20249pm} primarily assess task performance.\n    *   **Limitations of previous solutions**: These benchmarks do not adequately evaluate the reliability of LALM-generated content, specifically object hallucination. While hallucination has been studied in LLMs and LVLMs (e.g., in image captioning), its prevalence and characteristics in LALMs within speech and audio domains were unexplored, and no specific benchmarks existed to measure it.\n\n3.  **Technical Approach & Innovation**\n    *   **Core technical method or algorithm**: The paper introduces novel methods to assess object hallucination in LALMs through two main task types:\n        *   **Discriminative Tasks**: Formulated as binary classification (Yes/No) to determine the presence of a specific object's sound. Questions are generated using positive (ground truth) and negative samples (Random, Popular, and Adversarial sampling strategies for non-existent objects). Performance is measured by accuracy, precision, recall, F1 score, and \"Yes\" answer ratio.\n        *   **Generative Tasks**: Involves audio captioning (e.g., \"Describe the audio\") and noisy Automatic Speech Recognition (ASR). Nouns are extracted from generated captions using NLP tools (SpaCy \\cite{kuan20249pm}) and compared against ground truth labels to identify hallucinated objects.\n    *   **What makes this approach novel or different**: This is the first work to systematically explore and quantify object hallucination in LALMs. It introduces specific evaluation methodologies and metrics tailored for this problem in the audio domain, including novel negative sampling strategies for discriminative questions and the ECHO (Evaluation of Caption Hallucination in audiO) and Cover metrics for generative tasks. It also explores prompt engineering as a mitigation strategy.\n\n4.  **Key Technical Contributions**\n    *   **Novel algorithms, methods, or techniques**:\n        *   Introduction of discriminative and generative evaluation tasks specifically designed to measure object hallucination in LALMs.\n        *   Development of three negative sampling strategies (Random, Popular, Adversarial) for constructing discriminative questions.\n        *   Proposal of ECHO (instance-level ECHO_I and sentence-level ECHO_S) and Cover metrics for quantifying hallucination and coverage in generative audio captioning.\n        *   Investigation into prompt engineering techniques to improve LALM performance on discriminative tasks.\n    *   **Theoretical insights or analysis**: The paper reveals a critical discrepancy: LALMs perform well on audio captioning (generative tasks) but struggle significantly with discriminative questions, indicating a weakness in understanding the *nature* of discriminative queries rather than a fundamental inability to process audio content.\n\n5.  **Experimental Validation**\n    *   **Experiments conducted**:\n        *   Evaluated five publicly available LALMs: Qwen-Audio \\cite{kuan20249pm}, Qwen-Audio-Chat \\cite{kuan20249pm}, LTU-AS-7B \\cite{kuan20249pm}, SALMONN-7B \\cite{kuan20249pm}, and SALMONN-13B \\cite{kuan20249pm}.\n        *   Compared LALMs against a \"Specialized\" cascade pipeline (Whisper-based audio captioning \\cite{kuan20249pm} + ChatGPT \\cite{kuan20249pm} or LLaMA-7b-chat \\cite{kuan20249pm}).\n        *   Used AudioCaps \\cite{kuan20249pm} for audio captioning and CHIME-6 \\cite{kuan20249pm} for noisy ASR.\n        *   Tested five different prompts for both task types and explored greedy vs. sample decoding strategies.\n        *   Investigated eight different prefix prompts for prompt engineering on discriminative tasks.\n    *   **Key performance metrics and comparison results**:\n        *   **Discriminative Tasks**: LALMs exhibited significantly lower recall than precision, indicating a strong tendency to give affirmative answers (hallucinate). F1 scores decreased with more challenging negative sampling strategies (Adversarial < Popular < Random). LALMs were highly sensitive to prompt design. The \"Specialized\" cascade pipeline significantly outperformed all LALMs in F1 scores, highlighting a substantial gap.\n        *   **Generative Tasks**: LALMs' performance on ECHO and Cover metrics was comparable to specialized Whisper-based caption models, demonstrating their capability in understanding audio information and similar levels of object hallucination in captioning. Greedy decoding generally reduced hallucination in generative tasks.\n        *   **Prompt Engineering**: Specific prefix prompts, particularly those emphasizing careful consideration of the question (e.g., P3, P4, P6, P8), led to significant F1 score improvements for some LALMs (e.g., SALMONN models, Qwen-Audio-Chat-7B), but not universally (e.g., LTU-AS-7B showed degradation).\n\n6.  **Limitations & Scope**\n    *   **Technical limitations or assumptions**: The study primarily focuses on object hallucination and suggests that LALMs' struggle lies in comprehending discriminative queries rather than processing audio content itself. The effectiveness of prompt engineering was model-dependent.\n    *   **Scope of applicability**: The proposed evaluation methods and insights are directly applicable to assessing and improving the reliability of LALMs in audio captioning and related audio understanding tasks.\n\n7.  **Technical Significance**\n    *   **How does this advance the technical state-of-the-art**: This work is the first to systematically identify and quantify object hallucination in LALMs, filling a critical gap in LALM reliability research. It introduces novel and comprehensive evaluation methodologies (discriminative/generative tasks, ECHO/Cover metrics) that can serve as benchmarks for future LALM development. It highlights a crucial weakness in current LALMs regarding their understanding of discriminative queries, even when their audio comprehension is strong.\n    *   **Potential impact on future research**: The findings will guide future LALM research towards developing more robust models that are less prone to hallucination, particularly by focusing on improving query understanding mechanisms. It encourages the development of LALMs that can precisely discriminate information from audio, rather than just generating descriptive captions. The proposed evaluation framework provides a valuable tool for assessing the reliability of new LALM architectures.",
      "intriguing_abstract": "Large Audio-Language Models (LALMs) are revolutionizing multimodal understanding, yet their reliability in discerning factual audio content remains critically underexplored. This paper uncovers a pervasive vulnerability: **object hallucination**, where LALMs confidently assert the presence of non-existent sounds. We present the first systematic investigation into this phenomenon, introducing novel discriminative and generative evaluation tasks, alongside new metrics like ECHO, to quantify hallucination in audio captioning and binary classification.\n\nOur findings reveal a striking paradox: while LALMs excel at descriptive audio captioning, they profoundly struggle with discriminative \"yes/no\" queries, exhibiting a strong bias towards affirmative answers. This suggests a fundamental weakness in *query understanding* rather than audio perception itself. We demonstrate that targeted prompt engineering can mitigate this, but the core challenge persists. This work provides essential benchmarks and insights, urging a paradigm shift in LALM development towards models that not only hear sounds but truly comprehend the questions asked about them, fostering greater trustworthiness and precision in audio AI.",
      "keywords": [
        "Large Audio-Language Models (LALMs)",
        "object hallucination",
        "discriminative tasks",
        "generative tasks",
        "evaluation methodologies",
        "negative sampling strategies",
        "ECHO and Cover metrics",
        "prompt engineering",
        "query understanding",
        "audio captioning",
        "LALM reliability",
        "systematic quantification",
        "affirmative answer bias"
      ],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/576023f7cc3da5a36ac0cfda402af859cc90be10.pdf",
      "citation_key": "kuan20249pm",
      "metadata": {
        "title": "Understanding Sounds, Missing the Questions: The Challenge of Object Hallucination in Large Audio-Language Models",
        "authors": [
          "Chun-Yi Kuan",
          "Wei-Ping Huang",
          "Hung-yi Lee"
        ],
        "published_date": "2024",
        "abstract": "Large audio-language models (LALMs) enhance traditional large language models by integrating audio perception capabilities, allowing them to tackle audio-related tasks. Previous research has primarily focused on assessing the performance of LALMs across various tasks, yet overlooking their reliability, particularly concerning issues like object hallucination. In our study, we introduce methods to assess the extent of object hallucination of publicly available LALMs. Our findings reveal that LALMs are comparable to specialized audio captioning models in their understanding of audio content, but struggle to answer discriminative questions, specifically those requiring the identification of the presence of particular object sounds within an audio clip. This limitation highlights a critical weakness in current LALMs: their inadequate understanding of discriminative queries. Moreover, we explore the potential of prompt engineering to enhance LALMs' performance on discriminative questions.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/576023f7cc3da5a36ac0cfda402af859cc90be10.pdf",
        "venue": "Interspeech",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the paper for a literature review:\n\n### Technical Paper Analysis: Understanding Sounds, Missing the Questions: The Challenge of Object Hallucination in Large Audio-Language Models \\cite{kuan20249pm}\n\n1.  **Research Problem & Motivation**\n    *   **Specific technical problem**: The paper addresses the issue of \"object hallucination\" in Large Audio-Language Models (LALMs), where models generate or affirm the presence of objects in audio that are not actually there.\n    *   **Why important and challenging**: LALMs integrate audio perception with Large Language Models (LLMs) to handle diverse audio-related tasks. While previous research has focused on task performance, the reliability of LALMs, particularly concerning hallucination, has been largely overlooked. Hallucination is a known problem in LLMs and Large Vision-Language Models (LVLMs), but there was a lack of discussion and benchmarks for LALMs in the audio domain. This unreliability can lead to incorrect information and reduced trustworthiness in practical applications.\n\n2.  **Related Work & Positioning**\n    *   **Relation to existing approaches**: LALMs build upon traditional LLMs by adding audio perception capabilities. Existing LALM evaluation benchmarks like Dynamic-SUPERB \\cite{kuan20249pm} and AIR-Bench \\cite{kuan20249pm} primarily assess task performance.\n    *   **Limitations of previous solutions**: These benchmarks do not adequately evaluate the reliability of LALM-generated content, specifically object hallucination. While hallucination has been studied in LLMs and LVLMs (e.g., in image captioning), its prevalence and characteristics in LALMs within speech and audio domains were unexplored, and no specific benchmarks existed to measure it.\n\n3.  **Technical Approach & Innovation**\n    *   **Core technical method or algorithm**: The paper introduces novel methods to assess object hallucination in LALMs through two main task types:\n        *   **Discriminative Tasks**: Formulated as binary classification (Yes/No) to determine the presence of a specific object's sound. Questions are generated using positive (ground truth) and negative samples (Random, Popular, and Adversarial sampling strategies for non-existent objects). Performance is measured by accuracy, precision, recall, F1 score, and \"Yes\" answer ratio.\n        *   **Generative Tasks**: Involves audio captioning (e.g., \"Describe the audio\") and noisy Automatic Speech Recognition (ASR). Nouns are extracted from generated captions using NLP tools (SpaCy \\cite{kuan20249pm}) and compared against ground truth labels to identify hallucinated objects.\n    *   **What makes this approach novel or different**: This is the first work to systematically explore and quantify object hallucination in LALMs. It introduces specific evaluation methodologies and metrics tailored for this problem in the audio domain, including novel negative sampling strategies for discriminative questions and the ECHO (Evaluation of Caption Hallucination in audiO) and Cover metrics for generative tasks. It also explores prompt engineering as a mitigation strategy.\n\n4.  **Key Technical Contributions**\n    *   **Novel algorithms, methods, or techniques**:\n        *   Introduction of discriminative and generative evaluation tasks specifically designed to measure object hallucination in LALMs.\n        *   Development of three negative sampling strategies (Random, Popular, Adversarial) for constructing discriminative questions.\n        *   Proposal of ECHO (instance-level ECHO_I and sentence-level ECHO_S) and Cover metrics for quantifying hallucination and coverage in generative audio captioning.\n        *   Investigation into prompt engineering techniques to improve LALM performance on discriminative tasks.\n    *   **Theoretical insights or analysis**: The paper reveals a critical discrepancy: LALMs perform well on audio captioning (generative tasks) but struggle significantly with discriminative questions, indicating a weakness in understanding the *nature* of discriminative queries rather than a fundamental inability to process audio content.\n\n5.  **Experimental Validation**\n    *   **Experiments conducted**:\n        *   Evaluated five publicly available LALMs: Qwen-Audio \\cite{kuan20249pm}, Qwen-Audio-Chat \\cite{kuan20249pm}, LTU-AS-7B \\cite{kuan20249pm}, SALMONN-7B \\cite{kuan20249pm}, and SALMONN-13B \\cite{kuan20249pm}.\n        *   Compared LALMs against a \"Specialized\" cascade pipeline (Whisper-based audio captioning \\cite{kuan20249pm} + ChatGPT \\cite{kuan20249pm} or LLaMA-7b-chat \\cite{kuan20249pm}).\n        *   Used AudioCaps \\cite{kuan20249pm} for audio captioning and CHIME-6 \\cite{kuan20249pm} for noisy ASR.\n        *   Tested five different prompts for both task types and explored greedy vs. sample decoding strategies.\n        *   Investigated eight different prefix prompts for prompt engineering on discriminative tasks.\n    *   **Key performance metrics and comparison results**:\n        *   **Discriminative Tasks**: LALMs exhibited significantly lower recall than precision, indicating a strong tendency to give affirmative answers (hallucinate). F1 scores decreased with more challenging negative sampling strategies (Adversarial < Popular < Random). LALMs were highly sensitive to prompt design. The \"Specialized\" cascade pipeline significantly outperformed all LALMs in F1 scores, highlighting a substantial gap.\n        *   **Generative Tasks**: LALMs' performance on ECHO and Cover metrics was comparable to specialized Whisper-based caption models, demonstrating their capability in understanding audio information and similar levels of object hallucination in captioning. Greedy decoding generally reduced hallucination in generative tasks.\n        *   **Prompt Engineering**: Specific prefix prompts, particularly those emphasizing careful consideration of the question (e.g., P3, P4, P6, P8), led to significant F1 score improvements for some LALMs (e.g., SALMONN models, Qwen-Audio-Chat-7B), but not universally (e.g., LTU-AS-7B showed degradation).\n\n6.  **Limitations & Scope**\n    *   **Technical limitations or assumptions**: The study primarily focuses on object hallucination and suggests that LALMs' struggle lies in comprehending discriminative queries rather than processing audio content itself. The effectiveness of prompt engineering was model-dependent.\n    *   **Scope of applicability**: The proposed evaluation methods and insights are directly applicable to assessing and improving the reliability of LALMs in audio captioning and related audio understanding tasks.\n\n7.  **Technical Significance**\n    *   **How does this advance the technical state-of-the-art**: This work is the first to systematically identify and quantify object hallucination in LALMs, filling a critical gap in LALM reliability research. It introduces novel and comprehensive evaluation methodologies (discriminative/generative tasks, ECHO/Cover metrics) that can serve as benchmarks for future LALM development. It highlights a crucial weakness in current LALMs regarding their understanding of discriminative queries, even when their audio comprehension is strong.\n    *   **Potential impact on future research**: The findings will guide future LALM research towards developing more robust models that are less prone to hallucination, particularly by focusing on improving query understanding mechanisms. It encourages the development of LALMs that can precisely discriminate information from audio, rather than just generating descriptive captions. The proposed evaluation framework provides a valuable tool for assessing the reliability of new LALM architectures.",
        "keywords": [
          "Large Audio-Language Models (LALMs)",
          "object hallucination",
          "discriminative tasks",
          "generative tasks",
          "evaluation methodologies",
          "negative sampling strategies",
          "ECHO and Cover metrics",
          "prompt engineering",
          "query understanding",
          "audio captioning",
          "LALM reliability",
          "systematic quantification",
          "affirmative answer bias"
        ],
        "paper_type": "based on the abstract and introduction:\n\n*   the abstract explicitly states: \"in our study, we introduce methods to assess the extent of object hallucination of publicly available lalms. our findings reveal that lalms are comparable... but struggle to answer discriminative questions... this limitation highlights a critical weakness...\"\n*   the introduction discusses the gap in existing benchmarks regarding reliability assessment and object hallucination, setting the stage for their \"study\" to fill this gap.\n\nthese phrases strongly align with the criteria for an **empirical** paper:\n*   abstract mentions: \"study\", \"experiment\" (implied by \"assess\" and \"findings\"), \"data\" (implied by \"publicly available lalms\" being evaluated), \"findings\".\n*   introduction discusses: research questions (the lack of reliability assessment), methodology (introducing methods to assess).\n\nthe paper is conducting a data-driven investigation to evaluate the performance and reliability of existing lalms, presenting its observations and conclusions.\n\n**classification: empirical**"
      },
      "file_name": "576023f7cc3da5a36ac0cfda402af859cc90be10.pdf"
    },
    {
      "success": true,
      "doc_id": "6a584b17c9a2653721a2ba6ad41627b1",
      "summary": "Here's a focused summary of the paper for a literature review:\n\n*   **Research Problem & Motivation**\n    *   The paper addresses the critical problem of mitigating hallucinations in large vision-language models (LVLMs), particularly \"Type I hallucinations\" which occur in open-ended, free-form responses \\cite{kaul2024ta7}.\n    *   Existing benchmarks primarily focus on \"Type II hallucinations\" (responses to specific, fixed-format questions) and often rely on unstable external APIs \\cite{kaul2024ta7}.\n    *   A key motivation is the observation that reducing Type II hallucinations does not necessarily lead to a reduction in Type I hallucinations; in fact, they are often anti-correlated \\cite{kaul2024ta7}.\n    *   Hallucinations are a major barrier to deploying LVLMs in safety-critical applications, making their evaluation and mitigation crucial \\cite{kaul2024ta7}.\n\n*   **Related Work & Positioning**\n    *   **POPE \\cite{kaul2024ta7}:** An existing benchmark for Type II hallucinations (object existence via yes/no questions). Its limitation is systematically under-sampling negative object categories, leading to a significant underestimation of hallucinations.\n    *   **CHAIR \\cite{kaul2024ta7}:** Addresses Type I hallucinations in short image captions. However, it relies on simple exact text matching, which is inadequate for the rich, free-form responses of modern LVLMs, cannot comprehend abstract concepts, requires manual synonym curation, and can be trivially gamed.\n    *   **Comprehensive Benchmarks (e.g., MMBench, MM-Vet) \\cite{kaul2024ta7}:** Evaluate various LVLM abilities but often use evolving/discontinued APIs, leading to inconsistency, and conflate hallucination impact with other performance metrics.\n    *   **THRONE's Positioning:** This work positions itself as the first accurate and accessible benchmark specifically designed for Type I hallucinations in free-form LVLM outputs, overcoming the limitations of previous methods by employing advanced language model comprehension and robust evaluation strategies \\cite{kaul2024ta7}.\n\n*   **Technical Approach & Innovation**\n    *   **Core Method:** THRONE prompts an LVLM with a concept-neutral instruction (\"Describe this image in detail.\") to generate a long, free-form text response \\cite{kaul2024ta7}.\n    *   **Hallucination Detection:** An external, publicly available, open-source Language Model (LM) performs Abstractive Question Answering (AQA) on the LVLM's generated response \\cite{kaul2024ta7}.\n    *   **AQA Process:** For each object class of interest, the LM is queried with a yes/no question (e.g., \"Is there {a/an} {object class name} in this image?\") using the LVLM's response as context. This avoids the need for additional parsing \\cite{kaul2024ta7}.\n    *   **Novelty:**\n        *   Leverages the semantic understanding capabilities of LMs to accurately judge object existence (or hallucination) within complex, free-form text, a significant advancement over simple text matching \\cite{kaul2024ta7}.\n        *   Focuses specifically on Type I hallucinations, recognizing their distinct nature from Type II \\cite{kaul2024ta7}.\n        *   Employs open-source LMs for AQA, ensuring accessibility, reproducibility, and independence from proprietary, unstable commercial APIs \\cite{kaul2024ta7}.\n        *   Introduces an ensembling strategy using multiple LMs and varied question phrasings to enhance robustness against single-model biases and spurious performance \\cite{kaul2024ta7}.\n        *   Utilizes a precision-weighted F0.5-score (specifically F0.5_CLS) as the primary metric, emphasizing the higher cost of false positives (hallucinations) over false negatives \\cite{kaul2024ta7}.\n\n*   **Key Technical Contributions**\n    *   **Novel Benchmark Framework:** Introduction of THRONE, an automatic, object-based framework for quantitatively evaluating Type I hallucinations in LVLM free-form outputs \\cite{kaul2024ta7}.\n    *   **LM-based Semantic Judgement:** A method that uses public LMs for abstractive question answering to semantically identify object hallucinations in complex, free-form LVLM responses, demonstrating significantly reduced judgement errors compared to prior methods \\cite{kaul2024ta7}.\n    *   **Robustness via Ensembling:** A technique to combine multiple open-source LMs and question variations to ensure the reliability and consistency of hallucination detection, mitigating individual LM biases \\cite{kaul2024ta7}.\n    *   **Precision-Focused Metrics:** Adoption of the F0.5-score (F0.5_CLS) as the principal metric, which prioritizes precision to reflect the critical importance of minimizing hallucinations \\cite{kaul2024ta7}.\n    *   **Effective Data Augmentation Baseline:** A simple yet effective data augmentation method for visual instruction tuning data, shown to improve both Type I and Type II hallucination performance \\cite{kaul2024ta7}.\n\n*   **Experimental Validation**\n    *   The paper evaluates a diverse selection of recent LVLMs using public datasets \\cite{kaul2024ta7}.\n    *   It quantitatively demonstrates that THRONE halves the rate of hallucination misjudgement compared to CHAIR, validated by a human oracle \\cite{kaul2024ta7}.\n    *   Experiments confirm that improvements in Type II hallucination metrics do not translate to a reduction in Type I hallucinations, highlighting their distinct nature \\cite{kaul2024ta7}.\n    *   The study reveals that POPE significantly underestimates Type II hallucinations due to its sampling strategy, and provides results for a more complete evaluation \\cite{kaul2024ta7}.\n    *   The proposed data augmentation method is shown to significantly improve Type I hallucination performance while maintaining or improving Type II performance \\cite{kaul2024ta7}.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations:** The accuracy of THRONE is inherently tied to the performance and potential biases of the underlying external LMs used for AQA, although ensembling is employed to mitigate this \\cite{kaul2024ta7}. The current focus is on object-based hallucinations, which may not cover all forms of factual inaccuracies (e.g., complex relational or attribute errors).\n    *   **Scope of Applicability:** THRONE is specifically designed for evaluating \"Type I\" object hallucinations in free-form, open-ended image descriptions generated by modern instruction-tuned LVLMs. It evaluates against a pre-defined object vocabulary \\cite{kaul2024ta7}.\n\n*   **Technical Significance**\n    *   **Advances State-of-the-Art:** THRONE provides a crucial, accurate, and accessible benchmark for Type I hallucinations, filling a significant gap in LVLM evaluation and advancing the technical state-of-the-art in hallucination assessment \\cite{kaul2024ta7}.\n    *   **Fundamental Insight:** It empirically establishes the critical distinction and often anti-correlated nature of Type I and Type II hallucinations, challenging prior assumptions and guiding future research directions \\cite{kaul2024ta7}.\n    *   **Enables Safer AI:** By offering a robust and reproducible evaluation framework, THRONE directly contributes to the development of more reliable and less hallucinatory LVLMs, which is essential for their responsible deployment in real-world, safety-critical applications \\cite{kaul2024ta7}.\n    *   **Impact on Future Research:** The benchmark and its findings will likely spur new research into targeted mitigation strategies for Type I hallucinations and provide a standardized tool for comparing and improving LVLM performance in this critical area \\cite{kaul2024ta7}.",
      "intriguing_abstract": "Large Vision-Language Models (LVLMs) are plagued by insidious \"Type I hallucinations\" in open-ended, free-form responses, a critical barrier to their safe deployment. Existing benchmarks largely overlook this challenge, focusing on \"Type II\" hallucinations and relying on unstable APIs. Crucially, we unveil a surprising anti-correlation: reducing Type II hallucinations often *increases* Type I, highlighting their distinct nature and the inadequacy of current evaluation paradigms.\n\nWe introduce THRONE, the first accurate and accessible benchmark specifically designed for Type I hallucinations in LVLM free-form outputs. THRONE leverages publicly available Language Models (LMs) for Abstractive Question Answering (AQA), semantically judging object existence within complex generated textâ€”a significant leap beyond simple text matching. Our novel framework employs an ensembling strategy for robustness and utilizes a precision-weighted F0.5-score, prioritizing the minimization of costly false positives.\n\nTHRONE quantitatively halves hallucination misjudgement compared to prior methods and empirically confirms the distinct nature of Type I and Type II hallucinations. By providing a robust, reproducible evaluation, THRONE is pivotal for developing safer, more reliable LVLMs and guiding future research into targeted mitigation strategies, accelerating their responsible integration into safety-critical applications.",
      "keywords": [
        "Large Vision-Language Models (LVLMs)",
        "Type I hallucinations",
        "THRONE benchmark",
        "free-form LVLM outputs",
        "LM-based semantic judgment",
        "Abstractive Question Answering (AQA)",
        "ensembling strategy",
        "precision-weighted F0.5-score",
        "object-based hallucinations",
        "data augmentation (visual instruction tuning)",
        "Type II hallucinations",
        "hallucination mitigation",
        "safety-critical applications"
      ],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/a7f4deb9a1452374330f202bc8d36966a0f254e8.pdf",
      "citation_key": "kaul2024ta7",
      "metadata": {
        "title": "THRONE: An Object-Based Hallucination Benchmark for the Free-Form Generations of Large Vision-Language Models",
        "authors": [
          "Prannay Kaul",
          "Zhizhong Li",
          "Hao Yang",
          "Yonatan Dukler",
          "Ashwin Swaminathan",
          "C. Taylor",
          "Stefano Soatto"
        ],
        "published_date": "2024",
        "abstract": "Mitigating hallucinations in large vision-language models (LVLMs) remains an open problem. Recent benchmarks do not address hallucinations in open-ended free-form responses, which we term â€œType I hallucinationsâ€. Instead, they focus on hallucinations responding to very specific question formats-typically a multiple-choice response regarding a particular object or attribute-which we term â€œType II hallucinationsâ€. Additionally, such benchmarks often require external API calls to models which are subject to change. In practice, we observe that a reduction in Type II hallucinations does not lead to a reduction in Type I hallucinations but rather that the two forms of halluci-nations are often anti-correlated. To address this, we propose THRONE, a novel object-based automatic framework for quantitatively evaluating Type I hallucinations in LVLM free-form outputs. We use public language models (LMs) to identify hallucinations in LVLM responses and compute informative metrics. By evaluating a large selection of recent LVLMs using public datasets, we show that an improvement in existing metrics do not lead to a reduction in Type I hallucinations, and that established benchmarks for measuring Type I hallucinations are incomplete. Finally, we provide a simple and effective data augmentation method to reduce Type I and Type II hallucinations as a strong baseline.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/a7f4deb9a1452374330f202bc8d36966a0f254e8.pdf",
        "venue": "Computer Vision and Pattern Recognition",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the paper for a literature review:\n\n*   **Research Problem & Motivation**\n    *   The paper addresses the critical problem of mitigating hallucinations in large vision-language models (LVLMs), particularly \"Type I hallucinations\" which occur in open-ended, free-form responses \\cite{kaul2024ta7}.\n    *   Existing benchmarks primarily focus on \"Type II hallucinations\" (responses to specific, fixed-format questions) and often rely on unstable external APIs \\cite{kaul2024ta7}.\n    *   A key motivation is the observation that reducing Type II hallucinations does not necessarily lead to a reduction in Type I hallucinations; in fact, they are often anti-correlated \\cite{kaul2024ta7}.\n    *   Hallucinations are a major barrier to deploying LVLMs in safety-critical applications, making their evaluation and mitigation crucial \\cite{kaul2024ta7}.\n\n*   **Related Work & Positioning**\n    *   **POPE \\cite{kaul2024ta7}:** An existing benchmark for Type II hallucinations (object existence via yes/no questions). Its limitation is systematically under-sampling negative object categories, leading to a significant underestimation of hallucinations.\n    *   **CHAIR \\cite{kaul2024ta7}:** Addresses Type I hallucinations in short image captions. However, it relies on simple exact text matching, which is inadequate for the rich, free-form responses of modern LVLMs, cannot comprehend abstract concepts, requires manual synonym curation, and can be trivially gamed.\n    *   **Comprehensive Benchmarks (e.g., MMBench, MM-Vet) \\cite{kaul2024ta7}:** Evaluate various LVLM abilities but often use evolving/discontinued APIs, leading to inconsistency, and conflate hallucination impact with other performance metrics.\n    *   **THRONE's Positioning:** This work positions itself as the first accurate and accessible benchmark specifically designed for Type I hallucinations in free-form LVLM outputs, overcoming the limitations of previous methods by employing advanced language model comprehension and robust evaluation strategies \\cite{kaul2024ta7}.\n\n*   **Technical Approach & Innovation**\n    *   **Core Method:** THRONE prompts an LVLM with a concept-neutral instruction (\"Describe this image in detail.\") to generate a long, free-form text response \\cite{kaul2024ta7}.\n    *   **Hallucination Detection:** An external, publicly available, open-source Language Model (LM) performs Abstractive Question Answering (AQA) on the LVLM's generated response \\cite{kaul2024ta7}.\n    *   **AQA Process:** For each object class of interest, the LM is queried with a yes/no question (e.g., \"Is there {a/an} {object class name} in this image?\") using the LVLM's response as context. This avoids the need for additional parsing \\cite{kaul2024ta7}.\n    *   **Novelty:**\n        *   Leverages the semantic understanding capabilities of LMs to accurately judge object existence (or hallucination) within complex, free-form text, a significant advancement over simple text matching \\cite{kaul2024ta7}.\n        *   Focuses specifically on Type I hallucinations, recognizing their distinct nature from Type II \\cite{kaul2024ta7}.\n        *   Employs open-source LMs for AQA, ensuring accessibility, reproducibility, and independence from proprietary, unstable commercial APIs \\cite{kaul2024ta7}.\n        *   Introduces an ensembling strategy using multiple LMs and varied question phrasings to enhance robustness against single-model biases and spurious performance \\cite{kaul2024ta7}.\n        *   Utilizes a precision-weighted F0.5-score (specifically F0.5_CLS) as the primary metric, emphasizing the higher cost of false positives (hallucinations) over false negatives \\cite{kaul2024ta7}.\n\n*   **Key Technical Contributions**\n    *   **Novel Benchmark Framework:** Introduction of THRONE, an automatic, object-based framework for quantitatively evaluating Type I hallucinations in LVLM free-form outputs \\cite{kaul2024ta7}.\n    *   **LM-based Semantic Judgement:** A method that uses public LMs for abstractive question answering to semantically identify object hallucinations in complex, free-form LVLM responses, demonstrating significantly reduced judgement errors compared to prior methods \\cite{kaul2024ta7}.\n    *   **Robustness via Ensembling:** A technique to combine multiple open-source LMs and question variations to ensure the reliability and consistency of hallucination detection, mitigating individual LM biases \\cite{kaul2024ta7}.\n    *   **Precision-Focused Metrics:** Adoption of the F0.5-score (F0.5_CLS) as the principal metric, which prioritizes precision to reflect the critical importance of minimizing hallucinations \\cite{kaul2024ta7}.\n    *   **Effective Data Augmentation Baseline:** A simple yet effective data augmentation method for visual instruction tuning data, shown to improve both Type I and Type II hallucination performance \\cite{kaul2024ta7}.\n\n*   **Experimental Validation**\n    *   The paper evaluates a diverse selection of recent LVLMs using public datasets \\cite{kaul2024ta7}.\n    *   It quantitatively demonstrates that THRONE halves the rate of hallucination misjudgement compared to CHAIR, validated by a human oracle \\cite{kaul2024ta7}.\n    *   Experiments confirm that improvements in Type II hallucination metrics do not translate to a reduction in Type I hallucinations, highlighting their distinct nature \\cite{kaul2024ta7}.\n    *   The study reveals that POPE significantly underestimates Type II hallucinations due to its sampling strategy, and provides results for a more complete evaluation \\cite{kaul2024ta7}.\n    *   The proposed data augmentation method is shown to significantly improve Type I hallucination performance while maintaining or improving Type II performance \\cite{kaul2024ta7}.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations:** The accuracy of THRONE is inherently tied to the performance and potential biases of the underlying external LMs used for AQA, although ensembling is employed to mitigate this \\cite{kaul2024ta7}. The current focus is on object-based hallucinations, which may not cover all forms of factual inaccuracies (e.g., complex relational or attribute errors).\n    *   **Scope of Applicability:** THRONE is specifically designed for evaluating \"Type I\" object hallucinations in free-form, open-ended image descriptions generated by modern instruction-tuned LVLMs. It evaluates against a pre-defined object vocabulary \\cite{kaul2024ta7}.\n\n*   **Technical Significance**\n    *   **Advances State-of-the-Art:** THRONE provides a crucial, accurate, and accessible benchmark for Type I hallucinations, filling a significant gap in LVLM evaluation and advancing the technical state-of-the-art in hallucination assessment \\cite{kaul2024ta7}.\n    *   **Fundamental Insight:** It empirically establishes the critical distinction and often anti-correlated nature of Type I and Type II hallucinations, challenging prior assumptions and guiding future research directions \\cite{kaul2024ta7}.\n    *   **Enables Safer AI:** By offering a robust and reproducible evaluation framework, THRONE directly contributes to the development of more reliable and less hallucinatory LVLMs, which is essential for their responsible deployment in real-world, safety-critical applications \\cite{kaul2024ta7}.\n    *   **Impact on Future Research:** The benchmark and its findings will likely spur new research into targeted mitigation strategies for Type I hallucinations and provide a standardized tool for comparing and improving LVLM performance in this critical area \\cite{kaul2024ta7}.",
        "keywords": [
          "Large Vision-Language Models (LVLMs)",
          "Type I hallucinations",
          "THRONE benchmark",
          "free-form LVLM outputs",
          "LM-based semantic judgment",
          "Abstractive Question Answering (AQA)",
          "ensembling strategy",
          "precision-weighted F0.5-score",
          "object-based hallucinations",
          "data augmentation (visual instruction tuning)",
          "Type II hallucinations",
          "hallucination mitigation",
          "safety-critical applications"
        ],
        "paper_type": "the paper should be classified as **technical**.\n\nhere's why:\n\n1.  **\"propose throne, a novel object-based automatic framework\"**: the abstract explicitly states the proposal of a new system/framework. this is a direct match for the \"technical\" criterion (\"abstract mentions: 'propose', 'develop', 'present', 'algorithm', 'method'\").\n2.  **\"we provide a simple and effective data augmentation method\"**: this is another new method presented in the paper, further supporting the \"technical\" classification.\n3.  **\"this paper proposes a benchmark to evaluate hallucinations...\"**: the introduction immediately reinforces the core contribution as proposing a new benchmark, which is a type of system or method.\n4.  **problem and solution**: the paper clearly identifies a technical problem (lack of benchmarks for type i hallucinations) and proposes a technical solution (throne framework and data augmentation method).\n\nwhile the paper also includes an empirical evaluation (\"by evaluating a large selection of recent lvlms using public datasets, we show that...\"), this evaluation serves to demonstrate the effectiveness and necessity of their *proposed technical solution*. the primary contribution is the creation and presentation of the new benchmark and method, making it fundamentally a technical paper."
      },
      "file_name": "a7f4deb9a1452374330f202bc8d36966a0f254e8.pdf"
    },
    {
      "success": true,
      "doc_id": "83f775d7737edb462930d70ece8751ea",
      "summary": "Here's a focused summary of the paper for a literature review, adhering to your specified structure and citation requirements:\n\n---\n\n### Analysis of \"BEAF: Observing BEfore-AFter Changes to Evaluate Hallucination in Vision-language Models\"\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem:** Vision-language models (VLMs) are highly susceptible to hallucination, where their generated outputs do not accurately reflect the factual content of input images \\cite{yebin2024txh}. This includes incorrectly asserting the presence of objects not depicted in the image.\n    *   **Importance & Challenge:** This hallucination significantly degrades the reliability and credibility of VLMs, hindering genuine communication and eroding user trust \\cite{yebin2024txh}. Addressing this is crucial for improving system trustworthiness, especially in sensitive applications. The challenge lies in developing a comprehensive assessment framework that can deeply explore the underlying causes of hallucination in multi-modal VLMs, which process both visual and textual information.\n\n2.  **Related Work & Positioning**\n    *   **Existing Approaches:** Prior hallucination benchmarks, such as POPE \\cite{yebin2024txh}, CIEM \\cite{yebin2024txh}, and AMBER \\cite{yebin2024txh}, typically adopt a Visual Question Answering (VQA) style. They provide simple question-and-answer (QnA) evaluations, often using discriminative (yes/no, multi-choice) or generative (open-ended caption) question types.\n    *   **Limitations of Previous Solutions:** These prior studies primarily focus on \"text-axis\" evaluation, manipulating only the textual aspects of questions and answers \\cite{yebin2024txh}. This approach is insufficient for a thorough analysis of hallucination in VLMs, which inherently handle both visual and text modalities. It makes it difficult to disentangle the true sources of hallucination, particularly when objects frequently co-occur, preventing a clear assessment of whether VLMs genuinely understand scene information or merely rely on learned associations \\cite{yebin2024txh}.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method:** The paper proposes the BEfore-AFter (BEAF) hallucination evaluation benchmark, which innovatively manipulates visual scene information in addition to augmenting textual questions \\cite{yebin2024txh}.\n        *   **Vision-axis Manipulation:** The core idea involves manipulating visual scenes by selectively removing objects from original images. This is achieved through a three-stage pipeline: automatic object removal using mask extraction (SAM) and inpainting (LaMa), followed by a filtering stage, and finally, human-guided manipulation to refine images and eliminate artifacts \\cite{yebin2024txh}.\n        *   **Change-aware Metrics:** New evaluation metrics are introduced to assess how VLM answers change in response to these visual manipulations.\n    *   **Novelty/Difference:**\n        *   **Multi-modal Evaluation:** BEAF uniquely evaluates VLMs along both vision and text axes, providing a more granular and comprehensive analysis of hallucination compared to previous text-axis-only methods \\cite{yebin2024txh}.\n        *   **Perception of Change:** The benchmark's key innovation is its ability to observe whether VLMs correctly perceive and adapt their answers to explicit visual scene changes (e.g., an object being removed), thereby assessing true understanding rather than mere memorization or statistical correlation \\cite{yebin2024txh}.\n        *   **Novel Metrics:** The introduction of True Understanding (TU), IGnorance (IG), StuBbornness (SB), and InDecision (ID) provides specific, change-aware measures to characterize different facets of VLM hallucinatory behavior \\cite{yebin2024txh}.\n\n4.  **Key Technical Contributions**\n    *   **Novel Benchmark & Dataset:** Introduction of the BEAF benchmark, which includes a dataset of 500 original and 1,727 meticulously manipulated images (derived from MS-COCO), paired with 26,118 image-question pairs, enabling evaluation across both vision and text axes \\cite{yebin2024txh}.\n    *   **Novel Change-aware Metrics:** Development of four new metrics for detailed hallucination evaluation:\n        *   **True Understanding (TU):** Measures if a model correctly answers for a removed object both before (present) and after (absent) manipulation \\cite{yebin2024txh}.\n        *   **IGnorance (IG):** Quantifies instances where models consistently provide incorrect answers regarding removed objects \\cite{yebin2024txh}.\n        *   **StuBbornness (SB):** Identifies cases where models give the same answer despite a visual change that should alter the ground truth for removed objects \\cite{yebin2024txh}.\n        *   **InDecision (ID):** Captures situations where answers change even when the target object remains unchanged or was never present in the image \\cite{yebin2024txh}.\n    *   **Robust Image Manipulation Pipeline:** A three-stage pipeline for creating high-quality manipulated images, combining automatic object removal (using SAM for masks and LaMa for inpainting) with human filtering and refinement to ensure artifact-free results \\cite{yebin2024txh}.\n    *   **Visualization of Object Relationships:** The framework facilitates visualizing the impact of relationships between objects within hallucinatory images based on the BEAF evaluation results \\cite{yebin2024txh}.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted:** The paper evaluates various Vision-Language Models (VLMs) using the curated BEAF dataset and the newly proposed change-aware metrics \\cite{yebin2024txh}.\n    *   **Key Performance Metrics and Comparison Results:**\n        *   Evaluation utilizes the novel metrics (TU, IG, SB, ID) alongside traditional metrics like accuracy, precision, recall, and F1 score \\cite{yebin2024txh}.\n        *   **Key Findings:** The BEAF metrics reveal distinct aspects of VLM hallucination that were not previously reported or detectable by existing text-axis-only evaluation methods \\cite{yebin2024txh}. Notably, outcomes previously considered non-hallucinatory by prior benchmarks are shown to potentially be hallucinations when assessed with the BEAF framework \\cite{yebin2024txh}.\n        *   The study also includes an analysis of the influence of individual objects within a scene and a discussion on open-generation answers and CLIPScore results \\cite{yebin2024txh}.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions:** While not explicitly detailed as limitations in the provided text, the reliance on a meticulous human-guided manipulation stage for dataset creation implies that fully automated, high-fidelity object removal without any detectable artifacts remains a significant challenge \\cite{yebin2024txh}. The dataset's foundation on MS-COCO might also limit its generalizability to broader or more complex real-world visual scenarios.\n    *   **Scope of Applicability:** The BEAF benchmark is specifically designed for a granular evaluation of hallucination in Vision-Language Models (VLMs) by assessing their ability to perceive and respond to explicit visual scene changes \\cite{yebin2024txh}. It is particularly applicable to models that integrate both visual and textual inputs and aims to provide a deeper understanding of their multi-modal reasoning and perception capabilities.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art:** BEAF significantly advances the technical state-of-the-art in VLM hallucination evaluation by introducing a multi-modal assessment framework that incorporates vision-axis manipulations, moving beyond the limitations of text-only evaluations \\cite{yebin2024txh}. This provides a more comprehensive and nuanced understanding of VLM behavior, uncovering previously undetected forms of hallucination.\n    *   **Potential Impact on Future Research:**\n        *   **Improved VLM Development:** The benchmark and its novel metrics offer a more robust diagnostic tool for identifying and understanding VLM weaknesses, thereby guiding the development of more reliable, trustworthy, and factually grounded multi-modal models \\cite{yebin2024txh}.\n        *   **Deeper Hallucination Analysis:** It enables researchers to more effectively disentangle the sources of hallucination by directly observing how models react to explicit visual changes, fostering targeted research into mitigation strategies \\cite{yebin2024txh}.\n        *   **New Research Directions:** The ability to visualize object relationships and the granular evaluation metrics can inspire new research avenues into how VLMs process multi-modal information, how internal knowledge biases their responses, and how to build models with true visual understanding \\cite{yebin2024txh}.\n\n---",
      "intriguing_abstract": "Vision-language models (VLMs) are plagued by pervasive hallucination, where generated content misrepresents visual facts, severely eroding trust and hindering reliable communication. Existing evaluation benchmarks, limited to text-axis manipulations, fail to thoroughly diagnose the multi-modal roots of this critical problem. We introduce BEAF (BEfore-AFter), a novel benchmark that revolutionizes hallucination evaluation by innovatively manipulating *visual scene information*.\n\nBEAF meticulously creates \"before-after\" image pairs by automatically removing objects using advanced techniques like SAM and LaMa inpainting, followed by human refinement. This unique vision-axis manipulation allows us to directly observe how VLM outputs change in response to explicit visual alterations. We propose four novel, change-aware metricsâ€”True Understanding (TU), IGnorance (IG), StuBbornness (SB), and InDecision (ID)â€”to precisely characterize distinct facets of hallucinatory behavior. Our multi-modal framework provides unprecedented depth, revealing forms of hallucination previously undetectable by text-only methods. BEAF offers critical insights into VLM weaknesses, guiding the development of more trustworthy and factually grounded models with genuine visual understanding.",
      "keywords": [
        "Vision-language models (VLMs)",
        "VLM hallucination",
        "BEAF benchmark",
        "multi-modal evaluation",
        "vision-axis manipulation",
        "object removal and inpainting",
        "change-aware metrics",
        "True Understanding (TU)",
        "IGnorance (IG)",
        "StuBbornness (SB)",
        "InDecision (ID)",
        "perception of visual change",
        "VLM trustworthiness"
      ],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/7bcd5c0b17560ee560aec903ea42487a1a54e5d9.pdf",
      "citation_key": "yebin2024txh",
      "metadata": {
        "title": "BEAF: Observing BEfore-AFter Changes to Evaluate Hallucination in Vision-language Models",
        "authors": [
          "Moon Ye-Bin",
          "Nam Hyeon-Woo",
          "Wonseok Choi",
          "Tae-Hyun Oh"
        ],
        "published_date": "2024",
        "abstract": "Vision language models (VLMs) perceive the world through a combination of a visual encoder and a large language model (LLM). The visual encoder, pre-trained on large-scale vision-text datasets, provides zero-shot generalization to visual data, and the LLM endows its high reasoning ability to VLMs. It leads VLMs to achieve high performance on wide benchmarks without fine-tuning, exhibiting zero or few-shot capability. However, recent studies show that VLMs are vulnerable to hallucination. This undesirable behavior degrades reliability and credibility, thereby making users unable to fully trust the output from VLMs. To enhance trustworthiness and better tackle the hallucination of VLMs, we curate a new evaluation dataset, called the BEfore-AFter hallucination dataset (BEAF), and introduce new metrics: True Understanding (TU), IGnorance (IG), StuBbornness (SB), and InDecision (ID). Unlike prior works that focus only on constructing questions and answers, the key idea of our benchmark is to manipulate visual scene information by image editing models and to design the metrics based on scene changes. This allows us to clearly assess whether VLMs correctly understand a given scene by observing the ability to perceive changes. We also visualize image-wise object relationship by virtue of our two-axis view: vision and text. Upon evaluating VLMs with our dataset, we observed that our metrics reveal different aspects of VLM hallucination that have not been reported before. Project page: \\url{https://beafbench.github.io/}",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/7bcd5c0b17560ee560aec903ea42487a1a54e5d9.pdf",
        "venue": "European Conference on Computer Vision",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the paper for a literature review, adhering to your specified structure and citation requirements:\n\n---\n\n### Analysis of \"BEAF: Observing BEfore-AFter Changes to Evaluate Hallucination in Vision-language Models\"\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem:** Vision-language models (VLMs) are highly susceptible to hallucination, where their generated outputs do not accurately reflect the factual content of input images \\cite{yebin2024txh}. This includes incorrectly asserting the presence of objects not depicted in the image.\n    *   **Importance & Challenge:** This hallucination significantly degrades the reliability and credibility of VLMs, hindering genuine communication and eroding user trust \\cite{yebin2024txh}. Addressing this is crucial for improving system trustworthiness, especially in sensitive applications. The challenge lies in developing a comprehensive assessment framework that can deeply explore the underlying causes of hallucination in multi-modal VLMs, which process both visual and textual information.\n\n2.  **Related Work & Positioning**\n    *   **Existing Approaches:** Prior hallucination benchmarks, such as POPE \\cite{yebin2024txh}, CIEM \\cite{yebin2024txh}, and AMBER \\cite{yebin2024txh}, typically adopt a Visual Question Answering (VQA) style. They provide simple question-and-answer (QnA) evaluations, often using discriminative (yes/no, multi-choice) or generative (open-ended caption) question types.\n    *   **Limitations of Previous Solutions:** These prior studies primarily focus on \"text-axis\" evaluation, manipulating only the textual aspects of questions and answers \\cite{yebin2024txh}. This approach is insufficient for a thorough analysis of hallucination in VLMs, which inherently handle both visual and text modalities. It makes it difficult to disentangle the true sources of hallucination, particularly when objects frequently co-occur, preventing a clear assessment of whether VLMs genuinely understand scene information or merely rely on learned associations \\cite{yebin2024txh}.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method:** The paper proposes the BEfore-AFter (BEAF) hallucination evaluation benchmark, which innovatively manipulates visual scene information in addition to augmenting textual questions \\cite{yebin2024txh}.\n        *   **Vision-axis Manipulation:** The core idea involves manipulating visual scenes by selectively removing objects from original images. This is achieved through a three-stage pipeline: automatic object removal using mask extraction (SAM) and inpainting (LaMa), followed by a filtering stage, and finally, human-guided manipulation to refine images and eliminate artifacts \\cite{yebin2024txh}.\n        *   **Change-aware Metrics:** New evaluation metrics are introduced to assess how VLM answers change in response to these visual manipulations.\n    *   **Novelty/Difference:**\n        *   **Multi-modal Evaluation:** BEAF uniquely evaluates VLMs along both vision and text axes, providing a more granular and comprehensive analysis of hallucination compared to previous text-axis-only methods \\cite{yebin2024txh}.\n        *   **Perception of Change:** The benchmark's key innovation is its ability to observe whether VLMs correctly perceive and adapt their answers to explicit visual scene changes (e.g., an object being removed), thereby assessing true understanding rather than mere memorization or statistical correlation \\cite{yebin2024txh}.\n        *   **Novel Metrics:** The introduction of True Understanding (TU), IGnorance (IG), StuBbornness (SB), and InDecision (ID) provides specific, change-aware measures to characterize different facets of VLM hallucinatory behavior \\cite{yebin2024txh}.\n\n4.  **Key Technical Contributions**\n    *   **Novel Benchmark & Dataset:** Introduction of the BEAF benchmark, which includes a dataset of 500 original and 1,727 meticulously manipulated images (derived from MS-COCO), paired with 26,118 image-question pairs, enabling evaluation across both vision and text axes \\cite{yebin2024txh}.\n    *   **Novel Change-aware Metrics:** Development of four new metrics for detailed hallucination evaluation:\n        *   **True Understanding (TU):** Measures if a model correctly answers for a removed object both before (present) and after (absent) manipulation \\cite{yebin2024txh}.\n        *   **IGnorance (IG):** Quantifies instances where models consistently provide incorrect answers regarding removed objects \\cite{yebin2024txh}.\n        *   **StuBbornness (SB):** Identifies cases where models give the same answer despite a visual change that should alter the ground truth for removed objects \\cite{yebin2024txh}.\n        *   **InDecision (ID):** Captures situations where answers change even when the target object remains unchanged or was never present in the image \\cite{yebin2024txh}.\n    *   **Robust Image Manipulation Pipeline:** A three-stage pipeline for creating high-quality manipulated images, combining automatic object removal (using SAM for masks and LaMa for inpainting) with human filtering and refinement to ensure artifact-free results \\cite{yebin2024txh}.\n    *   **Visualization of Object Relationships:** The framework facilitates visualizing the impact of relationships between objects within hallucinatory images based on the BEAF evaluation results \\cite{yebin2024txh}.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted:** The paper evaluates various Vision-Language Models (VLMs) using the curated BEAF dataset and the newly proposed change-aware metrics \\cite{yebin2024txh}.\n    *   **Key Performance Metrics and Comparison Results:**\n        *   Evaluation utilizes the novel metrics (TU, IG, SB, ID) alongside traditional metrics like accuracy, precision, recall, and F1 score \\cite{yebin2024txh}.\n        *   **Key Findings:** The BEAF metrics reveal distinct aspects of VLM hallucination that were not previously reported or detectable by existing text-axis-only evaluation methods \\cite{yebin2024txh}. Notably, outcomes previously considered non-hallucinatory by prior benchmarks are shown to potentially be hallucinations when assessed with the BEAF framework \\cite{yebin2024txh}.\n        *   The study also includes an analysis of the influence of individual objects within a scene and a discussion on open-generation answers and CLIPScore results \\cite{yebin2024txh}.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions:** While not explicitly detailed as limitations in the provided text, the reliance on a meticulous human-guided manipulation stage for dataset creation implies that fully automated, high-fidelity object removal without any detectable artifacts remains a significant challenge \\cite{yebin2024txh}. The dataset's foundation on MS-COCO might also limit its generalizability to broader or more complex real-world visual scenarios.\n    *   **Scope of Applicability:** The BEAF benchmark is specifically designed for a granular evaluation of hallucination in Vision-Language Models (VLMs) by assessing their ability to perceive and respond to explicit visual scene changes \\cite{yebin2024txh}. It is particularly applicable to models that integrate both visual and textual inputs and aims to provide a deeper understanding of their multi-modal reasoning and perception capabilities.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art:** BEAF significantly advances the technical state-of-the-art in VLM hallucination evaluation by introducing a multi-modal assessment framework that incorporates vision-axis manipulations, moving beyond the limitations of text-only evaluations \\cite{yebin2024txh}. This provides a more comprehensive and nuanced understanding of VLM behavior, uncovering previously undetected forms of hallucination.\n    *   **Potential Impact on Future Research:**\n        *   **Improved VLM Development:** The benchmark and its novel metrics offer a more robust diagnostic tool for identifying and understanding VLM weaknesses, thereby guiding the development of more reliable, trustworthy, and factually grounded multi-modal models \\cite{yebin2024txh}.\n        *   **Deeper Hallucination Analysis:** It enables researchers to more effectively disentangle the sources of hallucination by directly observing how models react to explicit visual changes, fostering targeted research into mitigation strategies \\cite{yebin2024txh}.\n        *   **New Research Directions:** The ability to visualize object relationships and the granular evaluation metrics can inspire new research avenues into how VLMs process multi-modal information, how internal knowledge biases their responses, and how to build models with true visual understanding \\cite{yebin2024txh}.\n\n---",
        "keywords": [
          "Vision-language models (VLMs)",
          "VLM hallucination",
          "BEAF benchmark",
          "multi-modal evaluation",
          "vision-axis manipulation",
          "object removal and inpainting",
          "change-aware metrics",
          "True Understanding (TU)",
          "IGnorance (IG)",
          "StuBbornness (SB)",
          "InDecision (ID)",
          "perception of visual change",
          "VLM trustworthiness"
        ],
        "paper_type": "the paper presents a new evaluation dataset (beaf) and new metrics (tu, ig, sb, id) specifically designed to assess hallucination in vision-language models (vlms). it describes a novel methodology for this evaluation, which involves manipulating visual scene information using image editing models and designing metrics based on these scene changes. while the paper also reports observations from evaluating vlms with this new dataset, the primary contribution highlighted in the abstract and introduction is the **development and presentation of this new evaluation system and method**.\n\nthis aligns best with the **technical** classification criteria:\n*   **technical** - presents new methods, algorithms, or systems\n    *   abstract mentions: \"we curate a new evaluation dataset, called the before-after hallucination dataset (beaf), and introduce new metrics\", \"the key idea of our benchmark is to manipulate visual scene information by image editing models and to design the metrics based on scene changes.\"\n    *   introduction discusses: the problem of vlm hallucination and the proposed solution (implicitly, the new benchmark and metrics).\n\nwhile the paper does involve an empirical study (\"upon evaluating vlms with our dataset, we observed...\"), the core novelty and contribution described is the *creation* of the tools (dataset, metrics, methodology) for this study. papers that introduce new benchmarks or evaluation frameworks are typically classified as technical.\n\n**classification:** technical"
      },
      "file_name": "7bcd5c0b17560ee560aec903ea42487a1a54e5d9.pdf"
    },
    {
      "success": true,
      "doc_id": "982764f75827233f76146d2eb85442b8",
      "summary": "Here's a focused summary of the paper for a literature review, emphasizing technical innovations and empirical validation:\n\n### Analysis of \"Mitigating Object Hallucination in Large Vision-Language Models via Image-Grounded Guidance\" \\cite{zhao2024ge8}\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Large Vision-Language Models (LVLMs) frequently suffer from \"object hallucination,\" where they generate descriptions of non-existent objects in an image.\n    *   **Importance & Challenge**: This problem compromises model accuracy and reliability, making LVLMs unsuitable for safety-critical applications (e.g., medical imaging). Existing solutions often require costly training/fine-tuning, extensive human annotation, or reliance on proprietary LLM APIs for post-generation correction, which are often infeasible, expensive, and raise privacy concerns. Furthermore, these prior methods often do not address the intrinsic causes of hallucination within LVLMs.\n\n*   **Related Work & Positioning**\n    *   **Existing Approaches**: Previous efforts include fine-tuning LVLMs with specially curated datasets (e.g., Liu et al., 2023a;b), leveraging powerful LLMs like GPT-4 for post-processing (e.g., Yin et al., 2023), Visual Contrastive Decoding (VCD) which penalizes logit outputs of corrupted images (Leng et al., 2023), and enhanced beam-search decoding with over-trust penalties (OPERA) (Huang et al., 2023a). HALC (Chen et al., 2024) uses adaptive focal-contrast decoding, and BRAVE (Kar et al., 2024) combines features from multiple vision encoders.\n    *   **Limitations of Previous Solutions**:\n        *   **Costly & Infeasible**: Fine-tuning requires significant computational resources and human annotation for high-quality datasets.\n        *   **API Dependence & Privacy**: GPT-assisted methods are expensive and raise privacy concerns.\n        *   **Intrinsic Causes Unaddressed**: Many approaches do not tackle the root causes of hallucination, which stem from insufficient visual context from encoders or information distortion during vision-to-text projection.\n        *   **Computational Overhead**: Some methods like HALC use computationally intensive beam search.\n    *   **Positioning**: \\cite{zhao2024ge8} introduces MARINE as a novel, training-free, and API-free framework that addresses object hallucination during inference by targeting its intrinsic causes (deficiencies in visual encoding and cross-modal information distortion) through image-grounded guidance.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method**: MARINE (Mitigating hallucin Ation via image-g Rounded guIdaNcE) introduces image-grounded guidance to LVLMs during inference. It operates in two main steps:\n        1.  **Visual Guidance from Image-Grounded Features**: Leverages open-source, specialized vision models (e.g., object detection models like DETR and RAM++) to extract detailed, object-level visual information from the input image. This information is aggregated (potentially by a small language model or rule-based algorithm) and translated into a textual \"guidance prompt\" (e.g., \"focusing on the visible objects in this image: [aggregated object info]\").\n        2.  **Guided Text Generation**: Integrates this guidance prompt (`c`) into the LVLM's generation process using a classifier-free guidance mechanism in the logit space. The output token `yt` is sampled from a linearly combined logit space: `log p_guided(yt) = Î³ * log p_conditional(yt) + (1 - Î³) * log p_unconditional(yt)`. Here, `p_conditional` is the generation conditioned on the original visual tokens, textual prompt, *and* the guidance prompt `c`, while `p_unconditional` is conditioned only on the original visual tokens and textual prompt.\n    *   **Novelty & Differentiation**:\n        *   **Training-Free & API-Free**: Unlike most prior work, MARINE requires no additional training or fine-tuning of the LVLM and no access to proprietary LLM APIs.\n        *   **Addresses Intrinsic Causes**: Directly tackles hallucination arising from insufficient visual context and information distortion by enriching the visual input with fine-grained, externally derived object information.\n        *   **Flexible Integration**: Allows for the integration and aggregation of multiple open-source vision models, enhancing robustness and reliability.\n        *   **Inference-Time Control**: Applies guidance directly during the decoding process, offering real-time control over generation.\n\n*   **Key Technical Contributions**\n    *   **Novel Framework (MARINE)**: A universal, training-free, and API-free framework for mitigating object hallucination in LVLMs.\n    *   **Image-Grounded Guidance Mechanism**: Introduces a method to leverage external, specialized vision models to provide detailed object-level information, which is then converted into textual guidance.\n    *   **Logit-Space Guidance for LVLMs**: Adapts classifier-free guidance to the multi-modal context of LVLMs, allowing for controllable generation by balancing original LVLM output with image-grounded guidance.\n    *   **Aggregation of Multiple Vision Models**: Demonstrates the ability to aggregate information from diverse vision models (e.g., DETR, RAM++) for more robust guidance.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted**: Comprehensive evaluations were performed across 5 popular LVLMs (LLaVA, LLaVA-v1.5, MiniGPT-v2, mPLUG-Owl2, InstructBLIP) using guidance from DETR and RAM++.\n    *   **Benchmarks & Metrics**:\n        *   **Image Captioning**: CHAIR-I (instance-level hallucination), CHAIR-S (sentence-level hallucination), and Recall (inclusion of existing objects) on MSCOCO.\n        *   **Visual Question Answering (VQA)**: POPE (Polling-based Object Probing Evaluation) in adversarial settings (accuracy, F1, \"yes\" answer proportion) on LLaVA-QA90, A-OKVQA, and GQA.\n        *   **Qualitative Evaluation**: GPT-4V-aided evaluation for overall quality and hallucination assessment.\n    *   **Key Performance Metrics & Comparison Results**:\n        *   **Hallucination Reduction**: MARINE consistently and significantly reduces object hallucinations across all evaluated LVLMs and benchmarks, as measured by CHAIR-I, CHAIR-S, and POPE.\n        *   **Outperforms Baselines**: Achieves superior hallucination mitigation compared to state-of-the-art methods, including existing fine-tuning-based approaches.\n        *   **Maintains Detailedness**: While reducing hallucination, MARINE effectively maintains the detailedness and overall performance of LVLMs' generations (e.g., high Recall scores).\n        *   **Efficiency**: Provides a favorable trade-off between latency and accuracy, demonstrating the lowest computational overhead compared to existing baselines, making it a practical and scalable solution.\n        *   **GPT-4V Evaluation**: Consistently reduces hallucinations in GPT-4V-assisted evaluations.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations**: While addressing visual encoder and cross-modal projection issues, MARINE does not directly tackle \"inherent hallucinations common in general language models\" (i.e., language-prior-driven hallucinations). The effectiveness is dependent on the quality and coverage of the external vision models used for guidance.\n    *   **Scope of Applicability**: Primarily designed for inference-time mitigation of object hallucination in LVLMs. It is training-free and API-free, making it broadly applicable to existing LVLMs without requiring architectural changes or retraining.\n\n*   **Technical Significance**\n    *   **Advances State-of-the-Art**: \\cite{zhao2024ge8} presents the first training-free and API-free framework that effectively mitigates object hallucination in LVLMs during inference, directly addressing intrinsic causes related to visual information processing.\n    *   **Practical & Scalable Solution**: Its low computational overhead and independence from costly training or proprietary APIs make it a highly practical and scalable solution for real-world LVLM deployments.\n    *   **Impact on Future Research**: Opens new avenues for research into inference-time guidance mechanisms for multimodal models, potentially inspiring similar approaches to address other LVLM deficiencies without extensive retraining. It enhances the reliability and trustworthiness of LVLMs, paving the way for their safer adoption in critical applications.",
      "intriguing_abstract": "Large Vision-Language Models (LVLMs) are transforming AI, yet their pervasive \"object hallucination\"â€”the generation of descriptions for non-existent objectsâ€”critically undermines their reliability, particularly in safety-critical applications. Existing mitigation strategies are often computationally intensive, demand extensive fine-tuning, or rely on costly, privacy-sensitive proprietary APIs, frequently failing to address the intrinsic causes of this visual disconnect.\n\nWe introduce MARINE (Mitigating hallucin Ation via image-g Rounded guIdaNcE), a groundbreaking, training-free, and API-free framework that directly tackles object hallucination during inference. MARINE leverages specialized open-source vision models to extract fine-grained, image-grounded object features, transforming them into dynamic textual guidance. This guidance is then seamlessly integrated into the LVLM's logit space using a novel classifier-free guidance mechanism, steering generation towards visually verifiable content.\n\nOur comprehensive evaluations across five prominent LVLMs demonstrate MARINE's superior ability to significantly reduce object hallucination (measured by CHAIR-I, CHAIR-S, and POPE) while preserving descriptive richness and achieving unparalleled efficiency. MARINE represents a critical step towards trustworthy LVLMs, unlocking their potential for robust and reliable deployment in real-world applications.",
      "keywords": [
        "Large Vision-Language Models (LVLMs)",
        "object hallucination",
        "MARINE framework",
        "image-grounded guidance",
        "training-free",
        "API-free",
        "inference-time mitigation",
        "classifier-free logit-space guidance",
        "visual context deficiencies",
        "cross-modal information distortion",
        "object detection models",
        "hallucination reduction",
        "image captioning",
        "Visual Question Answering",
        "safety-critical applications"
      ],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/8ff45750057cc9452ae09aef6b9dfee3bd84b083.pdf",
      "citation_key": "zhao2024ge8",
      "metadata": {
        "title": "Mitigating Object Hallucination in Large Vision-Language Models via Image-Grounded Guidance",
        "authors": [
          "Linxi Zhao",
          "Yihe Deng",
          "Weitong Zhang",
          "Quanquan Gu"
        ],
        "published_date": "2024",
        "abstract": "The advancement of Large Vision-Language Models (LVLMs) has increasingly highlighted the critical issue of their tendency to hallucinate non-existing objects in the images. To address this issue, previous works focused on using specially curated datasets or powerful LLMs to rectify the outputs of LVLMs. However, these approaches require either costly training or fine-tuning, or API access to proprietary LLMs for post-generation correction. In response to these limitations, we propose Mitigating hallucinAtion via image-gRounded guIdaNcE (MARINE), a framework that is both training-free and API-free. MARINE effectively and efficiently reduces object hallucinations during inference by introducing image-grounded guidance to LVLMs. This is achieved by leveraging open-source vision models to extract object-level information, thereby enhancing the precision of LVLM-generated content. Our framework's flexibility further allows for the integration of multiple vision models, enabling more reliable and robust object-level guidance. Through comprehensive evaluations across 5 popular LVLMs with diverse evaluation metrics and benchmarks, we demonstrate the effectiveness of MARINE, which even outperforms existing fine-tuning-based methods. Remarkably, it reduces hallucinations consistently in GPT-4V-assisted evaluation while maintaining the detailedness of LVLMs' generations. We release our code at https://github.com/Linxi-ZHAO/MARINE.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/8ff45750057cc9452ae09aef6b9dfee3bd84b083.pdf",
        "venue": "",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the paper for a literature review, emphasizing technical innovations and empirical validation:\n\n### Analysis of \"Mitigating Object Hallucination in Large Vision-Language Models via Image-Grounded Guidance\" \\cite{zhao2024ge8}\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Large Vision-Language Models (LVLMs) frequently suffer from \"object hallucination,\" where they generate descriptions of non-existent objects in an image.\n    *   **Importance & Challenge**: This problem compromises model accuracy and reliability, making LVLMs unsuitable for safety-critical applications (e.g., medical imaging). Existing solutions often require costly training/fine-tuning, extensive human annotation, or reliance on proprietary LLM APIs for post-generation correction, which are often infeasible, expensive, and raise privacy concerns. Furthermore, these prior methods often do not address the intrinsic causes of hallucination within LVLMs.\n\n*   **Related Work & Positioning**\n    *   **Existing Approaches**: Previous efforts include fine-tuning LVLMs with specially curated datasets (e.g., Liu et al., 2023a;b), leveraging powerful LLMs like GPT-4 for post-processing (e.g., Yin et al., 2023), Visual Contrastive Decoding (VCD) which penalizes logit outputs of corrupted images (Leng et al., 2023), and enhanced beam-search decoding with over-trust penalties (OPERA) (Huang et al., 2023a). HALC (Chen et al., 2024) uses adaptive focal-contrast decoding, and BRAVE (Kar et al., 2024) combines features from multiple vision encoders.\n    *   **Limitations of Previous Solutions**:\n        *   **Costly & Infeasible**: Fine-tuning requires significant computational resources and human annotation for high-quality datasets.\n        *   **API Dependence & Privacy**: GPT-assisted methods are expensive and raise privacy concerns.\n        *   **Intrinsic Causes Unaddressed**: Many approaches do not tackle the root causes of hallucination, which stem from insufficient visual context from encoders or information distortion during vision-to-text projection.\n        *   **Computational Overhead**: Some methods like HALC use computationally intensive beam search.\n    *   **Positioning**: \\cite{zhao2024ge8} introduces MARINE as a novel, training-free, and API-free framework that addresses object hallucination during inference by targeting its intrinsic causes (deficiencies in visual encoding and cross-modal information distortion) through image-grounded guidance.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method**: MARINE (Mitigating hallucin Ation via image-g Rounded guIdaNcE) introduces image-grounded guidance to LVLMs during inference. It operates in two main steps:\n        1.  **Visual Guidance from Image-Grounded Features**: Leverages open-source, specialized vision models (e.g., object detection models like DETR and RAM++) to extract detailed, object-level visual information from the input image. This information is aggregated (potentially by a small language model or rule-based algorithm) and translated into a textual \"guidance prompt\" (e.g., \"focusing on the visible objects in this image: [aggregated object info]\").\n        2.  **Guided Text Generation**: Integrates this guidance prompt (`c`) into the LVLM's generation process using a classifier-free guidance mechanism in the logit space. The output token `yt` is sampled from a linearly combined logit space: `log p_guided(yt) = Î³ * log p_conditional(yt) + (1 - Î³) * log p_unconditional(yt)`. Here, `p_conditional` is the generation conditioned on the original visual tokens, textual prompt, *and* the guidance prompt `c`, while `p_unconditional` is conditioned only on the original visual tokens and textual prompt.\n    *   **Novelty & Differentiation**:\n        *   **Training-Free & API-Free**: Unlike most prior work, MARINE requires no additional training or fine-tuning of the LVLM and no access to proprietary LLM APIs.\n        *   **Addresses Intrinsic Causes**: Directly tackles hallucination arising from insufficient visual context and information distortion by enriching the visual input with fine-grained, externally derived object information.\n        *   **Flexible Integration**: Allows for the integration and aggregation of multiple open-source vision models, enhancing robustness and reliability.\n        *   **Inference-Time Control**: Applies guidance directly during the decoding process, offering real-time control over generation.\n\n*   **Key Technical Contributions**\n    *   **Novel Framework (MARINE)**: A universal, training-free, and API-free framework for mitigating object hallucination in LVLMs.\n    *   **Image-Grounded Guidance Mechanism**: Introduces a method to leverage external, specialized vision models to provide detailed object-level information, which is then converted into textual guidance.\n    *   **Logit-Space Guidance for LVLMs**: Adapts classifier-free guidance to the multi-modal context of LVLMs, allowing for controllable generation by balancing original LVLM output with image-grounded guidance.\n    *   **Aggregation of Multiple Vision Models**: Demonstrates the ability to aggregate information from diverse vision models (e.g., DETR, RAM++) for more robust guidance.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted**: Comprehensive evaluations were performed across 5 popular LVLMs (LLaVA, LLaVA-v1.5, MiniGPT-v2, mPLUG-Owl2, InstructBLIP) using guidance from DETR and RAM++.\n    *   **Benchmarks & Metrics**:\n        *   **Image Captioning**: CHAIR-I (instance-level hallucination), CHAIR-S (sentence-level hallucination), and Recall (inclusion of existing objects) on MSCOCO.\n        *   **Visual Question Answering (VQA)**: POPE (Polling-based Object Probing Evaluation) in adversarial settings (accuracy, F1, \"yes\" answer proportion) on LLaVA-QA90, A-OKVQA, and GQA.\n        *   **Qualitative Evaluation**: GPT-4V-aided evaluation for overall quality and hallucination assessment.\n    *   **Key Performance Metrics & Comparison Results**:\n        *   **Hallucination Reduction**: MARINE consistently and significantly reduces object hallucinations across all evaluated LVLMs and benchmarks, as measured by CHAIR-I, CHAIR-S, and POPE.\n        *   **Outperforms Baselines**: Achieves superior hallucination mitigation compared to state-of-the-art methods, including existing fine-tuning-based approaches.\n        *   **Maintains Detailedness**: While reducing hallucination, MARINE effectively maintains the detailedness and overall performance of LVLMs' generations (e.g., high Recall scores).\n        *   **Efficiency**: Provides a favorable trade-off between latency and accuracy, demonstrating the lowest computational overhead compared to existing baselines, making it a practical and scalable solution.\n        *   **GPT-4V Evaluation**: Consistently reduces hallucinations in GPT-4V-assisted evaluations.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations**: While addressing visual encoder and cross-modal projection issues, MARINE does not directly tackle \"inherent hallucinations common in general language models\" (i.e., language-prior-driven hallucinations). The effectiveness is dependent on the quality and coverage of the external vision models used for guidance.\n    *   **Scope of Applicability**: Primarily designed for inference-time mitigation of object hallucination in LVLMs. It is training-free and API-free, making it broadly applicable to existing LVLMs without requiring architectural changes or retraining.\n\n*   **Technical Significance**\n    *   **Advances State-of-the-Art**: \\cite{zhao2024ge8} presents the first training-free and API-free framework that effectively mitigates object hallucination in LVLMs during inference, directly addressing intrinsic causes related to visual information processing.\n    *   **Practical & Scalable Solution**: Its low computational overhead and independence from costly training or proprietary APIs make it a highly practical and scalable solution for real-world LVLM deployments.\n    *   **Impact on Future Research**: Opens new avenues for research into inference-time guidance mechanisms for multimodal models, potentially inspiring similar approaches to address other LVLM deficiencies without extensive retraining. It enhances the reliability and trustworthiness of LVLMs, paving the way for their safer adoption in critical applications.",
        "keywords": [
          "Large Vision-Language Models (LVLMs)",
          "object hallucination",
          "MARINE framework",
          "image-grounded guidance",
          "training-free",
          "API-free",
          "inference-time mitigation",
          "classifier-free logit-space guidance",
          "visual context deficiencies",
          "cross-modal information distortion",
          "object detection models",
          "hallucination reduction",
          "image captioning",
          "Visual Question Answering",
          "safety-critical applications"
        ],
        "paper_type": "the paper should be classified as **technical**.\n\nhere's why:\n\n1.  **proposes a new method/framework:** the abstract explicitly states, \"we propose mitigating hallucin ation via image-g rounded gu idance (marine), a framework that is both training-free and api-free.\" this directly aligns with the \"technical\" criterion: \"presents new methods, algorithms, or systems.\"\n2.  **describes the solution:** the abstract details *how* marine works (\"introducing image-grounded guidance,\" \"leveraging open-source vision models to extract object-level information\"). this further supports the \"technical\" classification, as it discusses the \"proposed solution.\"\n3.  **empirical evaluation supports the technical contribution:** while the paper also includes \"comprehensive evaluations across 5 popular lvlms with diverse evaluation metrics and benchmarks,\" and \"demonstrate the effectiveness of marine,\" these empirical findings serve to validate the effectiveness of the *new technical framework* being proposed. the primary contribution is the framework itself, with the experiments providing evidence for its utility."
      },
      "file_name": "8ff45750057cc9452ae09aef6b9dfee3bd84b083.pdf"
    },
    {
      "success": true,
      "doc_id": "f1b330b85ee4e56b07f0c8aacd7ee604",
      "summary": "Here's a focused summary of the paper \"Cognitive Mirage: A Review of Hallucinations in Large Language Models\" \\cite{ye2023yom} for a literature review:\n\n---\n\n*   **1. Research Problem & Motivation**\n    *   **Specific Technical Problem**: The paper addresses the pervasive and critical issue of \"hallucinations\" in Large Language Models (LLMs), where models generate text that is fluent and grammatically correct but factually incorrect, irrelevant, or unfaithful to source inputs.\n    *   **Importance and Challenge**: This problem is crucial because LLMs are increasingly used as knowledge repositories, and hallucinations severely undermine their reliability and trustworthiness. Challenges arise from:\n        *   **Memory Distortion**: Generalization of knowledge during pre-training can lead to inaccuracies.\n        *   **Knowledge Representation Limits**: Constraints of model scale and difficulty in addressing long-tailed knowledge problems.\n        *   **Data Timeliness/Privacy**: Difficulty in maintaining comprehensive and up-to-date factual understanding due to dynamic real-world data.\n        *   **High-Stakes Applications**: Hallucinations pose significant risks in knowledge-intensive fields like medical, financial, and legal applications where accuracy is paramount.\n\n*   **2. Related Work & Positioning**\n    *   **Relation to Existing Approaches**: This work is a comprehensive survey that synthesizes recent progress in understanding and mitigating hallucinations in LLMs.\n    *   **Limitations of Previous Solutions**: Existing surveys are nascent and often limited in scope:\n        *   Some focus on early works in natural language generation (NLG) or task-specific progress \\cite{ye2023yom}.\n        *   Others cover specific aspects like collecting high-quality instructions for LLM alignment or self-correction methods \\cite{ye2023yom}.\n        *   While benchmarks exist to evaluate factual responses, they are scattered across various tasks and have not been systematically reviewed and analyzed \\cite{ye2023yom}.\n    *   **Positioning**: This paper aims to provide a systematic and comprehensive understanding of methodologies, compare different approaches, and inspire new research directions, differentiating itself through its broad coverage and structured analysis.\n\n*   **3. Technical Approach & Innovation**\n    *   **Core Technical Method**: As a review paper, the core \"method\" is a systematic and comprehensive literature survey and analysis. It proposes a novel framework for understanding hallucinations.\n    *   **Novelty/Difference**: The approach is novel due to its structured organization and depth of analysis:\n        *   **Novel Taxonomy**: Presents a new, detailed taxonomy of hallucinations categorized by various text generation tasks (e.g., Machine Translation, QA, Dialog, Summarization, Knowledge Graph, Cross-modal systems) \\cite{ye2023yom}.\n        *   **Theoretical Insights**: Provides in-depth theoretical and mechanistic analyses of hallucination origins, attributing them to three primary factors: data collection, knowledge gaps, and the optimization process \\cite{ye2023yom}.\n        *   **Comprehensive Coverage**: Systematically reviews a wide range of emerging detection and correction methods for hallucinations in LLMs \\cite{ye2023yom}.\n        *   **Future Directions**: Proposes several potential research directions to address evolving challenges.\n\n*   **4. Key Technical Contributions**\n    *   **Theoretical Insights/Analysis**:\n        *   **Mechanism Analysis**: Identifies and elaborates on three crucial factors contributing to hallucinations:\n            *   **Data Collection**: Issues like incomplete/outdated pre-trained corpus knowledge, contextual learning biases, and challenges in multilingual/cross-modal data \\cite{ye2023yom}.\n            *   **Knowledge Gap**: Discrepancies between pre-training and fine-tuning, challenges in balancing internal memory with retrieved evidence, and inconsistencies with cache components \\cite{ye2023yom}.\n            *   **Optimization Process**: Problems like stochastic parroting (MLE, teacher-forcing), exposure bias, high uncertainty sampling, and \"snowballing\" hallucinations where LLMs maintain coherence with earlier incorrect statements \\cite{ye2023yom}.\n        *   **Detailed Taxonomy**: Offers a comprehensive categorization of hallucination types observed across diverse LLM applications, including specific examples like \"translation off-target,\" \"imitative falsehoods,\" \"uncooperativeness,\" \"intrinsic/extrinsic hallucinations,\" and \"subject/relation/object hallucination\" \\cite{ye2023yom}.\n    *   **Review of Methods**: Systematically categorizes and summarizes existing methods for hallucination detection (e.g., Inference Classifiers, Uncertainty Metrics, Self-Evaluation, Evidence Retrieval) and correction (e.g., Parameter Adaptation, Post-hoc Attribution, Leveraging External Knowledge, Assessment Feedback) \\cite{ye2023yom}.\n\n*   **5. Experimental Validation**\n    *   **Experiments Conducted**: This paper is a literature review and does not conduct its own experiments.\n    *   **Key Performance Metrics/Comparison Results**: The paper *summarizes* the experimental validation and evaluation metrics used in the reviewed literature. It references various benchmarks (e.g., TruthfulQA, HotpotQA, WoW, CNN/DM, XSum, MSCOCO) and evaluation approaches (e.g., manual analysis, human feedback, pathology detection, fact-checking) that other researchers have employed to identify and measure hallucinations \\cite{ye2023yom}.\n\n*   **6. Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: As a review, its primary limitation is that it synthesizes existing work rather than proposing new technical solutions or conducting novel experiments. The field of LLM hallucinations is rapidly evolving, and while the authors commit to maintaining updates, any review is a snapshot in time.\n    *   **Scope of Applicability**: The review focuses specifically on hallucinations in Large Language Models across various text generation tasks, including machine translation, question answering, dialog systems, summarization, knowledge graph generation, and cross-modal systems \\cite{ye2023yom}.\n\n*   **7. Technical Significance**\n    *   **Advancement of State-of-the-Art**: This paper significantly advances the technical state-of-the-art by providing the first detailed and complete taxonomy of hallucinations in LLMs across diverse tasks, coupled with a systematic analysis of their underlying mechanisms \\cite{ye2023yom}. It consolidates fragmented research into a coherent framework.\n    *   **Potential Impact on Future Research**: By offering theoretical insights, categorizing existing detection and correction methods, and proposing concrete future research directions (e.g., data construction management, downstream task alignment, reasoning mechanism exploitation, multi-modal hallucination survey), the paper serves as a foundational resource to guide and inspire future efforts to understand, detect, and mitigate hallucinations in LLMs \\cite{ye2023yom}. This systematic understanding is crucial for improving the reliability and trustworthiness of LLM-based systems.",
      "intriguing_abstract": "Large Language Models (LLMs) often present a \"cognitive mirage,\" generating fluent text that is factually incorrect or unfaithful to source inputs â€“ a pervasive problem known as hallucination. As LLMs become critical knowledge repositories, these hallucinations severely undermine their reliability, particularly in high-stakes applications. This comprehensive review systematically dissects the multifaceted phenomenon of LLM hallucinations, offering a novel, detailed taxonomy categorized across diverse text generation tasks, including Machine Translation, Question Answering, and cross-modal systems.\n\nWe delve into the theoretical mechanisms underpinning hallucinations, attributing their origins to critical issues in data collection, inherent knowledge gaps, and the optimization process, revealing insights into phenomena like stochastic parroting and \"snowballing\" errors. Furthermore, we provide a structured analysis of emerging detection methods (e.g., uncertainty metrics, self-evaluation) and correction strategies (e.g., external knowledge leveraging, parameter adaptation). By consolidating fragmented research and proposing concrete future directions, this paper serves as a foundational resource, crucial for advancing the state-of-the-art in mitigating LLM hallucinations and fostering truly trustworthy AI.",
      "keywords": [
        "Large Language Models (LLMs)",
        "Hallucinations",
        "Factual incorrectness",
        "Reliability and trustworthiness",
        "Systematic literature review",
        "Novel taxonomy of hallucinations",
        "Hallucination mechanisms",
        "Hallucination detection methods",
        "Hallucination correction methods",
        "Text generation tasks",
        "Cross-modal systems",
        "Knowledge representation",
        "Future research directions",
        "High-stakes applications"
      ],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/396305230ddcf915b19a19683a89e34d76321a33.pdf",
      "citation_key": "ye2023yom",
      "metadata": {
        "title": "Cognitive Mirage: A Review of Hallucinations in Large Language Models",
        "authors": [
          "Hongbin Ye",
          "Tong Liu",
          "Aijia Zhang",
          "Wei Hua",
          "Weiqiang Jia"
        ],
        "published_date": "2023",
        "abstract": "As large language models continue to develop in the field of AI, text generation systems are susceptible to a worrisome phenomenon known as hallucination. In this study, we summarize recent compelling insights into hallucinations in LLMs. We present a novel taxonomy of hallucinations from various text generation tasks, thus provide theoretical insights, detection methods and improvement approaches. Based on this, future research directions are proposed. Our contribution are threefold: (1) We provide a detailed and complete taxonomy for hallucinations appearing in text generation tasks; (2) We provide theoretical analyses of hallucinations in LLMs and provide existing detection and improvement methods; (3) We propose several research directions that can be developed in the future. As hallucinations garner significant attention from the community, we will maintain updates on relevant research progress.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/396305230ddcf915b19a19683a89e34d76321a33.pdf",
        "venue": "LKM@IJCAI",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the paper \"Cognitive Mirage: A Review of Hallucinations in Large Language Models\" \\cite{ye2023yom} for a literature review:\n\n---\n\n*   **1. Research Problem & Motivation**\n    *   **Specific Technical Problem**: The paper addresses the pervasive and critical issue of \"hallucinations\" in Large Language Models (LLMs), where models generate text that is fluent and grammatically correct but factually incorrect, irrelevant, or unfaithful to source inputs.\n    *   **Importance and Challenge**: This problem is crucial because LLMs are increasingly used as knowledge repositories, and hallucinations severely undermine their reliability and trustworthiness. Challenges arise from:\n        *   **Memory Distortion**: Generalization of knowledge during pre-training can lead to inaccuracies.\n        *   **Knowledge Representation Limits**: Constraints of model scale and difficulty in addressing long-tailed knowledge problems.\n        *   **Data Timeliness/Privacy**: Difficulty in maintaining comprehensive and up-to-date factual understanding due to dynamic real-world data.\n        *   **High-Stakes Applications**: Hallucinations pose significant risks in knowledge-intensive fields like medical, financial, and legal applications where accuracy is paramount.\n\n*   **2. Related Work & Positioning**\n    *   **Relation to Existing Approaches**: This work is a comprehensive survey that synthesizes recent progress in understanding and mitigating hallucinations in LLMs.\n    *   **Limitations of Previous Solutions**: Existing surveys are nascent and often limited in scope:\n        *   Some focus on early works in natural language generation (NLG) or task-specific progress \\cite{ye2023yom}.\n        *   Others cover specific aspects like collecting high-quality instructions for LLM alignment or self-correction methods \\cite{ye2023yom}.\n        *   While benchmarks exist to evaluate factual responses, they are scattered across various tasks and have not been systematically reviewed and analyzed \\cite{ye2023yom}.\n    *   **Positioning**: This paper aims to provide a systematic and comprehensive understanding of methodologies, compare different approaches, and inspire new research directions, differentiating itself through its broad coverage and structured analysis.\n\n*   **3. Technical Approach & Innovation**\n    *   **Core Technical Method**: As a review paper, the core \"method\" is a systematic and comprehensive literature survey and analysis. It proposes a novel framework for understanding hallucinations.\n    *   **Novelty/Difference**: The approach is novel due to its structured organization and depth of analysis:\n        *   **Novel Taxonomy**: Presents a new, detailed taxonomy of hallucinations categorized by various text generation tasks (e.g., Machine Translation, QA, Dialog, Summarization, Knowledge Graph, Cross-modal systems) \\cite{ye2023yom}.\n        *   **Theoretical Insights**: Provides in-depth theoretical and mechanistic analyses of hallucination origins, attributing them to three primary factors: data collection, knowledge gaps, and the optimization process \\cite{ye2023yom}.\n        *   **Comprehensive Coverage**: Systematically reviews a wide range of emerging detection and correction methods for hallucinations in LLMs \\cite{ye2023yom}.\n        *   **Future Directions**: Proposes several potential research directions to address evolving challenges.\n\n*   **4. Key Technical Contributions**\n    *   **Theoretical Insights/Analysis**:\n        *   **Mechanism Analysis**: Identifies and elaborates on three crucial factors contributing to hallucinations:\n            *   **Data Collection**: Issues like incomplete/outdated pre-trained corpus knowledge, contextual learning biases, and challenges in multilingual/cross-modal data \\cite{ye2023yom}.\n            *   **Knowledge Gap**: Discrepancies between pre-training and fine-tuning, challenges in balancing internal memory with retrieved evidence, and inconsistencies with cache components \\cite{ye2023yom}.\n            *   **Optimization Process**: Problems like stochastic parroting (MLE, teacher-forcing), exposure bias, high uncertainty sampling, and \"snowballing\" hallucinations where LLMs maintain coherence with earlier incorrect statements \\cite{ye2023yom}.\n        *   **Detailed Taxonomy**: Offers a comprehensive categorization of hallucination types observed across diverse LLM applications, including specific examples like \"translation off-target,\" \"imitative falsehoods,\" \"uncooperativeness,\" \"intrinsic/extrinsic hallucinations,\" and \"subject/relation/object hallucination\" \\cite{ye2023yom}.\n    *   **Review of Methods**: Systematically categorizes and summarizes existing methods for hallucination detection (e.g., Inference Classifiers, Uncertainty Metrics, Self-Evaluation, Evidence Retrieval) and correction (e.g., Parameter Adaptation, Post-hoc Attribution, Leveraging External Knowledge, Assessment Feedback) \\cite{ye2023yom}.\n\n*   **5. Experimental Validation**\n    *   **Experiments Conducted**: This paper is a literature review and does not conduct its own experiments.\n    *   **Key Performance Metrics/Comparison Results**: The paper *summarizes* the experimental validation and evaluation metrics used in the reviewed literature. It references various benchmarks (e.g., TruthfulQA, HotpotQA, WoW, CNN/DM, XSum, MSCOCO) and evaluation approaches (e.g., manual analysis, human feedback, pathology detection, fact-checking) that other researchers have employed to identify and measure hallucinations \\cite{ye2023yom}.\n\n*   **6. Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: As a review, its primary limitation is that it synthesizes existing work rather than proposing new technical solutions or conducting novel experiments. The field of LLM hallucinations is rapidly evolving, and while the authors commit to maintaining updates, any review is a snapshot in time.\n    *   **Scope of Applicability**: The review focuses specifically on hallucinations in Large Language Models across various text generation tasks, including machine translation, question answering, dialog systems, summarization, knowledge graph generation, and cross-modal systems \\cite{ye2023yom}.\n\n*   **7. Technical Significance**\n    *   **Advancement of State-of-the-Art**: This paper significantly advances the technical state-of-the-art by providing the first detailed and complete taxonomy of hallucinations in LLMs across diverse tasks, coupled with a systematic analysis of their underlying mechanisms \\cite{ye2023yom}. It consolidates fragmented research into a coherent framework.\n    *   **Potential Impact on Future Research**: By offering theoretical insights, categorizing existing detection and correction methods, and proposing concrete future research directions (e.g., data construction management, downstream task alignment, reasoning mechanism exploitation, multi-modal hallucination survey), the paper serves as a foundational resource to guide and inspire future efforts to understand, detect, and mitigate hallucinations in LLMs \\cite{ye2023yom}. This systematic understanding is crucial for improving the reliability and trustworthiness of LLM-based systems.",
        "keywords": [
          "Large Language Models (LLMs)",
          "Hallucinations",
          "Factual incorrectness",
          "Reliability and trustworthiness",
          "Systematic literature review",
          "Novel taxonomy of hallucinations",
          "Hallucination mechanisms",
          "Hallucination detection methods",
          "Hallucination correction methods",
          "Text generation tasks",
          "Cross-modal systems",
          "Knowledge representation",
          "Future research directions",
          "High-stakes applications"
        ],
        "paper_type": "**survey**\n\n**reasoning:**\n\n1.  **title:** the title \"cognitive mirage: a **review** of hallucinations in large language models\" explicitly states it is a \"review,\" which is a direct synonym for \"survey\" in this context.\n2.  **abstract keywords:**\n    *   \"we **summarize** recent compelling insights\"\n    *   \"we present a novel **taxonomy** of hallucinations\"\n    *   \"provide theoretical insights, detection methods and improvement approaches\" (implying a review of existing ones)\n    *   \"future research directions are proposed\" (common in surveys)\n    *   \"our contribution are threefold: (1) we provide a detailed and complete **taxonomy**...; (2) we provide theoretical **analyses** of hallucinations in llms and provide **existing** detection and improvement methods; (3) we propose several research directions...\"\n    these points directly match the \"survey\" criteria: \"reviews existing literature comprehensively,\" \"abstract mentions: 'survey', 'review', 'comprehensive analysis', 'state-of-the-art',\" and \"introduction discusses: literature organization, classification schemes.\" the \"theoretical analyses\" here refers to analyzing *existing* theories, not proposing new mathematical proofs.\n3.  **introduction content:** the introduction sets the stage by discussing the emergence of llms and the problem of \"memory distortion\" and \"inaccuracies\" (hallucinations), which is typical for a paper that will then comprehensively review this phenomenon.\n\nthe paper's primary goal is to organize, summarize, and classify existing knowledge about hallucinations in llms, which is the definition of a survey paper."
      },
      "file_name": "396305230ddcf915b19a19683a89e34d76321a33.pdf"
    },
    {
      "success": true,
      "doc_id": "7e769c6643985bda5e0a7867f347c739",
      "summary": "Here's a focused summary of the paper for a literature review, emphasizing technical innovations and empirical validation:\n\n### Analysis of \"R-Tuning: Instructing Large Language Models to Say â€˜I Donâ€™t Knowâ€™\" \\cite{zhang2023k5a}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Large Language Models (LLMs) frequently generate non-existent facts, a phenomenon known as hallucination. This occurs because traditional instruction tuning methods compel models to complete sentences even when they lack the underlying knowledge, leading them to \"make up something\" rather than admit ignorance.\n    *   **Importance & Challenge**: Hallucination undermines the trustworthiness and reliability of LLMs, especially in critical applications. The challenge lies in teaching LLMs to recognize the boundaries of their parametric knowledge and to explicitly refuse to answer questions beyond these boundaries, rather than fabricating responses.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: Current mainstream approaches to mitigate hallucination include retrieval-based methods (e.g., Peng et al., 2023), which augment LLMs with external knowledge, and verification-based methods (e.g., Manakul et al., 2023), which check the factual consistency of generated answers.\n    *   **Limitations of Previous Solutions**: These methods primarily focus on *correcting* or *augmenting* LLM outputs. In contrast, \\cite{zhang2023k5a} addresses the root cause by teaching the model to *refuse* to answer when knowledge is absent, a capability largely overlooked by prior instruction tuning paradigms that implicitly train models to always provide an answer.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes **Refusal-Aware Instruction Tuning (R-Tuning)**, a novel instruction tuning method designed to endow LLMs with the ability to recognize and express uncertainty. R-Tuning involves two main steps:\n        1.  **Refusal-Aware Data Identification**: Measures the knowledge gap between the LLM's parametric knowledge and the instruction tuning data. This is done by inferring the pre-trained model on the training data and splitting questions into \"certain\" (D1, where prediction matches label) and \"uncertain\" (D0, where prediction mismatches label) sets. An unsupervised variant is also explored, which identifies uncertain data by prompting the LLM multiple times and identifying low-consistency answers.\n        2.  **Refusal-Aware Data Construction**: Creates a new dataset by incorporating a prompt template. For certain data (D1), the template appends \"I am sure\" after the answer. For uncertain data (D0), it appends \"I am unsure\" after the answer, using a specific prompt like \"Are you sure you accurately answered the question based on your internal knowledge?\". This teaches the model to express uncertainty while still providing the ground truth for learning.\n    *   **Novelty/Difference**: R-Tuning's novelty lies in explicitly training LLMs to articulate \"I don't know\" by constructing a refusal-aware dataset based on the model's *own* knowledge boundaries. This contrasts with traditional instruction tuning that implicitly forces an answer. The method introduces a mechanism to learn uncertainty as part of the training process, leading to better calibration and uncertainty estimation, rather than just filtering at inference time.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithm/Method**: Introduction of R-Tuning, a novel instruction tuning approach that differentiates instruction tuning data based on the model's parametric knowledge and constructs a refusal-aware dataset.\n    *   **Data Construction Technique**: A specific \"padding\" method for refusal-aware data construction, where uncertainty expressions are appended to labels, allowing the model to learn both the answer and its confidence.\n    *   **Unsupervised Identification Strategy**: Development of an effective unsupervised method for identifying uncertain training data, which prompts LLMs multiple times and uses answer consistency.\n    *   **Theoretical Insight/Analysis**: Demonstrates that learning uncertainty during training (R-Tuning) leads to better calibration and improved uncertainty estimation compared to applying uncertainty filtering directly on test data.\n    *   **Meta-Skill Discovery**: Empirically shows that the learned refusal ability functions as a \"meta-skill\" that is task-agnostic and can be enhanced through multi-task training, generalizing to out-of-domain datasets.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**:\n        *   **Single-task experiments**: Evaluated R-Tuning on individual tasks (ParaRel, MMLU) to verify refusal-aware answering ability and accuracy on willingly answered questions.\n        *   **Multi-task experiments**: Combined five datasets (ParaRel, MMLU, WiCE, HotpotQA, FEVER) for training and tested on both in-domain and an unseen out-of-domain dataset (HaluEval-QA) to assess generalization.\n        *   **Ablation studies**: Investigated the unsupervised identification strategy and compared \"padding\" vs. \"replacement\" methods for data construction.\n    *   **Key Performance Metrics**:\n        *   **Accuracy**: For willingly answered questions.\n        *   **Average Precision (AP) score**: A primary metric measuring the precision-recall trade-off, indicating how well the model ranks correct answers with high confidence and incorrect/hallucinated answers with low confidence.\n    *   **Comparison Results**:\n        *   R-Tuning significantly outperforms baselines (Pretrain-T, Pretrain-W, Vanilla) in terms of accuracy on willingly answered questions and consistently achieves higher AP scores across various datasets and model sizes.\n        *   Larger models show greater improvement with R-Tuning, indicating good scalability.\n        *   R-Tuning demonstrates superior generalization performance on out-of-domain datasets (e.g., HaluEval-QA), confirming that refusal ability is a transferable meta-skill.\n        *   The unsupervised identification method is surprisingly effective, yielding comparable results to the supervised approach.\n        *   R-Tuning leads to better calibrated models, as evidenced by its stable AP score growth compared to Vanilla, which can suffer from confidence miscalibration.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The initial refusal-aware data identification relies on a supervised strategy requiring ground-truth labels, though an unsupervised alternative is presented to mitigate this. The effectiveness of the \"padding\" method over \"replacement\" is noted, suggesting design choices in prompt engineering are crucial.\n    *   **Scope of Applicability**: Primarily focused on mitigating hallucination in LLMs by teaching them to express uncertainty. While demonstrated across various QA and multiple-choice tasks, its direct applicability to generative tasks requiring creative or open-ended responses where \"I don't know\" might be less appropriate is not explicitly detailed.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: R-Tuning advances the technical state-of-the-art by introducing a principled method to train LLMs to recognize and articulate their knowledge boundaries, directly addressing the hallucination problem from a novel angle (refusal rather than correction). It shifts the paradigm from \"always answer\" to \"know when to refuse.\"\n    *   **Potential Impact on Future Research**: This work opens avenues for developing more reliable, trustworthy, and well-calibrated LLMs. The finding that refusal is a generalizable meta-skill suggests that future LLM training could explicitly incorporate uncertainty learning to build models that are inherently more aware of their limitations, leading to safer and more robust AI systems. It also highlights the potential for multi-task learning to enhance such meta-cognitive abilities.",
      "intriguing_abstract": "Large Language Models (LLMs) frequently undermine their utility by confidently generating non-existent facts, a critical issue known as hallucination. Traditional instruction tuning inadvertently compels models to answer even when lacking knowledge, fostering this unreliable behavior. We introduce **R-Tuning: Refusal-Aware Instruction Tuning**, a novel paradigm that teaches LLMs to explicitly say \"I don't know\" when confronted with questions beyond their parametric knowledge boundaries.\n\nR-Tuning identifies knowledge gaps within training data and constructs a refusal-aware dataset, where models learn to append expressions of certainty or uncertainty to their responses. This innovative approach transforms hallucination mitigation from post-hoc correction to intrinsic knowledge boundary recognition. Our experiments demonstrate that R-Tuning significantly improves accuracy on willingly answered questions and achieves superior Average Precision (AP) scores, leading to better calibrated models. Crucially, we show that this learned refusal ability functions as a generalizable \"meta-skill,\" transferable across tasks and enhanced through multi-task training, even generalizing to out-of-domain datasets. R-Tuning paves the way for more trustworthy, reliable, and inherently safer AI systems by equipping LLMs with a fundamental understanding of their own limitations.",
      "keywords": [
        "Large Language Models (LLMs)",
        "hallucination mitigation",
        "instruction tuning",
        "R-Tuning",
        "expressing uncertainty",
        "parametric knowledge boundaries",
        "refusal-aware data construction",
        "unsupervised data identification",
        "uncertainty calibration",
        "meta-skill discovery",
        "multi-task training",
        "generalization",
        "trustworthiness and reliability"
      ],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/97d24f9f0d81007d57cc43e61bf2b0c9081fe184.pdf",
      "citation_key": "zhang2023k5a",
      "metadata": {
        "title": "R-Tuning: Instructing Large Language Models to Say â€˜I Donâ€™t Knowâ€™",
        "authors": [
          "Hanning Zhang",
          "Shizhe Diao",
          "Yong Lin",
          "Y. Fung",
          "Qing Lian",
          "Xingyao Wang",
          "Yangyi Chen",
          "Heng Ji",
          "Tong Zhang"
        ],
        "published_date": "2023",
        "abstract": "Large language models (LLMs) have revolutionized numerous domains with their impressive performance but still face their challenges. A predominant issue is the propensity for these models to generate non-existent facts, a concern termed hallucination. Our research is motivated by the observation that previous instruction tuning methods force the model to complete a sentence no matter whether the model knows the knowledge or not. When the question is out of the parametric knowledge, it will try to make up something and fail to indicate when it lacks knowledge. In this paper, we present a new approach called Refusal-Aware Instruction Tuning (R-Tuning). This approach is formalized by first identifying the disparity in knowledge encompassed by pre-trained parameters compared to that of instruction tuning data. Then, we construct the refusal-aware data based on the knowledge intersection, to tune LLMs to refrain from responding to questions beyond its parametric knowledge. Experimental results demonstrate R-Tuning effectively improves a modelâ€™s ability to answer known questions and refrain from answering unknown questions. Furthermore, when tested on out-of-domain datasets, the refusal ability was found to be a meta-skill that could be generalized to other tasks. Further analysis surprisingly finds that learning the uncertainty results in better calibration and an improved ability to estimate the uncertainty than uncertainty-based testing. Our code is available at https://github.com/shizhediao/R-Tuning",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/97d24f9f0d81007d57cc43e61bf2b0c9081fe184.pdf",
        "venue": "North American Chapter of the Association for Computational Linguistics",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the paper for a literature review, emphasizing technical innovations and empirical validation:\n\n### Analysis of \"R-Tuning: Instructing Large Language Models to Say â€˜I Donâ€™t Knowâ€™\" \\cite{zhang2023k5a}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Large Language Models (LLMs) frequently generate non-existent facts, a phenomenon known as hallucination. This occurs because traditional instruction tuning methods compel models to complete sentences even when they lack the underlying knowledge, leading them to \"make up something\" rather than admit ignorance.\n    *   **Importance & Challenge**: Hallucination undermines the trustworthiness and reliability of LLMs, especially in critical applications. The challenge lies in teaching LLMs to recognize the boundaries of their parametric knowledge and to explicitly refuse to answer questions beyond these boundaries, rather than fabricating responses.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: Current mainstream approaches to mitigate hallucination include retrieval-based methods (e.g., Peng et al., 2023), which augment LLMs with external knowledge, and verification-based methods (e.g., Manakul et al., 2023), which check the factual consistency of generated answers.\n    *   **Limitations of Previous Solutions**: These methods primarily focus on *correcting* or *augmenting* LLM outputs. In contrast, \\cite{zhang2023k5a} addresses the root cause by teaching the model to *refuse* to answer when knowledge is absent, a capability largely overlooked by prior instruction tuning paradigms that implicitly train models to always provide an answer.\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method**: The paper proposes **Refusal-Aware Instruction Tuning (R-Tuning)**, a novel instruction tuning method designed to endow LLMs with the ability to recognize and express uncertainty. R-Tuning involves two main steps:\n        1.  **Refusal-Aware Data Identification**: Measures the knowledge gap between the LLM's parametric knowledge and the instruction tuning data. This is done by inferring the pre-trained model on the training data and splitting questions into \"certain\" (D1, where prediction matches label) and \"uncertain\" (D0, where prediction mismatches label) sets. An unsupervised variant is also explored, which identifies uncertain data by prompting the LLM multiple times and identifying low-consistency answers.\n        2.  **Refusal-Aware Data Construction**: Creates a new dataset by incorporating a prompt template. For certain data (D1), the template appends \"I am sure\" after the answer. For uncertain data (D0), it appends \"I am unsure\" after the answer, using a specific prompt like \"Are you sure you accurately answered the question based on your internal knowledge?\". This teaches the model to express uncertainty while still providing the ground truth for learning.\n    *   **Novelty/Difference**: R-Tuning's novelty lies in explicitly training LLMs to articulate \"I don't know\" by constructing a refusal-aware dataset based on the model's *own* knowledge boundaries. This contrasts with traditional instruction tuning that implicitly forces an answer. The method introduces a mechanism to learn uncertainty as part of the training process, leading to better calibration and uncertainty estimation, rather than just filtering at inference time.\n\n4.  **Key Technical Contributions**\n    *   **Novel Algorithm/Method**: Introduction of R-Tuning, a novel instruction tuning approach that differentiates instruction tuning data based on the model's parametric knowledge and constructs a refusal-aware dataset.\n    *   **Data Construction Technique**: A specific \"padding\" method for refusal-aware data construction, where uncertainty expressions are appended to labels, allowing the model to learn both the answer and its confidence.\n    *   **Unsupervised Identification Strategy**: Development of an effective unsupervised method for identifying uncertain training data, which prompts LLMs multiple times and uses answer consistency.\n    *   **Theoretical Insight/Analysis**: Demonstrates that learning uncertainty during training (R-Tuning) leads to better calibration and improved uncertainty estimation compared to applying uncertainty filtering directly on test data.\n    *   **Meta-Skill Discovery**: Empirically shows that the learned refusal ability functions as a \"meta-skill\" that is task-agnostic and can be enhanced through multi-task training, generalizing to out-of-domain datasets.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted**:\n        *   **Single-task experiments**: Evaluated R-Tuning on individual tasks (ParaRel, MMLU) to verify refusal-aware answering ability and accuracy on willingly answered questions.\n        *   **Multi-task experiments**: Combined five datasets (ParaRel, MMLU, WiCE, HotpotQA, FEVER) for training and tested on both in-domain and an unseen out-of-domain dataset (HaluEval-QA) to assess generalization.\n        *   **Ablation studies**: Investigated the unsupervised identification strategy and compared \"padding\" vs. \"replacement\" methods for data construction.\n    *   **Key Performance Metrics**:\n        *   **Accuracy**: For willingly answered questions.\n        *   **Average Precision (AP) score**: A primary metric measuring the precision-recall trade-off, indicating how well the model ranks correct answers with high confidence and incorrect/hallucinated answers with low confidence.\n    *   **Comparison Results**:\n        *   R-Tuning significantly outperforms baselines (Pretrain-T, Pretrain-W, Vanilla) in terms of accuracy on willingly answered questions and consistently achieves higher AP scores across various datasets and model sizes.\n        *   Larger models show greater improvement with R-Tuning, indicating good scalability.\n        *   R-Tuning demonstrates superior generalization performance on out-of-domain datasets (e.g., HaluEval-QA), confirming that refusal ability is a transferable meta-skill.\n        *   The unsupervised identification method is surprisingly effective, yielding comparable results to the supervised approach.\n        *   R-Tuning leads to better calibrated models, as evidenced by its stable AP score growth compared to Vanilla, which can suffer from confidence miscalibration.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions**: The initial refusal-aware data identification relies on a supervised strategy requiring ground-truth labels, though an unsupervised alternative is presented to mitigate this. The effectiveness of the \"padding\" method over \"replacement\" is noted, suggesting design choices in prompt engineering are crucial.\n    *   **Scope of Applicability**: Primarily focused on mitigating hallucination in LLMs by teaching them to express uncertainty. While demonstrated across various QA and multiple-choice tasks, its direct applicability to generative tasks requiring creative or open-ended responses where \"I don't know\" might be less appropriate is not explicitly detailed.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art**: R-Tuning advances the technical state-of-the-art by introducing a principled method to train LLMs to recognize and articulate their knowledge boundaries, directly addressing the hallucination problem from a novel angle (refusal rather than correction). It shifts the paradigm from \"always answer\" to \"know when to refuse.\"\n    *   **Potential Impact on Future Research**: This work opens avenues for developing more reliable, trustworthy, and well-calibrated LLMs. The finding that refusal is a generalizable meta-skill suggests that future LLM training could explicitly incorporate uncertainty learning to build models that are inherently more aware of their limitations, leading to safer and more robust AI systems. It also highlights the potential for multi-task learning to enhance such meta-cognitive abilities.",
        "keywords": [
          "Large Language Models (LLMs)",
          "hallucination mitigation",
          "instruction tuning",
          "R-Tuning",
          "expressing uncertainty",
          "parametric knowledge boundaries",
          "refusal-aware data construction",
          "unsupervised data identification",
          "uncertainty calibration",
          "meta-skill discovery",
          "multi-task training",
          "generalization",
          "trustworthiness and reliability"
        ],
        "paper_type": "based on the abstract and introduction:\n\n1.  **\"in this paper, we present a new approach called refusal-aware instruction tuning (r-tuning).\"** this directly aligns with the \"technical\" criterion: \"abstract mentions: 'propose', 'develop', 'present', 'algorithm', 'method'\".\n2.  the abstract describes how this approach is \"formalized\" and how they \"construct the refusal-aware data\" to \"tune llms.\" this details the new method.\n3.  while the abstract also mentions \"experimental results demonstrate\" and \"tested on out-of-domain datasets,\" which are indicators for \"empirical,\" these experiments are conducted to validate the effectiveness of the *new approach* (r-tuning). the primary contribution is the development and presentation of this novel method.\n\ntherefore, the paper's core contribution is the introduction of a new method, making it primarily **technical**.\n\n**classification: technical**"
      },
      "file_name": "97d24f9f0d81007d57cc43e61bf2b0c9081fe184.pdf"
    },
    {
      "success": true,
      "doc_id": "5c451b5409a30432efd2776a8f05b52f",
      "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Problem:** The paper addresses the challenge of providing external knowledge for unsupervised knowledge-grounded conversation, specifically exploring how to leverage large pre-trained models (LPMs) as dynamic knowledge bases \\cite{li2022ypy}.\n    *   **Importance & Challenge:** Traditional knowledge bases are expensive and time-consuming to construct and maintain. While LPMs show potential for internalizing knowledge and even performing implicit reasoning, they are prone to \"hallucinations\" (generating plausible but factually incorrect information), making their direct use risky \\cite{li2022ypy}. The core challenge is to effectively elicit and exploit this noisy, generated knowledge.\n\n*   **Related Work & Positioning**\n    *   **Existing Approaches:** Prior work in knowledge-grounded conversation typically relies on retrieving knowledge from pre-defined external sources like unstructured documents (e.g., Wikipedia) or structured knowledge graphs \\cite{li2022ypy}. Models like PLATO-KAG, SKT, and KnowledGPT are representative of these approaches, featuring knowledge selection and response generation modules \\cite{li2022ypy}.\n    *   **Limitations of Previous Solutions:** These methods are constrained by the availability and cost of maintaining explicit knowledge bases. Furthermore, existing response generators may not be robust to imperfect or noisy knowledge selection, leading to a training-inference discrepancy \\cite{li2022ypy}.\n\n*   **Technical Approach & Innovation**\n    *   **Core Method:**\n        *   **Knowledge Elicitation:** The paper systematically investigates various tuning methods (fine-tuning, prefix-tuning) and large pre-trained models (PLMs like T5, PDMs like DialoGPT) to generate relevant knowledge from dialogue history \\cite{li2022ypy}.\n        *   **Posterior-based Reweighing:** It treats the generated knowledge from LPMs as a *noisy knowledge source*. This generated knowledge is used to refine the initial knowledge selection distribution by measuring its similarity to retrieved knowledge candidates. This refined distribution is then further updated using a posterior estimation, leveraging the response generator's likelihood to select more appropriate knowledge given the potential future response \\cite{li2022ypy}.\n        *   **Noisy Training Strategy:** To make the response generator more resilient to noisy knowledge during inference, the Gumbel-TopK trick is employed during training. This introduces noise into the knowledge selection process, perturbing the ranking of candidates and strengthening the model's ability to handle imperfect inputs \\cite{li2022ypy}.\n    *   **Novelty/Difference:** This work is novel in systematically exploring LPMs as dynamic knowledge bases for unsupervised knowledge-grounded conversation. It introduces a unique strategy to mitigate LPM hallucinations by treating their output as a noisy signal for reweighing existing knowledge, rather than direct input. The combination of posterior estimation with generated knowledge-based reweighing and a specific noisy training strategy (Gumbel-TopK) is a key innovation \\cite{li2022ypy}. A human study also highlights LPMs' unique ability to generate common sense and summarize facts, going beyond simple retrieval \\cite{li2022ypy}.\n\n*   **Key Technical Contributions**\n    *   **Novel Algorithms/Methods:**\n        *   A comprehensive framework for eliciting and evaluating knowledge from large pre-trained models for dialogue generation \\cite{li2022ypy}.\n        *   **Posterior-based Reweighing:** A novel method to refine knowledge selection by integrating similarity to noisy generated knowledge and a posterior update using the response generator's likelihood \\cite{li2022ypy}.\n        *   **Noisy Training Strategy:** The application of Gumbel-TopK to inject noise into the knowledge selection during training, enhancing the response generator's robustness to noisy knowledge \\cite{li2022ypy}.\n    *   **Theoretical Insights/Analysis:** Demonstrates that LPMs can generate valuable, non-trivial knowledge (common sense, summarized facts) beyond simple memorization, underscoring their potential as dynamic knowledge sources despite inherent noise \\cite{li2022ypy}.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted:**\n        *   **Knowledge Generation Quality:** Automatic (Unigram F1) and extensive human evaluation (using a custom tagset for context understanding, tuning effectiveness, and fact-checking) were performed on knowledge generated by various T5 and DialoGPT models with fine-tuning and prefix-tuning \\cite{li2022ypy}.\n        *   **Response Generation Performance:** Automatic metrics (Perplexity, Unigram F1, P@1 knowledge accuracy) and human evaluation (Coherence, Informativeness, Engagingness, Hallucination) were used to assess dialogue responses \\cite{li2022ypy}.\n    *   **Key Performance Metrics & Comparison Results:**\n        *   Human evaluation of generated knowledge showed high relatedness (98% seen, 88% unseen) and verifiability. Notably, a significant portion of supported facts were *implicitly supported* (summarized/reasoned, not direct copies), and some unvalidated facts aligned with *common sense* \\cite{li2022ypy}.\n        *   The proposed method achieved significant improvements in automatic metrics (PPL, F1, P@1) over state-of-the-art baselines (TMN, SKT, KnowledGPT, PLATO-KAG) on the Wizard of Wikipedia (WoW) and Holl-E datasets \\cite{li2022ypy}.\n        *   Human evaluation confirmed improvements in Coherence, Informativeness, and Engagingness, while maintaining low Hallucination rates, validating the effectiveness of the noisy knowledge handling strategies \\cite{li2022ypy}.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations/Assumptions:** Hallucinations from LPMs persist, necessitating the proposed robust handling mechanisms. The posterior estimation relies on an approximation (mean token probability), and the method requires hyper-parameter tuning for reweighing and posterior sharpness \\cite{li2022ypy}.\n    *   **Scope of Applicability:** The approach is primarily focused on unsupervised knowledge-grounded conversation. It is particularly applicable in scenarios where dynamic, on-demand knowledge is preferred over static, pre-constructed knowledge bases \\cite{li2022ypy}.\n\n*   **Technical Significance**\n    *   **Advances State-of-the-Art:** This paper advances the state-of-the-art by demonstrating a viable and effective method for leveraging large pre-trained models as dynamic knowledge sources for knowledge-grounded conversation, moving beyond the limitations of static knowledge bases \\cite{li2022ypy}. It provides robust strategies to manage the inherent noise (hallucinations) in LPM-generated knowledge, making their outputs practically usable \\cite{li2022ypy}.\n    *   **Potential Impact:** This work opens new research directions for dynamic knowledge acquisition in dialogue systems, potentially reducing the reliance on costly manual knowledge base construction. The proposed noisy training and reweighing strategies could inspire similar approaches for other NLP tasks dealing with uncertain or noisy information from large models, contributing to more scalable and adaptable conversational AI \\cite{li2022ypy}.",
      "intriguing_abstract": "Unlocking truly dynamic and unsupervised knowledge-grounded conversation remains a significant challenge, as traditional knowledge bases are costly and large pre-trained models (LPMs) are prone to factual hallucinations. This paper pioneers a novel framework to harness LPMs as *dynamic knowledge bases*, systematically addressing their inherent noise. We introduce a sophisticated **posterior-based reweighing** mechanism that refines knowledge selection by integrating noisy LPM-generated knowledge with the response generator's likelihood. Crucially, a **Gumbel-TopK noisy training strategy** is employed to bolster the response generator's robustness against imperfect knowledge during inference. Our comprehensive evaluations on Wizard of Wikipedia and Holl-E datasets demonstrate significant improvements over state-of-the-art baselines in coherence, informativeness, and reduced hallucination. This work not only validates LPMs' unique ability to generate common sense and summarized facts but also offers a robust pathway towards scalable, adaptable conversational AI, fundamentally shifting away from static knowledge dependencies.",
      "keywords": [
        "unsupervised knowledge-grounded conversation",
        "large pre-trained models (LPMs)",
        "dynamic knowledge bases",
        "LPM hallucinations",
        "knowledge elicitation",
        "posterior-based reweighing",
        "noisy training strategy",
        "Gumbel-TopK",
        "mitigating hallucinations",
        "common sense generation",
        "robustness to noisy knowledge",
        "dialogue systems",
        "human evaluation"
      ],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/0f7a6c557e376d8c77d684bcda0daee74fc29acf.pdf",
      "citation_key": "li2022ypy",
      "metadata": {
        "title": "Eliciting Knowledge from Large Pre-Trained Models for Unsupervised Knowledge-Grounded Conversation",
        "authors": [
          "Yanyang Li",
          "Jianqiao Zhao",
          "M. Lyu",
          "Liwei Wang"
        ],
        "published_date": "2022",
        "abstract": "Recent advances in large-scale pre-training provide large models with the potential to learn knowledge from the raw text. It is thus natural to ask whether it is possible to leverage these large models as knowledge bases for downstream tasks. In this work, we answer the aforementioned question in unsupervised knowledge-grounded conversation. We explore various methods that best elicit knowledge from large models. Our human study indicates that, though hallucinations exist, large models post the unique advantage of being able to output common sense and summarize facts that cannot be directly retrieved from the search engine. To better exploit such generated knowledge in dialogue generation, we treat the generated knowledge as a noisy knowledge source and propose the posterior-based reweighing as well as the noisy training strategy. Empirical results on two benchmarks show advantages over the state-of-the-art methods.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/0f7a6c557e376d8c77d684bcda0daee74fc29acf.pdf",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Problem:** The paper addresses the challenge of providing external knowledge for unsupervised knowledge-grounded conversation, specifically exploring how to leverage large pre-trained models (LPMs) as dynamic knowledge bases \\cite{li2022ypy}.\n    *   **Importance & Challenge:** Traditional knowledge bases are expensive and time-consuming to construct and maintain. While LPMs show potential for internalizing knowledge and even performing implicit reasoning, they are prone to \"hallucinations\" (generating plausible but factually incorrect information), making their direct use risky \\cite{li2022ypy}. The core challenge is to effectively elicit and exploit this noisy, generated knowledge.\n\n*   **Related Work & Positioning**\n    *   **Existing Approaches:** Prior work in knowledge-grounded conversation typically relies on retrieving knowledge from pre-defined external sources like unstructured documents (e.g., Wikipedia) or structured knowledge graphs \\cite{li2022ypy}. Models like PLATO-KAG, SKT, and KnowledGPT are representative of these approaches, featuring knowledge selection and response generation modules \\cite{li2022ypy}.\n    *   **Limitations of Previous Solutions:** These methods are constrained by the availability and cost of maintaining explicit knowledge bases. Furthermore, existing response generators may not be robust to imperfect or noisy knowledge selection, leading to a training-inference discrepancy \\cite{li2022ypy}.\n\n*   **Technical Approach & Innovation**\n    *   **Core Method:**\n        *   **Knowledge Elicitation:** The paper systematically investigates various tuning methods (fine-tuning, prefix-tuning) and large pre-trained models (PLMs like T5, PDMs like DialoGPT) to generate relevant knowledge from dialogue history \\cite{li2022ypy}.\n        *   **Posterior-based Reweighing:** It treats the generated knowledge from LPMs as a *noisy knowledge source*. This generated knowledge is used to refine the initial knowledge selection distribution by measuring its similarity to retrieved knowledge candidates. This refined distribution is then further updated using a posterior estimation, leveraging the response generator's likelihood to select more appropriate knowledge given the potential future response \\cite{li2022ypy}.\n        *   **Noisy Training Strategy:** To make the response generator more resilient to noisy knowledge during inference, the Gumbel-TopK trick is employed during training. This introduces noise into the knowledge selection process, perturbing the ranking of candidates and strengthening the model's ability to handle imperfect inputs \\cite{li2022ypy}.\n    *   **Novelty/Difference:** This work is novel in systematically exploring LPMs as dynamic knowledge bases for unsupervised knowledge-grounded conversation. It introduces a unique strategy to mitigate LPM hallucinations by treating their output as a noisy signal for reweighing existing knowledge, rather than direct input. The combination of posterior estimation with generated knowledge-based reweighing and a specific noisy training strategy (Gumbel-TopK) is a key innovation \\cite{li2022ypy}. A human study also highlights LPMs' unique ability to generate common sense and summarize facts, going beyond simple retrieval \\cite{li2022ypy}.\n\n*   **Key Technical Contributions**\n    *   **Novel Algorithms/Methods:**\n        *   A comprehensive framework for eliciting and evaluating knowledge from large pre-trained models for dialogue generation \\cite{li2022ypy}.\n        *   **Posterior-based Reweighing:** A novel method to refine knowledge selection by integrating similarity to noisy generated knowledge and a posterior update using the response generator's likelihood \\cite{li2022ypy}.\n        *   **Noisy Training Strategy:** The application of Gumbel-TopK to inject noise into the knowledge selection during training, enhancing the response generator's robustness to noisy knowledge \\cite{li2022ypy}.\n    *   **Theoretical Insights/Analysis:** Demonstrates that LPMs can generate valuable, non-trivial knowledge (common sense, summarized facts) beyond simple memorization, underscoring their potential as dynamic knowledge sources despite inherent noise \\cite{li2022ypy}.\n\n*   **Experimental Validation**\n    *   **Experiments Conducted:**\n        *   **Knowledge Generation Quality:** Automatic (Unigram F1) and extensive human evaluation (using a custom tagset for context understanding, tuning effectiveness, and fact-checking) were performed on knowledge generated by various T5 and DialoGPT models with fine-tuning and prefix-tuning \\cite{li2022ypy}.\n        *   **Response Generation Performance:** Automatic metrics (Perplexity, Unigram F1, P@1 knowledge accuracy) and human evaluation (Coherence, Informativeness, Engagingness, Hallucination) were used to assess dialogue responses \\cite{li2022ypy}.\n    *   **Key Performance Metrics & Comparison Results:**\n        *   Human evaluation of generated knowledge showed high relatedness (98% seen, 88% unseen) and verifiability. Notably, a significant portion of supported facts were *implicitly supported* (summarized/reasoned, not direct copies), and some unvalidated facts aligned with *common sense* \\cite{li2022ypy}.\n        *   The proposed method achieved significant improvements in automatic metrics (PPL, F1, P@1) over state-of-the-art baselines (TMN, SKT, KnowledGPT, PLATO-KAG) on the Wizard of Wikipedia (WoW) and Holl-E datasets \\cite{li2022ypy}.\n        *   Human evaluation confirmed improvements in Coherence, Informativeness, and Engagingness, while maintaining low Hallucination rates, validating the effectiveness of the noisy knowledge handling strategies \\cite{li2022ypy}.\n\n*   **Limitations & Scope**\n    *   **Technical Limitations/Assumptions:** Hallucinations from LPMs persist, necessitating the proposed robust handling mechanisms. The posterior estimation relies on an approximation (mean token probability), and the method requires hyper-parameter tuning for reweighing and posterior sharpness \\cite{li2022ypy}.\n    *   **Scope of Applicability:** The approach is primarily focused on unsupervised knowledge-grounded conversation. It is particularly applicable in scenarios where dynamic, on-demand knowledge is preferred over static, pre-constructed knowledge bases \\cite{li2022ypy}.\n\n*   **Technical Significance**\n    *   **Advances State-of-the-Art:** This paper advances the state-of-the-art by demonstrating a viable and effective method for leveraging large pre-trained models as dynamic knowledge sources for knowledge-grounded conversation, moving beyond the limitations of static knowledge bases \\cite{li2022ypy}. It provides robust strategies to manage the inherent noise (hallucinations) in LPM-generated knowledge, making their outputs practically usable \\cite{li2022ypy}.\n    *   **Potential Impact:** This work opens new research directions for dynamic knowledge acquisition in dialogue systems, potentially reducing the reliance on costly manual knowledge base construction. The proposed noisy training and reweighing strategies could inspire similar approaches for other NLP tasks dealing with uncertain or noisy information from large models, contributing to more scalable and adaptable conversational AI \\cite{li2022ypy}.",
        "keywords": [
          "unsupervised knowledge-grounded conversation",
          "large pre-trained models (LPMs)",
          "dynamic knowledge bases",
          "LPM hallucinations",
          "knowledge elicitation",
          "posterior-based reweighing",
          "noisy training strategy",
          "Gumbel-TopK",
          "mitigating hallucinations",
          "common sense generation",
          "robustness to noisy knowledge",
          "dialogue systems",
          "human evaluation"
        ],
        "paper_type": "based on the abstract and introduction:\n\n*   the abstract explicitly states: \"we... **propose** the posterior-based reweighing as well as the noisy training strategy.\" this directly matches the \"propose\", \"develop\", \"present\", \"algorithm\", \"method\" criteria for a **technical** paper.\n*   the introduction discusses a \"technical problem\" (leveraging large models as knowledge bases for conversation) and sets up the \"proposed solution\" (investigating methods to elicit knowledge and then proposing strategies to exploit it).\n*   while the paper also mentions \"our human study\" and \"empirical results on two benchmarks,\" these are used to validate the effectiveness of the *proposed* methods. the core contribution is the development and proposal of these new strategies.\n\ntherefore, the primary classification is **technical**."
      },
      "file_name": "0f7a6c557e376d8c77d684bcda0daee74fc29acf.pdf"
    },
    {
      "success": true,
      "doc_id": "1968dea72bbf65d5f41f318dc9be7310",
      "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **CITATION**: \\cite{huang2023du3}\n\n### Technical Paper Analysis:\n\n1.  **Research Problem & Motivation**\n    *   **Specific technical problem**: Addresses the pervasive challenge of \"hallucination\" in Multi-Modal Large Language Models (MLLMs), where models generate incorrect, irrelevant, or nonsensical statements about user-provided images \\cite{huang2023du3}.\n    *   **Importance and challenge**: Hallucination significantly impedes the real-world usage of MLLMs, especially in applications demanding precise judgment (e.g., autonomous driving). Existing mitigation methods incur substantial additional costs, such as training with specific data or inferencing with external knowledge \\cite{huang2023du3}. The paper aims to provide a solution without these additional costs.\n\n2.  **Related Work & Positioning**\n    *   **Relation to existing approaches**:\n        *   Acknowledges the impressive capabilities of recent MLLMs (e.g., LLaVA, InstructBLIP, MiniGPT-4, Shikra) but highlights their shared vulnerability to severe hallucination problems \\cite{huang2023du3}.\n        *   Positions its work as a decoding strategy, contrasting with other common decoding methods like Greedy Decoding, Beam Search, Top-k Sampling, Top-p Sampling, and DoLa \\cite{huang2023du3}.\n    *   **Limitations of previous solutions**:\n        *   Existing MLLM hallucination countermeasures (e.g., [29, 42, 47]) typically introduce significant additional costs, such as large quantities of extra data, more powerful external models, or external knowledge \\cite{huang2023du3}.\n        *   The paper aims to overcome these limitations by offering a \"nearly free lunch\" solution that operates during inference without extra training, data, or knowledge \\cite{huang2023du3}.\n\n3.  **Technical Approach & Innovation**\n    *   **Core technical method**: Introduces OPERA (Over-trust Penalty and Retrospection-Allocation), a novel MLLM decoding method applied during beam-search decoding \\cite{huang2023du3}.\n    *   **Novelty/Difference**:\n        *   **Observation-driven**: Grounded in the observation that most hallucinations are closely tied to \"knowledge aggregation patterns\" in the self-attention matrix, where MLLMs \"over-trust\" a few \"summary tokens\" and neglect image tokens \\cite{huang2023du3}.\n        *   **Over-trust Penalty**: Introduces a penalty term on the model logits during beam-search decoding. This penalty is derived from a column-wise metric calculated on a local window of the self-attention map, indicating the \"over-trust degree\" towards summary tokens \\cite{huang2023du3}.\n        *   **Retrospection-Allocation Strategy**: Addresses the hysteresis of aggregation patterns by allowing the decoding process to \"roll back\" to the position of a summary token and re-select better candidates if a strong over-trust pattern is detected (e.g., when the location overlap of maximum in-window penalty scores reaches a threshold) \\cite{huang2023du3}.\n        *   **Cost-effectiveness**: Achieves hallucination mitigation without requiring additional data, external knowledge, or training, making it a \"nearly free lunch\" solution \\cite{huang2023du3}.\n\n4.  **Key Technical Contributions**\n    *   **Novel algorithms, methods, or techniques**:\n        *   **Over-Trust Logit Penalty**: A mechanism to detect and penalize knowledge aggregation patterns in the self-attention map during decoding, integrated into the beam search score calculation \\cite{huang2023du3}.\n        *   **Retrospection-Allocation Strategy**: A dynamic rollback and re-selection mechanism that allows the model to correct its decoding path when over-trust patterns are strongly detected, mitigating the hysteresis issue \\cite{huang2023du3}.\n    *   **Theoretical insights or analysis**:\n        *   Identifies and characterizes \"partial over-trust\" and \"knowledge aggregation patterns\" in MLLM self-attention as a root cause of hallucination, showing that MLLMs tend to generate new tokens by focusing on a few summary tokens rather than all previous tokens, leading to neglect of visual information \\cite{huang2023du3}.\n        *   Demonstrates a clear positive correlation between the appearance of summary tokens (aggregation patterns) and the increase in hallucination scores (CHAIR scores) \\cite{huang2023du3}.\n\n5.  **Experimental Validation**\n    *   **Experiments conducted**: Extensive experiments were conducted on various MLLM models (InstructBLIP, MiniGPT-4, LLaVA-1.5, Shikra) using standard benchmarks and hallucination metrics \\cite{huang2023du3}. GPT-4/GPT-4V assessments were also utilized for evaluation \\cite{huang2023du3}.\n    *   **Key performance metrics and comparison results**: OPERA demonstrated significant hallucination-mitigating performance across different MLLMs and metrics, proving its effectiveness and generality \\cite{huang2023du3}. The evaluations, including GPT assessments, confirmed its superior performance in reducing hallucinations \\cite{huang2023du3}.\n\n6.  **Limitations & Scope**\n    *   **Technical limitations or assumptions**: The method's effectiveness relies on the accurate detection and characterization of \"over-trust patterns\" within the self-attention mechanism. The performance may be sensitive to parameters like the local window size `k`, scaling factor `Ïƒ`, and the retrospection threshold `r` \\cite{huang2023du3}.\n    *   **Scope of applicability**: OPERA is a decoding-time intervention specifically designed for MLLMs using beam search. It does not involve modifications to the model architecture or training process, nor does it require external data or knowledge \\cite{huang2023du3}.\n\n7.  **Technical Significance**\n    *   **Advances the technical state-of-the-art**: OPERA provides a novel, efficient, and cost-effective approach to alleviate MLLM hallucination during inference, without the need for additional training, data, or external knowledge, which is a significant advancement over existing methods \\cite{huang2023du3}.\n    *   **Potential impact on future research**: The insights into the relationship between self-attention patterns, \"over-trust,\" and hallucination could inspire future research into more robust MLLM architectures, training objectives that explicitly discourage such patterns, or other decoding-time interventions for improving factual consistency and faithfulness to visual input \\cite{huang2023du3}.",
      "intriguing_abstract": "The pervasive problem of hallucination critically undermines the reliability of Multi-Modal Large Language Models (MLLMs), hindering their deployment in real-world applications. We uncover a fundamental cause: MLLMs often 'over-trust' summary tokens within their self-attention mechanism, neglecting crucial visual information and leading to knowledge aggregation patterns that precede hallucinatory outputs. To combat this, we propose OPERA (Over-trust Penalty and Retrospection-Allocation), a novel, cost-effective decoding strategy for beam search. OPERA introduces an **Over-trust Penalty** on model logits, derived from local self-attention patterns, to actively discourage over-reliance on summary tokens. Furthermore, its **Retrospection-Allocation strategy** dynamically allows the decoding process to 'roll back' and re-evaluate candidates when strong over-trust patterns are detected, mitigating historical biases. Crucially, OPERA operates entirely during inference, requiring no additional training, data, or external knowledgeâ€”a 'nearly free lunch' solution. Extensive experiments across diverse MLLMs (e.g., InstructBLIP, MiniGPT-4, LLaVA-1.5, Shikra) and benchmarks, validated by GPT-4V, demonstrate significant reductions in hallucination, substantially enhancing factual consistency. This work not only advances the state-of-the-art in MLLM reliability but also offers profound insights into the mechanistic origins of hallucination, paving the way for more trustworthy and robust multi-modal AI.",
      "keywords": [
        "Multi-Modal Large Language Models (MLLMs)",
        "Hallucination mitigation",
        "OPERA (Over-trust Penalty and Retrospection-Allocation)",
        "Decoding strategy",
        "Beam-search decoding",
        "Self-attention mechanism",
        "Knowledge aggregation patterns",
        "Over-trust summary tokens",
        "Over-Trust Logit Penalty",
        "Retrospection-Allocation Strategy",
        "Cost-effective solution",
        "Inference-time intervention"
      ],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/49b79d61ffc2db6dce8c2cd9cda06e1876ed8b4c.pdf",
      "citation_key": "huang2023du3",
      "metadata": {
        "title": "OPERA: Alleviating Hallucination in Multi-Modal Large Language Models via Over-Trust Penalty and Retrospection-Allocation",
        "authors": [
          "Qidong Huang",
          "Xiao-wen Dong",
          "Pan Zhang",
          "Bin Wang",
          "Conghui He",
          "Jiaqi Wang",
          "Dahua Lin",
          "Weiming Zhang",
          "Neng H. Yu"
        ],
        "published_date": "2023",
        "abstract": "Hallucination, posed as a pervasive challenge of multi-modal large language models (MLLMs), has significantly impeded their real-world usage that demands precise judgment. Existing methods mitigate this issue with either training with specific designed data or inferencing with external knowledge from other sources, incurring inevitable additional costs. In this paper, we present OPERA, a novel MLLM decoding method grounded in an Over-trust Penalty and a Retrospection-Allocation strategy, serving as a nearly free lunch to alleviate the hallucination issue without additional data, knowledge, or training. Our approach begins with an interesting observation that, most hallucinations are closely tied to the knowledge aggregation patterns manifested in the self-attention matrix, i.e., MLLMs tend to generate new tokens by focusing on a few summary tokens, but not all the previous tokens. Such partial overtrust inclination results in the neglecting of image tokens and describes the image content with hallucination. Based on the observation, OPERA introduces a penalty term on the model logits during the beam-search decoding to mitigate the over-trust issue, along with a rollback strategy that retrospects the presence of summary tokens in the previously generated tokens, and re-allocate the token selection if necessary. With extensive experiments, OPERA shows significant hallucination-mitigating performance on different MLLMs and metrics, proving its effectiveness and generality. Our code is at: https://github.com/shikiw/OPERA.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/49b79d61ffc2db6dce8c2cd9cda06e1876ed8b4c.pdf",
        "venue": "Computer Vision and Pattern Recognition",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the technical paper for a literature review:\n\n*   **CITATION**: \\cite{huang2023du3}\n\n### Technical Paper Analysis:\n\n1.  **Research Problem & Motivation**\n    *   **Specific technical problem**: Addresses the pervasive challenge of \"hallucination\" in Multi-Modal Large Language Models (MLLMs), where models generate incorrect, irrelevant, or nonsensical statements about user-provided images \\cite{huang2023du3}.\n    *   **Importance and challenge**: Hallucination significantly impedes the real-world usage of MLLMs, especially in applications demanding precise judgment (e.g., autonomous driving). Existing mitigation methods incur substantial additional costs, such as training with specific data or inferencing with external knowledge \\cite{huang2023du3}. The paper aims to provide a solution without these additional costs.\n\n2.  **Related Work & Positioning**\n    *   **Relation to existing approaches**:\n        *   Acknowledges the impressive capabilities of recent MLLMs (e.g., LLaVA, InstructBLIP, MiniGPT-4, Shikra) but highlights their shared vulnerability to severe hallucination problems \\cite{huang2023du3}.\n        *   Positions its work as a decoding strategy, contrasting with other common decoding methods like Greedy Decoding, Beam Search, Top-k Sampling, Top-p Sampling, and DoLa \\cite{huang2023du3}.\n    *   **Limitations of previous solutions**:\n        *   Existing MLLM hallucination countermeasures (e.g., [29, 42, 47]) typically introduce significant additional costs, such as large quantities of extra data, more powerful external models, or external knowledge \\cite{huang2023du3}.\n        *   The paper aims to overcome these limitations by offering a \"nearly free lunch\" solution that operates during inference without extra training, data, or knowledge \\cite{huang2023du3}.\n\n3.  **Technical Approach & Innovation**\n    *   **Core technical method**: Introduces OPERA (Over-trust Penalty and Retrospection-Allocation), a novel MLLM decoding method applied during beam-search decoding \\cite{huang2023du3}.\n    *   **Novelty/Difference**:\n        *   **Observation-driven**: Grounded in the observation that most hallucinations are closely tied to \"knowledge aggregation patterns\" in the self-attention matrix, where MLLMs \"over-trust\" a few \"summary tokens\" and neglect image tokens \\cite{huang2023du3}.\n        *   **Over-trust Penalty**: Introduces a penalty term on the model logits during beam-search decoding. This penalty is derived from a column-wise metric calculated on a local window of the self-attention map, indicating the \"over-trust degree\" towards summary tokens \\cite{huang2023du3}.\n        *   **Retrospection-Allocation Strategy**: Addresses the hysteresis of aggregation patterns by allowing the decoding process to \"roll back\" to the position of a summary token and re-select better candidates if a strong over-trust pattern is detected (e.g., when the location overlap of maximum in-window penalty scores reaches a threshold) \\cite{huang2023du3}.\n        *   **Cost-effectiveness**: Achieves hallucination mitigation without requiring additional data, external knowledge, or training, making it a \"nearly free lunch\" solution \\cite{huang2023du3}.\n\n4.  **Key Technical Contributions**\n    *   **Novel algorithms, methods, or techniques**:\n        *   **Over-Trust Logit Penalty**: A mechanism to detect and penalize knowledge aggregation patterns in the self-attention map during decoding, integrated into the beam search score calculation \\cite{huang2023du3}.\n        *   **Retrospection-Allocation Strategy**: A dynamic rollback and re-selection mechanism that allows the model to correct its decoding path when over-trust patterns are strongly detected, mitigating the hysteresis issue \\cite{huang2023du3}.\n    *   **Theoretical insights or analysis**:\n        *   Identifies and characterizes \"partial over-trust\" and \"knowledge aggregation patterns\" in MLLM self-attention as a root cause of hallucination, showing that MLLMs tend to generate new tokens by focusing on a few summary tokens rather than all previous tokens, leading to neglect of visual information \\cite{huang2023du3}.\n        *   Demonstrates a clear positive correlation between the appearance of summary tokens (aggregation patterns) and the increase in hallucination scores (CHAIR scores) \\cite{huang2023du3}.\n\n5.  **Experimental Validation**\n    *   **Experiments conducted**: Extensive experiments were conducted on various MLLM models (InstructBLIP, MiniGPT-4, LLaVA-1.5, Shikra) using standard benchmarks and hallucination metrics \\cite{huang2023du3}. GPT-4/GPT-4V assessments were also utilized for evaluation \\cite{huang2023du3}.\n    *   **Key performance metrics and comparison results**: OPERA demonstrated significant hallucination-mitigating performance across different MLLMs and metrics, proving its effectiveness and generality \\cite{huang2023du3}. The evaluations, including GPT assessments, confirmed its superior performance in reducing hallucinations \\cite{huang2023du3}.\n\n6.  **Limitations & Scope**\n    *   **Technical limitations or assumptions**: The method's effectiveness relies on the accurate detection and characterization of \"over-trust patterns\" within the self-attention mechanism. The performance may be sensitive to parameters like the local window size `k`, scaling factor `Ïƒ`, and the retrospection threshold `r` \\cite{huang2023du3}.\n    *   **Scope of applicability**: OPERA is a decoding-time intervention specifically designed for MLLMs using beam search. It does not involve modifications to the model architecture or training process, nor does it require external data or knowledge \\cite{huang2023du3}.\n\n7.  **Technical Significance**\n    *   **Advances the technical state-of-the-art**: OPERA provides a novel, efficient, and cost-effective approach to alleviate MLLM hallucination during inference, without the need for additional training, data, or external knowledge, which is a significant advancement over existing methods \\cite{huang2023du3}.\n    *   **Potential impact on future research**: The insights into the relationship between self-attention patterns, \"over-trust,\" and hallucination could inspire future research into more robust MLLM architectures, training objectives that explicitly discourage such patterns, or other decoding-time interventions for improving factual consistency and faithfulness to visual input \\cite{huang2023du3}.",
        "keywords": [
          "Multi-Modal Large Language Models (MLLMs)",
          "Hallucination mitigation",
          "OPERA (Over-trust Penalty and Retrospection-Allocation)",
          "Decoding strategy",
          "Beam-search decoding",
          "Self-attention mechanism",
          "Knowledge aggregation patterns",
          "Over-trust summary tokens",
          "Over-Trust Logit Penalty",
          "Retrospection-Allocation Strategy",
          "Cost-effective solution",
          "Inference-time intervention"
        ],
        "paper_type": "based on the abstract and introduction, this paper is a **technical** paper.\n\nhere's why:\n\n*   **abstract mentions:** \"we present opera, a novel mllm decoding method\", \"our approach begins with an interesting observation...\", \"opera introduces a penalty term... along with a rollback strategy\". these phrases clearly indicate the development and presentation of a new method or algorithm.\n*   **introduction discusses:** the problem of hallucination in mllms and then immediately introduces \"operaâ€™s performance on reducing hallucinations\" (figure 1), implying a proposed solution to this technical problem.\n*   the core contribution is a new \"decoding method\" and \"strategy\" to alleviate a specific technical challenge (hallucination). while it includes \"extensive experiments\" (which could suggest empirical), these experiments are conducted to validate the effectiveness of the *new method* being proposed, making the primary classification \"technical\"."
      },
      "file_name": "49b79d61ffc2db6dce8c2cd9cda06e1876ed8b4c.pdf"
    },
    {
      "success": true,
      "doc_id": "8090f9a8a38dc15d0f1b631f0efd8823",
      "summary": "Here's a focused summary of the paper for a literature review, adhering to your requirements:\n\n---\n\n### Analysis of \"GRAPH ARENA : EVALUATING AND EXPLORING LARGE LANGUAGE MODELS ON GRAPH COMPUTATION\" \\cite{tang2024a1j}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem:** The paper addresses the challenge of rigorously evaluating Large Language Models (LLMs) on their advanced reasoning capabilities, particularly in the domain of real-world graph computational problems.\n    *   **Importance & Challenge:**\n        *   Existing LLM benchmarks suffer from potential data leakage (memorization vs. genuine reasoning) and often rely on synthetic data lacking real-world relevance.\n        *   Current graph problem-solving benchmarks for LLMs are limited by predominantly synthetic graphs (e.g., ErdÅ‘s-RÃ©nyi), tasks confined to basic structural understanding, and evaluation methods that rely on simple string matching, allowing for guesswork rather than demonstrating true logical understanding.\n        *   Graphs are crucial for evaluating LLMs' ability to interpret relational information, process non-sequential data, and generalize across diverse structures, making them an ideal testbed for higher-order reasoning.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches:** \\cite{tang2024a1j} positions GraphArena as a significant advancement over prior graph problem-solving benchmarks like NLGraph \\cite{wang2023nlgraph} and GraphQA \\cite{fatemi2023graphqa}, and algorithmic reasoning datasets such as CLRS-Text \\cite{markeeva2024clrstext} and MAGMA \\cite{taylor2024magma}.\n    *   **Limitations of Previous Solutions:**\n        *   **Synthetic Data:** Previous datasets predominantly use synthetic graphs, failing to capture real-world diversity and complexity.\n        *   **Limited Task Complexity:** Tasks are generally confined to basic structural understanding and direct algorithm execution (e.g., BFS), neglecting higher-order reasoning skills like problem abstraction, strategy comparison, and solving NP-complete problems.\n        *   **Weak Evaluation Metrics:** Evaluation typically relies on string matching of final answers, which can be gamed by models through guesswork and lacks nuanced categorization of failure modes (e.g., infeasible vs. suboptimal solutions).\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method:** GraphArena introduces a comprehensive benchmarking tool for LLMs on graph computational problems, featuring:\n        *   **Realistic Graph Collection:** Utilizes subgraphs sampled from diverse real-world datasets (DBLP, Social Network, DBpedia, OpenFlights, PubChemQC) using a random walk with restart strategy to preserve original graph topology and attributes.\n        *   **Comprehensive Task Selection:** Includes a suite of 10 tasks: four polynomial-time tasks (e.g., Shortest Distance) testing direct algorithmic reasoning, and six NP-complete challenges (e.g., Traveling Salesman Problem) demanding meta-algorithmic planning.\n        *   **Rigorous Path-based Evaluation Framework:** Requires LLMs to generate the entire solution path or its critical components, not just the final answer. This involves a three-step process:\n            1.  **Path Extraction:** Using regular expressions to extract the proposed solution path.\n            2.  **Feasibility Check:** Script-based verification of whether the extracted path meets basic problem requirements.\n            3.  **Optimality Verification:** Calculation of a path score and comparison against the ground truth optimal solution (generated by exact algorithms).\n    *   **Novelty/Differentiation:**\n        *   The use of real-world, contextualized graphs for evaluation, moving beyond synthetic data.\n        *   Inclusion of NP-complete problems to assess higher-level meta-algorithmic planning and strategic decision-making, not just direct algorithm execution.\n        *   The fine-grained, path-based evaluation protocol that differentiates between correct, suboptimal, hallucinatory (infeasible but formatted), and missing responses, providing deeper insights into LLM reasoning failures and preventing pattern-based guessing.\n\n4.  **Key Technical Contributions**\n    *   **Novel Benchmark Design:** Introduction of GraphArena, a novel benchmark specifically designed for evaluating LLMs on real-world graph computational problems, encompassing both polynomial-time and NP-complete tasks.\n    *   **Advanced Graph Sampling Methodology:** Employs a random walk with restart strategy to extract topologically representative subgraphs from large real-world datasets, ensuring ecological validity.\n    *   **Multi-dimensional Task Taxonomy:** Categorization of tasks into \"direct algorithmic reasoning\" (polynomial-time) and \"meta-algorithmic planning\" (NP-complete) to probe different facets of LLM intelligence.\n    *   **Fine-grained Evaluation Protocol:** Development of a three-step (Path Extraction, Feasibility Check, Optimality Verification) evaluation framework that moves beyond simple answer matching to assess the correctness, feasibility, and optimality of the *solution process* generated by LLMs.\n    *   **Exploration of Improvement Strategies:** Investigates the effectiveness of Chain-of-Thought prompting, instruction tuning, code generation, and scaling test-time compute as methods to enhance LLM performance on graph reasoning.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted:**\n        *   Evaluated over 10 prominent LLMs (including GPT-4o, Claude-3.5-sonnet, Llama3, Deepseek-V2) on 10,000 GraphArena problems (500 small, 500 large for each of 10 tasks).\n        *   Compared LLM performance against traditional graph algorithms, Graph Neural Networks (GNNs), and Graph-LLM hybrid approaches.\n        *   Explored four strategies to improve LLM performance: Chain-of-Thought (CoT) prompting, instruction tuning, code writing, and increasing test-time compute.\n    *   **Key Performance Metrics & Results:**\n        *   **LLM Performance:** Top-performing LLMs (e.g., Claude-3.5-sonnet, GPT-4o) struggle significantly with larger and more complex graph problems, particularly NP-complete tasks, exhibiting high hallucination rates (e.g., up to 40.7% for GPT-4o-mini on large polynomial tasks).\n        *   **Task Difficulty:** LLMs perform significantly better on polynomial-time tasks (direct algorithmic reasoning) than on NP-complete tasks (meta-algorithmic planning), where they often default to greedy algorithms.\n        *   **Graph Size Impact:** Performance degrades substantially as graph size increases, with hallucination rates rising.\n        *   **Improvement Strategies:**\n            *   **Chain-of-Thought (CoT):** Showed limited effectiveness on GraphArena.\n            *   **Instruction Tuning:** Enhanced performance on small-scale polynomial tasks but was less effective for large-scale NP problems.\n            *   **Code Generation:** Demonstrated significant promise for solving large graphs and complex tasks, though some performance degradation was observed on small graphs.\n            *   **Scaling Test-Time Compute:** Yielded modest but consistent performance improvements.\n        *   **Baselines:** The paper notes comparison with traditional algorithms, GNNs, and Graph-LLM hybrids, providing a comprehensive assessment of LLMs' capabilities relative to established methods.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions:**\n        *   The benchmark focuses on specific graph computational problems and does not cover all possible graph-related reasoning tasks (e.g., graph generation, graph representation learning).\n        *   The problem encoding into text can result in long contexts (up to 6,000 tokens), which itself poses a challenge for LLMs, potentially conflating long-context understanding with graph reasoning.\n        *   The study conducted a single run per model due to computational demands, which might not capture the full variability of LLM responses.\n    *   **Scope of Applicability:** GraphArena is primarily applicable for evaluating LLMs' algorithmic reasoning and planning capabilities on discrete graph structures, particularly for problems requiring systematic traversal, search, and optimization. It highlights LLMs' current limitations in handling complex, large-scale graph computations and their tendency to hallucinate infeasible solutions.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art:** GraphArena significantly advances the technical state-of-the-art in LLM evaluation by providing a more realistic, comprehensive, and rigorously evaluated benchmark for graph computational problems. It moves beyond superficial evaluations to probe genuine reasoning and planning abilities.\n    *   **Identification of Key Weaknesses:** The benchmark clearly identifies critical weaknesses in current LLMs, such as their struggle with meta-algorithmic planning, high hallucination rates on complex tasks, and sensitivity to graph size, even for top-tier models.\n    *   **Guidance for Future Research:** The exploration of improvement strategies offers concrete directions for future research, highlighting the potential of code generation and the limitations of simpler prompting techniques and instruction tuning for complex graph reasoning. It also suggests LLMs could serve as alternative heuristics for NP tasks, complementing existing approximation methods.\n    *   **Open-Source Contribution:** The open-sourcing of GraphArena (problems, responses, and codebase) provides a valuable resource for the research community to foster reproducible and comparable advancements in LLM graph reasoning.",
      "intriguing_abstract": "Despite impressive advancements, the true algorithmic reasoning capabilities of Large Language Models (LLMs) on complex, real-world problems remain underexplored. Existing benchmarks often rely on synthetic data or simplistic tasks, failing to rigorously test genuine logical understanding. We introduce **GraphArena**, a novel, comprehensive benchmark designed to evaluate LLMs on diverse graph computational problems, ranging from polynomial-time tasks to challenging NP-complete problems. Unlike prior work, GraphArena leverages subgraphs sampled from real-world datasets, ensuring ecological validity. Our innovative path-based evaluation protocol moves beyond simple answer matching, meticulously assessing the feasibility and optimality of the entire solution process, revealing nuanced failure modes like hallucinations and suboptimal reasoning.\n\nOur extensive evaluation of over ten leading LLMs, including GPT-4o and Claude-3.5-sonnet, reveals significant struggles with larger and NP-complete graph problems, exhibiting high hallucination rates and performance degradation with increasing graph size. While Chain-of-Thought prompting showed limited impact, code generation emerged as a promising strategy for enhancing performance on complex tasks. GraphArena not only exposes critical limitations in current LLM algorithmic reasoning and meta-algorithmic planning but also provides a robust, open-source framework to guide future research towards more capable and reliable AI systems for graph computation.",
      "keywords": [
        "Large Language Models (LLMs)",
        "graph computation",
        "GraphArena benchmark",
        "real-world graphs",
        "NP-complete problems",
        "meta-algorithmic planning",
        "path-based evaluation",
        "hallucination rates",
        "code generation",
        "algorithmic reasoning",
        "graph sampling methodology",
        "LLM evaluation framework",
        "complex graph problems",
        "performance degradation (graph size)"
      ],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/1cc347c97a8f9d30edc809e4f207d64c7b8247b4.pdf",
      "citation_key": "tang2024a1j",
      "metadata": {
        "title": "GraphArena: Evaluating and Exploring Large Language Models on Graph Computation",
        "authors": [
          "Jianheng Tang",
          "Qifan Zhang",
          "Yuhan Li",
          "Nuo Chen",
          "Jia Li"
        ],
        "published_date": "2024",
        "abstract": "The ``arms race'' of Large Language Models (LLMs) demands new benchmarks to examine their progresses. In this paper, we introduce GraphArena, a benchmarking tool designed to evaluate LLMs on real-world graph computational problems. It offers a suite of four polynomial-time tasks (e.g., Shortest Distance) and six NP-complete challenges (e.g., Traveling Salesman Problem). GraphArena features a rigorous evaluation framework that classifies LLM outputs as correct, suboptimal (feasible but not optimal), hallucinatory (properly formatted but infeasible), or missing. Evaluation of over 10 LLMs reveals that even top-performing LLMs struggle with larger, more complex graph problems and exhibit hallucination issues. We further explore four potential solutions to address this issue and improve LLMs on graph computation, including chain-of-thought prompting, instruction tuning, code writing, and scaling test-time compute, each demonstrating unique strengths and limitations. GraphArena complements the existing LLM benchmarks and is open-sourced at https://github.com/squareRoot3/GraphArena.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/1cc347c97a8f9d30edc809e4f207d64c7b8247b4.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 0,
        "score": 0,
        "summary": "Here's a focused summary of the paper for a literature review, adhering to your requirements:\n\n---\n\n### Analysis of \"GRAPH ARENA : EVALUATING AND EXPLORING LARGE LANGUAGE MODELS ON GRAPH COMPUTATION\" \\cite{tang2024a1j}\n\n1.  **Research Problem & Motivation**\n    *   **Specific Technical Problem:** The paper addresses the challenge of rigorously evaluating Large Language Models (LLMs) on their advanced reasoning capabilities, particularly in the domain of real-world graph computational problems.\n    *   **Importance & Challenge:**\n        *   Existing LLM benchmarks suffer from potential data leakage (memorization vs. genuine reasoning) and often rely on synthetic data lacking real-world relevance.\n        *   Current graph problem-solving benchmarks for LLMs are limited by predominantly synthetic graphs (e.g., ErdÅ‘s-RÃ©nyi), tasks confined to basic structural understanding, and evaluation methods that rely on simple string matching, allowing for guesswork rather than demonstrating true logical understanding.\n        *   Graphs are crucial for evaluating LLMs' ability to interpret relational information, process non-sequential data, and generalize across diverse structures, making them an ideal testbed for higher-order reasoning.\n\n2.  **Related Work & Positioning**\n    *   **Relation to Existing Approaches:** \\cite{tang2024a1j} positions GraphArena as a significant advancement over prior graph problem-solving benchmarks like NLGraph \\cite{wang2023nlgraph} and GraphQA \\cite{fatemi2023graphqa}, and algorithmic reasoning datasets such as CLRS-Text \\cite{markeeva2024clrstext} and MAGMA \\cite{taylor2024magma}.\n    *   **Limitations of Previous Solutions:**\n        *   **Synthetic Data:** Previous datasets predominantly use synthetic graphs, failing to capture real-world diversity and complexity.\n        *   **Limited Task Complexity:** Tasks are generally confined to basic structural understanding and direct algorithm execution (e.g., BFS), neglecting higher-order reasoning skills like problem abstraction, strategy comparison, and solving NP-complete problems.\n        *   **Weak Evaluation Metrics:** Evaluation typically relies on string matching of final answers, which can be gamed by models through guesswork and lacks nuanced categorization of failure modes (e.g., infeasible vs. suboptimal solutions).\n\n3.  **Technical Approach & Innovation**\n    *   **Core Technical Method:** GraphArena introduces a comprehensive benchmarking tool for LLMs on graph computational problems, featuring:\n        *   **Realistic Graph Collection:** Utilizes subgraphs sampled from diverse real-world datasets (DBLP, Social Network, DBpedia, OpenFlights, PubChemQC) using a random walk with restart strategy to preserve original graph topology and attributes.\n        *   **Comprehensive Task Selection:** Includes a suite of 10 tasks: four polynomial-time tasks (e.g., Shortest Distance) testing direct algorithmic reasoning, and six NP-complete challenges (e.g., Traveling Salesman Problem) demanding meta-algorithmic planning.\n        *   **Rigorous Path-based Evaluation Framework:** Requires LLMs to generate the entire solution path or its critical components, not just the final answer. This involves a three-step process:\n            1.  **Path Extraction:** Using regular expressions to extract the proposed solution path.\n            2.  **Feasibility Check:** Script-based verification of whether the extracted path meets basic problem requirements.\n            3.  **Optimality Verification:** Calculation of a path score and comparison against the ground truth optimal solution (generated by exact algorithms).\n    *   **Novelty/Differentiation:**\n        *   The use of real-world, contextualized graphs for evaluation, moving beyond synthetic data.\n        *   Inclusion of NP-complete problems to assess higher-level meta-algorithmic planning and strategic decision-making, not just direct algorithm execution.\n        *   The fine-grained, path-based evaluation protocol that differentiates between correct, suboptimal, hallucinatory (infeasible but formatted), and missing responses, providing deeper insights into LLM reasoning failures and preventing pattern-based guessing.\n\n4.  **Key Technical Contributions**\n    *   **Novel Benchmark Design:** Introduction of GraphArena, a novel benchmark specifically designed for evaluating LLMs on real-world graph computational problems, encompassing both polynomial-time and NP-complete tasks.\n    *   **Advanced Graph Sampling Methodology:** Employs a random walk with restart strategy to extract topologically representative subgraphs from large real-world datasets, ensuring ecological validity.\n    *   **Multi-dimensional Task Taxonomy:** Categorization of tasks into \"direct algorithmic reasoning\" (polynomial-time) and \"meta-algorithmic planning\" (NP-complete) to probe different facets of LLM intelligence.\n    *   **Fine-grained Evaluation Protocol:** Development of a three-step (Path Extraction, Feasibility Check, Optimality Verification) evaluation framework that moves beyond simple answer matching to assess the correctness, feasibility, and optimality of the *solution process* generated by LLMs.\n    *   **Exploration of Improvement Strategies:** Investigates the effectiveness of Chain-of-Thought prompting, instruction tuning, code generation, and scaling test-time compute as methods to enhance LLM performance on graph reasoning.\n\n5.  **Experimental Validation**\n    *   **Experiments Conducted:**\n        *   Evaluated over 10 prominent LLMs (including GPT-4o, Claude-3.5-sonnet, Llama3, Deepseek-V2) on 10,000 GraphArena problems (500 small, 500 large for each of 10 tasks).\n        *   Compared LLM performance against traditional graph algorithms, Graph Neural Networks (GNNs), and Graph-LLM hybrid approaches.\n        *   Explored four strategies to improve LLM performance: Chain-of-Thought (CoT) prompting, instruction tuning, code writing, and increasing test-time compute.\n    *   **Key Performance Metrics & Results:**\n        *   **LLM Performance:** Top-performing LLMs (e.g., Claude-3.5-sonnet, GPT-4o) struggle significantly with larger and more complex graph problems, particularly NP-complete tasks, exhibiting high hallucination rates (e.g., up to 40.7% for GPT-4o-mini on large polynomial tasks).\n        *   **Task Difficulty:** LLMs perform significantly better on polynomial-time tasks (direct algorithmic reasoning) than on NP-complete tasks (meta-algorithmic planning), where they often default to greedy algorithms.\n        *   **Graph Size Impact:** Performance degrades substantially as graph size increases, with hallucination rates rising.\n        *   **Improvement Strategies:**\n            *   **Chain-of-Thought (CoT):** Showed limited effectiveness on GraphArena.\n            *   **Instruction Tuning:** Enhanced performance on small-scale polynomial tasks but was less effective for large-scale NP problems.\n            *   **Code Generation:** Demonstrated significant promise for solving large graphs and complex tasks, though some performance degradation was observed on small graphs.\n            *   **Scaling Test-Time Compute:** Yielded modest but consistent performance improvements.\n        *   **Baselines:** The paper notes comparison with traditional algorithms, GNNs, and Graph-LLM hybrids, providing a comprehensive assessment of LLMs' capabilities relative to established methods.\n\n6.  **Limitations & Scope**\n    *   **Technical Limitations/Assumptions:**\n        *   The benchmark focuses on specific graph computational problems and does not cover all possible graph-related reasoning tasks (e.g., graph generation, graph representation learning).\n        *   The problem encoding into text can result in long contexts (up to 6,000 tokens), which itself poses a challenge for LLMs, potentially conflating long-context understanding with graph reasoning.\n        *   The study conducted a single run per model due to computational demands, which might not capture the full variability of LLM responses.\n    *   **Scope of Applicability:** GraphArena is primarily applicable for evaluating LLMs' algorithmic reasoning and planning capabilities on discrete graph structures, particularly for problems requiring systematic traversal, search, and optimization. It highlights LLMs' current limitations in handling complex, large-scale graph computations and their tendency to hallucinate infeasible solutions.\n\n7.  **Technical Significance**\n    *   **Advancement of State-of-the-Art:** GraphArena significantly advances the technical state-of-the-art in LLM evaluation by providing a more realistic, comprehensive, and rigorously evaluated benchmark for graph computational problems. It moves beyond superficial evaluations to probe genuine reasoning and planning abilities.\n    *   **Identification of Key Weaknesses:** The benchmark clearly identifies critical weaknesses in current LLMs, such as their struggle with meta-algorithmic planning, high hallucination rates on complex tasks, and sensitivity to graph size, even for top-tier models.\n    *   **Guidance for Future Research:** The exploration of improvement strategies offers concrete directions for future research, highlighting the potential of code generation and the limitations of simpler prompting techniques and instruction tuning for complex graph reasoning. It also suggests LLMs could serve as alternative heuristics for NP tasks, complementing existing approximation methods.\n    *   **Open-Source Contribution:** The open-sourcing of GraphArena (problems, responses, and codebase) provides a valuable resource for the research community to foster reproducible and comparable advancements in LLM graph reasoning.",
        "keywords": [
          "Large Language Models (LLMs)",
          "graph computation",
          "GraphArena benchmark",
          "real-world graphs",
          "NP-complete problems",
          "meta-algorithmic planning",
          "path-based evaluation",
          "hallucination rates",
          "code generation",
          "algorithmic reasoning",
          "graph sampling methodology",
          "LLM evaluation framework",
          "complex graph problems",
          "performance degradation (graph size)"
        ],
        "paper_type": "the paper introduces \"grapharena,\" a new benchmarking tool designed to evaluate large language models (llms) on graph computational problems. while the creation of grapharena itself involves technical development (new dataset collection, task selection, and a rigorous evaluation framework), the abstract and introduction heavily emphasize the *application* of this tool to conduct a data-driven study.\n\nkey indicators:\n*   **abstract:** \"evaluation of over 10 llms reveals that even top-performing llms struggle...\", \"we further explore four potential solutions to address this issue and improve llms on graph computation...\"\n*   **introduction:** explicitly states, \"our study seeks to address two key research questions:\", followed by the questions themselves (\"to what degree can current llms solve graph computational problems?\" and \"how can we further enhance llms on graph reasoning?\"). it then summarizes the \"findings\" for these questions.\n*   **methodology:** describes evaluating \"ten popular llms on 10,000 grapharena problems.\"\n*   **analysis:** discusses llm performance on different task types, hallucination issues, and the effectiveness of various improvement strategies (chain-of-thought, instruction tuning, code generation, scaling test-time compute).\n\nthe paper's primary goal, as presented in these sections, is to conduct a comprehensive, data-driven investigation into llm capabilities on graph computation and explore ways to improve them, using the newly developed grapharena as its experimental platform. this aligns strongly with the criteria for an empirical paper.\n\n**classification: empirical**"
      },
      "file_name": "1cc347c97a8f9d30edc809e4f207d64c7b8247b4.pdf"
    },
    {
      "success": true,
      "doc_id": "0a5232e5de01e7092f2cbc7d4438069f",
      "summary": "This paper introduces Hallucination-targeted Direct Preference Optimization (HDPO) to mitigate diverse forms of hallucinations in Multimodal Large Language Models (MLLMs) \\cite{fu2024yqj}.\n\n### 1. Research Problem & Motivation\n\n*   **Specific Technical Problem**: MLLMs are prone to \"hallucinations,\" generating content that is unfaithful or misaligned with visual input \\cite{fu2024yqj}. This issue manifests in various forms, including insufficient visual understanding, errors in long-context generation, and conflicts between multimodal information \\cite{fu2024yqj}.\n*   **Importance and Challenge**: Hallucinations severely undermine the reliability and practicality of MLLMs, especially in critical applications like autonomous driving or medical tasks \\cite{fu2024yqj}. Existing methods often show inconsistent improvements across different hallucination types or fail to address the fundamental causes, making comprehensive mitigation challenging \\cite{fu2024yqj}.\n\n### 2. Related Work & Positioning\n\n*   **Relation to Existing Approaches**:\n    *   **Training-Free Methods (e.g., VCD, OPERA, WoodPecker)**: These methods (e.g., contrastive decoding, external visual models) are simple and training-free but do not fundamentally enhance the MLLM's intrinsic capabilities, often increasing inference load or complexity \\cite{fu2024yqj}.\n    *   **Direct Preference Optimization (DPO) Methods (e.g., HA-DPO, SeVa, BPO)**: Pioneer DPO methods encourage learning from positive/negative sample comparisons to alleviate hallucinations \\cite{fu2024yqj}.\n*   **Limitations of Previous Solutions**:\n    *   Training-free methods are superficial, not improving core model capabilities \\cite{fu2024yqj}.\n    *   Existing DPO methods show inconsistent improvements across different MLLM hallucination tasks (e.g., VQA vs. captioning) \\cite{fu2024yqj}.\n    *   Their effectiveness is often tied to the format of training data (e.g., SeVa performs well on VQA due to VQA-centric data but poorly on captioning) \\cite{fu2024yqj}.\n    *   Crucially, previous DPO methods do not explicitly consider or target the diverse underlying causes and forms of MLLM hallucinations \\cite{fu2024yqj}.\n\n### 3. Technical Approach & Innovation\n\n*   **Core Technical Method**: The paper proposes Hallucination-targeted Direct Preference Optimization (HDPO), which constructs specific preference pair data designed to address three identified causes of MLLM hallucinations \\cite{fu2024yqj}.\n*   **Novelty/Differentiation**: Unlike previous DPO approaches that use general preference data or suboptimal negative examples, HDPO explicitly targets hallucinations from their diverse forms and causes through novel data construction strategies \\cite{fu2024yqj}.\n    *   **Visual Distracted Hallucination (VDH)**: Negative samples are generated by preserving only visual tokens with the lowest attention scores during autoregressive decoding, amplifying the influence of irrelevant visual information to train the model to focus on more important visual cues \\cite{fu2024yqj}.\n    *   **Long Context Hallucination (LCH)**: Positive examples are high-quality long-form captions. Negative examples are created by truncating the end of a positive example and having the MLLM continue generation from the prefix, often leading to content deviation from the image. A \"hint phrase\" and modified system prompt are used to encourage longer, error-prone generations \\cite{fu2024yqj}.\n    *   **Multimodal Conflict Hallucination (MCH)**: Negative examples are generated by prompting the MLLM with questions containing conflicting information (rewritten by GPT-4o-mini) at the beginning of the prompt, which often leads the model to hallucinate based on the text rather than the image \\cite{fu2024yqj}. The model is then trained to correctly respond despite these conflicting prefixes \\cite{fu2024yqj}.\n\n### 4. Key Technical Contributions\n\n*   **Novel Analysis of Hallucination Causes**: Identification and analysis of three distinct, key causes of MLLM hallucinations: insufficient visual capability, incapable long-context generation, and multimodal conflicts \\cite{fu2024yqj}.\n*   **Hallucination-targeted DPO Framework (HDPO)**: A novel DPO framework that jointly addresses multiple types of MLLM hallucinations by explicitly designing preference data for each identified cause \\cite{fu2024yqj}.\n*   **Innovative DPO Data Construction Strategies**: Development of three specific methods for generating positive/negative preference pairs (VDH, LCH, MCH) that directly target and simulate different hallucination types, guiding the model to learn robust alignment \\cite{fu2024yqj}.\n\n### 5. Experimental Validation\n\n*   **Experiments Conducted**: Extensive experiments were conducted to evaluate HDPO across various types of MLLM hallucination tasks \\cite{fu2024yqj}. Ablation studies and in-depth analyses were also performed \\cite{fu2024yqj}.\n*   **Key Performance Metrics and Comparison Results**:\n    *   The abstract states that HDPO achieves \"superior performance across multiple hallucination evaluation datasets\" \\cite{fu2024yqj}.\n    *   It \"surpasses most state-of-the-art (SOTA) methods\" \\cite{fu2024yqj}.\n    *   The method demonstrates \"consistent improvements in all types of M-hallu tasks\" \\cite{fu2024yqj}.\n    *   Ablation studies confirmed the effectiveness of the method and suggested potential for further improvements through scaling up \\cite{fu2024yqj}.\n    *   (Specific datasets and quantitative results are not provided in the excerpt, but the abstract and introduction strongly indicate comprehensive evaluation.)\n\n### 6. Limitations & Scope\n\n*   **Technical Limitations/Assumptions**: The paper does not explicitly state technical limitations within the provided excerpt. However, the reliance on GPT-4o-mini for generating conflicting information in MCH implies a dependency on external LLMs for data creation \\cite{fu2024yqj}. The effectiveness of the \"hint phrase\" and system prompt for LCH also suggests a degree of prompt engineering \\cite{fu2024yqj}.\n*   **Scope of Applicability**: HDPO is designed for general MLLMs and aims to mitigate hallucinations in both discriminative tasks (e.g., VQA) and generative tasks (e.g., image captioning) \\cite{fu2024yqj}. The approach is broadly applicable to MLLMs that utilize DPO for fine-tuning.\n\n### 7. Technical Significance\n\n*   **Advancement of State-of-the-Art**: HDPO advances the technical state-of-the-art by providing a more targeted and comprehensive approach to MLLM hallucination mitigation compared to previous DPO methods \\cite{fu2024yqj}. By addressing the root causes and diverse forms of hallucinations, it achieves more consistent and superior performance across various tasks \\cite{fu2024yqj}.\n*   **Potential Impact on Future Research**: The detailed analysis of hallucination causes and the novel data construction strategies offer valuable insights for future research in MLLM alignment and safety \\cite{fu2024yqj}. It highlights the importance of designing preference optimization data that specifically targets known failure modes, potentially leading to more robust and reliable multimodal AI systems \\cite{fu2024yqj}. The suggestion for further improvements through scaling up also points to future research directions \\cite{fu2024yqj}.",
      "intriguing_abstract": "Multimodal Large Language Models (MLLMs) are revolutionizing AI, yet their pervasive \"hallucinations\"â€”generating content unfaithful or misaligned with visual inputâ€”severely limit their reliability, especially in critical applications. Current Direct Preference Optimization (DPO) methods offer inconsistent mitigation, often failing to address the diverse root causes of these complex errors.\n\nWe introduce **Hallucination-targeted Direct Preference Optimization (HDPO)**, a novel framework that fundamentally redefines MLLM hallucination mitigation. Unlike prior approaches, HDPO meticulously dissects and directly targets three distinct, critical hallucination types: **Visual Distracted Hallucination (VDH)**, **Long Context Hallucination (LCH)**, and **Multimodal Conflict Hallucination (MCH)**. Our core innovation lies in developing sophisticated, cause-specific preference data construction strategies for each type, guiding the MLLM to learn robust visual understanding, coherent long-context generation, and accurate multimodal alignment.\n\nExtensive experiments demonstrate HDPO's superior and consistent performance across various hallucination evaluation datasets, significantly surpassing state-of-the-art methods. This targeted approach not only advances the reliability of MLLMs but also offers a crucial paradigm for building safer, more trustworthy multimodal AI systems.",
      "keywords": [
        "Hallucination-targeted Direct Preference Optimization (HDPO)",
        "Multimodal Large Language Models (MLLMs)",
        "MLLM hallucinations",
        "Direct Preference Optimization (DPO)",
        "novel preference data construction",
        "Visual Distracted Hallucination (VDH)",
        "Long Context Hallucination (LCH)",
        "Multimodal Conflict Hallucination (MCH)",
        "analysis of hallucination causes",
        "robust MLLM alignment",
        "superior performance",
        "state-of-the-art mitigation"
      ],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/dd6b124606e3696dcddc93c889a824feaa322117.pdf",
      "citation_key": "fu2024yqj",
      "metadata": {
        "title": "Mitigating Hallucination in Multimodal Large Language Model via Hallucination-targeted Direct Preference Optimization",
        "authors": [
          "Yuhan Fu",
          "Ruobing Xie",
          "Xingwu Sun",
          "Zhanhui Kang",
          "Xirong Li"
        ],
        "published_date": "2024",
        "abstract": "Multimodal Large Language Models (MLLMs) are known to hallucinate, which limits their practical applications. Recent works have attempted to apply Direct Preference Optimization (DPO) to enhance the performance of MLLMs, but have shown inconsistent improvements in mitigating hallucinations. To address this issue more effectively, we introduce Hallucination-targeted Direct Preference Optimization (HDPO) to reduce hallucinations in MLLMs. Unlike previous approaches, our method tackles hallucinations from their diverse forms and causes. Specifically, we develop three types of preference pair data targeting the following causes of MLLM hallucinations: (1) insufficient visual capabilities, (2) long context generation, and (3) multimodal conflicts. Experimental results demonstrate that our method achieves superior performance across multiple hallucination evaluation datasets, surpassing most state-of-the-art (SOTA) methods and highlighting the potential of our approach. Ablation studies and in-depth analyses further confirm the effectiveness of our method and suggest the potential for further improvements through scaling up.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/dd6b124606e3696dcddc93c889a824feaa322117.pdf",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "citationCount": 0,
        "score": 0,
        "summary": "This paper introduces Hallucination-targeted Direct Preference Optimization (HDPO) to mitigate diverse forms of hallucinations in Multimodal Large Language Models (MLLMs) \\cite{fu2024yqj}.\n\n### 1. Research Problem & Motivation\n\n*   **Specific Technical Problem**: MLLMs are prone to \"hallucinations,\" generating content that is unfaithful or misaligned with visual input \\cite{fu2024yqj}. This issue manifests in various forms, including insufficient visual understanding, errors in long-context generation, and conflicts between multimodal information \\cite{fu2024yqj}.\n*   **Importance and Challenge**: Hallucinations severely undermine the reliability and practicality of MLLMs, especially in critical applications like autonomous driving or medical tasks \\cite{fu2024yqj}. Existing methods often show inconsistent improvements across different hallucination types or fail to address the fundamental causes, making comprehensive mitigation challenging \\cite{fu2024yqj}.\n\n### 2. Related Work & Positioning\n\n*   **Relation to Existing Approaches**:\n    *   **Training-Free Methods (e.g., VCD, OPERA, WoodPecker)**: These methods (e.g., contrastive decoding, external visual models) are simple and training-free but do not fundamentally enhance the MLLM's intrinsic capabilities, often increasing inference load or complexity \\cite{fu2024yqj}.\n    *   **Direct Preference Optimization (DPO) Methods (e.g., HA-DPO, SeVa, BPO)**: Pioneer DPO methods encourage learning from positive/negative sample comparisons to alleviate hallucinations \\cite{fu2024yqj}.\n*   **Limitations of Previous Solutions**:\n    *   Training-free methods are superficial, not improving core model capabilities \\cite{fu2024yqj}.\n    *   Existing DPO methods show inconsistent improvements across different MLLM hallucination tasks (e.g., VQA vs. captioning) \\cite{fu2024yqj}.\n    *   Their effectiveness is often tied to the format of training data (e.g., SeVa performs well on VQA due to VQA-centric data but poorly on captioning) \\cite{fu2024yqj}.\n    *   Crucially, previous DPO methods do not explicitly consider or target the diverse underlying causes and forms of MLLM hallucinations \\cite{fu2024yqj}.\n\n### 3. Technical Approach & Innovation\n\n*   **Core Technical Method**: The paper proposes Hallucination-targeted Direct Preference Optimization (HDPO), which constructs specific preference pair data designed to address three identified causes of MLLM hallucinations \\cite{fu2024yqj}.\n*   **Novelty/Differentiation**: Unlike previous DPO approaches that use general preference data or suboptimal negative examples, HDPO explicitly targets hallucinations from their diverse forms and causes through novel data construction strategies \\cite{fu2024yqj}.\n    *   **Visual Distracted Hallucination (VDH)**: Negative samples are generated by preserving only visual tokens with the lowest attention scores during autoregressive decoding, amplifying the influence of irrelevant visual information to train the model to focus on more important visual cues \\cite{fu2024yqj}.\n    *   **Long Context Hallucination (LCH)**: Positive examples are high-quality long-form captions. Negative examples are created by truncating the end of a positive example and having the MLLM continue generation from the prefix, often leading to content deviation from the image. A \"hint phrase\" and modified system prompt are used to encourage longer, error-prone generations \\cite{fu2024yqj}.\n    *   **Multimodal Conflict Hallucination (MCH)**: Negative examples are generated by prompting the MLLM with questions containing conflicting information (rewritten by GPT-4o-mini) at the beginning of the prompt, which often leads the model to hallucinate based on the text rather than the image \\cite{fu2024yqj}. The model is then trained to correctly respond despite these conflicting prefixes \\cite{fu2024yqj}.\n\n### 4. Key Technical Contributions\n\n*   **Novel Analysis of Hallucination Causes**: Identification and analysis of three distinct, key causes of MLLM hallucinations: insufficient visual capability, incapable long-context generation, and multimodal conflicts \\cite{fu2024yqj}.\n*   **Hallucination-targeted DPO Framework (HDPO)**: A novel DPO framework that jointly addresses multiple types of MLLM hallucinations by explicitly designing preference data for each identified cause \\cite{fu2024yqj}.\n*   **Innovative DPO Data Construction Strategies**: Development of three specific methods for generating positive/negative preference pairs (VDH, LCH, MCH) that directly target and simulate different hallucination types, guiding the model to learn robust alignment \\cite{fu2024yqj}.\n\n### 5. Experimental Validation\n\n*   **Experiments Conducted**: Extensive experiments were conducted to evaluate HDPO across various types of MLLM hallucination tasks \\cite{fu2024yqj}. Ablation studies and in-depth analyses were also performed \\cite{fu2024yqj}.\n*   **Key Performance Metrics and Comparison Results**:\n    *   The abstract states that HDPO achieves \"superior performance across multiple hallucination evaluation datasets\" \\cite{fu2024yqj}.\n    *   It \"surpasses most state-of-the-art (SOTA) methods\" \\cite{fu2024yqj}.\n    *   The method demonstrates \"consistent improvements in all types of M-hallu tasks\" \\cite{fu2024yqj}.\n    *   Ablation studies confirmed the effectiveness of the method and suggested potential for further improvements through scaling up \\cite{fu2024yqj}.\n    *   (Specific datasets and quantitative results are not provided in the excerpt, but the abstract and introduction strongly indicate comprehensive evaluation.)\n\n### 6. Limitations & Scope\n\n*   **Technical Limitations/Assumptions**: The paper does not explicitly state technical limitations within the provided excerpt. However, the reliance on GPT-4o-mini for generating conflicting information in MCH implies a dependency on external LLMs for data creation \\cite{fu2024yqj}. The effectiveness of the \"hint phrase\" and system prompt for LCH also suggests a degree of prompt engineering \\cite{fu2024yqj}.\n*   **Scope of Applicability**: HDPO is designed for general MLLMs and aims to mitigate hallucinations in both discriminative tasks (e.g., VQA) and generative tasks (e.g., image captioning) \\cite{fu2024yqj}. The approach is broadly applicable to MLLMs that utilize DPO for fine-tuning.\n\n### 7. Technical Significance\n\n*   **Advancement of State-of-the-Art**: HDPO advances the technical state-of-the-art by providing a more targeted and comprehensive approach to MLLM hallucination mitigation compared to previous DPO methods \\cite{fu2024yqj}. By addressing the root causes and diverse forms of hallucinations, it achieves more consistent and superior performance across various tasks \\cite{fu2024yqj}.\n*   **Potential Impact on Future Research**: The detailed analysis of hallucination causes and the novel data construction strategies offer valuable insights for future research in MLLM alignment and safety \\cite{fu2024yqj}. It highlights the importance of designing preference optimization data that specifically targets known failure modes, potentially leading to more robust and reliable multimodal AI systems \\cite{fu2024yqj}. The suggestion for further improvements through scaling up also points to future research directions \\cite{fu2024yqj}.",
        "keywords": [
          "Hallucination-targeted Direct Preference Optimization (HDPO)",
          "Multimodal Large Language Models (MLLMs)",
          "MLLM hallucinations",
          "Direct Preference Optimization (DPO)",
          "novel preference data construction",
          "Visual Distracted Hallucination (VDH)",
          "Long Context Hallucination (LCH)",
          "Multimodal Conflict Hallucination (MCH)",
          "analysis of hallucination causes",
          "robust MLLM alignment",
          "superior performance",
          "state-of-the-art mitigation"
        ],
        "paper_type": "the paper should be classified as **technical**.\n\nhere's why:\n\n*   **abstract:** explicitly states \"we introduce hallucination-targeted direct preference optimization (hdpo)\", \"our method tackles hallucinations\", and \"we develop three types of preference pair data\". it then discusses \"experimental results\" demonstrating the \"superior performance\" of \"our method\". these are strong indicators of presenting a new method or system.\n*   **introduction:** sets up a technical problem (hallucination in mllms), discusses limitations of existing \"approaches\" and \"methods\", and implicitly positions the paper to offer a new, more effective solution.\n*   **keywords from criteria:** \"propose\", \"develop\", \"present\", \"algorithm\", \"method\" are all strongly reflected in the abstract. the introduction discusses a \"technical problem\" and the need for a \"proposed solution\".\n\nwhile the paper also includes \"empirical\" elements (experimental results, ablation studies), its primary contribution is the *development and presentation of a new method* (hdpo) to solve a specific technical problem. the empirical results serve to validate this new technical contribution."
      },
      "file_name": "dd6b124606e3696dcddc93c889a824feaa322117.pdf"
    },
    {
      "success": true,
      "doc_id": "7a52d3278d4316d02648f7db66668fd6",
      "summary": "Here's a focused summary of the paper \"ReAct: Synergizing Reasoning and Acting in Language Models\" by `\\cite{yao20229uz}` for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Large Language Models (LLMs) have shown impressive capabilities in both reasoning (e.g., chain-of-thought prompting) and acting (e.g., action plan generation), but these abilities have primarily been studied and applied in isolation. This separation limits their effectiveness in complex tasks requiring both.\n    *   **Importance and Challenge**:\n        *   **Reasoning-only (CoT)**: Suffers from issues like fact hallucination and error propagation because it relies solely on internal representations and is not grounded in the external world, preventing reactive reasoning or knowledge updates `\\cite{yao20229uz}`.\n        *   **Acting-only**: Lacks the ability for LLMs to reason abstractly about high-level goals, maintain a working memory, or handle exceptions, leading to less robust decision-making in interactive environments `\\cite{yao20229uz}`.\n        *   **Human-like Intelligence**: Humans seamlessly combine verbal reasoning (inner speech) with task-oriented actions for self-regulation, strategization, and robust decision-making, even under uncertainty. Emulating this synergy in LLMs is a key challenge for general task solving `\\cite{yao20229uz}`.\n\n*   **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: `\\cite{yao20229uz}` positions ReAct as a paradigm that bridges the gap between Chain-of-Thought (CoT) reasoning and action generation in LLMs.\n    *   **Limitations of Previous Solutions**:\n        *   **Chain-of-Thought (CoT)**: While effective for multi-step reasoning, CoT is a \"static black box\" that uses internal representations, making it ungrounded, prone to hallucination, and unable to update its knowledge based on external feedback `\\cite{yao20229uz}`.\n        *   **Action-only approaches**: These focus on predicting domain-specific actions from textual observations but typically do not employ LLMs for abstract high-level reasoning or maintaining a working memory to support acting `\\cite{yao20229uz}`. Some limited forms of verbal reasoning exist but are not synergistic.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method**: `\\cite{yao20229uz}` introduces ReAct, a general paradigm that prompts LLMs to generate both verbal reasoning traces (\"thoughts\") and task-specific actions in an *interleaved manner*.\n    *   **Novelty**:\n        *   **Augmented Action Space**: ReAct augments the agent's action space to include language (`^A = A \\cup L`), where `L` is the space of language for \"thoughts.\" Thoughts do not affect the external environment but compose useful information by reasoning over the current context and updating it to support future reasoning or acting `\\cite{yao20229uz}`.\n        *   **Dynamic Synergy**: This interleaved generation allows for dynamic reasoning to create, maintain, and adjust high-level plans for acting (\"reason to act\"), while simultaneously interacting with external environments (e.g., Wikipedia API) to incorporate additional information into reasoning (\"act to reason\") `\\cite{yao20229uz}`.\n        *   **Few-shot Prompting**: ReAct primarily operates by prompting a frozen LLM (PaLM-540B) with few-shot in-context examples, demonstrating strong generalization without extensive training `\\cite{yao20229uz}`.\n        *   **Flexible Thought Occurrence**: For reasoning-intensive tasks, thoughts and actions alternate densely. For decision-making tasks with many actions, thoughts can appear sparsely and asynchronously, allowing the model to decide when reasoning is needed `\\cite{yao20229uz}`.\n\n*   **Key Technical Contributions**\n    *   **Novel Paradigm**: Introduction of ReAct, a prompt-based paradigm to synergize reasoning and acting in language models for general task solving `\\cite{yao20229uz}`.\n    *   **Extensive Empirical Validation**: Demonstrates the advantage of ReAct in a few-shot learning setup across diverse benchmarks over prior approaches that perform either reasoning or action generation in isolation `\\cite{yao20229uz}`.\n    *   **Systematic Ablations and Analysis**: Provides insights into the importance of acting in reasoning tasks and reasoning in interactive tasks `\\cite{yao20229uz}`.\n    *   **Combined Internal and External Knowledge**: Proposes and evaluates methods (e.g., `CoT-SC!ReAct`, `ReAct!CoT-SC`) to combine ReAct's external grounding with CoT-SC's internal reasoning for enhanced performance `\\cite{yao20229uz}`.\n    *   **Interpretability and Controllability**: The interleaved thoughts provide an interpretable, human-aligned, and diagnosable decision-making process, allowing humans to inspect reasoning and factual correctness `\\cite{yao20229uz}`.\n\n*   **Experimental Validation**\n    *   **Tasks**: Evaluated on four diverse benchmarks:\n        *   **Knowledge-intensive Reasoning**: HotpotQA (multi-hop QA) and FEVER (fact verification), using a simple Wikipedia API for external interaction `\\cite{yao20229uz}`.\n        *   **Interactive Decision Making**: ALFWorld (text-based games) and WebShop (webpage navigation) `\\cite{yao20229uz}`.\n    *   **Baselines**: Compared against Standard prompting, Chain-of-Thought (CoT), CoT with Self-Consistency (CoT-SC), and Act-only prompting `\\cite{yao20229uz}`. For interactive tasks, also compared against imitation and reinforcement learning methods.\n    *   **Key Performance Metrics & Results**:\n        *   **HotpotQA & FEVER**: ReAct, with access to a Wikipedia API, outperforms vanilla action generation models and is competitive with CoT. The best approach is a combination of ReAct and CoT-SC (`CoT-SC!ReAct` or `ReAct!CoT-SC`), achieving 35.1 EM on HotpotQA and 64.6 Acc on FEVER, demonstrating the synergy of internal and external knowledge `\\cite{yao20229uz}`.\n        *   **ALFWorld & WebShop**: Few-shot (one or two examples) ReAct prompting significantly outperforms imitation and reinforcement learning methods trained with 10^3-10^5 task instances, with absolute success rate improvements of 34% and 10% respectively `\\cite{yao20229uz}`.\n        *   **Interpretability**: ReAct generates human-like task-solving trajectories that are more interpretable and trustworthy than baselines without reasoning traces `\\cite{yao20229uz}`.\n\n*   **Limitations & Scope**\n    *   **Prompting Setup Limitations**: The current prompting setup has limited support for complex reasoning and acting behaviors `\\cite{yao20229uz}`.\n    *   **Data Scarcity**: Manual annotation of reasoning traces and actions at scale is challenging, suggesting a need for bootstrapping or other data generation methods `\\cite{yao20229uz}`.\n    *   **Future Scaling**: The paper notes that scaling up ReAct with additional training data (e.g., through finetuning) and combining it with complementary paradigms like reinforcement learning could further unlock its potential `\\cite{yao20229uz}`.\n\n*   **Technical Significance**\n    *   **Advances State-of-the-Art**: `\\cite{yao20229uz}` significantly advances the state-of-the-art in LLM capabilities by demonstrating that synergizing reasoning and acting leads to more robust, grounded, and performant agents for diverse tasks, especially in few-shot settings.\n    *   **Improved Robustness and Grounding**: ReAct overcomes prevalent issues of hallucination and error propagation in CoT by grounding reasoning in external information obtained through actions `\\cite{yao20229uz}`.\n    *   **Enhanced Interpretability**: The explicit reasoning traces make LLM decision-making processes more transparent, interpretable, and diagnosable for humans `\\cite{yao20229uz}`.\n    *   **Potential Impact on Future Research**: ReAct opens new avenues for research into more general, human-aligned, and controllable AI agents, suggesting that combining LLMs with interactive environments and explicit reasoning is a powerful direction for developing more capable autonomous systems `\\cite{yao20229uz}`.",
      "intriguing_abstract": "Large Language Models (LLMs) excel at complex reasoning (e.g., Chain-of-Thought) and task-specific acting, but their isolated application leads to critical issues like hallucination and ungrounded knowledge. Inspired by human cognition, we introduce ReAct, a novel prompting paradigm that synergistically interleaves verbal reasoning \"thoughts\" with task-specific \"actions.\" ReAct dynamically augments the agent's action space to include language, enabling LLMs to \"reason to act\" by formulating plans and \"act to reason\" by gathering external information to update their understanding. This dynamic interplay grounds LLM reasoning, mitigating hallucination and error propagation, while fostering robust decision-making in interactive environments. Operating with few-shot examples, ReAct achieves state-of-the-art performance across diverse benchmarks, including knowledge-intensive QA (HotpotQA, FEVER) and interactive decision-making (ALFWorld, WebShop), significantly outperforming prior methods. Furthermore, ReAct's explicit thought traces offer unprecedented interpretability and diagnosability. This work represents a significant step towards building more general, grounded, and human-aligned AI agents.",
      "keywords": [
        "ReAct paradigm",
        "synergizing reasoning and acting",
        "Large Language Models (LLMs)",
        "Chain-of-Thought (CoT)",
        "interleaved generation",
        "few-shot prompting",
        "external environment grounding",
        "augmented action space",
        "knowledge-intensive reasoning",
        "interactive decision making",
        "hallucination mitigation",
        "enhanced interpretability",
        "robustness",
        "general task solving"
      ],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/99832586d55f540f603637e458a292406a0ed75d.pdf",
      "citation_key": "yao20229uz",
      "metadata": {
        "title": "ReAct: Synergizing Reasoning and Acting in Language Models",
        "authors": [
          "Shunyu Yao",
          "Jeffrey Zhao",
          "Dian Yu",
          "Nan Du",
          "Izhak Shafran",
          "Karthik Narasimhan",
          "Yuan Cao"
        ],
        "published_date": "2022",
        "abstract": "While large language models (LLMs) have demonstrated impressive capabilities across tasks in language understanding and interactive decision making, their abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action plan generation) have primarily been studied as separate topics. In this paper, we explore the use of LLMs to generate both reasoning traces and task-specific actions in an interleaved manner, allowing for greater synergy between the two: reasoning traces help the model induce, track, and update action plans as well as handle exceptions, while actions allow it to interface with external sources, such as knowledge bases or environments, to gather additional information. We apply our approach, named ReAct, to a diverse set of language and decision making tasks and demonstrate its effectiveness over state-of-the-art baselines, as well as improved human interpretability and trustworthiness over methods without reasoning or acting components. Concretely, on question answering (HotpotQA) and fact verification (Fever), ReAct overcomes issues of hallucination and error propagation prevalent in chain-of-thought reasoning by interacting with a simple Wikipedia API, and generates human-like task-solving trajectories that are more interpretable than baselines without reasoning traces. On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples. Project site with code: https://react-lm.github.io",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/99832586d55f540f603637e458a292406a0ed75d.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 3859,
        "score": 1286.3333333333333,
        "summary": "Here's a focused summary of the paper \"ReAct: Synergizing Reasoning and Acting in Language Models\" by `\\cite{yao20229uz}` for a literature review:\n\n*   **Research Problem & Motivation**\n    *   **Specific Technical Problem**: Large Language Models (LLMs) have shown impressive capabilities in both reasoning (e.g., chain-of-thought prompting) and acting (e.g., action plan generation), but these abilities have primarily been studied and applied in isolation. This separation limits their effectiveness in complex tasks requiring both.\n    *   **Importance and Challenge**:\n        *   **Reasoning-only (CoT)**: Suffers from issues like fact hallucination and error propagation because it relies solely on internal representations and is not grounded in the external world, preventing reactive reasoning or knowledge updates `\\cite{yao20229uz}`.\n        *   **Acting-only**: Lacks the ability for LLMs to reason abstractly about high-level goals, maintain a working memory, or handle exceptions, leading to less robust decision-making in interactive environments `\\cite{yao20229uz}`.\n        *   **Human-like Intelligence**: Humans seamlessly combine verbal reasoning (inner speech) with task-oriented actions for self-regulation, strategization, and robust decision-making, even under uncertainty. Emulating this synergy in LLMs is a key challenge for general task solving `\\cite{yao20229uz}`.\n\n*   **Related Work & Positioning**\n    *   **Relation to Existing Approaches**: `\\cite{yao20229uz}` positions ReAct as a paradigm that bridges the gap between Chain-of-Thought (CoT) reasoning and action generation in LLMs.\n    *   **Limitations of Previous Solutions**:\n        *   **Chain-of-Thought (CoT)**: While effective for multi-step reasoning, CoT is a \"static black box\" that uses internal representations, making it ungrounded, prone to hallucination, and unable to update its knowledge based on external feedback `\\cite{yao20229uz}`.\n        *   **Action-only approaches**: These focus on predicting domain-specific actions from textual observations but typically do not employ LLMs for abstract high-level reasoning or maintaining a working memory to support acting `\\cite{yao20229uz}`. Some limited forms of verbal reasoning exist but are not synergistic.\n\n*   **Technical Approach & Innovation**\n    *   **Core Technical Method**: `\\cite{yao20229uz}` introduces ReAct, a general paradigm that prompts LLMs to generate both verbal reasoning traces (\"thoughts\") and task-specific actions in an *interleaved manner*.\n    *   **Novelty**:\n        *   **Augmented Action Space**: ReAct augments the agent's action space to include language (`^A = A \\cup L`), where `L` is the space of language for \"thoughts.\" Thoughts do not affect the external environment but compose useful information by reasoning over the current context and updating it to support future reasoning or acting `\\cite{yao20229uz}`.\n        *   **Dynamic Synergy**: This interleaved generation allows for dynamic reasoning to create, maintain, and adjust high-level plans for acting (\"reason to act\"), while simultaneously interacting with external environments (e.g., Wikipedia API) to incorporate additional information into reasoning (\"act to reason\") `\\cite{yao20229uz}`.\n        *   **Few-shot Prompting**: ReAct primarily operates by prompting a frozen LLM (PaLM-540B) with few-shot in-context examples, demonstrating strong generalization without extensive training `\\cite{yao20229uz}`.\n        *   **Flexible Thought Occurrence**: For reasoning-intensive tasks, thoughts and actions alternate densely. For decision-making tasks with many actions, thoughts can appear sparsely and asynchronously, allowing the model to decide when reasoning is needed `\\cite{yao20229uz}`.\n\n*   **Key Technical Contributions**\n    *   **Novel Paradigm**: Introduction of ReAct, a prompt-based paradigm to synergize reasoning and acting in language models for general task solving `\\cite{yao20229uz}`.\n    *   **Extensive Empirical Validation**: Demonstrates the advantage of ReAct in a few-shot learning setup across diverse benchmarks over prior approaches that perform either reasoning or action generation in isolation `\\cite{yao20229uz}`.\n    *   **Systematic Ablations and Analysis**: Provides insights into the importance of acting in reasoning tasks and reasoning in interactive tasks `\\cite{yao20229uz}`.\n    *   **Combined Internal and External Knowledge**: Proposes and evaluates methods (e.g., `CoT-SC!ReAct`, `ReAct!CoT-SC`) to combine ReAct's external grounding with CoT-SC's internal reasoning for enhanced performance `\\cite{yao20229uz}`.\n    *   **Interpretability and Controllability**: The interleaved thoughts provide an interpretable, human-aligned, and diagnosable decision-making process, allowing humans to inspect reasoning and factual correctness `\\cite{yao20229uz}`.\n\n*   **Experimental Validation**\n    *   **Tasks**: Evaluated on four diverse benchmarks:\n        *   **Knowledge-intensive Reasoning**: HotpotQA (multi-hop QA) and FEVER (fact verification), using a simple Wikipedia API for external interaction `\\cite{yao20229uz}`.\n        *   **Interactive Decision Making**: ALFWorld (text-based games) and WebShop (webpage navigation) `\\cite{yao20229uz}`.\n    *   **Baselines**: Compared against Standard prompting, Chain-of-Thought (CoT), CoT with Self-Consistency (CoT-SC), and Act-only prompting `\\cite{yao20229uz}`. For interactive tasks, also compared against imitation and reinforcement learning methods.\n    *   **Key Performance Metrics & Results**:\n        *   **HotpotQA & FEVER**: ReAct, with access to a Wikipedia API, outperforms vanilla action generation models and is competitive with CoT. The best approach is a combination of ReAct and CoT-SC (`CoT-SC!ReAct` or `ReAct!CoT-SC`), achieving 35.1 EM on HotpotQA and 64.6 Acc on FEVER, demonstrating the synergy of internal and external knowledge `\\cite{yao20229uz}`.\n        *   **ALFWorld & WebShop**: Few-shot (one or two examples) ReAct prompting significantly outperforms imitation and reinforcement learning methods trained with 10^3-10^5 task instances, with absolute success rate improvements of 34% and 10% respectively `\\cite{yao20229uz}`.\n        *   **Interpretability**: ReAct generates human-like task-solving trajectories that are more interpretable and trustworthy than baselines without reasoning traces `\\cite{yao20229uz}`.\n\n*   **Limitations & Scope**\n    *   **Prompting Setup Limitations**: The current prompting setup has limited support for complex reasoning and acting behaviors `\\cite{yao20229uz}`.\n    *   **Data Scarcity**: Manual annotation of reasoning traces and actions at scale is challenging, suggesting a need for bootstrapping or other data generation methods `\\cite{yao20229uz}`.\n    *   **Future Scaling**: The paper notes that scaling up ReAct with additional training data (e.g., through finetuning) and combining it with complementary paradigms like reinforcement learning could further unlock its potential `\\cite{yao20229uz}`.\n\n*   **Technical Significance**\n    *   **Advances State-of-the-Art**: `\\cite{yao20229uz}` significantly advances the state-of-the-art in LLM capabilities by demonstrating that synergizing reasoning and acting leads to more robust, grounded, and performant agents for diverse tasks, especially in few-shot settings.\n    *   **Improved Robustness and Grounding**: ReAct overcomes prevalent issues of hallucination and error propagation in CoT by grounding reasoning in external information obtained through actions `\\cite{yao20229uz}`.\n    *   **Enhanced Interpretability**: The explicit reasoning traces make LLM decision-making processes more transparent, interpretable, and diagnosable for humans `\\cite{yao20229uz}`.\n    *   **Potential Impact on Future Research**: ReAct opens new avenues for research into more general, human-aligned, and controllable AI agents, suggesting that combining LLMs with interactive environments and explicit reasoning is a powerful direction for developing more capable autonomous systems `\\cite{yao20229uz}`.",
        "keywords": [
          "ReAct paradigm",
          "synergizing reasoning and acting",
          "Large Language Models (LLMs)",
          "Chain-of-Thought (CoT)",
          "interleaved generation",
          "few-shot prompting",
          "external environment grounding",
          "augmented action space",
          "knowledge-intensive reasoning",
          "interactive decision making",
          "hallucination mitigation",
          "enhanced interpretability",
          "robustness",
          "general task solving"
        ],
        "paper_type": "based on the abstract and introduction:\n\n1.  **\"in this paper, we explore the use of llms to generate both reasoning traces and task-speciï¬c actions in an interleaved manner...\"** - this indicates the proposal of a new approach or method.\n2.  **\"we apply our approach, named react...\"** - clearly states they have developed and are presenting a new system/method called react.\n3.  **\"...demonstrate its effectiveness over state-of-the-art baselines...\"** - this shows empirical evaluation, which is common for technical papers validating their proposed solution.\n4.  **\"react overcomes prevalent issues... by interacting with a simple wikipedia api, and generating human-like task-solving trajectories...\"** - describes the technical mechanism and solution.\n\nthe paper introduces a novel approach/system (\"react\") and then demonstrates its effectiveness through experiments. while it includes strong empirical evidence, its primary contribution is the *development and presentation of a new method*.\n\ntherefore, this paper is best classified as **technical**."
      },
      "file_name": "99832586d55f540f603637e458a292406a0ed75d.pdf"
    },
    {
      "success": true,
      "doc_id": "a3498007bbc909a26921bcc9996249a3",
      "summary": "Charts are commonly used for exploring data and communicating insights. Generating natural language summaries from charts can be very helpful for people in inferring key insights that would otherwise require a lot of cognitive and perceptual efforts. We present Chart-to-text, a large-scale benchmark with two datasets and a total of 44,096 charts covering a wide range of topics and chart types. We explain the dataset construction process and analyze the datasets. We also introduce a number of state-of-the-art neural models as baselines that utilize image captioning and data-to-text generation techniques to tackle two problem variations: one assumes the underlying data table of the chart is available while the other needs to extract data from chart images. Our analysis with automatic and human evaluation shows that while our best models usually generate fluent summaries and yield reasonable BLEU scores, they also suffer from hallucinations and factual errors as well as difficulties in correctly explaining complex patterns and trends in charts.",
      "intriguing_abstract": "Charts are commonly used for exploring data and communicating insights. Generating natural language summaries from charts can be very helpful for people in inferring key insights that would otherwise require a lot of cognitive and perceptual efforts. We present Chart-to-text, a large-scale benchmark with two datasets and a total of 44,096 charts covering a wide range of topics and chart types. We explain the dataset construction process and analyze the datasets. We also introduce a number of state-of-the-art neural models as baselines that utilize image captioning and data-to-text generation techniques to tackle two problem variations: one assumes the underlying data table of the chart is available while the other needs to extract data from chart images. Our analysis with automatic and human evaluation shows that while our best models usually generate fluent summaries and yield reasonable BLEU scores, they also suffer from hallucinations and factual errors as well as difficulties in correctly explaining complex patterns and trends in charts.",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/ca2f63950685a97e5ab6b8e6b2db78a8995e94a2.pdf",
      "citation_key": "kanthara2022kuj",
      "metadata": {
        "title": "Chart-to-Text: A Large-Scale Benchmark for Chart Summarization",
        "authors": [
          "Shankar Kanthara",
          "Rixie Tiffany Ko Leong",
          "Xiang Lin",
          "Ahmed Masry",
          "Megh Thakkar",
          "Enamul Hoque",
          "Shafiq R. Joty"
        ],
        "published_date": "2022",
        "abstract": "Charts are commonly used for exploring data and communicating insights. Generating natural language summaries from charts can be very helpful for people in inferring key insights that would otherwise require a lot of cognitive and perceptual efforts. We present Chart-to-text, a large-scale benchmark with two datasets and a total of 44,096 charts covering a wide range of topics and chart types. We explain the dataset construction process and analyze the datasets. We also introduce a number of state-of-the-art neural models as baselines that utilize image captioning and data-to-text generation techniques to tackle two problem variations: one assumes the underlying data table of the chart is available while the other needs to extract data from chart images. Our analysis with automatic and human evaluation shows that while our best models usually generate fluent summaries and yield reasonable BLEU scores, they also suffer from hallucinations and factual errors as well as difficulties in correctly explaining complex patterns and trends in charts.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/ca2f63950685a97e5ab6b8e6b2db78a8995e94a2.pdf",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "citationCount": 176,
        "score": 58.666666666666664,
        "summary": "Charts are commonly used for exploring data and communicating insights. Generating natural language summaries from charts can be very helpful for people in inferring key insights that would otherwise require a lot of cognitive and perceptual efforts. We present Chart-to-text, a large-scale benchmark with two datasets and a total of 44,096 charts covering a wide range of topics and chart types. We explain the dataset construction process and analyze the datasets. We also introduce a number of state-of-the-art neural models as baselines that utilize image captioning and data-to-text generation techniques to tackle two problem variations: one assumes the underlying data table of the chart is available while the other needs to extract data from chart images. Our analysis with automatic and human evaluation shows that while our best models usually generate fluent summaries and yield reasonable BLEU scores, they also suffer from hallucinations and factual errors as well as difficulties in correctly explaining complex patterns and trends in charts.",
        "keywords": []
      },
      "file_name": "ca2f63950685a97e5ab6b8e6b2db78a8995e94a2.pdf"
    },
    {
      "success": true,
      "doc_id": "1aa6658234b43d7db7fdb6f1cff4bf26",
      "summary": "GPT-3 shows remarkable in-context learning ability of large-scale language models (LMs) trained on hundreds of billion scale data. Here we address some remaining issues less reported by the GPT-3 paper, such as a non-English LM, the performances of different sized models, and the effect of recently introduced prompt optimization on in-context learning. To achieve this, we introduce HyperCLOVA, a Korean variant of 82B GPT-3 trained on a Korean-centric corpus of 560B tokens. Enhanced by our Korean-specific tokenization, HyperCLOVA with our training configuration shows state-of-the-art in-context zero-shot and few-shot learning performances on various downstream tasks in Korean. Also, we show the performance benefits of prompt-based learning and demonstrate how it can be integrated into the prompt engineering pipeline. Then we discuss the possibility of materializing the No Code AI paradigm by providing AI prototyping capabilities to non-experts of ML by introducing HyperCLOVA studio, an interactive prompt engineering interface. Lastly, we demonstrate the potential of our methods with three successful in-house applications.",
      "intriguing_abstract": "GPT-3 shows remarkable in-context learning ability of large-scale language models (LMs) trained on hundreds of billion scale data. Here we address some remaining issues less reported by the GPT-3 paper, such as a non-English LM, the performances of different sized models, and the effect of recently introduced prompt optimization on in-context learning. To achieve this, we introduce HyperCLOVA, a Korean variant of 82B GPT-3 trained on a Korean-centric corpus of 560B tokens. Enhanced by our Korean-specific tokenization, HyperCLOVA with our training configuration shows state-of-the-art in-context zero-shot and few-shot learning performances on various downstream tasks in Korean. Also, we show the performance benefits of prompt-based learning and demonstrate how it can be integrated into the prompt engineering pipeline. Then we discuss the possibility of materializing the No Code AI paradigm by providing AI prototyping capabilities to non-experts of ML by introducing HyperCLOVA studio, an interactive prompt engineering interface. Lastly, we demonstrate the potential of our methods with three successful in-house applications.",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/a6d8d04962f84ae6225e72723869a002b9fc8036.pdf",
      "citation_key": "kim2021obx",
      "metadata": {
        "title": "What Changes Can Large-scale Language Models Bring? Intensive Study on HyperCLOVA: Billions-scale Korean Generative Pretrained Transformers",
        "authors": [
          "Boseop Kim",
          "Hyoungseok Kim",
          "Sang-Woo Lee",
          "Gichang Lee",
          "Donghyun Kwak",
          "D. Jeon",
          "Sunghyun Park",
          "Sungju Kim",
          "Seonhoon Kim",
          "D. Seo",
          "Heungsub Lee",
          "Minyoung Jeong",
          "Sungjae Lee",
          "Minsub Kim",
          "SukHyun Ko",
          "Seokhun Kim",
          "Taeyong Park",
          "Jinuk Kim",
          "Soyoung Kang",
          "Nahyeon Ryu",
          "Kang Min Yoo",
          "Minsuk Chang",
          "Soobin Suh",
          "Sookyo In",
          "Jinseong Park",
          "Kyungduk Kim",
          "Hiun Kim",
          "Jisu Jeong",
          "Y. Yeo",
          "Dong-hyun Ham",
          "Dongju Park",
          "Min Young Lee",
          "Jaewook Kang",
          "Inho Kang",
          "Jung-Woo Ha",
          "W. Park",
          "Nako Sung"
        ],
        "published_date": "2021",
        "abstract": "GPT-3 shows remarkable in-context learning ability of large-scale language models (LMs) trained on hundreds of billion scale data. Here we address some remaining issues less reported by the GPT-3 paper, such as a non-English LM, the performances of different sized models, and the effect of recently introduced prompt optimization on in-context learning. To achieve this, we introduce HyperCLOVA, a Korean variant of 82B GPT-3 trained on a Korean-centric corpus of 560B tokens. Enhanced by our Korean-specific tokenization, HyperCLOVA with our training configuration shows state-of-the-art in-context zero-shot and few-shot learning performances on various downstream tasks in Korean. Also, we show the performance benefits of prompt-based learning and demonstrate how it can be integrated into the prompt engineering pipeline. Then we discuss the possibility of materializing the No Code AI paradigm by providing AI prototyping capabilities to non-experts of ML by introducing HyperCLOVA studio, an interactive prompt engineering interface. Lastly, we demonstrate the potential of our methods with three successful in-house applications.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/a6d8d04962f84ae6225e72723869a002b9fc8036.pdf",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "citationCount": 124,
        "score": 31.0,
        "summary": "GPT-3 shows remarkable in-context learning ability of large-scale language models (LMs) trained on hundreds of billion scale data. Here we address some remaining issues less reported by the GPT-3 paper, such as a non-English LM, the performances of different sized models, and the effect of recently introduced prompt optimization on in-context learning. To achieve this, we introduce HyperCLOVA, a Korean variant of 82B GPT-3 trained on a Korean-centric corpus of 560B tokens. Enhanced by our Korean-specific tokenization, HyperCLOVA with our training configuration shows state-of-the-art in-context zero-shot and few-shot learning performances on various downstream tasks in Korean. Also, we show the performance benefits of prompt-based learning and demonstrate how it can be integrated into the prompt engineering pipeline. Then we discuss the possibility of materializing the No Code AI paradigm by providing AI prototyping capabilities to non-experts of ML by introducing HyperCLOVA studio, an interactive prompt engineering interface. Lastly, we demonstrate the potential of our methods with three successful in-house applications.",
        "keywords": []
      },
      "file_name": "a6d8d04962f84ae6225e72723869a002b9fc8036.pdf"
    },
    {
      "success": true,
      "doc_id": "a5b93191632abd054e8920441c79911b",
      "summary": "Scaling up language models has led to unprecedented performance gains, but little is understood about how the training dynamics change as models get larger. How do language models of different sizes learn during pre-training? Why do larger language models demonstrate more desirable behaviors? In this paper, we analyze the intermediate training checkpoints of differently sized OPT models (Zhang et al., 2022)â€”from 125M to 175B parametersâ€”on next-token prediction, sequence-level generation and downstream tasks. We find that 1) at a given perplexity and independent of model sizes, a similar subset of training tokens see the most significant reduction in loss, with the rest stagnating or showing double-descent behavior (Nakkiran et al., 2020); 2) early in training, all models learn to reduce the perplexity of grammatical sequences that contain hallucinations, with small models halting at this suboptimal distribution and larger ones eventually learning to assign these sequences lower probabilities; and 3) perplexity is a strong predictor of in-context learning performance on 74 multiple-choice tasks from BIG-Bench, and this holds independent of the model size. Together, these results show that perplexity is more predictive of model behaviors than model size or training computation.",
      "intriguing_abstract": "Scaling up language models has led to unprecedented performance gains, but little is understood about how the training dynamics change as models get larger. How do language models of different sizes learn during pre-training? Why do larger language models demonstrate more desirable behaviors? In this paper, we analyze the intermediate training checkpoints of differently sized OPT models (Zhang et al., 2022)â€”from 125M to 175B parametersâ€”on next-token prediction, sequence-level generation and downstream tasks. We find that 1) at a given perplexity and independent of model sizes, a similar subset of training tokens see the most significant reduction in loss, with the rest stagnating or showing double-descent behavior (Nakkiran et al., 2020); 2) early in training, all models learn to reduce the perplexity of grammatical sequences that contain hallucinations, with small models halting at this suboptimal distribution and larger ones eventually learning to assign these sequences lower probabilities; and 3) perplexity is a strong predictor of in-context learning performance on 74 multiple-choice tasks from BIG-Bench, and this holds independent of the model size. Together, these results show that perplexity is more predictive of model behaviors than model size or training computation.",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/5bb3bd2ec1e99b11a84ccd0e4dce4bdb2a776a5e.pdf",
      "citation_key": "xia20224cl",
      "metadata": {
        "title": "Training Trajectories of Language Models Across Scales",
        "authors": [
          "Mengzhou Xia",
          "Mikel Artetxe",
          "Chunting Zhou",
          "Xi Victoria Lin",
          "Ramakanth Pasunuru",
          "Danqi Chen",
          "Luke Zettlemoyer",
          "Ves Stoyanov"
        ],
        "published_date": "2022",
        "abstract": "Scaling up language models has led to unprecedented performance gains, but little is understood about how the training dynamics change as models get larger. How do language models of different sizes learn during pre-training? Why do larger language models demonstrate more desirable behaviors? In this paper, we analyze the intermediate training checkpoints of differently sized OPT models (Zhang et al., 2022)â€”from 125M to 175B parametersâ€”on next-token prediction, sequence-level generation and downstream tasks. We find that 1) at a given perplexity and independent of model sizes, a similar subset of training tokens see the most significant reduction in loss, with the rest stagnating or showing double-descent behavior (Nakkiran et al., 2020); 2) early in training, all models learn to reduce the perplexity of grammatical sequences that contain hallucinations, with small models halting at this suboptimal distribution and larger ones eventually learning to assign these sequences lower probabilities; and 3) perplexity is a strong predictor of in-context learning performance on 74 multiple-choice tasks from BIG-Bench, and this holds independent of the model size. Together, these results show that perplexity is more predictive of model behaviors than model size or training computation.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/5bb3bd2ec1e99b11a84ccd0e4dce4bdb2a776a5e.pdf",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "citationCount": 67,
        "score": 22.333333333333332,
        "summary": "Scaling up language models has led to unprecedented performance gains, but little is understood about how the training dynamics change as models get larger. How do language models of different sizes learn during pre-training? Why do larger language models demonstrate more desirable behaviors? In this paper, we analyze the intermediate training checkpoints of differently sized OPT models (Zhang et al., 2022)â€”from 125M to 175B parametersâ€”on next-token prediction, sequence-level generation and downstream tasks. We find that 1) at a given perplexity and independent of model sizes, a similar subset of training tokens see the most significant reduction in loss, with the rest stagnating or showing double-descent behavior (Nakkiran et al., 2020); 2) early in training, all models learn to reduce the perplexity of grammatical sequences that contain hallucinations, with small models halting at this suboptimal distribution and larger ones eventually learning to assign these sequences lower probabilities; and 3) perplexity is a strong predictor of in-context learning performance on 74 multiple-choice tasks from BIG-Bench, and this holds independent of the model size. Together, these results show that perplexity is more predictive of model behaviors than model size or training computation.",
        "keywords": []
      },
      "file_name": "5bb3bd2ec1e99b11a84ccd0e4dce4bdb2a776a5e.pdf"
    },
    {
      "success": true,
      "doc_id": "97505be595176b5f75764dbf00559c15",
      "summary": "Abstractive summarization has enjoyed renewed interest in recent years, thanks to pre-trained language models and the availability of large-scale datasets. Despite promising results, current models still suffer from generating factually inconsistent summaries, reducing their utility for real-world application. Several recent efforts attempt to address this by devising models that automatically detect factual inconsistencies in machine generated summaries. However, they focus exclusively on English, a language with abundant resources. In this work, we leverage factual consistency evaluation models to improve multilingual summarization. We explore two intuitive approaches to mitigate hallucinations based on the signal provided by a multilingual NLI model, namely data filtering and controlled generation. Experimental results in the 45 languages from the XLSum dataset show gains over strong baselines in both automatic and human evaluation.",
      "intriguing_abstract": "Abstractive summarization has enjoyed renewed interest in recent years, thanks to pre-trained language models and the availability of large-scale datasets. Despite promising results, current models still suffer from generating factually inconsistent summaries, reducing their utility for real-world application. Several recent efforts attempt to address this by devising models that automatically detect factual inconsistencies in machine generated summaries. However, they focus exclusively on English, a language with abundant resources. In this work, we leverage factual consistency evaluation models to improve multilingual summarization. We explore two intuitive approaches to mitigate hallucinations based on the signal provided by a multilingual NLI model, namely data filtering and controlled generation. Experimental results in the 45 languages from the XLSum dataset show gains over strong baselines in both automatic and human evaluation.",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/c4f26bc007343c59bedd1423250a3b453d3d2d22.pdf",
      "citation_key": "aharoni2022ioz",
      "metadata": {
        "title": "mFACE: Multilingual Summarization with Factual Consistency Evaluation",
        "authors": [
          "Roee Aharoni",
          "Shashi Narayan",
          "Joshua Maynez",
          "Jonathan Herzig",
          "Elizabeth Clark",
          "Mirella Lapata"
        ],
        "published_date": "2022",
        "abstract": "Abstractive summarization has enjoyed renewed interest in recent years, thanks to pre-trained language models and the availability of large-scale datasets. Despite promising results, current models still suffer from generating factually inconsistent summaries, reducing their utility for real-world application. Several recent efforts attempt to address this by devising models that automatically detect factual inconsistencies in machine generated summaries. However, they focus exclusively on English, a language with abundant resources. In this work, we leverage factual consistency evaluation models to improve multilingual summarization. We explore two intuitive approaches to mitigate hallucinations based on the signal provided by a multilingual NLI model, namely data filtering and controlled generation. Experimental results in the 45 languages from the XLSum dataset show gains over strong baselines in both automatic and human evaluation.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/c4f26bc007343c59bedd1423250a3b453d3d2d22.pdf",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "citationCount": 48,
        "score": 16.0,
        "summary": "Abstractive summarization has enjoyed renewed interest in recent years, thanks to pre-trained language models and the availability of large-scale datasets. Despite promising results, current models still suffer from generating factually inconsistent summaries, reducing their utility for real-world application. Several recent efforts attempt to address this by devising models that automatically detect factual inconsistencies in machine generated summaries. However, they focus exclusively on English, a language with abundant resources. In this work, we leverage factual consistency evaluation models to improve multilingual summarization. We explore two intuitive approaches to mitigate hallucinations based on the signal provided by a multilingual NLI model, namely data filtering and controlled generation. Experimental results in the 45 languages from the XLSum dataset show gains over strong baselines in both automatic and human evaluation.",
        "keywords": []
      },
      "file_name": "c4f26bc007343c59bedd1423250a3b453d3d2d22.pdf"
    },
    {
      "success": true,
      "doc_id": "bf69e13c4729b74428e75a93542dfad4",
      "summary": "Abstractive summarization systems leveraging pre-training language models have achieved superior results on benchmark datasets. However, such models have been shown to be more prone to hallucinate facts that are unfaithful to the input context. In this paper, we propose a method to remedy entity-level extrinsic hallucinations with Entity Coverage Control (ECC). We first compute entity coverage precision and prepend the corresponding control code for each training example, which implicitly guides the model to recognize faithfulness contents in the training phase. We further extend our method via intermediate fine-tuning on large but noisy data extracted from Wikipedia to unlock zero-shot summarization. We show that the proposed method leads to more faithful and salient abstractive summarization in supervised fine-tuning and zero-shot settings according to our experimental results on three benchmark datasets XSum, Pubmed, and SAMSum of very different domains and styles.",
      "intriguing_abstract": "Abstractive summarization systems leveraging pre-training language models have achieved superior results on benchmark datasets. However, such models have been shown to be more prone to hallucinate facts that are unfaithful to the input context. In this paper, we propose a method to remedy entity-level extrinsic hallucinations with Entity Coverage Control (ECC). We first compute entity coverage precision and prepend the corresponding control code for each training example, which implicitly guides the model to recognize faithfulness contents in the training phase. We further extend our method via intermediate fine-tuning on large but noisy data extracted from Wikipedia to unlock zero-shot summarization. We show that the proposed method leads to more faithful and salient abstractive summarization in supervised fine-tuning and zero-shot settings according to our experimental results on three benchmark datasets XSum, Pubmed, and SAMSum of very different domains and styles.",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/ebb4826b717798918bebed4dfac28c917e44577d.pdf",
      "citation_key": "zhang2022p55",
      "metadata": {
        "title": "Improving the Faithfulness of Abstractive Summarization via Entity Coverage Control",
        "authors": [
          "Haopeng Zhang",
          "Semih Yavuz",
          "Wojciech Kryscinski",
          "Kazuma Hashimoto",
          "Yingbo Zhou"
        ],
        "published_date": "2022",
        "abstract": "Abstractive summarization systems leveraging pre-training language models have achieved superior results on benchmark datasets. However, such models have been shown to be more prone to hallucinate facts that are unfaithful to the input context. In this paper, we propose a method to remedy entity-level extrinsic hallucinations with Entity Coverage Control (ECC). We first compute entity coverage precision and prepend the corresponding control code for each training example, which implicitly guides the model to recognize faithfulness contents in the training phase. We further extend our method via intermediate fine-tuning on large but noisy data extracted from Wikipedia to unlock zero-shot summarization. We show that the proposed method leads to more faithful and salient abstractive summarization in supervised fine-tuning and zero-shot settings according to our experimental results on three benchmark datasets XSum, Pubmed, and SAMSum of very different domains and styles.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/ebb4826b717798918bebed4dfac28c917e44577d.pdf",
        "venue": "NAACL-HLT",
        "citationCount": 36,
        "score": 12.0,
        "summary": "Abstractive summarization systems leveraging pre-training language models have achieved superior results on benchmark datasets. However, such models have been shown to be more prone to hallucinate facts that are unfaithful to the input context. In this paper, we propose a method to remedy entity-level extrinsic hallucinations with Entity Coverage Control (ECC). We first compute entity coverage precision and prepend the corresponding control code for each training example, which implicitly guides the model to recognize faithfulness contents in the training phase. We further extend our method via intermediate fine-tuning on large but noisy data extracted from Wikipedia to unlock zero-shot summarization. We show that the proposed method leads to more faithful and salient abstractive summarization in supervised fine-tuning and zero-shot settings according to our experimental results on three benchmark datasets XSum, Pubmed, and SAMSum of very different domains and styles.",
        "keywords": []
      },
      "file_name": "ebb4826b717798918bebed4dfac28c917e44577d.pdf"
    },
    {
      "success": true,
      "doc_id": "c49acbe324c7e0f55224281c7feb6f3d",
      "summary": "Attention mechanisms have been extensively adopted in vision and language tasks such as image captioning. It encourages a captioning model to dynamically ground appropriate image regions when generating words or phrases, and it is critical to alleviate the problems of object hallucinations and language bias. However, current studies show that the grounding accuracy of existing captioners is still far from satisfactory. Recently, much effort is devoted to improving the grounding accuracy by linking the words to the full content of objects in images. However, due to the noisy grounding annotations and large variations of object appearance, such strict word-object alignment regularization may not be optimal for improving captioning performance. In this paper, to improve the performance of both grounding and captioning, we propose a novel grounding model which implicitly links the words to the evidence in the image. The proposed model encourages the captioner to dynamically focus on informative regions of the objects, which could be either discriminative parts or full object content. With slacked constraints, the proposed captioning model can capture correct linguistic characteristics and visual relevance, and then generate more grounded image captions. In addition, we propose a novel quantitative metric for evaluating the correctness of the soft attention mechanism by considering the overall contribution of all object proposals when generating certain words. The proposed grounding model can be seamlessly plugged into most attention-based architectures without introducing inference complexity. We conduct extensive experiments on Flickr30k (Young et al., 2014) and MS COCO datasets (Lin et al., 2014), demonstrating that the proposed method consistently improves image captioning in both grounding and captioning. Besides, the proposed attention evaluation metric shows better consistency with the captioning performance.",
      "intriguing_abstract": "Attention mechanisms have been extensively adopted in vision and language tasks such as image captioning. It encourages a captioning model to dynamically ground appropriate image regions when generating words or phrases, and it is critical to alleviate the problems of object hallucinations and language bias. However, current studies show that the grounding accuracy of existing captioners is still far from satisfactory. Recently, much effort is devoted to improving the grounding accuracy by linking the words to the full content of objects in images. However, due to the noisy grounding annotations and large variations of object appearance, such strict word-object alignment regularization may not be optimal for improving captioning performance. In this paper, to improve the performance of both grounding and captioning, we propose a novel grounding model which implicitly links the words to the evidence in the image. The proposed model encourages the captioner to dynamically focus on informative regions of the objects, which could be either discriminative parts or full object content. With slacked constraints, the proposed captioning model can capture correct linguistic characteristics and visual relevance, and then generate more grounded image captions. In addition, we propose a novel quantitative metric for evaluating the correctness of the soft attention mechanism by considering the overall contribution of all object proposals when generating certain words. The proposed grounding model can be seamlessly plugged into most attention-based architectures without introducing inference complexity. We conduct extensive experiments on Flickr30k (Young et al., 2014) and MS COCO datasets (Lin et al., 2014), demonstrating that the proposed method consistently improves image captioning in both grounding and captioning. Besides, the proposed attention evaluation metric shows better consistency with the captioning performance.",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/35872510c095b1189105e9f902f04f51bd0a88e3.pdf",
      "citation_key": "jiang2022reg",
      "metadata": {
        "title": "Visual Cluster Grounding for Image Captioning",
        "authors": [
          "Wenhui Jiang",
          "Minwei Zhu",
          "Yuming Fang",
          "Guangming Shi",
          "Xiaowei Zhao",
          "Yang Liu"
        ],
        "published_date": "2022",
        "abstract": "Attention mechanisms have been extensively adopted in vision and language tasks such as image captioning. It encourages a captioning model to dynamically ground appropriate image regions when generating words or phrases, and it is critical to alleviate the problems of object hallucinations and language bias. However, current studies show that the grounding accuracy of existing captioners is still far from satisfactory. Recently, much effort is devoted to improving the grounding accuracy by linking the words to the full content of objects in images. However, due to the noisy grounding annotations and large variations of object appearance, such strict word-object alignment regularization may not be optimal for improving captioning performance. In this paper, to improve the performance of both grounding and captioning, we propose a novel grounding model which implicitly links the words to the evidence in the image. The proposed model encourages the captioner to dynamically focus on informative regions of the objects, which could be either discriminative parts or full object content. With slacked constraints, the proposed captioning model can capture correct linguistic characteristics and visual relevance, and then generate more grounded image captions. In addition, we propose a novel quantitative metric for evaluating the correctness of the soft attention mechanism by considering the overall contribution of all object proposals when generating certain words. The proposed grounding model can be seamlessly plugged into most attention-based architectures without introducing inference complexity. We conduct extensive experiments on Flickr30k (Young et al., 2014) and MS COCO datasets (Lin et al., 2014), demonstrating that the proposed method consistently improves image captioning in both grounding and captioning. Besides, the proposed attention evaluation metric shows better consistency with the captioning performance.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/35872510c095b1189105e9f902f04f51bd0a88e3.pdf",
        "venue": "IEEE Transactions on Image Processing",
        "citationCount": 34,
        "score": 11.333333333333332,
        "summary": "Attention mechanisms have been extensively adopted in vision and language tasks such as image captioning. It encourages a captioning model to dynamically ground appropriate image regions when generating words or phrases, and it is critical to alleviate the problems of object hallucinations and language bias. However, current studies show that the grounding accuracy of existing captioners is still far from satisfactory. Recently, much effort is devoted to improving the grounding accuracy by linking the words to the full content of objects in images. However, due to the noisy grounding annotations and large variations of object appearance, such strict word-object alignment regularization may not be optimal for improving captioning performance. In this paper, to improve the performance of both grounding and captioning, we propose a novel grounding model which implicitly links the words to the evidence in the image. The proposed model encourages the captioner to dynamically focus on informative regions of the objects, which could be either discriminative parts or full object content. With slacked constraints, the proposed captioning model can capture correct linguistic characteristics and visual relevance, and then generate more grounded image captions. In addition, we propose a novel quantitative metric for evaluating the correctness of the soft attention mechanism by considering the overall contribution of all object proposals when generating certain words. The proposed grounding model can be seamlessly plugged into most attention-based architectures without introducing inference complexity. We conduct extensive experiments on Flickr30k (Young et al., 2014) and MS COCO datasets (Lin et al., 2014), demonstrating that the proposed method consistently improves image captioning in both grounding and captioning. Besides, the proposed attention evaluation metric shows better consistency with the captioning performance.",
        "keywords": []
      },
      "file_name": "35872510c095b1189105e9f902f04f51bd0a88e3.pdf"
    },
    {
      "success": true,
      "doc_id": "5afe5f1b46a729c985088cfd70ae54c8",
      "summary": "Data-to-text generation models face challenges in ensuring data fidelity by referring to the correct input source. To inspire studies in this area, Wiseman et al. (2017) introduced the RotoWire corpus on generating NBA game summaries from the box- and line-score tables. However, limited attempts have been made in this direction and the challenges remain. We observe a prominent bottleneck in the corpus where only about 60% of the summary contents can be grounded to the boxscore records. Such information deficiency tends to misguide a conditioned language model to produce unconditioned random facts and thus leads to factual hallucinations. In this work, we restore the information balance and revamp this task to focus on fact-grounded data-to-text generation. We introduce a purified and larger-scale dataset, RotoWire-FG (Fact-Grounding), with 50% more data from the year 2017-19 and enriched input tables, and hope to attract research focuses in this direction. Moreover, we achieve improved data fidelity over the state-of-the-art models by integrating a new form of table reconstruction as an auxiliary task to boost the generation quality.",
      "intriguing_abstract": "Data-to-text generation models face challenges in ensuring data fidelity by referring to the correct input source. To inspire studies in this area, Wiseman et al. (2017) introduced the RotoWire corpus on generating NBA game summaries from the box- and line-score tables. However, limited attempts have been made in this direction and the challenges remain. We observe a prominent bottleneck in the corpus where only about 60% of the summary contents can be grounded to the boxscore records. Such information deficiency tends to misguide a conditioned language model to produce unconditioned random facts and thus leads to factual hallucinations. In this work, we restore the information balance and revamp this task to focus on fact-grounded data-to-text generation. We introduce a purified and larger-scale dataset, RotoWire-FG (Fact-Grounding), with 50% more data from the year 2017-19 and enriched input tables, and hope to attract research focuses in this direction. Moreover, we achieve improved data fidelity over the state-of-the-art models by integrating a new form of table reconstruction as an auxiliary task to boost the generation quality.",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/c2c3220b9faf95db43d90a5ed42fa824b4b3d2f0.pdf",
      "citation_key": "wang2020vz6",
      "metadata": {
        "title": "Revisiting Challenges in Data-to-Text Generation with Fact Grounding",
        "authors": [
          "Hongmin Wang"
        ],
        "published_date": "2020",
        "abstract": "Data-to-text generation models face challenges in ensuring data fidelity by referring to the correct input source. To inspire studies in this area, Wiseman et al. (2017) introduced the RotoWire corpus on generating NBA game summaries from the box- and line-score tables. However, limited attempts have been made in this direction and the challenges remain. We observe a prominent bottleneck in the corpus where only about 60% of the summary contents can be grounded to the boxscore records. Such information deficiency tends to misguide a conditioned language model to produce unconditioned random facts and thus leads to factual hallucinations. In this work, we restore the information balance and revamp this task to focus on fact-grounded data-to-text generation. We introduce a purified and larger-scale dataset, RotoWire-FG (Fact-Grounding), with 50% more data from the year 2017-19 and enriched input tables, and hope to attract research focuses in this direction. Moreover, we achieve improved data fidelity over the state-of-the-art models by integrating a new form of table reconstruction as an auxiliary task to boost the generation quality.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/c2c3220b9faf95db43d90a5ed42fa824b4b3d2f0.pdf",
        "venue": "International Conference on Natural Language Generation",
        "citationCount": 51,
        "score": 10.200000000000001,
        "summary": "Data-to-text generation models face challenges in ensuring data fidelity by referring to the correct input source. To inspire studies in this area, Wiseman et al. (2017) introduced the RotoWire corpus on generating NBA game summaries from the box- and line-score tables. However, limited attempts have been made in this direction and the challenges remain. We observe a prominent bottleneck in the corpus where only about 60% of the summary contents can be grounded to the boxscore records. Such information deficiency tends to misguide a conditioned language model to produce unconditioned random facts and thus leads to factual hallucinations. In this work, we restore the information balance and revamp this task to focus on fact-grounded data-to-text generation. We introduce a purified and larger-scale dataset, RotoWire-FG (Fact-Grounding), with 50% more data from the year 2017-19 and enriched input tables, and hope to attract research focuses in this direction. Moreover, we achieve improved data fidelity over the state-of-the-art models by integrating a new form of table reconstruction as an auxiliary task to boost the generation quality.",
        "keywords": []
      },
      "file_name": "c2c3220b9faf95db43d90a5ed42fa824b4b3d2f0.pdf"
    },
    {
      "success": true,
      "doc_id": "73d5680dd640b85d5d73f21ec0839396",
      "summary": "The widely studied task of Natural Language Inference (NLI) requires a system to recognize whether one piece of text is textually entailed by another, i.e. whether the entirety of its meaning can be inferred from the other. In current NLI datasets and models, textual entailment relations are typically defined on the sentence- or paragraph-level. However, even a simple sentence often contains multiple propositions, i.e. distinct units of meaning conveyed by the sentence. As these propositions can carry different truth values in the context of a given premise, we argue for the need to recognize the textual entailment relation of each proposition in a sentence individually. We propose PropSegmEnt, a corpus of over 45K propositions annotated by expert human raters. Our dataset structure resembles the tasks of (1) segmenting sentences within a document to the set of propositions, and (2) classifying the entailment relation of each proposition with respect to a different yet topically-aligned document, i.e. documents describing the same event or entity. We establish strong baselines for the segmentation and entailment tasks. Through case studies on summary hallucination detection and document-level NLI, we demonstrate that our conceptual framework is potentially useful for understanding and explaining the compositionality of NLI labels.",
      "intriguing_abstract": "The widely studied task of Natural Language Inference (NLI) requires a system to recognize whether one piece of text is textually entailed by another, i.e. whether the entirety of its meaning can be inferred from the other. In current NLI datasets and models, textual entailment relations are typically defined on the sentence- or paragraph-level. However, even a simple sentence often contains multiple propositions, i.e. distinct units of meaning conveyed by the sentence. As these propositions can carry different truth values in the context of a given premise, we argue for the need to recognize the textual entailment relation of each proposition in a sentence individually. We propose PropSegmEnt, a corpus of over 45K propositions annotated by expert human raters. Our dataset structure resembles the tasks of (1) segmenting sentences within a document to the set of propositions, and (2) classifying the entailment relation of each proposition with respect to a different yet topically-aligned document, i.e. documents describing the same event or entity. We establish strong baselines for the segmentation and entailment tasks. Through case studies on summary hallucination detection and document-level NLI, we demonstrate that our conceptual framework is potentially useful for understanding and explaining the compositionality of NLI labels.",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/72da4a646a31d72bcae90b916e120cd7df5f9dae.pdf",
      "citation_key": "chen2022gkm",
      "metadata": {
        "title": "PropSegmEnt: A Large-Scale Corpus for Proposition-Level Segmentation and Entailment Recognition",
        "authors": [
          "Sihao Chen",
          "S. Buthpitiya",
          "Alex Fabrikant",
          "D. Roth",
          "Tal Schuster"
        ],
        "published_date": "2022",
        "abstract": "The widely studied task of Natural Language Inference (NLI) requires a system to recognize whether one piece of text is textually entailed by another, i.e. whether the entirety of its meaning can be inferred from the other. In current NLI datasets and models, textual entailment relations are typically defined on the sentence- or paragraph-level. However, even a simple sentence often contains multiple propositions, i.e. distinct units of meaning conveyed by the sentence. As these propositions can carry different truth values in the context of a given premise, we argue for the need to recognize the textual entailment relation of each proposition in a sentence individually. We propose PropSegmEnt, a corpus of over 45K propositions annotated by expert human raters. Our dataset structure resembles the tasks of (1) segmenting sentences within a document to the set of propositions, and (2) classifying the entailment relation of each proposition with respect to a different yet topically-aligned document, i.e. documents describing the same event or entity. We establish strong baselines for the segmentation and entailment tasks. Through case studies on summary hallucination detection and document-level NLI, we demonstrate that our conceptual framework is potentially useful for understanding and explaining the compositionality of NLI labels.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/72da4a646a31d72bcae90b916e120cd7df5f9dae.pdf",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "citationCount": 28,
        "score": 9.333333333333332,
        "summary": "The widely studied task of Natural Language Inference (NLI) requires a system to recognize whether one piece of text is textually entailed by another, i.e. whether the entirety of its meaning can be inferred from the other. In current NLI datasets and models, textual entailment relations are typically defined on the sentence- or paragraph-level. However, even a simple sentence often contains multiple propositions, i.e. distinct units of meaning conveyed by the sentence. As these propositions can carry different truth values in the context of a given premise, we argue for the need to recognize the textual entailment relation of each proposition in a sentence individually. We propose PropSegmEnt, a corpus of over 45K propositions annotated by expert human raters. Our dataset structure resembles the tasks of (1) segmenting sentences within a document to the set of propositions, and (2) classifying the entailment relation of each proposition with respect to a different yet topically-aligned document, i.e. documents describing the same event or entity. We establish strong baselines for the segmentation and entailment tasks. Through case studies on summary hallucination detection and document-level NLI, we demonstrate that our conceptual framework is potentially useful for understanding and explaining the compositionality of NLI labels.",
        "keywords": []
      },
      "file_name": "72da4a646a31d72bcae90b916e120cd7df5f9dae.pdf"
    },
    {
      "success": true,
      "doc_id": "d9558f6bf4e85dd88500495e088169e2",
      "summary": "Machine learning is shifting towards general-purpose pretrained generative models, trained in a self-supervised manner on large amounts of data, which can then be applied to solve a large number of tasks. However, due to their generic training methodology, these models often fail to meet some of the downstream requirements (e.g., hallucinations in abstractive summarization or style violations in code generation). This raises the important question of how to adapt pre-trained generative models to meet all requirements without destroying their general capabilities (\"catastrophic forgetting\"). Recent work has proposed to solve this problem by representing task-specific requirements through energy-based models (EBMs) and approximating these EBMs using distributional policy gradients (DPG). Despite its effectiveness, this approach is however limited to unconditional distributions. In this paper, we extend DPG to conditional tasks by proposing Conditional DPG (CDPG). We evaluate CDPG on four different control objectives across three tasks (translation, summarization and code generation) and two pretrained models (T5 and GPT-Neo). Our results show that fine-tuning using CDPG robustly moves these pretrained models closer towards meeting control objectives and -- in contrast with baseline approaches -- does not result in catastrophic forgetting.",
      "intriguing_abstract": "Machine learning is shifting towards general-purpose pretrained generative models, trained in a self-supervised manner on large amounts of data, which can then be applied to solve a large number of tasks. However, due to their generic training methodology, these models often fail to meet some of the downstream requirements (e.g., hallucinations in abstractive summarization or style violations in code generation). This raises the important question of how to adapt pre-trained generative models to meet all requirements without destroying their general capabilities (\"catastrophic forgetting\"). Recent work has proposed to solve this problem by representing task-specific requirements through energy-based models (EBMs) and approximating these EBMs using distributional policy gradients (DPG). Despite its effectiveness, this approach is however limited to unconditional distributions. In this paper, we extend DPG to conditional tasks by proposing Conditional DPG (CDPG). We evaluate CDPG on four different control objectives across three tasks (translation, summarization and code generation) and two pretrained models (T5 and GPT-Neo). Our results show that fine-tuning using CDPG robustly moves these pretrained models closer towards meeting control objectives and -- in contrast with baseline approaches -- does not result in catastrophic forgetting.",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/ecb5a6fe2f5261e4e717ece1e82c464c63cb4862.pdf",
      "citation_key": "korbak202191w",
      "metadata": {
        "title": "Controlling Conditional Language Models without Catastrophic Forgetting",
        "authors": [
          "Tomasz Korbak",
          "Hady ElSahar",
          "GermÃ¡n Kruszewski",
          "Marc Dymetman"
        ],
        "published_date": "2021",
        "abstract": "Machine learning is shifting towards general-purpose pretrained generative models, trained in a self-supervised manner on large amounts of data, which can then be applied to solve a large number of tasks. However, due to their generic training methodology, these models often fail to meet some of the downstream requirements (e.g., hallucinations in abstractive summarization or style violations in code generation). This raises the important question of how to adapt pre-trained generative models to meet all requirements without destroying their general capabilities (\"catastrophic forgetting\"). Recent work has proposed to solve this problem by representing task-specific requirements through energy-based models (EBMs) and approximating these EBMs using distributional policy gradients (DPG). Despite its effectiveness, this approach is however limited to unconditional distributions. In this paper, we extend DPG to conditional tasks by proposing Conditional DPG (CDPG). We evaluate CDPG on four different control objectives across three tasks (translation, summarization and code generation) and two pretrained models (T5 and GPT-Neo). Our results show that fine-tuning using CDPG robustly moves these pretrained models closer towards meeting control objectives and -- in contrast with baseline approaches -- does not result in catastrophic forgetting.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/ecb5a6fe2f5261e4e717ece1e82c464c63cb4862.pdf",
        "venue": "International Conference on Machine Learning",
        "citationCount": 36,
        "score": 9.0,
        "summary": "Machine learning is shifting towards general-purpose pretrained generative models, trained in a self-supervised manner on large amounts of data, which can then be applied to solve a large number of tasks. However, due to their generic training methodology, these models often fail to meet some of the downstream requirements (e.g., hallucinations in abstractive summarization or style violations in code generation). This raises the important question of how to adapt pre-trained generative models to meet all requirements without destroying their general capabilities (\"catastrophic forgetting\"). Recent work has proposed to solve this problem by representing task-specific requirements through energy-based models (EBMs) and approximating these EBMs using distributional policy gradients (DPG). Despite its effectiveness, this approach is however limited to unconditional distributions. In this paper, we extend DPG to conditional tasks by proposing Conditional DPG (CDPG). We evaluate CDPG on four different control objectives across three tasks (translation, summarization and code generation) and two pretrained models (T5 and GPT-Neo). Our results show that fine-tuning using CDPG robustly moves these pretrained models closer towards meeting control objectives and -- in contrast with baseline approaches -- does not result in catastrophic forgetting.",
        "keywords": []
      },
      "file_name": "ecb5a6fe2f5261e4e717ece1e82c464c63cb4862.pdf"
    },
    {
      "success": true,
      "doc_id": "860169c5ad6616574faab6f97977d59f",
      "summary": "Pretrained, large, generative language models (LMs) have had great success in a wide range of sequence tagging and structured prediction tasks. Casting a sequence tagging task as a Seq2Seq one requires deciding the formats of the input and output sequences. However, we lack a principled understanding of the trade-offs associated with these formats (such as the effect on model accuracy, sequence length, multilingual generalization, hallucination). In this paper, we rigorously study different formats one could use for casting input text sentences and their output labels into the input and target (i.e., output) of a Seq2Seq model. Along the way, we introduce a new format, which we show to to be both simpler and more effective. Additionally the new format demonstrates significant gains in the multilingual settings â€“ both zero-shot transfer learning and joint training. Lastly, we find that the new format is more robust and almost completely devoid of hallucination â€“ an issue we find common in existing formats. With well over a 1000 experiments studying 14 different formats, over 7 diverse public benchmarks â€“ including 3 multilingual datasets spanning 7 languages â€“ we believe our findings provide a strong empirical basis in understanding how we should tackle sequence tagging tasks.",
      "intriguing_abstract": "Pretrained, large, generative language models (LMs) have had great success in a wide range of sequence tagging and structured prediction tasks. Casting a sequence tagging task as a Seq2Seq one requires deciding the formats of the input and output sequences. However, we lack a principled understanding of the trade-offs associated with these formats (such as the effect on model accuracy, sequence length, multilingual generalization, hallucination). In this paper, we rigorously study different formats one could use for casting input text sentences and their output labels into the input and target (i.e., output) of a Seq2Seq model. Along the way, we introduce a new format, which we show to to be both simpler and more effective. Additionally the new format demonstrates significant gains in the multilingual settings â€“ both zero-shot transfer learning and joint training. Lastly, we find that the new format is more robust and almost completely devoid of hallucination â€“ an issue we find common in existing formats. With well over a 1000 experiments studying 14 different formats, over 7 diverse public benchmarks â€“ including 3 multilingual datasets spanning 7 languages â€“ we believe our findings provide a strong empirical basis in understanding how we should tackle sequence tagging tasks.",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/f35dbee22c1572d149b7c1e20d69672cae931451.pdf",
      "citation_key": "raman20229ce",
      "metadata": {
        "title": "Transforming Sequence Tagging Into A Seq2Seq Task",
        "authors": [
          "K. Raman",
          "Iftekhar Naim",
          "Jiecao Chen",
          "Kazuma Hashimoto",
          "Kiran Yalasangi",
          "Krishna Srinivasan"
        ],
        "published_date": "2022",
        "abstract": "Pretrained, large, generative language models (LMs) have had great success in a wide range of sequence tagging and structured prediction tasks. Casting a sequence tagging task as a Seq2Seq one requires deciding the formats of the input and output sequences. However, we lack a principled understanding of the trade-offs associated with these formats (such as the effect on model accuracy, sequence length, multilingual generalization, hallucination). In this paper, we rigorously study different formats one could use for casting input text sentences and their output labels into the input and target (i.e., output) of a Seq2Seq model. Along the way, we introduce a new format, which we show to to be both simpler and more effective. Additionally the new format demonstrates significant gains in the multilingual settings â€“ both zero-shot transfer learning and joint training. Lastly, we find that the new format is more robust and almost completely devoid of hallucination â€“ an issue we find common in existing formats. With well over a 1000 experiments studying 14 different formats, over 7 diverse public benchmarks â€“ including 3 multilingual datasets spanning 7 languages â€“ we believe our findings provide a strong empirical basis in understanding how we should tackle sequence tagging tasks.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/f35dbee22c1572d149b7c1e20d69672cae931451.pdf",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "citationCount": 23,
        "score": 7.666666666666666,
        "summary": "Pretrained, large, generative language models (LMs) have had great success in a wide range of sequence tagging and structured prediction tasks. Casting a sequence tagging task as a Seq2Seq one requires deciding the formats of the input and output sequences. However, we lack a principled understanding of the trade-offs associated with these formats (such as the effect on model accuracy, sequence length, multilingual generalization, hallucination). In this paper, we rigorously study different formats one could use for casting input text sentences and their output labels into the input and target (i.e., output) of a Seq2Seq model. Along the way, we introduce a new format, which we show to to be both simpler and more effective. Additionally the new format demonstrates significant gains in the multilingual settings â€“ both zero-shot transfer learning and joint training. Lastly, we find that the new format is more robust and almost completely devoid of hallucination â€“ an issue we find common in existing formats. With well over a 1000 experiments studying 14 different formats, over 7 diverse public benchmarks â€“ including 3 multilingual datasets spanning 7 languages â€“ we believe our findings provide a strong empirical basis in understanding how we should tackle sequence tagging tasks.",
        "keywords": []
      },
      "file_name": "f35dbee22c1572d149b7c1e20d69672cae931451.pdf"
    },
    {
      "success": true,
      "doc_id": "a1bdcf49fd4ee0e1a7ee4b423e9af780",
      "summary": "Deep learning sequence models have been successful with morphological inflection generation. The SIGMORPHON shared task results in the past several years indicate that such models can perform well, but only if the training data covers a good amount of different lemmata, or if the lemmata to be inflected at test time have also been seen in training, as has indeed been largely the case in these tasks. Surprisingly, we find that standard models such as the Transformer almost completely fail at generalizing inflection patterns when trained on a limited number of lemmata and asked to inflect previously unseen lemmataâ€”i.e. under â€œwug testâ€-like circumstances. This is true even though the actual number of training examples is very large. While established data augmentation techniques can be employed to alleviate this shortcoming by introducing a copying bias through hallucinating synthetic new word forms using the alphabet in the language at hand, our experiment results show that, to be more effective, the hallucination process needs to pay attention to substrings of syllable-like length rather than individual characters.",
      "intriguing_abstract": "Deep learning sequence models have been successful with morphological inflection generation. The SIGMORPHON shared task results in the past several years indicate that such models can perform well, but only if the training data covers a good amount of different lemmata, or if the lemmata to be inflected at test time have also been seen in training, as has indeed been largely the case in these tasks. Surprisingly, we find that standard models such as the Transformer almost completely fail at generalizing inflection patterns when trained on a limited number of lemmata and asked to inflect previously unseen lemmataâ€”i.e. under â€œwug testâ€-like circumstances. This is true even though the actual number of training examples is very large. While established data augmentation techniques can be employed to alleviate this shortcoming by introducing a copying bias through hallucinating synthetic new word forms using the alphabet in the language at hand, our experiment results show that, to be more effective, the hallucination process needs to pay attention to substrings of syllable-like length rather than individual characters.",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/fda76a1411e16722ebd2d8278c3143ca4363da6b.pdf",
      "citation_key": "liu2021h6c",
      "metadata": {
        "title": "Can a Transformer Pass the Wug Test? Tuning Copying Bias in Neural Morphological Inflection Models",
        "authors": [
          "Ling Liu",
          "Mans Hulden"
        ],
        "published_date": "2021",
        "abstract": "Deep learning sequence models have been successful with morphological inflection generation. The SIGMORPHON shared task results in the past several years indicate that such models can perform well, but only if the training data covers a good amount of different lemmata, or if the lemmata to be inflected at test time have also been seen in training, as has indeed been largely the case in these tasks. Surprisingly, we find that standard models such as the Transformer almost completely fail at generalizing inflection patterns when trained on a limited number of lemmata and asked to inflect previously unseen lemmataâ€”i.e. under â€œwug testâ€-like circumstances. This is true even though the actual number of training examples is very large. While established data augmentation techniques can be employed to alleviate this shortcoming by introducing a copying bias through hallucinating synthetic new word forms using the alphabet in the language at hand, our experiment results show that, to be more effective, the hallucination process needs to pay attention to substrings of syllable-like length rather than individual characters.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/fda76a1411e16722ebd2d8278c3143ca4363da6b.pdf",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "citationCount": 29,
        "score": 7.25,
        "summary": "Deep learning sequence models have been successful with morphological inflection generation. The SIGMORPHON shared task results in the past several years indicate that such models can perform well, but only if the training data covers a good amount of different lemmata, or if the lemmata to be inflected at test time have also been seen in training, as has indeed been largely the case in these tasks. Surprisingly, we find that standard models such as the Transformer almost completely fail at generalizing inflection patterns when trained on a limited number of lemmata and asked to inflect previously unseen lemmataâ€”i.e. under â€œwug testâ€-like circumstances. This is true even though the actual number of training examples is very large. While established data augmentation techniques can be employed to alleviate this shortcoming by introducing a copying bias through hallucinating synthetic new word forms using the alphabet in the language at hand, our experiment results show that, to be more effective, the hallucination process needs to pay attention to substrings of syllable-like length rather than individual characters.",
        "keywords": []
      },
      "file_name": "fda76a1411e16722ebd2d8278c3143ca4363da6b.pdf"
    },
    {
      "success": true,
      "doc_id": "73db1093d82e4580f1c753f5706a0f70",
      "summary": "Large language models are known to suffer from the hallucination problem in that they are prone to output statements that are false or inconsistent, indicating a lack of knowledge. A proposed solution to this is to provide the model with additional data modalities that complements the knowledge obtained through text. We investigate the use of visual data to complement the knowledge of large language models by proposing a method for evaluating visual knowledge transfer to text for uni- or multimodal language models. The method is based on two steps, 1) a novel task querying for knowledge of memory colors, i.e. typical colors of well-known objects, and 2) filtering of model training data to clearly separate knowledge contributions. Additionally, we introduce a model architecture that involves a visual imagination step and evaluate it with our proposed method. We find that our method can successfully be used to measure visual knowledge transfer capabilities in models and that our novel model architecture shows promising results for leveraging multimodal knowledge in a unimodal setting.",
      "intriguing_abstract": "Large language models are known to suffer from the hallucination problem in that they are prone to output statements that are false or inconsistent, indicating a lack of knowledge. A proposed solution to this is to provide the model with additional data modalities that complements the knowledge obtained through text. We investigate the use of visual data to complement the knowledge of large language models by proposing a method for evaluating visual knowledge transfer to text for uni- or multimodal language models. The method is based on two steps, 1) a novel task querying for knowledge of memory colors, i.e. typical colors of well-known objects, and 2) filtering of model training data to clearly separate knowledge contributions. Additionally, we introduce a model architecture that involves a visual imagination step and evaluate it with our proposed method. We find that our method can successfully be used to measure visual knowledge transfer capabilities in models and that our novel model architecture shows promising results for leveraging multimodal knowledge in a unimodal setting.",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/3e075efc541c7d2b357199655e11f084686e8575.pdf",
      "citation_key": "norlund2021462",
      "metadata": {
        "title": "Transferring Knowledge from Vision to Language: How to Achieve it and how to Measure it?",
        "authors": [
          "Tobias Norlund",
          "Lovisa HagstrÃ¶m",
          "Richard Johansson"
        ],
        "published_date": "2021",
        "abstract": "Large language models are known to suffer from the hallucination problem in that they are prone to output statements that are false or inconsistent, indicating a lack of knowledge. A proposed solution to this is to provide the model with additional data modalities that complements the knowledge obtained through text. We investigate the use of visual data to complement the knowledge of large language models by proposing a method for evaluating visual knowledge transfer to text for uni- or multimodal language models. The method is based on two steps, 1) a novel task querying for knowledge of memory colors, i.e. typical colors of well-known objects, and 2) filtering of model training data to clearly separate knowledge contributions. Additionally, we introduce a model architecture that involves a visual imagination step and evaluate it with our proposed method. We find that our method can successfully be used to measure visual knowledge transfer capabilities in models and that our novel model architecture shows promising results for leveraging multimodal knowledge in a unimodal setting.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/3e075efc541c7d2b357199655e11f084686e8575.pdf",
        "venue": "BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP",
        "citationCount": 25,
        "score": 6.25,
        "summary": "Large language models are known to suffer from the hallucination problem in that they are prone to output statements that are false or inconsistent, indicating a lack of knowledge. A proposed solution to this is to provide the model with additional data modalities that complements the knowledge obtained through text. We investigate the use of visual data to complement the knowledge of large language models by proposing a method for evaluating visual knowledge transfer to text for uni- or multimodal language models. The method is based on two steps, 1) a novel task querying for knowledge of memory colors, i.e. typical colors of well-known objects, and 2) filtering of model training data to clearly separate knowledge contributions. Additionally, we introduce a model architecture that involves a visual imagination step and evaluate it with our proposed method. We find that our method can successfully be used to measure visual knowledge transfer capabilities in models and that our novel model architecture shows promising results for leveraging multimodal knowledge in a unimodal setting.",
        "keywords": []
      },
      "file_name": "3e075efc541c7d2b357199655e11f084686e8575.pdf"
    },
    {
      "success": true,
      "doc_id": "d2a143776bf8fb70ae5cd363c8c905a0",
      "summary": "Many recent advances in neural information retrieval models, which predict top-K items given a query, learn directly from a large training set of (query, item) pairs. However, they are often insufficient when there are many previously unseen (query, item) combinations, often referred to as the cold start problem. Furthermore, the search system can be biased towards items that are frequently shown to a query previously, also known as the 'rich get richer' (a.k.a. feedback loop) problem. In light of these problems, we observed that most online content platforms have both a search and a recommender system that, while having heterogeneous input spaces, can be connected through their common output item space and a shared semantic representation. In this paper, we propose a new Zero-Shot Heterogeneous Transfer Learning framework that transfers learned knowledge from the recommender system component to improve the search component of a content platform. First, it learns representations of items and their natural-language features by predicting (item, item) correlation graphs derived from the recommender system as an auxiliary task. Then, the learned representations are transferred to solve the target search retrieval task, performing query-to-item prediction without having seen any (query, item) pairs in training. We conduct online and offline experiments on one of the world's largest search and recommender systems from Google, and present the results and lessons learned. We demonstrate that the proposed approach can achieve high performance on offline search retrieval tasks, and more importantly, achieved significant improvements on relevance and user interactions over the highly-optimized production system in online experiments.",
      "intriguing_abstract": "Many recent advances in neural information retrieval models, which predict top-K items given a query, learn directly from a large training set of (query, item) pairs. However, they are often insufficient when there are many previously unseen (query, item) combinations, often referred to as the cold start problem. Furthermore, the search system can be biased towards items that are frequently shown to a query previously, also known as the 'rich get richer' (a.k.a. feedback loop) problem. In light of these problems, we observed that most online content platforms have both a search and a recommender system that, while having heterogeneous input spaces, can be connected through their common output item space and a shared semantic representation. In this paper, we propose a new Zero-Shot Heterogeneous Transfer Learning framework that transfers learned knowledge from the recommender system component to improve the search component of a content platform. First, it learns representations of items and their natural-language features by predicting (item, item) correlation graphs derived from the recommender system as an auxiliary task. Then, the learned representations are transferred to solve the target search retrieval task, performing query-to-item prediction without having seen any (query, item) pairs in training. We conduct online and offline experiments on one of the world's largest search and recommender systems from Google, and present the results and lessons learned. We demonstrate that the proposed approach can achieve high performance on offline search retrieval tasks, and more importantly, achieved significant improvements on relevance and user interactions over the highly-optimized production system in online experiments.",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/334bf07262320eb895a22973c948b4111e782daa.pdf",
      "citation_key": "wu20206gt",
      "metadata": {
        "title": "Zero-Shot Heterogeneous Transfer Learning from Recommender Systems to Cold-Start Search Retrieval",
        "authors": [
          "Tao Wu",
          "E. Chio",
          "Heng-Tze Cheng",
          "Yu Du",
          "Steffen Rendle",
          "D. Kuzmin",
          "Ritesh Agarwal",
          "Li Zhang",
          "John R. Anderson",
          "Sarvjeet Singh",
          "Tushar Chandra",
          "Ed H. Chi",
          "Wen Li",
          "Ankit Kumar",
          "Xiang Ma",
          "A. Soares",
          "Nitin Jindal",
          "Pei Cao"
        ],
        "published_date": "2020",
        "abstract": "Many recent advances in neural information retrieval models, which predict top-K items given a query, learn directly from a large training set of (query, item) pairs. However, they are often insufficient when there are many previously unseen (query, item) combinations, often referred to as the cold start problem. Furthermore, the search system can be biased towards items that are frequently shown to a query previously, also known as the 'rich get richer' (a.k.a. feedback loop) problem. In light of these problems, we observed that most online content platforms have both a search and a recommender system that, while having heterogeneous input spaces, can be connected through their common output item space and a shared semantic representation. In this paper, we propose a new Zero-Shot Heterogeneous Transfer Learning framework that transfers learned knowledge from the recommender system component to improve the search component of a content platform. First, it learns representations of items and their natural-language features by predicting (item, item) correlation graphs derived from the recommender system as an auxiliary task. Then, the learned representations are transferred to solve the target search retrieval task, performing query-to-item prediction without having seen any (query, item) pairs in training. We conduct online and offline experiments on one of the world's largest search and recommender systems from Google, and present the results and lessons learned. We demonstrate that the proposed approach can achieve high performance on offline search retrieval tasks, and more importantly, achieved significant improvements on relevance and user interactions over the highly-optimized production system in online experiments.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/334bf07262320eb895a22973c948b4111e782daa.pdf",
        "venue": "International Conference on Information and Knowledge Management",
        "citationCount": 27,
        "score": 5.4,
        "summary": "Many recent advances in neural information retrieval models, which predict top-K items given a query, learn directly from a large training set of (query, item) pairs. However, they are often insufficient when there are many previously unseen (query, item) combinations, often referred to as the cold start problem. Furthermore, the search system can be biased towards items that are frequently shown to a query previously, also known as the 'rich get richer' (a.k.a. feedback loop) problem. In light of these problems, we observed that most online content platforms have both a search and a recommender system that, while having heterogeneous input spaces, can be connected through their common output item space and a shared semantic representation. In this paper, we propose a new Zero-Shot Heterogeneous Transfer Learning framework that transfers learned knowledge from the recommender system component to improve the search component of a content platform. First, it learns representations of items and their natural-language features by predicting (item, item) correlation graphs derived from the recommender system as an auxiliary task. Then, the learned representations are transferred to solve the target search retrieval task, performing query-to-item prediction without having seen any (query, item) pairs in training. We conduct online and offline experiments on one of the world's largest search and recommender systems from Google, and present the results and lessons learned. We demonstrate that the proposed approach can achieve high performance on offline search retrieval tasks, and more importantly, achieved significant improvements on relevance and user interactions over the highly-optimized production system in online experiments.",
        "keywords": []
      },
      "file_name": "334bf07262320eb895a22973c948b4111e782daa.pdf"
    },
    {
      "success": true,
      "doc_id": "11624e0e44499803733d63972b52a92e",
      "summary": "We present DADS , a novel D ata A ugmentation 001 technique for low-resource D ialogue 002 S ummarization. Our method generates 003 synthetic examples by replacing sections of 004 text from both the input dialogue and summary 005 while preserving the augmented summary 006 to correspond to a viable summary for the 007 augmented dialogue. We utilize pretrained 008 language models that produce highly likely 009 dialogue alternatives while still being free to 010 generate diverse alternatives. We applied our 011 data augmentation method to the SAMSum 012 dataset in low resource scenarios, mimicking 013 real world problems such as chat, thread, and 014 meeting summarization where large scale 015 supervised datasets with human-written sum-016 maries are scarce. Through both automatic 017 and human evaluations, we show that DADS 018 shows strong improvements for low resource 019 scenarios while generating topically diverse 020 summaries without introducing additional 021 hallucinations to the summaries. 022",
      "intriguing_abstract": "We present DADS , a novel D ata A ugmentation 001 technique for low-resource D ialogue 002 S ummarization. Our method generates 003 synthetic examples by replacing sections of 004 text from both the input dialogue and summary 005 while preserving the augmented summary 006 to correspond to a viable summary for the 007 augmented dialogue. We utilize pretrained 008 language models that produce highly likely 009 dialogue alternatives while still being free to 010 generate diverse alternatives. We applied our 011 data augmentation method to the SAMSum 012 dataset in low resource scenarios, mimicking 013 real world problems such as chat, thread, and 014 meeting summarization where large scale 015 supervised datasets with human-written sum-016 maries are scarce. Through both automatic 017 and human evaluations, we show that DADS 018 shows strong improvements for low resource 019 scenarios while generating topically diverse 020 summaries without introducing additional 021 hallucinations to the summaries. 022",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/5b824faaea855eb1700b16d7a67ee58a8f75e7d4.pdf",
      "citation_key": "liu2022hw7",
      "metadata": {
        "title": "Data Augmentation for Low-Resource Dialogue Summarization",
        "authors": [
          "Yongtai Liu",
          "Joshua Maynez",
          "GonÃ§alo SimÃµes",
          "Shashi Narayan"
        ],
        "published_date": "2022",
        "abstract": "We present DADS , a novel D ata A ugmentation 001 technique for low-resource D ialogue 002 S ummarization. Our method generates 003 synthetic examples by replacing sections of 004 text from both the input dialogue and summary 005 while preserving the augmented summary 006 to correspond to a viable summary for the 007 augmented dialogue. We utilize pretrained 008 language models that produce highly likely 009 dialogue alternatives while still being free to 010 generate diverse alternatives. We applied our 011 data augmentation method to the SAMSum 012 dataset in low resource scenarios, mimicking 013 real world problems such as chat, thread, and 014 meeting summarization where large scale 015 supervised datasets with human-written sum-016 maries are scarce. Through both automatic 017 and human evaluations, we show that DADS 018 shows strong improvements for low resource 019 scenarios while generating topically diverse 020 summaries without introducing additional 021 hallucinations to the summaries. 022",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/5b824faaea855eb1700b16d7a67ee58a8f75e7d4.pdf",
        "venue": "NAACL-HLT",
        "citationCount": 16,
        "score": 5.333333333333333,
        "summary": "We present DADS , a novel D ata A ugmentation 001 technique for low-resource D ialogue 002 S ummarization. Our method generates 003 synthetic examples by replacing sections of 004 text from both the input dialogue and summary 005 while preserving the augmented summary 006 to correspond to a viable summary for the 007 augmented dialogue. We utilize pretrained 008 language models that produce highly likely 009 dialogue alternatives while still being free to 010 generate diverse alternatives. We applied our 011 data augmentation method to the SAMSum 012 dataset in low resource scenarios, mimicking 013 real world problems such as chat, thread, and 014 meeting summarization where large scale 015 supervised datasets with human-written sum-016 maries are scarce. Through both automatic 017 and human evaluations, we show that DADS 018 shows strong improvements for low resource 019 scenarios while generating topically diverse 020 summaries without introducing additional 021 hallucinations to the summaries. 022",
        "keywords": []
      },
      "file_name": "5b824faaea855eb1700b16d7a67ee58a8f75e7d4.pdf"
    },
    {
      "success": true,
      "doc_id": "dfd465e6a12696fc676f5c386255dd84",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/c8373577bcce9b8811672ddeb372a5b780397117.pdf",
      "citation_key": "jelinek2016205",
      "metadata": {
        "title": "Efficacy of Metacognitive Training for Depression: A Randomized Controlled Trial",
        "authors": [
          "L. Jelinek",
          "M. Hauschildt",
          "C. Wittekind",
          "Brooke C. Schneider",
          "L. Kriston",
          "S. Moritz"
        ],
        "published_date": "2016",
        "abstract": "",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/c8373577bcce9b8811672ddeb372a5b780397117.pdf",
        "venue": "Psychotherapy and Psychosomatics",
        "citationCount": 47,
        "score": 5.222222222222222,
        "summary": "",
        "keywords": []
      },
      "file_name": "c8373577bcce9b8811672ddeb372a5b780397117.pdf"
    },
    {
      "success": true,
      "doc_id": "198cf336d15a849c7177358f335e0810",
      "summary": "Memorization presents a challenge for several constrained Natural Language Generation (NLG) tasks such as Neural Machine Translation (NMT), wherein the proclivity of neural models to memorize noisy and atypical samples reacts adversely with the noisy (web crawled) datasets. However, previous studies of memorization in constrained NLG tasks have only focused on counterfactual memorization, linking it to the problem of hallucinations. In this work, we propose a new, inexpensive algorithm for extractive memorization (exact training data generation under insufficient context) in constrained sequence generation tasks and use it to study extractive memorization and its effects in NMT. We demonstrate that extractive memorization poses a serious threat to NMT reliability by qualitatively and quantitatively characterizing the memorized samples as well as the model behavior in their vicinity. Based on empirical observations, we develop a simple algorithm which elicits non-memorized translations of memorized samples from the same model, for a large fraction of such samples. Finally, we show that the proposed algorithm could also be leveraged to mitigate memorization in the model through finetuning. We have released the code to reproduce our results at https://github.com/vyraun/Finding-Memo.",
      "intriguing_abstract": "Memorization presents a challenge for several constrained Natural Language Generation (NLG) tasks such as Neural Machine Translation (NMT), wherein the proclivity of neural models to memorize noisy and atypical samples reacts adversely with the noisy (web crawled) datasets. However, previous studies of memorization in constrained NLG tasks have only focused on counterfactual memorization, linking it to the problem of hallucinations. In this work, we propose a new, inexpensive algorithm for extractive memorization (exact training data generation under insufficient context) in constrained sequence generation tasks and use it to study extractive memorization and its effects in NMT. We demonstrate that extractive memorization poses a serious threat to NMT reliability by qualitatively and quantitatively characterizing the memorized samples as well as the model behavior in their vicinity. Based on empirical observations, we develop a simple algorithm which elicits non-memorized translations of memorized samples from the same model, for a large fraction of such samples. Finally, we show that the proposed algorithm could also be leveraged to mitigate memorization in the model through finetuning. We have released the code to reproduce our results at https://github.com/vyraun/Finding-Memo.",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/6ab78343ab82fa9d7baa68027f9f7e8cd9863737.pdf",
      "citation_key": "raunak2022r58",
      "metadata": {
        "title": "Finding Memo: Extractive Memorization in Constrained Sequence Generation Tasks",
        "authors": [
          "Vikas Raunak",
          "Arul Menezes"
        ],
        "published_date": "2022",
        "abstract": "Memorization presents a challenge for several constrained Natural Language Generation (NLG) tasks such as Neural Machine Translation (NMT), wherein the proclivity of neural models to memorize noisy and atypical samples reacts adversely with the noisy (web crawled) datasets. However, previous studies of memorization in constrained NLG tasks have only focused on counterfactual memorization, linking it to the problem of hallucinations. In this work, we propose a new, inexpensive algorithm for extractive memorization (exact training data generation under insufficient context) in constrained sequence generation tasks and use it to study extractive memorization and its effects in NMT. We demonstrate that extractive memorization poses a serious threat to NMT reliability by qualitatively and quantitatively characterizing the memorized samples as well as the model behavior in their vicinity. Based on empirical observations, we develop a simple algorithm which elicits non-memorized translations of memorized samples from the same model, for a large fraction of such samples. Finally, we show that the proposed algorithm could also be leveraged to mitigate memorization in the model through finetuning. We have released the code to reproduce our results at https://github.com/vyraun/Finding-Memo.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/6ab78343ab82fa9d7baa68027f9f7e8cd9863737.pdf",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "citationCount": 14,
        "score": 4.666666666666666,
        "summary": "Memorization presents a challenge for several constrained Natural Language Generation (NLG) tasks such as Neural Machine Translation (NMT), wherein the proclivity of neural models to memorize noisy and atypical samples reacts adversely with the noisy (web crawled) datasets. However, previous studies of memorization in constrained NLG tasks have only focused on counterfactual memorization, linking it to the problem of hallucinations. In this work, we propose a new, inexpensive algorithm for extractive memorization (exact training data generation under insufficient context) in constrained sequence generation tasks and use it to study extractive memorization and its effects in NMT. We demonstrate that extractive memorization poses a serious threat to NMT reliability by qualitatively and quantitatively characterizing the memorized samples as well as the model behavior in their vicinity. Based on empirical observations, we develop a simple algorithm which elicits non-memorized translations of memorized samples from the same model, for a large fraction of such samples. Finally, we show that the proposed algorithm could also be leveraged to mitigate memorization in the model through finetuning. We have released the code to reproduce our results at https://github.com/vyraun/Finding-Memo.",
        "keywords": []
      },
      "file_name": "6ab78343ab82fa9d7baa68027f9f7e8cd9863737.pdf"
    },
    {
      "success": true,
      "doc_id": "6b42f1d51d8a38dde1b20473a1b1cdd2",
      "summary": "Prompting large language models has enabled significant recent progress in multi-step reasoning over text. However, when applied to text generation from semi-structured data (e.g., graphs or tables), these methods typically suffer from low semantic coverage, hallucination, and logical inconsistency. We propose MURMUR, a neuro-symbolic modular approach to text generation from semi-structured data with multi-step reasoning. MURMUR is a best-first search method that generates reasoning paths using: (1) neural and symbolic modules with specific linguistic and logical skills, (2) a grammar whose production rules define valid compositions of modules, and (3) value functions that assess the quality of each reasoning step. We conduct experiments on two diverse data-to-text generation tasks like WebNLG and LogicNLG. These tasks differ in their data representations (graphs and tables) and span multiple linguistic and logical skills. MURMUR obtains significant improvements over recent few-shot baselines like direct prompting and chain-of-thought prompting, while also achieving comparable performance to fine-tuned GPT-2 on out-of-domain data. Moreover, human evaluation shows that MURMUR generates highly faithful and correct reasoning paths that lead to 26% more logically consistent summaries on LogicNLG, compared to direct prompting.",
      "intriguing_abstract": "Prompting large language models has enabled significant recent progress in multi-step reasoning over text. However, when applied to text generation from semi-structured data (e.g., graphs or tables), these methods typically suffer from low semantic coverage, hallucination, and logical inconsistency. We propose MURMUR, a neuro-symbolic modular approach to text generation from semi-structured data with multi-step reasoning. MURMUR is a best-first search method that generates reasoning paths using: (1) neural and symbolic modules with specific linguistic and logical skills, (2) a grammar whose production rules define valid compositions of modules, and (3) value functions that assess the quality of each reasoning step. We conduct experiments on two diverse data-to-text generation tasks like WebNLG and LogicNLG. These tasks differ in their data representations (graphs and tables) and span multiple linguistic and logical skills. MURMUR obtains significant improvements over recent few-shot baselines like direct prompting and chain-of-thought prompting, while also achieving comparable performance to fine-tuned GPT-2 on out-of-domain data. Moreover, human evaluation shows that MURMUR generates highly faithful and correct reasoning paths that lead to 26% more logically consistent summaries on LogicNLG, compared to direct prompting.",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/5791c2b41dd23310c53d6738a4c0d587107c2dc8.pdf",
      "citation_key": "saha20229lo",
      "metadata": {
        "title": "MURMUR: Modular Multi-Step Reasoning for Semi-Structured Data-to-Text Generation",
        "authors": [
          "Swarnadeep Saha",
          "Xinyan Velocity Yu",
          "Mohit Bansal",
          "Ramakanth Pasunuru",
          "Asli Celikyilmaz"
        ],
        "published_date": "2022",
        "abstract": "Prompting large language models has enabled significant recent progress in multi-step reasoning over text. However, when applied to text generation from semi-structured data (e.g., graphs or tables), these methods typically suffer from low semantic coverage, hallucination, and logical inconsistency. We propose MURMUR, a neuro-symbolic modular approach to text generation from semi-structured data with multi-step reasoning. MURMUR is a best-first search method that generates reasoning paths using: (1) neural and symbolic modules with specific linguistic and logical skills, (2) a grammar whose production rules define valid compositions of modules, and (3) value functions that assess the quality of each reasoning step. We conduct experiments on two diverse data-to-text generation tasks like WebNLG and LogicNLG. These tasks differ in their data representations (graphs and tables) and span multiple linguistic and logical skills. MURMUR obtains significant improvements over recent few-shot baselines like direct prompting and chain-of-thought prompting, while also achieving comparable performance to fine-tuned GPT-2 on out-of-domain data. Moreover, human evaluation shows that MURMUR generates highly faithful and correct reasoning paths that lead to 26% more logically consistent summaries on LogicNLG, compared to direct prompting.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/5791c2b41dd23310c53d6738a4c0d587107c2dc8.pdf",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "citationCount": 13,
        "score": 4.333333333333333,
        "summary": "Prompting large language models has enabled significant recent progress in multi-step reasoning over text. However, when applied to text generation from semi-structured data (e.g., graphs or tables), these methods typically suffer from low semantic coverage, hallucination, and logical inconsistency. We propose MURMUR, a neuro-symbolic modular approach to text generation from semi-structured data with multi-step reasoning. MURMUR is a best-first search method that generates reasoning paths using: (1) neural and symbolic modules with specific linguistic and logical skills, (2) a grammar whose production rules define valid compositions of modules, and (3) value functions that assess the quality of each reasoning step. We conduct experiments on two diverse data-to-text generation tasks like WebNLG and LogicNLG. These tasks differ in their data representations (graphs and tables) and span multiple linguistic and logical skills. MURMUR obtains significant improvements over recent few-shot baselines like direct prompting and chain-of-thought prompting, while also achieving comparable performance to fine-tuned GPT-2 on out-of-domain data. Moreover, human evaluation shows that MURMUR generates highly faithful and correct reasoning paths that lead to 26% more logically consistent summaries on LogicNLG, compared to direct prompting.",
        "keywords": []
      },
      "file_name": "5791c2b41dd23310c53d6738a4c0d587107c2dc8.pdf"
    },
    {
      "success": true,
      "doc_id": "ac9e6bc55c3bc76611f8474b1afa3bf7",
      "summary": "Many advancements in computer vision and machine learning have shown potential for significantly improving the lives of people with disabilities. In particular, recent research has demonstrated that deep neural network models could be used to bridge the gap between the deaf who use sign language and hearing people. The major impediment to advancing such models is the lack of high-quality and large-scale training data. Moreover, previously released sign language datasets include few or no interrogative sentences compared to declarative sentences. In this paper, we introduce a new publicly available large-scale Korean Sign Language (KSL) dataset-KSL-Guide-that includes both declarative sentences and comparable interrogative sentences, which are required for a model to achieve high performance in real-world interactive tasks deployed on service applications. Our dataset contains a total of 121K sign language video samples featuring sentences and words spoken by native KSL speakers with extensive annotations (e.g., gloss, translation, keypoints, and timestamps). We exploit a multi-camera system to produce 3D human pose keypoints as well as 2D keypoints from multi-view RGB. Our experiments quantitatively demonstrate that the inclusion of interrogative sentences in training for sign language recognition and translation tasks greatly improves their performance. Furthermore, we empirically show the qualitative results by developing a prototype application using our dataset, providing an interactive guide service that helps to lower the communication barrier between sign language speakers and hearing people.",
      "intriguing_abstract": "Many advancements in computer vision and machine learning have shown potential for significantly improving the lives of people with disabilities. In particular, recent research has demonstrated that deep neural network models could be used to bridge the gap between the deaf who use sign language and hearing people. The major impediment to advancing such models is the lack of high-quality and large-scale training data. Moreover, previously released sign language datasets include few or no interrogative sentences compared to declarative sentences. In this paper, we introduce a new publicly available large-scale Korean Sign Language (KSL) dataset-KSL-Guide-that includes both declarative sentences and comparable interrogative sentences, which are required for a model to achieve high performance in real-world interactive tasks deployed on service applications. Our dataset contains a total of 121K sign language video samples featuring sentences and words spoken by native KSL speakers with extensive annotations (e.g., gloss, translation, keypoints, and timestamps). We exploit a multi-camera system to produce 3D human pose keypoints as well as 2D keypoints from multi-view RGB. Our experiments quantitatively demonstrate that the inclusion of interrogative sentences in training for sign language recognition and translation tasks greatly improves their performance. Furthermore, we empirically show the qualitative results by developing a prototype application using our dataset, providing an interactive guide service that helps to lower the communication barrier between sign language speakers and hearing people.",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/fbe549b5b30f54a4f76cb4bf4c6b8a9fb8e24657.pdf",
      "citation_key": "ham20213fx",
      "metadata": {
        "title": "KSL-Guide: A Large-scale Korean Sign Language Dataset Including Interrogative Sentences for Guiding the Deaf and Hard-of-Hearing",
        "authors": [
          "Soomin Ham",
          "Kibaek Park",
          "Yeongjun Jang",
          "Youngtaek Oh",
          "Seokmin Yun",
          "Sukwon Yoon",
          "Chang Jo Kim",
          "Han-Mu Park",
          "In-So Kweon"
        ],
        "published_date": "2021",
        "abstract": "Many advancements in computer vision and machine learning have shown potential for significantly improving the lives of people with disabilities. In particular, recent research has demonstrated that deep neural network models could be used to bridge the gap between the deaf who use sign language and hearing people. The major impediment to advancing such models is the lack of high-quality and large-scale training data. Moreover, previously released sign language datasets include few or no interrogative sentences compared to declarative sentences. In this paper, we introduce a new publicly available large-scale Korean Sign Language (KSL) dataset-KSL-Guide-that includes both declarative sentences and comparable interrogative sentences, which are required for a model to achieve high performance in real-world interactive tasks deployed on service applications. Our dataset contains a total of 121K sign language video samples featuring sentences and words spoken by native KSL speakers with extensive annotations (e.g., gloss, translation, keypoints, and timestamps). We exploit a multi-camera system to produce 3D human pose keypoints as well as 2D keypoints from multi-view RGB. Our experiments quantitatively demonstrate that the inclusion of interrogative sentences in training for sign language recognition and translation tasks greatly improves their performance. Furthermore, we empirically show the qualitative results by developing a prototype application using our dataset, providing an interactive guide service that helps to lower the communication barrier between sign language speakers and hearing people.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/fbe549b5b30f54a4f76cb4bf4c6b8a9fb8e24657.pdf",
        "venue": "IEEE International Conference on Automatic Face & Gesture Recognition",
        "citationCount": 15,
        "score": 3.75,
        "summary": "Many advancements in computer vision and machine learning have shown potential for significantly improving the lives of people with disabilities. In particular, recent research has demonstrated that deep neural network models could be used to bridge the gap between the deaf who use sign language and hearing people. The major impediment to advancing such models is the lack of high-quality and large-scale training data. Moreover, previously released sign language datasets include few or no interrogative sentences compared to declarative sentences. In this paper, we introduce a new publicly available large-scale Korean Sign Language (KSL) dataset-KSL-Guide-that includes both declarative sentences and comparable interrogative sentences, which are required for a model to achieve high performance in real-world interactive tasks deployed on service applications. Our dataset contains a total of 121K sign language video samples featuring sentences and words spoken by native KSL speakers with extensive annotations (e.g., gloss, translation, keypoints, and timestamps). We exploit a multi-camera system to produce 3D human pose keypoints as well as 2D keypoints from multi-view RGB. Our experiments quantitatively demonstrate that the inclusion of interrogative sentences in training for sign language recognition and translation tasks greatly improves their performance. Furthermore, we empirically show the qualitative results by developing a prototype application using our dataset, providing an interactive guide service that helps to lower the communication barrier between sign language speakers and hearing people.",
        "keywords": []
      },
      "file_name": "fbe549b5b30f54a4f76cb4bf4c6b8a9fb8e24657.pdf"
    },
    {
      "success": true,
      "doc_id": "1c3489f6307e324accbf26122c0e0c50",
      "summary": "The rapid evolution of deep neural networks is demanding deep learning (DL) frameworks not only to satisfy the requirement of quickly executing large computations, but also to support straightforward programming models for quickly implementing and experimenting with complex network structures. However, existing frameworks fail to excel in both departments simultaneously, leading to diverged efforts for optimizing performance and improving usability. \nThis paper presents JANUS, a system that combines the advantages from both sides by transparently converting an imperative DL program written in Python, the de-facto scripting language for DL, into an efficiently executable symbolic dataflow graph. JANUS can convert various dynamic features of Python, including dynamic control flow, dynamic types, and impure functions, into elements of a symbolic dataflow graph. Experiments demonstrate that JANUS can achieve fast DL training by exploiting the techniques imposed by symbolic graph-based DL frameworks, while maintaining the simple and flexible programmability of imperative DL frameworks at the same time.",
      "intriguing_abstract": "The rapid evolution of deep neural networks is demanding deep learning (DL) frameworks not only to satisfy the requirement of quickly executing large computations, but also to support straightforward programming models for quickly implementing and experimenting with complex network structures. However, existing frameworks fail to excel in both departments simultaneously, leading to diverged efforts for optimizing performance and improving usability. \nThis paper presents JANUS, a system that combines the advantages from both sides by transparently converting an imperative DL program written in Python, the de-facto scripting language for DL, into an efficiently executable symbolic dataflow graph. JANUS can convert various dynamic features of Python, including dynamic control flow, dynamic types, and impure functions, into elements of a symbolic dataflow graph. Experiments demonstrate that JANUS can achieve fast DL training by exploiting the techniques imposed by symbolic graph-based DL frameworks, while maintaining the simple and flexible programmability of imperative DL frameworks at the same time.",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/a996fb4b9f3d4315138f8773b1e995f8386b11eb.pdf",
      "citation_key": "jeong20180d2",
      "metadata": {
        "title": "JANUS: Fast and Flexible Deep Learning via Symbolic Graph Execution of Imperative Programs",
        "authors": [
          "Eunji Jeong",
          "Sungwoo Cho",
          "Gyeong-In Yu",
          "Joo Seong Jeong",
          "Dongjin Shin",
          "Byung-Gon Chun"
        ],
        "published_date": "2018",
        "abstract": "The rapid evolution of deep neural networks is demanding deep learning (DL) frameworks not only to satisfy the requirement of quickly executing large computations, but also to support straightforward programming models for quickly implementing and experimenting with complex network structures. However, existing frameworks fail to excel in both departments simultaneously, leading to diverged efforts for optimizing performance and improving usability. \nThis paper presents JANUS, a system that combines the advantages from both sides by transparently converting an imperative DL program written in Python, the de-facto scripting language for DL, into an efficiently executable symbolic dataflow graph. JANUS can convert various dynamic features of Python, including dynamic control flow, dynamic types, and impure functions, into elements of a symbolic dataflow graph. Experiments demonstrate that JANUS can achieve fast DL training by exploiting the techniques imposed by symbolic graph-based DL frameworks, while maintaining the simple and flexible programmability of imperative DL frameworks at the same time.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/a996fb4b9f3d4315138f8773b1e995f8386b11eb.pdf",
        "venue": "Symposium on Networked Systems Design and Implementation",
        "citationCount": 25,
        "score": 3.571428571428571,
        "summary": "The rapid evolution of deep neural networks is demanding deep learning (DL) frameworks not only to satisfy the requirement of quickly executing large computations, but also to support straightforward programming models for quickly implementing and experimenting with complex network structures. However, existing frameworks fail to excel in both departments simultaneously, leading to diverged efforts for optimizing performance and improving usability. \nThis paper presents JANUS, a system that combines the advantages from both sides by transparently converting an imperative DL program written in Python, the de-facto scripting language for DL, into an efficiently executable symbolic dataflow graph. JANUS can convert various dynamic features of Python, including dynamic control flow, dynamic types, and impure functions, into elements of a symbolic dataflow graph. Experiments demonstrate that JANUS can achieve fast DL training by exploiting the techniques imposed by symbolic graph-based DL frameworks, while maintaining the simple and flexible programmability of imperative DL frameworks at the same time.",
        "keywords": []
      },
      "file_name": "a996fb4b9f3d4315138f8773b1e995f8386b11eb.pdf"
    },
    {
      "success": true,
      "doc_id": "fac61f72c803eba40a6c650dc6e6c19b",
      "summary": "Generative models have recently started to outperform extractive models in Open Domain Question Answering, largely by leveraging their decoder to attend over multiple encoded passages and combining their information. However, generative models tend to be larger than extractive models due to the need for a decoder, run slower during inference due to auto-regressive decoder beam search, and their generated output often suffers from hallucinations. We propose to extend transformer encoders with the ability to fuse information from multiple passages, using global representation to provide cross-sample attention over all tokens across samples. Furthermore, we propose an alternative answer span probability calculation to better aggregate answer scores in the global space of all samples. Using our proposed method, we outperform the current state-of-the-art method by 2.5 Exact Match score on the Natural Question dataset while using only 25% of parameters and 35% of the latency during inference, and 4.4 Exact Match on WebQuestions dataset. When coupled with synthetic data augmentation, we outperform larger models on the TriviaQA dataset as well. The latency and parameter savings of our method make it particularly attractive for open-domain question answering, as these models are often compute-intensive.",
      "intriguing_abstract": "Generative models have recently started to outperform extractive models in Open Domain Question Answering, largely by leveraging their decoder to attend over multiple encoded passages and combining their information. However, generative models tend to be larger than extractive models due to the need for a decoder, run slower during inference due to auto-regressive decoder beam search, and their generated output often suffers from hallucinations. We propose to extend transformer encoders with the ability to fuse information from multiple passages, using global representation to provide cross-sample attention over all tokens across samples. Furthermore, we propose an alternative answer span probability calculation to better aggregate answer scores in the global space of all samples. Using our proposed method, we outperform the current state-of-the-art method by 2.5 Exact Match score on the Natural Question dataset while using only 25% of parameters and 35% of the latency during inference, and 4.4 Exact Match on WebQuestions dataset. When coupled with synthetic data augmentation, we outperform larger models on the TriviaQA dataset as well. The latency and parameter savings of our method make it particularly attractive for open-domain question answering, as these models are often compute-intensive.",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/471a49220cea2069e8b8a76821b1d2434204a732.pdf",
      "citation_key": "kedia2022c03",
      "metadata": {
        "title": "FiE: Building a Global Probability Space by Leveraging Early Fusion in Encoder for Open-Domain Question Answering",
        "authors": [
          "Akhil Kedia",
          "Mohd Abbas Zaidi",
          "Haejun Lee"
        ],
        "published_date": "2022",
        "abstract": "Generative models have recently started to outperform extractive models in Open Domain Question Answering, largely by leveraging their decoder to attend over multiple encoded passages and combining their information. However, generative models tend to be larger than extractive models due to the need for a decoder, run slower during inference due to auto-regressive decoder beam search, and their generated output often suffers from hallucinations. We propose to extend transformer encoders with the ability to fuse information from multiple passages, using global representation to provide cross-sample attention over all tokens across samples. Furthermore, we propose an alternative answer span probability calculation to better aggregate answer scores in the global space of all samples. Using our proposed method, we outperform the current state-of-the-art method by 2.5 Exact Match score on the Natural Question dataset while using only 25% of parameters and 35% of the latency during inference, and 4.4 Exact Match on WebQuestions dataset. When coupled with synthetic data augmentation, we outperform larger models on the TriviaQA dataset as well. The latency and parameter savings of our method make it particularly attractive for open-domain question answering, as these models are often compute-intensive.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/471a49220cea2069e8b8a76821b1d2434204a732.pdf",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "citationCount": 10,
        "score": 3.333333333333333,
        "summary": "Generative models have recently started to outperform extractive models in Open Domain Question Answering, largely by leveraging their decoder to attend over multiple encoded passages and combining their information. However, generative models tend to be larger than extractive models due to the need for a decoder, run slower during inference due to auto-regressive decoder beam search, and their generated output often suffers from hallucinations. We propose to extend transformer encoders with the ability to fuse information from multiple passages, using global representation to provide cross-sample attention over all tokens across samples. Furthermore, we propose an alternative answer span probability calculation to better aggregate answer scores in the global space of all samples. Using our proposed method, we outperform the current state-of-the-art method by 2.5 Exact Match score on the Natural Question dataset while using only 25% of parameters and 35% of the latency during inference, and 4.4 Exact Match on WebQuestions dataset. When coupled with synthetic data augmentation, we outperform larger models on the TriviaQA dataset as well. The latency and parameter savings of our method make it particularly attractive for open-domain question answering, as these models are often compute-intensive.",
        "keywords": []
      },
      "file_name": "471a49220cea2069e8b8a76821b1d2434204a732.pdf"
    },
    {
      "success": true,
      "doc_id": "e892c476278ba77492f673034212832e",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/3b7cdcbe19fdbda17f7cce1eb1b6c8f5a8a60e3b.pdf",
      "citation_key": "gallace201080s",
      "metadata": {
        "title": "Touch and the body: The role of the somatosensory cortex in tactile awareness",
        "authors": [
          "A. Gallace",
          "A. Gallace"
        ],
        "published_date": "2010",
        "abstract": "",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/3b7cdcbe19fdbda17f7cce1eb1b6c8f5a8a60e3b.pdf",
        "venue": "",
        "citationCount": 49,
        "score": 3.2666666666666666,
        "summary": "",
        "keywords": []
      },
      "file_name": "3b7cdcbe19fdbda17f7cce1eb1b6c8f5a8a60e3b.pdf"
    },
    {
      "success": true,
      "doc_id": "e53834e975cce85d18156bb4145be1ee",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/6aed9dffe3246efd9d19b2873994ba112e2ad422.pdf",
      "citation_key": "li20203k7",
      "metadata": {
        "title": "Leveraging Large Pretrained Models for WebNLG 2020",
        "authors": [
          "Xintong Li",
          "Aleksandre Maskharashvili",
          "S. Stevens-Guille",
          "Michael White"
        ],
        "published_date": "2020",
        "abstract": "",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/6aed9dffe3246efd9d19b2873994ba112e2ad422.pdf",
        "venue": "WEBNLG",
        "citationCount": 15,
        "score": 3.0,
        "summary": "",
        "keywords": []
      },
      "file_name": "6aed9dffe3246efd9d19b2873994ba112e2ad422.pdf"
    },
    {
      "success": true,
      "doc_id": "4f5cb537de21e22e666253d216ba86f9",
      "summary": "\n \n Online forums contain interactive and semantically related discussions on various questions. Extracted question-answer archive is invaluable knowledge, which can be used to improve Question Answering services. In this paper, we address the problem of Question Suggestion, which targets at suggesting questions that are semantically related to a queried question. Existing bag-of-words approaches suffer from the shortcoming that they could not bridge the lexical chasm between semantically related questions. Therefore, we present a new framework to suggest questions, and propose the Topicenhanced Translation-based Language Model (TopicTRLM) which fuses both the lexical and latent semantic knowledge. Extensive experiments have been conducted with a large real world data set. Experimental results indicate our approach is very effective and outperforms other popular methods in several metrics.\n \n",
      "intriguing_abstract": "\n \n Online forums contain interactive and semantically related discussions on various questions. Extracted question-answer archive is invaluable knowledge, which can be used to improve Question Answering services. In this paper, we address the problem of Question Suggestion, which targets at suggesting questions that are semantically related to a queried question. Existing bag-of-words approaches suffer from the shortcoming that they could not bridge the lexical chasm between semantically related questions. Therefore, we present a new framework to suggest questions, and propose the Topicenhanced Translation-based Language Model (TopicTRLM) which fuses both the lexical and latent semantic knowledge. Extensive experiments have been conducted with a large real world data set. Experimental results indicate our approach is very effective and outperforms other popular methods in several metrics.\n \n",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/b32cf25dffbdbe6d838fc7b8781c126c8fea7d3c.pdf",
      "citation_key": "zhou2011j8m",
      "metadata": {
        "title": "Learning to Suggest Questions in Online Forums",
        "authors": [
          "Tom Chao Zhou",
          "Chin-Yew Lin",
          "Irwin King",
          "Michael R. Lyu",
          "Young-In Song",
          "Yunbo Cao"
        ],
        "published_date": "2011",
        "abstract": "\n \n Online forums contain interactive and semantically related discussions on various questions. Extracted question-answer archive is invaluable knowledge, which can be used to improve Question Answering services. In this paper, we address the problem of Question Suggestion, which targets at suggesting questions that are semantically related to a queried question. Existing bag-of-words approaches suffer from the shortcoming that they could not bridge the lexical chasm between semantically related questions. Therefore, we present a new framework to suggest questions, and propose the Topicenhanced Translation-based Language Model (TopicTRLM) which fuses both the lexical and latent semantic knowledge. Extensive experiments have been conducted with a large real world data set. Experimental results indicate our approach is very effective and outperforms other popular methods in several metrics.\n \n",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/b32cf25dffbdbe6d838fc7b8781c126c8fea7d3c.pdf",
        "venue": "AAAI Conference on Artificial Intelligence",
        "citationCount": 28,
        "score": 2.0,
        "summary": "\n \n Online forums contain interactive and semantically related discussions on various questions. Extracted question-answer archive is invaluable knowledge, which can be used to improve Question Answering services. In this paper, we address the problem of Question Suggestion, which targets at suggesting questions that are semantically related to a queried question. Existing bag-of-words approaches suffer from the shortcoming that they could not bridge the lexical chasm between semantically related questions. Therefore, we present a new framework to suggest questions, and propose the Topicenhanced Translation-based Language Model (TopicTRLM) which fuses both the lexical and latent semantic knowledge. Extensive experiments have been conducted with a large real world data set. Experimental results indicate our approach is very effective and outperforms other popular methods in several metrics.\n \n",
        "keywords": []
      },
      "file_name": "b32cf25dffbdbe6d838fc7b8781c126c8fea7d3c.pdf"
    },
    {
      "success": true,
      "doc_id": "3b756901e1fd9ed2762a8072684bc26c",
      "summary": "The open-platform communication (OPC) unified architecture (UA) (IEC62541) is introduced as a key technology for realizing a variety of smart grid (SG) use cases enabling relevant automation and control tasks. The OPC UA can expand interoperability between power systems. The top-level SG management platform needs independent middleware to transparently manage the power information technology (IT) systems, including the IEC 61850. To expand interoperability between the power system for a large number of stakeholders and various standards, this paper focuses on the IEC 61850 for the digital substation. In this paper, we propose the interconnection method to integrate communication with OPC UA and convert OPC UA AddressSpace using system configuration description language (SCL) of IEC 61850. We implemented the mapping process for the verification of the interconnection method. The interconnection method in this paper can expand interoperability between power systems for OPC UA integration for various data structures in the smart grid.",
      "intriguing_abstract": "The open-platform communication (OPC) unified architecture (UA) (IEC62541) is introduced as a key technology for realizing a variety of smart grid (SG) use cases enabling relevant automation and control tasks. The OPC UA can expand interoperability between power systems. The top-level SG management platform needs independent middleware to transparently manage the power information technology (IT) systems, including the IEC 61850. To expand interoperability between the power system for a large number of stakeholders and various standards, this paper focuses on the IEC 61850 for the digital substation. In this paper, we propose the interconnection method to integrate communication with OPC UA and convert OPC UA AddressSpace using system configuration description language (SCL) of IEC 61850. We implemented the mapping process for the verification of the interconnection method. The interconnection method in this paper can expand interoperability between power systems for OPC UA integration for various data structures in the smart grid.",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/20013690616f6e781c05feaa08a2247a97640a87.pdf",
      "citation_key": "injae2016yq6",
      "metadata": {
        "title": "Auto-Mapping and Configuration Method of IEC 61850 Information Model Based on OPC UA",
        "authors": [
          "Shin In-Jae",
          "Byungkwen Song",
          "D. Eom"
        ],
        "published_date": "2016",
        "abstract": "The open-platform communication (OPC) unified architecture (UA) (IEC62541) is introduced as a key technology for realizing a variety of smart grid (SG) use cases enabling relevant automation and control tasks. The OPC UA can expand interoperability between power systems. The top-level SG management platform needs independent middleware to transparently manage the power information technology (IT) systems, including the IEC 61850. To expand interoperability between the power system for a large number of stakeholders and various standards, this paper focuses on the IEC 61850 for the digital substation. In this paper, we propose the interconnection method to integrate communication with OPC UA and convert OPC UA AddressSpace using system configuration description language (SCL) of IEC 61850. We implemented the mapping process for the verification of the interconnection method. The interconnection method in this paper can expand interoperability between power systems for OPC UA integration for various data structures in the smart grid.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/20013690616f6e781c05feaa08a2247a97640a87.pdf",
        "venue": "",
        "citationCount": 17,
        "score": 1.8888888888888888,
        "summary": "The open-platform communication (OPC) unified architecture (UA) (IEC62541) is introduced as a key technology for realizing a variety of smart grid (SG) use cases enabling relevant automation and control tasks. The OPC UA can expand interoperability between power systems. The top-level SG management platform needs independent middleware to transparently manage the power information technology (IT) systems, including the IEC 61850. To expand interoperability between the power system for a large number of stakeholders and various standards, this paper focuses on the IEC 61850 for the digital substation. In this paper, we propose the interconnection method to integrate communication with OPC UA and convert OPC UA AddressSpace using system configuration description language (SCL) of IEC 61850. We implemented the mapping process for the verification of the interconnection method. The interconnection method in this paper can expand interoperability between power systems for OPC UA integration for various data structures in the smart grid.",
        "keywords": []
      },
      "file_name": "20013690616f6e781c05feaa08a2247a97640a87.pdf"
    },
    {
      "success": true,
      "doc_id": "e32cc6978bb79a91ab3e623a7d72ad66",
      "summary": "The rapid evolution of deep neural networks is demanding deep learning (DL) frameworks not only to satisfy the requirement of quickly executing large computations, but also to support straightforward programming models for quickly implementing and experimenting with complex network structures. However, existing frameworks fail to excel in both departments simultaneously, leading to diverged efforts for optimizing performance and improving usability. This paper presents JANUS, a system that combines the advantages from both sides by transparently converting an imperative DL program written in Python, a de-facto scripting language for DL, into an efficiently executable symbolic dataflow graph. JANUS can convert various dynamic features of Python, including dynamic control flow, dynamic types, and impure functions, into elements of a symbolic dataflow graph. Our experiments show that JANUS can achieve fast DL training by exploiting the techniques imposed by symbolic graph-based DL frameworks, while maintaining the simple and flexible programmability of imperative DL frameworks at the same time.",
      "intriguing_abstract": "The rapid evolution of deep neural networks is demanding deep learning (DL) frameworks not only to satisfy the requirement of quickly executing large computations, but also to support straightforward programming models for quickly implementing and experimenting with complex network structures. However, existing frameworks fail to excel in both departments simultaneously, leading to diverged efforts for optimizing performance and improving usability. This paper presents JANUS, a system that combines the advantages from both sides by transparently converting an imperative DL program written in Python, a de-facto scripting language for DL, into an efficiently executable symbolic dataflow graph. JANUS can convert various dynamic features of Python, including dynamic control flow, dynamic types, and impure functions, into elements of a symbolic dataflow graph. Our experiments show that JANUS can achieve fast DL training by exploiting the techniques imposed by symbolic graph-based DL frameworks, while maintaining the simple and flexible programmability of imperative DL frameworks at the same time.",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/e416b5fb1ab75c0770cf7cbd6976f5444b0ee89f.pdf",
      "citation_key": "jeong2019z3k",
      "metadata": {
        "title": "Speculative Symbolic Graph Execution of Imperative Deep Learning Programs",
        "authors": [
          "Eunji Jeong",
          "Sungwoo Cho",
          "Gyeong-In Yu",
          "Joo Seong Jeong",
          "Dongjin Shin",
          "Taebum Kim",
          "Byung-Gon Chun"
        ],
        "published_date": "2019",
        "abstract": "The rapid evolution of deep neural networks is demanding deep learning (DL) frameworks not only to satisfy the requirement of quickly executing large computations, but also to support straightforward programming models for quickly implementing and experimenting with complex network structures. However, existing frameworks fail to excel in both departments simultaneously, leading to diverged efforts for optimizing performance and improving usability. This paper presents JANUS, a system that combines the advantages from both sides by transparently converting an imperative DL program written in Python, a de-facto scripting language for DL, into an efficiently executable symbolic dataflow graph. JANUS can convert various dynamic features of Python, including dynamic control flow, dynamic types, and impure functions, into elements of a symbolic dataflow graph. Our experiments show that JANUS can achieve fast DL training by exploiting the techniques imposed by symbolic graph-based DL frameworks, while maintaining the simple and flexible programmability of imperative DL frameworks at the same time.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/e416b5fb1ab75c0770cf7cbd6976f5444b0ee89f.pdf",
        "venue": "ACM SIGOPS Operating Systems Review",
        "citationCount": 11,
        "score": 1.8333333333333333,
        "summary": "The rapid evolution of deep neural networks is demanding deep learning (DL) frameworks not only to satisfy the requirement of quickly executing large computations, but also to support straightforward programming models for quickly implementing and experimenting with complex network structures. However, existing frameworks fail to excel in both departments simultaneously, leading to diverged efforts for optimizing performance and improving usability. This paper presents JANUS, a system that combines the advantages from both sides by transparently converting an imperative DL program written in Python, a de-facto scripting language for DL, into an efficiently executable symbolic dataflow graph. JANUS can convert various dynamic features of Python, including dynamic control flow, dynamic types, and impure functions, into elements of a symbolic dataflow graph. Our experiments show that JANUS can achieve fast DL training by exploiting the techniques imposed by symbolic graph-based DL frameworks, while maintaining the simple and flexible programmability of imperative DL frameworks at the same time.",
        "keywords": []
      },
      "file_name": "e416b5fb1ab75c0770cf7cbd6976f5444b0ee89f.pdf"
    },
    {
      "success": true,
      "doc_id": "5f8b958cb0f29f2a8e3a2e83a54b13c3",
      "summary": "Linking spans of natural language text to concepts in a structured source is an important task for many problems. It allows intelligent systems to leverage rich knowledge available in those sources (such as concept properties and relations) to enhance the semantics of the mentions of these concepts in text. In the medical domain, it is common to link text spans to medical concepts in large, curated knowledge repositories such as the Unified Medical Language System. Different approaches have different strengths: some are precision-oriented, some recall-oriented; some better at considering context but more prone to hallucination. The variety of techniques suggests that ensembling could outperform component technologies at this task. In this paper, we describe our process for building a Stacking ensemble using additional, auxiliary features for Entity Linking in the medical domain. We report experiments that show that naive ensembling does not always outperform component Entity Linking systems, that stacking usually outperforms naive ensembling, and that auxiliary features added to the stacker further improve its performance on three distinct datasets. Our best model produces state-of-the-art results on several medical datasets.",
      "intriguing_abstract": "Linking spans of natural language text to concepts in a structured source is an important task for many problems. It allows intelligent systems to leverage rich knowledge available in those sources (such as concept properties and relations) to enhance the semantics of the mentions of these concepts in text. In the medical domain, it is common to link text spans to medical concepts in large, curated knowledge repositories such as the Unified Medical Language System. Different approaches have different strengths: some are precision-oriented, some recall-oriented; some better at considering context but more prone to hallucination. The variety of techniques suggests that ensembling could outperform component technologies at this task. In this paper, we describe our process for building a Stacking ensemble using additional, auxiliary features for Entity Linking in the medical domain. We report experiments that show that naive ensembling does not always outperform component Entity Linking systems, that stacking usually outperforms naive ensembling, and that auxiliary features added to the stacker further improve its performance on three distinct datasets. Our best model produces state-of-the-art results on several medical datasets.",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/b7d484357f5f84c065ef7fbc77d0460b9795964d.pdf",
      "citation_key": "rajani20171n9",
      "metadata": {
        "title": "Stacking With Auxiliary Features for Entity Linking in the Medical Domain",
        "authors": [
          "Nazneen Rajani",
          "Mihaela A. Bornea",
          "Ken Barker"
        ],
        "published_date": "2017",
        "abstract": "Linking spans of natural language text to concepts in a structured source is an important task for many problems. It allows intelligent systems to leverage rich knowledge available in those sources (such as concept properties and relations) to enhance the semantics of the mentions of these concepts in text. In the medical domain, it is common to link text spans to medical concepts in large, curated knowledge repositories such as the Unified Medical Language System. Different approaches have different strengths: some are precision-oriented, some recall-oriented; some better at considering context but more prone to hallucination. The variety of techniques suggests that ensembling could outperform component technologies at this task. In this paper, we describe our process for building a Stacking ensemble using additional, auxiliary features for Entity Linking in the medical domain. We report experiments that show that naive ensembling does not always outperform component Entity Linking systems, that stacking usually outperforms naive ensembling, and that auxiliary features added to the stacker further improve its performance on three distinct datasets. Our best model produces state-of-the-art results on several medical datasets.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/b7d484357f5f84c065ef7fbc77d0460b9795964d.pdf",
        "venue": "Workshop on Biomedical Natural Language Processing",
        "citationCount": 14,
        "score": 1.75,
        "summary": "Linking spans of natural language text to concepts in a structured source is an important task for many problems. It allows intelligent systems to leverage rich knowledge available in those sources (such as concept properties and relations) to enhance the semantics of the mentions of these concepts in text. In the medical domain, it is common to link text spans to medical concepts in large, curated knowledge repositories such as the Unified Medical Language System. Different approaches have different strengths: some are precision-oriented, some recall-oriented; some better at considering context but more prone to hallucination. The variety of techniques suggests that ensembling could outperform component technologies at this task. In this paper, we describe our process for building a Stacking ensemble using additional, auxiliary features for Entity Linking in the medical domain. We report experiments that show that naive ensembling does not always outperform component Entity Linking systems, that stacking usually outperforms naive ensembling, and that auxiliary features added to the stacker further improve its performance on three distinct datasets. Our best model produces state-of-the-art results on several medical datasets.",
        "keywords": []
      },
      "file_name": "b7d484357f5f84c065ef7fbc77d0460b9795964d.pdf"
    },
    {
      "success": true,
      "doc_id": "f0de7a94088f554fdf0d49fb68db2d3f",
      "summary": "Despite the fact that large-scale Language Models (LLM) have achieved SOTA performances on a variety of NLP tasks, its performance on NER is still significantly below supervised baselines. This is due to the gap between the two tasks the NER and LLMs: the former is a sequence labeling task in nature while the latter is a text-generation model. In this paper, we propose GPT-NER to resolve this issue. GPT-NER bridges the gap by transforming the sequence labeling task to a generation task that can be easily adapted by LLMs e.g., the task of finding location entities in the input text\"Columbus is a city\"is transformed to generate the text sequence\"@@Columbus## is a city\", where special tokens @@## marks the entity to extract. To efficiently address the\"hallucination\"issue of LLMs, where LLMs have a strong inclination to over-confidently label NULL inputs as entities, we propose a self-verification strategy by prompting LLMs to ask itself whether the extracted entities belong to a labeled entity tag. We conduct experiments on five widely adopted NER datasets, and GPT-NER achieves comparable performances to fully supervised baselines, which is the first time as far as we are concerned. More importantly, we find that GPT-NER exhibits a greater ability in the low-resource and few-shot setups, when the amount of training data is extremely scarce, GPT-NER performs significantly better than supervised models. This demonstrates the capabilities of GPT-NER in real-world NER applications where the number of labeled examples is limited.",
      "intriguing_abstract": "Despite the fact that large-scale Language Models (LLM) have achieved SOTA performances on a variety of NLP tasks, its performance on NER is still significantly below supervised baselines. This is due to the gap between the two tasks the NER and LLMs: the former is a sequence labeling task in nature while the latter is a text-generation model. In this paper, we propose GPT-NER to resolve this issue. GPT-NER bridges the gap by transforming the sequence labeling task to a generation task that can be easily adapted by LLMs e.g., the task of finding location entities in the input text\"Columbus is a city\"is transformed to generate the text sequence\"@@Columbus## is a city\", where special tokens @@## marks the entity to extract. To efficiently address the\"hallucination\"issue of LLMs, where LLMs have a strong inclination to over-confidently label NULL inputs as entities, we propose a self-verification strategy by prompting LLMs to ask itself whether the extracted entities belong to a labeled entity tag. We conduct experiments on five widely adopted NER datasets, and GPT-NER achieves comparable performances to fully supervised baselines, which is the first time as far as we are concerned. More importantly, we find that GPT-NER exhibits a greater ability in the low-resource and few-shot setups, when the amount of training data is extremely scarce, GPT-NER performs significantly better than supervised models. This demonstrates the capabilities of GPT-NER in real-world NER applications where the number of labeled examples is limited.",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/7c9f69848d28e0a7cbb00942ee83dab9773c23e4.pdf",
      "citation_key": "wang202379k",
      "metadata": {
        "title": "GPT-NER: Named Entity Recognition via Large Language Models",
        "authors": [
          "Shuhe Wang",
          "Xiaofei Sun",
          "Xiaoya Li",
          "Rongbin Ouyang",
          "Fei Wu",
          "Tianwei Zhang",
          "Jiwei Li",
          "Guoyin Wang"
        ],
        "published_date": "2023",
        "abstract": "Despite the fact that large-scale Language Models (LLM) have achieved SOTA performances on a variety of NLP tasks, its performance on NER is still significantly below supervised baselines. This is due to the gap between the two tasks the NER and LLMs: the former is a sequence labeling task in nature while the latter is a text-generation model. In this paper, we propose GPT-NER to resolve this issue. GPT-NER bridges the gap by transforming the sequence labeling task to a generation task that can be easily adapted by LLMs e.g., the task of finding location entities in the input text\"Columbus is a city\"is transformed to generate the text sequence\"@@Columbus## is a city\", where special tokens @@## marks the entity to extract. To efficiently address the\"hallucination\"issue of LLMs, where LLMs have a strong inclination to over-confidently label NULL inputs as entities, we propose a self-verification strategy by prompting LLMs to ask itself whether the extracted entities belong to a labeled entity tag. We conduct experiments on five widely adopted NER datasets, and GPT-NER achieves comparable performances to fully supervised baselines, which is the first time as far as we are concerned. More importantly, we find that GPT-NER exhibits a greater ability in the low-resource and few-shot setups, when the amount of training data is extremely scarce, GPT-NER performs significantly better than supervised models. This demonstrates the capabilities of GPT-NER in real-world NER applications where the number of labeled examples is limited.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/7c9f69848d28e0a7cbb00942ee83dab9773c23e4.pdf",
        "venue": "North American Chapter of the Association for Computational Linguistics",
        "citationCount": 248,
        "score": 124.0,
        "summary": "Despite the fact that large-scale Language Models (LLM) have achieved SOTA performances on a variety of NLP tasks, its performance on NER is still significantly below supervised baselines. This is due to the gap between the two tasks the NER and LLMs: the former is a sequence labeling task in nature while the latter is a text-generation model. In this paper, we propose GPT-NER to resolve this issue. GPT-NER bridges the gap by transforming the sequence labeling task to a generation task that can be easily adapted by LLMs e.g., the task of finding location entities in the input text\"Columbus is a city\"is transformed to generate the text sequence\"@@Columbus## is a city\", where special tokens @@## marks the entity to extract. To efficiently address the\"hallucination\"issue of LLMs, where LLMs have a strong inclination to over-confidently label NULL inputs as entities, we propose a self-verification strategy by prompting LLMs to ask itself whether the extracted entities belong to a labeled entity tag. We conduct experiments on five widely adopted NER datasets, and GPT-NER achieves comparable performances to fully supervised baselines, which is the first time as far as we are concerned. More importantly, we find that GPT-NER exhibits a greater ability in the low-resource and few-shot setups, when the amount of training data is extremely scarce, GPT-NER performs significantly better than supervised models. This demonstrates the capabilities of GPT-NER in real-world NER applications where the number of labeled examples is limited.",
        "keywords": []
      },
      "file_name": "7c9f69848d28e0a7cbb00942ee83dab9773c23e4.pdf"
    },
    {
      "success": true,
      "doc_id": "c96858c7cbf024ec0274a3c0a538b95a",
      "summary": "This paper describes Metaâ€™s TestGen-LLM tool, which uses LLMs to automatically improve existing human-written tests. TestGen-LLM verifies that its generated test classes successfully clear a set of filters that assure measurable improvement over the original test suite, thereby eliminating problems due to LLM hallucination. We describe the deployment of TestGen-LLM at Meta test-a-thons for the Instagram and Facebook platforms. In an evaluation on Reels and Stories products for Instagram, 75% of TestGen-LLMâ€™s test cases built correctly, 57% passed reliably, and 25% increased coverage. During Metaâ€™s Instagram and Facebook test-a-thons, it improved 11.5% of all classes to which it was applied, with 73% of its recommendations being accepted for production deployment by Meta software engineers. We believe this is the first report on industrial scale deployment of LLM-generated code backed by such assurances of code improvement.",
      "intriguing_abstract": "This paper describes Metaâ€™s TestGen-LLM tool, which uses LLMs to automatically improve existing human-written tests. TestGen-LLM verifies that its generated test classes successfully clear a set of filters that assure measurable improvement over the original test suite, thereby eliminating problems due to LLM hallucination. We describe the deployment of TestGen-LLM at Meta test-a-thons for the Instagram and Facebook platforms. In an evaluation on Reels and Stories products for Instagram, 75% of TestGen-LLMâ€™s test cases built correctly, 57% passed reliably, and 25% increased coverage. During Metaâ€™s Instagram and Facebook test-a-thons, it improved 11.5% of all classes to which it was applied, with 73% of its recommendations being accepted for production deployment by Meta software engineers. We believe this is the first report on industrial scale deployment of LLM-generated code backed by such assurances of code improvement.",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/ce4e101950554c41d3b35f5b297722abd1ce6403.pdf",
      "citation_key": "alshahwan2024v64",
      "metadata": {
        "title": "Automated Unit Test Improvement using Large Language Models at Meta",
        "authors": [
          "N. Alshahwan",
          "Jubin Chheda",
          "Anastasia Finogenova",
          "Beliz Gokkaya",
          "Mark Harman",
          "Inna Harper",
          "Alexandru Marginean",
          "Shubho Sengupta",
          "Eddy Wang"
        ],
        "published_date": "2024",
        "abstract": "This paper describes Metaâ€™s TestGen-LLM tool, which uses LLMs to automatically improve existing human-written tests. TestGen-LLM verifies that its generated test classes successfully clear a set of filters that assure measurable improvement over the original test suite, thereby eliminating problems due to LLM hallucination. We describe the deployment of TestGen-LLM at Meta test-a-thons for the Instagram and Facebook platforms. In an evaluation on Reels and Stories products for Instagram, 75% of TestGen-LLMâ€™s test cases built correctly, 57% passed reliably, and 25% increased coverage. During Metaâ€™s Instagram and Facebook test-a-thons, it improved 11.5% of all classes to which it was applied, with 73% of its recommendations being accepted for production deployment by Meta software engineers. We believe this is the first report on industrial scale deployment of LLM-generated code backed by such assurances of code improvement.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/ce4e101950554c41d3b35f5b297722abd1ce6403.pdf",
        "venue": "SIGSOFT FSE Companion",
        "citationCount": 120,
        "score": 120.0,
        "summary": "This paper describes Metaâ€™s TestGen-LLM tool, which uses LLMs to automatically improve existing human-written tests. TestGen-LLM verifies that its generated test classes successfully clear a set of filters that assure measurable improvement over the original test suite, thereby eliminating problems due to LLM hallucination. We describe the deployment of TestGen-LLM at Meta test-a-thons for the Instagram and Facebook platforms. In an evaluation on Reels and Stories products for Instagram, 75% of TestGen-LLMâ€™s test cases built correctly, 57% passed reliably, and 25% increased coverage. During Metaâ€™s Instagram and Facebook test-a-thons, it improved 11.5% of all classes to which it was applied, with 73% of its recommendations being accepted for production deployment by Meta software engineers. We believe this is the first report on industrial scale deployment of LLM-generated code backed by such assurances of code improvement.",
        "keywords": []
      },
      "file_name": "ce4e101950554c41d3b35f5b297722abd1ce6403.pdf"
    },
    {
      "success": true,
      "doc_id": "09ec8258dc856fe03f7a4f6ed3d7f95e",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/5c204b2421d05b83d3c96a6c515cc03143073935.pdf",
      "citation_key": "zou2024ucl",
      "metadata": {
        "title": "PoisonedRAG: Knowledge Poisoning Attacks to Retrieval-Augmented Generation of Large Language Models",
        "authors": [
          "Wei Zou",
          "Runpeng Geng",
          "Binghui Wang",
          "Jinyuan Jia"
        ],
        "published_date": "2024",
        "abstract": "",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/5c204b2421d05b83d3c96a6c515cc03143073935.pdf",
        "venue": "arXiv.org",
        "citationCount": 76,
        "score": 76.0,
        "summary": "",
        "keywords": []
      },
      "file_name": "5c204b2421d05b83d3c96a6c515cc03143073935.pdf"
    },
    {
      "success": true,
      "doc_id": "2ee991b3e991fca4816202b12b86ddec",
      "summary": "Large language models (LLMs) are notorious for hallucinating, i.e., producing erroneous claims in their output. Such hallucinations can be dangerous, as occasional factual inaccuracies in the generated text might be obscured by the rest of the output being generally factually correct, making it extremely hard for the users to spot them. Current services that leverage LLMs usually do not provide any means for detecting unreliable generations. Here, we aim to bridge this gap. In particular, we propose a novel fact-checking and hallucination detection pipeline based on token-level uncertainty quantification. Uncertainty scores leverage information encapsulated in the output of a neural network or its layers to detect unreliable predictions, and we show that they can be used to fact-check the atomic claims in the LLM output. Moreover, we present a novel token-level uncertainty quantification method that removes the impact of uncertainty about what claim to generate on the current step and what surface form to use. Our method Claim Conditioned Probability (CCP) measures only the uncertainty of a particular claim value expressed by the model. Experiments on the task of biography generation demonstrate strong improvements for CCP compared to the baselines for seven LLMs and four languages. Human evaluation reveals that the fact-checking pipeline based on uncertainty quantification is competitive with a fact-checking tool that leverages external knowledge.",
      "intriguing_abstract": "Large language models (LLMs) are notorious for hallucinating, i.e., producing erroneous claims in their output. Such hallucinations can be dangerous, as occasional factual inaccuracies in the generated text might be obscured by the rest of the output being generally factually correct, making it extremely hard for the users to spot them. Current services that leverage LLMs usually do not provide any means for detecting unreliable generations. Here, we aim to bridge this gap. In particular, we propose a novel fact-checking and hallucination detection pipeline based on token-level uncertainty quantification. Uncertainty scores leverage information encapsulated in the output of a neural network or its layers to detect unreliable predictions, and we show that they can be used to fact-check the atomic claims in the LLM output. Moreover, we present a novel token-level uncertainty quantification method that removes the impact of uncertainty about what claim to generate on the current step and what surface form to use. Our method Claim Conditioned Probability (CCP) measures only the uncertainty of a particular claim value expressed by the model. Experiments on the task of biography generation demonstrate strong improvements for CCP compared to the baselines for seven LLMs and four languages. Human evaluation reveals that the fact-checking pipeline based on uncertainty quantification is competitive with a fact-checking tool that leverages external knowledge.",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/8c5acaafe43e710d55b08c63d567550ad26ec437.pdf",
      "citation_key": "fadeeva2024lt8",
      "metadata": {
        "title": "Fact-Checking the Output of Large Language Models via Token-Level Uncertainty Quantification",
        "authors": [
          "Ekaterina Fadeeva",
          "Aleksandr Rubashevskii",
          "Artem Shelmanov",
          "Sergey Petrakov",
          "Haonan Li",
          "Hamdy Mubarak",
          "Evgenii Tsymbalov",
          "Gleb Kuzmin",
          "Alexander Panchenko",
          "Timothy Baldwin",
          "Preslav Nakov",
          "Maxim Panov"
        ],
        "published_date": "2024",
        "abstract": "Large language models (LLMs) are notorious for hallucinating, i.e., producing erroneous claims in their output. Such hallucinations can be dangerous, as occasional factual inaccuracies in the generated text might be obscured by the rest of the output being generally factually correct, making it extremely hard for the users to spot them. Current services that leverage LLMs usually do not provide any means for detecting unreliable generations. Here, we aim to bridge this gap. In particular, we propose a novel fact-checking and hallucination detection pipeline based on token-level uncertainty quantification. Uncertainty scores leverage information encapsulated in the output of a neural network or its layers to detect unreliable predictions, and we show that they can be used to fact-check the atomic claims in the LLM output. Moreover, we present a novel token-level uncertainty quantification method that removes the impact of uncertainty about what claim to generate on the current step and what surface form to use. Our method Claim Conditioned Probability (CCP) measures only the uncertainty of a particular claim value expressed by the model. Experiments on the task of biography generation demonstrate strong improvements for CCP compared to the baselines for seven LLMs and four languages. Human evaluation reveals that the fact-checking pipeline based on uncertainty quantification is competitive with a fact-checking tool that leverages external knowledge.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/8c5acaafe43e710d55b08c63d567550ad26ec437.pdf",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "citationCount": 73,
        "score": 73.0,
        "summary": "Large language models (LLMs) are notorious for hallucinating, i.e., producing erroneous claims in their output. Such hallucinations can be dangerous, as occasional factual inaccuracies in the generated text might be obscured by the rest of the output being generally factually correct, making it extremely hard for the users to spot them. Current services that leverage LLMs usually do not provide any means for detecting unreliable generations. Here, we aim to bridge this gap. In particular, we propose a novel fact-checking and hallucination detection pipeline based on token-level uncertainty quantification. Uncertainty scores leverage information encapsulated in the output of a neural network or its layers to detect unreliable predictions, and we show that they can be used to fact-check the atomic claims in the LLM output. Moreover, we present a novel token-level uncertainty quantification method that removes the impact of uncertainty about what claim to generate on the current step and what surface form to use. Our method Claim Conditioned Probability (CCP) measures only the uncertainty of a particular claim value expressed by the model. Experiments on the task of biography generation demonstrate strong improvements for CCP compared to the baselines for seven LLMs and four languages. Human evaluation reveals that the fact-checking pipeline based on uncertainty quantification is competitive with a fact-checking tool that leverages external knowledge.",
        "keywords": []
      },
      "file_name": "8c5acaafe43e710d55b08c63d567550ad26ec437.pdf"
    },
    {
      "success": true,
      "doc_id": "8794d3adb8429cdcf38dd4b25a1d1337",
      "summary": "Extensive training datasets represent one of the important factors for the impressive learning capabilities of large language models (LLMs). However, these training datasets for current LLMs, especially the recent state-of-the-art models, are often not fully disclosed. Creating training data for high-performing LLMs involves extensive cleaning and deduplication to ensure the necessary level of quality. The lack of transparency for training data has thus hampered research on attributing and addressing hallucination and bias issues in LLMs, hindering replication efforts and further advancements in the community. These challenges become even more pronounced in multilingual learning scenarios, where the available multilingual text datasets are often inadequately collected and cleaned. Consequently, there is a lack of open-source and readily usable dataset to effectively train LLMs in multiple languages. To overcome this issue, we present CulturaX, a substantial multilingual dataset with 6.3 trillion tokens in 167 languages, tailored for LLM development. Our dataset undergoes meticulous cleaning and deduplication through a rigorous pipeline of multiple stages to accomplish the best quality for model training, including language identification, URL-based filtering, metric-based cleaning, document refinement, and data deduplication. CulturaX is released in Hugging Face facilitate research and advancements in multilingual LLMs: https://huggingface.co/datasets/uonlp/CulturaX.",
      "intriguing_abstract": "Extensive training datasets represent one of the important factors for the impressive learning capabilities of large language models (LLMs). However, these training datasets for current LLMs, especially the recent state-of-the-art models, are often not fully disclosed. Creating training data for high-performing LLMs involves extensive cleaning and deduplication to ensure the necessary level of quality. The lack of transparency for training data has thus hampered research on attributing and addressing hallucination and bias issues in LLMs, hindering replication efforts and further advancements in the community. These challenges become even more pronounced in multilingual learning scenarios, where the available multilingual text datasets are often inadequately collected and cleaned. Consequently, there is a lack of open-source and readily usable dataset to effectively train LLMs in multiple languages. To overcome this issue, we present CulturaX, a substantial multilingual dataset with 6.3 trillion tokens in 167 languages, tailored for LLM development. Our dataset undergoes meticulous cleaning and deduplication through a rigorous pipeline of multiple stages to accomplish the best quality for model training, including language identification, URL-based filtering, metric-based cleaning, document refinement, and data deduplication. CulturaX is released in Hugging Face facilitate research and advancements in multilingual LLMs: https://huggingface.co/datasets/uonlp/CulturaX.",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/1ebcf1884390c28f24b3adaf5a7aba5b9453b48b.pdf",
      "citation_key": "nguyen2023obn",
      "metadata": {
        "title": "CulturaX: A Cleaned, Enormous, and Multilingual Dataset for Large Language Models in 167 Languages",
        "authors": [
          "Thuat Nguyen",
          "C. Nguyen",
          "Viet Dac Lai",
          "Hieu Man",
          "Nghia Trung Ngo",
          "Franck Dernoncourt",
          "Ryan A. Rossi",
          "Thien Huu Nguyen"
        ],
        "published_date": "2023",
        "abstract": "Extensive training datasets represent one of the important factors for the impressive learning capabilities of large language models (LLMs). However, these training datasets for current LLMs, especially the recent state-of-the-art models, are often not fully disclosed. Creating training data for high-performing LLMs involves extensive cleaning and deduplication to ensure the necessary level of quality. The lack of transparency for training data has thus hampered research on attributing and addressing hallucination and bias issues in LLMs, hindering replication efforts and further advancements in the community. These challenges become even more pronounced in multilingual learning scenarios, where the available multilingual text datasets are often inadequately collected and cleaned. Consequently, there is a lack of open-source and readily usable dataset to effectively train LLMs in multiple languages. To overcome this issue, we present CulturaX, a substantial multilingual dataset with 6.3 trillion tokens in 167 languages, tailored for LLM development. Our dataset undergoes meticulous cleaning and deduplication through a rigorous pipeline of multiple stages to accomplish the best quality for model training, including language identification, URL-based filtering, metric-based cleaning, document refinement, and data deduplication. CulturaX is released in Hugging Face facilitate research and advancements in multilingual LLMs: https://huggingface.co/datasets/uonlp/CulturaX.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/1ebcf1884390c28f24b3adaf5a7aba5b9453b48b.pdf",
        "venue": "International Conference on Language Resources and Evaluation",
        "citationCount": 129,
        "score": 64.5,
        "summary": "Extensive training datasets represent one of the important factors for the impressive learning capabilities of large language models (LLMs). However, these training datasets for current LLMs, especially the recent state-of-the-art models, are often not fully disclosed. Creating training data for high-performing LLMs involves extensive cleaning and deduplication to ensure the necessary level of quality. The lack of transparency for training data has thus hampered research on attributing and addressing hallucination and bias issues in LLMs, hindering replication efforts and further advancements in the community. These challenges become even more pronounced in multilingual learning scenarios, where the available multilingual text datasets are often inadequately collected and cleaned. Consequently, there is a lack of open-source and readily usable dataset to effectively train LLMs in multiple languages. To overcome this issue, we present CulturaX, a substantial multilingual dataset with 6.3 trillion tokens in 167 languages, tailored for LLM development. Our dataset undergoes meticulous cleaning and deduplication through a rigorous pipeline of multiple stages to accomplish the best quality for model training, including language identification, URL-based filtering, metric-based cleaning, document refinement, and data deduplication. CulturaX is released in Hugging Face facilitate research and advancements in multilingual LLMs: https://huggingface.co/datasets/uonlp/CulturaX.",
        "keywords": []
      },
      "file_name": "1ebcf1884390c28f24b3adaf5a7aba5b9453b48b.pdf"
    },
    {
      "success": true,
      "doc_id": "954461512b7d4876a701c3d21b8fc34b",
      "summary": "Large language models (LLMs) have achieved remarkable success due to their exceptional generative capabilities. Despite their success, they also have inherent limitations such as a lack of up-to-date knowledge and hallucination. Retrieval-Augmented Generation (RAG) is a state-of-the-art technique to mitigate these limitations. The key idea of RAG is to ground the answer generation of an LLM on external knowledge retrieved from a knowledge database. Existing studies mainly focus on improving the accuracy or efficiency of RAG, leaving its security largely unexplored. We aim to bridge the gap in this work. We find that the knowledge database in a RAG system introduces a new and practical attack surface. Based on this attack surface, we propose PoisonedRAG, the first knowledge corruption attack to RAG, where an attacker could inject a few malicious texts into the knowledge database of a RAG system to induce an LLM to generate an attacker-chosen target answer for an attacker-chosen target question. We formulate knowledge corruption attacks as an optimization problem, whose solution is a set of malicious texts. Depending on the background knowledge (e.g., black-box and white-box settings) of an attacker on a RAG system, we propose two solutions to solve the optimization problem, respectively. Our results show PoisonedRAG could achieve a 90% attack success rate when injecting five malicious texts for each target question into a knowledge database with millions of texts. We also evaluate several defenses and our results show they are insufficient to defend against PoisonedRAG, highlighting the need for new defenses.",
      "intriguing_abstract": "Large language models (LLMs) have achieved remarkable success due to their exceptional generative capabilities. Despite their success, they also have inherent limitations such as a lack of up-to-date knowledge and hallucination. Retrieval-Augmented Generation (RAG) is a state-of-the-art technique to mitigate these limitations. The key idea of RAG is to ground the answer generation of an LLM on external knowledge retrieved from a knowledge database. Existing studies mainly focus on improving the accuracy or efficiency of RAG, leaving its security largely unexplored. We aim to bridge the gap in this work. We find that the knowledge database in a RAG system introduces a new and practical attack surface. Based on this attack surface, we propose PoisonedRAG, the first knowledge corruption attack to RAG, where an attacker could inject a few malicious texts into the knowledge database of a RAG system to induce an LLM to generate an attacker-chosen target answer for an attacker-chosen target question. We formulate knowledge corruption attacks as an optimization problem, whose solution is a set of malicious texts. Depending on the background knowledge (e.g., black-box and white-box settings) of an attacker on a RAG system, we propose two solutions to solve the optimization problem, respectively. Our results show PoisonedRAG could achieve a 90% attack success rate when injecting five malicious texts for each target question into a knowledge database with millions of texts. We also evaluate several defenses and our results show they are insufficient to defend against PoisonedRAG, highlighting the need for new defenses.",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/f4e06256ab07727ff4e0465deea83fcf45012354.pdf",
      "citation_key": "zou2024c26",
      "metadata": {
        "title": "PoisonedRAG: Knowledge Corruption Attacks to Retrieval-Augmented Generation of Large Language Models",
        "authors": [
          "Wei Zou",
          "Runpeng Geng",
          "Binghui Wang",
          "Jinyuan Jia"
        ],
        "published_date": "2024",
        "abstract": "Large language models (LLMs) have achieved remarkable success due to their exceptional generative capabilities. Despite their success, they also have inherent limitations such as a lack of up-to-date knowledge and hallucination. Retrieval-Augmented Generation (RAG) is a state-of-the-art technique to mitigate these limitations. The key idea of RAG is to ground the answer generation of an LLM on external knowledge retrieved from a knowledge database. Existing studies mainly focus on improving the accuracy or efficiency of RAG, leaving its security largely unexplored. We aim to bridge the gap in this work. We find that the knowledge database in a RAG system introduces a new and practical attack surface. Based on this attack surface, we propose PoisonedRAG, the first knowledge corruption attack to RAG, where an attacker could inject a few malicious texts into the knowledge database of a RAG system to induce an LLM to generate an attacker-chosen target answer for an attacker-chosen target question. We formulate knowledge corruption attacks as an optimization problem, whose solution is a set of malicious texts. Depending on the background knowledge (e.g., black-box and white-box settings) of an attacker on a RAG system, we propose two solutions to solve the optimization problem, respectively. Our results show PoisonedRAG could achieve a 90% attack success rate when injecting five malicious texts for each target question into a knowledge database with millions of texts. We also evaluate several defenses and our results show they are insufficient to defend against PoisonedRAG, highlighting the need for new defenses.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/f4e06256ab07727ff4e0465deea83fcf45012354.pdf",
        "venue": "",
        "citationCount": 53,
        "score": 53.0,
        "summary": "Large language models (LLMs) have achieved remarkable success due to their exceptional generative capabilities. Despite their success, they also have inherent limitations such as a lack of up-to-date knowledge and hallucination. Retrieval-Augmented Generation (RAG) is a state-of-the-art technique to mitigate these limitations. The key idea of RAG is to ground the answer generation of an LLM on external knowledge retrieved from a knowledge database. Existing studies mainly focus on improving the accuracy or efficiency of RAG, leaving its security largely unexplored. We aim to bridge the gap in this work. We find that the knowledge database in a RAG system introduces a new and practical attack surface. Based on this attack surface, we propose PoisonedRAG, the first knowledge corruption attack to RAG, where an attacker could inject a few malicious texts into the knowledge database of a RAG system to induce an LLM to generate an attacker-chosen target answer for an attacker-chosen target question. We formulate knowledge corruption attacks as an optimization problem, whose solution is a set of malicious texts. Depending on the background knowledge (e.g., black-box and white-box settings) of an attacker on a RAG system, we propose two solutions to solve the optimization problem, respectively. Our results show PoisonedRAG could achieve a 90% attack success rate when injecting five malicious texts for each target question into a knowledge database with millions of texts. We also evaluate several defenses and our results show they are insufficient to defend against PoisonedRAG, highlighting the need for new defenses.",
        "keywords": []
      },
      "file_name": "f4e06256ab07727ff4e0465deea83fcf45012354.pdf"
    },
    {
      "success": true,
      "doc_id": "b818cabfabb2e143ee81824b433d7155",
      "summary": "While Large Language Models (LLMs) have demonstrated impressive accomplishments in both reasoning and planning, their abilities in multi-agent collaborations remains largely unexplored. This study evaluates LLM-based agents in a multi-agent cooperative text game with Theory of Mind (ToM) inference tasks, comparing their performance with Multi-Agent Reinforcement Learning (MARL) and planning-based baselines. We observed evidence of emergent collaborative behaviors and high-order Theory of Mind capabilities among LLM-based agents. Our results reveal limitations in LLM-based agents' planning optimization due to systematic failures in managing long-horizon contexts and hallucination about the task state. We explore the use of explicit belief state representations to mitigate these issues, finding that it enhances task performance and the accuracy of ToM inferences for LLM-based agents.",
      "intriguing_abstract": "While Large Language Models (LLMs) have demonstrated impressive accomplishments in both reasoning and planning, their abilities in multi-agent collaborations remains largely unexplored. This study evaluates LLM-based agents in a multi-agent cooperative text game with Theory of Mind (ToM) inference tasks, comparing their performance with Multi-Agent Reinforcement Learning (MARL) and planning-based baselines. We observed evidence of emergent collaborative behaviors and high-order Theory of Mind capabilities among LLM-based agents. Our results reveal limitations in LLM-based agents' planning optimization due to systematic failures in managing long-horizon contexts and hallucination about the task state. We explore the use of explicit belief state representations to mitigate these issues, finding that it enhances task performance and the accuracy of ToM inferences for LLM-based agents.",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/e17c58d7a48b6b811df023484161a3b9c03e0d6b.pdf",
      "citation_key": "li2023f7d",
      "metadata": {
        "title": "Theory of Mind for Multi-Agent Collaboration via Large Language Models",
        "authors": [
          "Huao Li",
          "Yu Quan Chong",
          "Simon Stepputtis",
          "Joseph Campbell",
          "Dana Hughes",
          "Michael Lewis",
          "Katia P. Sycara"
        ],
        "published_date": "2023",
        "abstract": "While Large Language Models (LLMs) have demonstrated impressive accomplishments in both reasoning and planning, their abilities in multi-agent collaborations remains largely unexplored. This study evaluates LLM-based agents in a multi-agent cooperative text game with Theory of Mind (ToM) inference tasks, comparing their performance with Multi-Agent Reinforcement Learning (MARL) and planning-based baselines. We observed evidence of emergent collaborative behaviors and high-order Theory of Mind capabilities among LLM-based agents. Our results reveal limitations in LLM-based agents' planning optimization due to systematic failures in managing long-horizon contexts and hallucination about the task state. We explore the use of explicit belief state representations to mitigate these issues, finding that it enhances task performance and the accuracy of ToM inferences for LLM-based agents.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/e17c58d7a48b6b811df023484161a3b9c03e0d6b.pdf",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "citationCount": 103,
        "score": 51.5,
        "summary": "While Large Language Models (LLMs) have demonstrated impressive accomplishments in both reasoning and planning, their abilities in multi-agent collaborations remains largely unexplored. This study evaluates LLM-based agents in a multi-agent cooperative text game with Theory of Mind (ToM) inference tasks, comparing their performance with Multi-Agent Reinforcement Learning (MARL) and planning-based baselines. We observed evidence of emergent collaborative behaviors and high-order Theory of Mind capabilities among LLM-based agents. Our results reveal limitations in LLM-based agents' planning optimization due to systematic failures in managing long-horizon contexts and hallucination about the task state. We explore the use of explicit belief state representations to mitigate these issues, finding that it enhances task performance and the accuracy of ToM inferences for LLM-based agents.",
        "keywords": []
      },
      "file_name": "e17c58d7a48b6b811df023484161a3b9c03e0d6b.pdf"
    },
    {
      "success": true,
      "doc_id": "97ec66b049d4f385c99977d8ec1a65f2",
      "summary": "Automatic summarization generates concise summaries that contain key ideas of source documents.As the most mainstream datasets for the news sub-domain, CNN/DailyMail and BBC XSum have been widely used for performance benchmarking. However, the reference summaries of those datasets turn out to be noisy, mainly in terms of factual hallucination and information redundancy. To address this challenge, we first annotate new expert-writing Element-aware test sets following the â€œLasswell Communication Modelâ€ proposed by Lasswell, allowing reference summaries to focus on more fine-grained news elements objectively and comprehensively. Utilizing the new test sets, we observe the surprising zero-shot summary ability of LLMs, which addresses the issue of the inconsistent results between human preference and automatic evaluation metrics of LLMsâ€™ zero-shot summaries in prior work. Further, we propose a Summary Chain-of-Thought (SumCoT) technique to elicit LLMs to generate summaries step by step, which helps them integrate more fine-grained details of source documents into the final summaries that correlate with the human writing mindset. Experimental results show our method outperforms state-of-the-art fine-tuned PLMs and zero-shot LLMs by +4.33/+4.77 in ROUGE-L on the two datasets, respectively. Dataset and code are publicly available at https://github.com/Alsace08/SumCoT.",
      "intriguing_abstract": "Automatic summarization generates concise summaries that contain key ideas of source documents.As the most mainstream datasets for the news sub-domain, CNN/DailyMail and BBC XSum have been widely used for performance benchmarking. However, the reference summaries of those datasets turn out to be noisy, mainly in terms of factual hallucination and information redundancy. To address this challenge, we first annotate new expert-writing Element-aware test sets following the â€œLasswell Communication Modelâ€ proposed by Lasswell, allowing reference summaries to focus on more fine-grained news elements objectively and comprehensively. Utilizing the new test sets, we observe the surprising zero-shot summary ability of LLMs, which addresses the issue of the inconsistent results between human preference and automatic evaluation metrics of LLMsâ€™ zero-shot summaries in prior work. Further, we propose a Summary Chain-of-Thought (SumCoT) technique to elicit LLMs to generate summaries step by step, which helps them integrate more fine-grained details of source documents into the final summaries that correlate with the human writing mindset. Experimental results show our method outperforms state-of-the-art fine-tuned PLMs and zero-shot LLMs by +4.33/+4.77 in ROUGE-L on the two datasets, respectively. Dataset and code are publicly available at https://github.com/Alsace08/SumCoT.",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/2338d7c9ab07e6d0f4160335dce0e6e6a87c4749.pdf",
      "citation_key": "wang2023hgw",
      "metadata": {
        "title": "Element-aware Summarization with Large Language Models: Expert-aligned Evaluation and Chain-of-Thought Method",
        "authors": [
          "Yiming Wang",
          "Zhuosheng Zhang",
          "Rui Wang"
        ],
        "published_date": "2023",
        "abstract": "Automatic summarization generates concise summaries that contain key ideas of source documents.As the most mainstream datasets for the news sub-domain, CNN/DailyMail and BBC XSum have been widely used for performance benchmarking. However, the reference summaries of those datasets turn out to be noisy, mainly in terms of factual hallucination and information redundancy. To address this challenge, we first annotate new expert-writing Element-aware test sets following the â€œLasswell Communication Modelâ€ proposed by Lasswell, allowing reference summaries to focus on more fine-grained news elements objectively and comprehensively. Utilizing the new test sets, we observe the surprising zero-shot summary ability of LLMs, which addresses the issue of the inconsistent results between human preference and automatic evaluation metrics of LLMsâ€™ zero-shot summaries in prior work. Further, we propose a Summary Chain-of-Thought (SumCoT) technique to elicit LLMs to generate summaries step by step, which helps them integrate more fine-grained details of source documents into the final summaries that correlate with the human writing mindset. Experimental results show our method outperforms state-of-the-art fine-tuned PLMs and zero-shot LLMs by +4.33/+4.77 in ROUGE-L on the two datasets, respectively. Dataset and code are publicly available at https://github.com/Alsace08/SumCoT.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/2338d7c9ab07e6d0f4160335dce0e6e6a87c4749.pdf",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "citationCount": 101,
        "score": 50.5,
        "summary": "Automatic summarization generates concise summaries that contain key ideas of source documents.As the most mainstream datasets for the news sub-domain, CNN/DailyMail and BBC XSum have been widely used for performance benchmarking. However, the reference summaries of those datasets turn out to be noisy, mainly in terms of factual hallucination and information redundancy. To address this challenge, we first annotate new expert-writing Element-aware test sets following the â€œLasswell Communication Modelâ€ proposed by Lasswell, allowing reference summaries to focus on more fine-grained news elements objectively and comprehensively. Utilizing the new test sets, we observe the surprising zero-shot summary ability of LLMs, which addresses the issue of the inconsistent results between human preference and automatic evaluation metrics of LLMsâ€™ zero-shot summaries in prior work. Further, we propose a Summary Chain-of-Thought (SumCoT) technique to elicit LLMs to generate summaries step by step, which helps them integrate more fine-grained details of source documents into the final summaries that correlate with the human writing mindset. Experimental results show our method outperforms state-of-the-art fine-tuned PLMs and zero-shot LLMs by +4.33/+4.77 in ROUGE-L on the two datasets, respectively. Dataset and code are publicly available at https://github.com/Alsace08/SumCoT.",
        "keywords": []
      },
      "file_name": "2338d7c9ab07e6d0f4160335dce0e6e6a87c4749.pdf"
    },
    {
      "success": true,
      "doc_id": "2e08af591d8c18391d355d7f819a2a08",
      "summary": "Large language models (LLMs) have gained widespread adoption in various natural language processing tasks, including question answering and dialogue systems. However, a major drawback of LLMs is the issue of hallucination, where they generate unfaithful or inconsistent content that deviates from the input source, leading to severe consequences. In this paper, we propose a robust discriminator named RelD to effectively detect hallucination in LLMs' generated answers. RelD is trained on the constructed RelQA, a bilingual question-answering dialogue dataset along with answers generated by LLMs and a comprehensive set of metrics. Our experimental results demonstrate that the proposed RelD successfully detects hallucination in the answers generated by diverse LLMs. Moreover, it performs well in distinguishing hallucination in LLMs' generated answers from both in-distribution and out-of-distribution datasets. Additionally, we also conduct a thorough analysis of the types of hallucinations that occur and present valuable insights. This research significantly contributes to the detection of reliable answers generated by LLMs and holds noteworthy implications for mitigating hallucination in the future work.",
      "intriguing_abstract": "Large language models (LLMs) have gained widespread adoption in various natural language processing tasks, including question answering and dialogue systems. However, a major drawback of LLMs is the issue of hallucination, where they generate unfaithful or inconsistent content that deviates from the input source, leading to severe consequences. In this paper, we propose a robust discriminator named RelD to effectively detect hallucination in LLMs' generated answers. RelD is trained on the constructed RelQA, a bilingual question-answering dialogue dataset along with answers generated by LLMs and a comprehensive set of metrics. Our experimental results demonstrate that the proposed RelD successfully detects hallucination in the answers generated by diverse LLMs. Moreover, it performs well in distinguishing hallucination in LLMs' generated answers from both in-distribution and out-of-distribution datasets. Additionally, we also conduct a thorough analysis of the types of hallucinations that occur and present valuable insights. This research significantly contributes to the detection of reliable answers generated by LLMs and holds noteworthy implications for mitigating hallucination in the future work.",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/fac468032e0c38ea10dfb95ba6cdeac51a473050.pdf",
      "citation_key": "chen2023gii",
      "metadata": {
        "title": "Hallucination Detection: Robustly Discerning Reliable Answers in Large Language Models",
        "authors": [
          "Yuyan Chen",
          "Qiang Fu",
          "Yichen Yuan",
          "Zhihao Wen",
          "Ge Fan",
          "Dayiheng Liu",
          "Dongmei Zhang",
          "Zhixu Li",
          "Yanghua Xiao"
        ],
        "published_date": "2023",
        "abstract": "Large language models (LLMs) have gained widespread adoption in various natural language processing tasks, including question answering and dialogue systems. However, a major drawback of LLMs is the issue of hallucination, where they generate unfaithful or inconsistent content that deviates from the input source, leading to severe consequences. In this paper, we propose a robust discriminator named RelD to effectively detect hallucination in LLMs' generated answers. RelD is trained on the constructed RelQA, a bilingual question-answering dialogue dataset along with answers generated by LLMs and a comprehensive set of metrics. Our experimental results demonstrate that the proposed RelD successfully detects hallucination in the answers generated by diverse LLMs. Moreover, it performs well in distinguishing hallucination in LLMs' generated answers from both in-distribution and out-of-distribution datasets. Additionally, we also conduct a thorough analysis of the types of hallucinations that occur and present valuable insights. This research significantly contributes to the detection of reliable answers generated by LLMs and holds noteworthy implications for mitigating hallucination in the future work.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/fac468032e0c38ea10dfb95ba6cdeac51a473050.pdf",
        "venue": "International Conference on Information and Knowledge Management",
        "citationCount": 92,
        "score": 46.0,
        "summary": "Large language models (LLMs) have gained widespread adoption in various natural language processing tasks, including question answering and dialogue systems. However, a major drawback of LLMs is the issue of hallucination, where they generate unfaithful or inconsistent content that deviates from the input source, leading to severe consequences. In this paper, we propose a robust discriminator named RelD to effectively detect hallucination in LLMs' generated answers. RelD is trained on the constructed RelQA, a bilingual question-answering dialogue dataset along with answers generated by LLMs and a comprehensive set of metrics. Our experimental results demonstrate that the proposed RelD successfully detects hallucination in the answers generated by diverse LLMs. Moreover, it performs well in distinguishing hallucination in LLMs' generated answers from both in-distribution and out-of-distribution datasets. Additionally, we also conduct a thorough analysis of the types of hallucinations that occur and present valuable insights. This research significantly contributes to the detection of reliable answers generated by LLMs and holds noteworthy implications for mitigating hallucination in the future work.",
        "keywords": []
      },
      "file_name": "fac468032e0c38ea10dfb95ba6cdeac51a473050.pdf"
    },
    {
      "success": true,
      "doc_id": "191fb2e03ab03b89cf40ab954a68d8f0",
      "summary": "Reliably processing and interlinking medical information has been recognized as a critical foundation to the digital transformation of medical workflows, and despite the development of medical ontologies, the optimization of these has been a major bottleneck to digital medicine. The advent of large language models has brought great excitement, and maybe a solution to the medicinesâ€™ â€˜communication problemâ€™ is in sight, but how can the known weaknesses of these models, such as hallucination and non-determinism, be tempered? Retrieval Augmented Generation, particularly through knowledge graphs, is an automated approach that can deliver structured reasoning and a model of truth alongside LLMs, relevant to information structuring and therefore also to decision support.",
      "intriguing_abstract": "Reliably processing and interlinking medical information has been recognized as a critical foundation to the digital transformation of medical workflows, and despite the development of medical ontologies, the optimization of these has been a major bottleneck to digital medicine. The advent of large language models has brought great excitement, and maybe a solution to the medicinesâ€™ â€˜communication problemâ€™ is in sight, but how can the known weaknesses of these models, such as hallucination and non-determinism, be tempered? Retrieval Augmented Generation, particularly through knowledge graphs, is an automated approach that can deliver structured reasoning and a model of truth alongside LLMs, relevant to information structuring and therefore also to decision support.",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/3e7b421f9df1bd34d9cb42ece2760269f0314f05.pdf",
      "citation_key": "gilbert2024uu2",
      "metadata": {
        "title": "Augmented non-hallucinating large language models as medical information curators",
        "authors": [
          "S. Gilbert",
          "J. Kather",
          "Aidan Hogan"
        ],
        "published_date": "2024",
        "abstract": "Reliably processing and interlinking medical information has been recognized as a critical foundation to the digital transformation of medical workflows, and despite the development of medical ontologies, the optimization of these has been a major bottleneck to digital medicine. The advent of large language models has brought great excitement, and maybe a solution to the medicinesâ€™ â€˜communication problemâ€™ is in sight, but how can the known weaknesses of these models, such as hallucination and non-determinism, be tempered? Retrieval Augmented Generation, particularly through knowledge graphs, is an automated approach that can deliver structured reasoning and a model of truth alongside LLMs, relevant to information structuring and therefore also to decision support.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/3e7b421f9df1bd34d9cb42ece2760269f0314f05.pdf",
        "venue": "npj Digit. Medicine",
        "citationCount": 46,
        "score": 46.0,
        "summary": "Reliably processing and interlinking medical information has been recognized as a critical foundation to the digital transformation of medical workflows, and despite the development of medical ontologies, the optimization of these has been a major bottleneck to digital medicine. The advent of large language models has brought great excitement, and maybe a solution to the medicinesâ€™ â€˜communication problemâ€™ is in sight, but how can the known weaknesses of these models, such as hallucination and non-determinism, be tempered? Retrieval Augmented Generation, particularly through knowledge graphs, is an automated approach that can deliver structured reasoning and a model of truth alongside LLMs, relevant to information structuring and therefore also to decision support.",
        "keywords": []
      },
      "file_name": "3e7b421f9df1bd34d9cb42ece2760269f0314f05.pdf"
    },
    {
      "success": true,
      "doc_id": "6333dab6bd5fffa1857d0076eb0f641e",
      "summary": "Large language models (LLMs) have revolutionized the global landscape of technology beyond natural language processing. Owing to their extensive pre-training on vast datasets, contemporary LLMs can handle tasks ranging from general functionalities to domain-specific areas, such as radiology, without additional fine-tuning. General-purpose chatbots based on LLMs can optimize the efficiency of radiologists in terms of their professional work and research endeavors. Importantly, these LLMs are on a trajectory of rapid evolution, wherein challenges such as â€œhallucination,â€ high training cost, and efficiency issues are addressed, along with the inclusion of multimodal inputs. In this review, we aim to offer conceptual knowledge and actionable guidance to radiologists interested in utilizing LLMs through a succinct overview of the topic and a summary of radiology-specific aspects, from the beginning to potential future directions.",
      "intriguing_abstract": "Large language models (LLMs) have revolutionized the global landscape of technology beyond natural language processing. Owing to their extensive pre-training on vast datasets, contemporary LLMs can handle tasks ranging from general functionalities to domain-specific areas, such as radiology, without additional fine-tuning. General-purpose chatbots based on LLMs can optimize the efficiency of radiologists in terms of their professional work and research endeavors. Importantly, these LLMs are on a trajectory of rapid evolution, wherein challenges such as â€œhallucination,â€ high training cost, and efficiency issues are addressed, along with the inclusion of multimodal inputs. In this review, we aim to offer conceptual knowledge and actionable guidance to radiologists interested in utilizing LLMs through a succinct overview of the topic and a summary of radiology-specific aspects, from the beginning to potential future directions.",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/25a9c0946925cc860c4600dba91b313cdbe7c8a8.pdf",
      "citation_key": "kim2024vgn",
      "metadata": {
        "title": "Large Language Models: A Guide for Radiologists",
        "authors": [
          "Sunkyu Kim",
          "Choong-kun Lee",
          "Seung-seob Kim"
        ],
        "published_date": "2024",
        "abstract": "Large language models (LLMs) have revolutionized the global landscape of technology beyond natural language processing. Owing to their extensive pre-training on vast datasets, contemporary LLMs can handle tasks ranging from general functionalities to domain-specific areas, such as radiology, without additional fine-tuning. General-purpose chatbots based on LLMs can optimize the efficiency of radiologists in terms of their professional work and research endeavors. Importantly, these LLMs are on a trajectory of rapid evolution, wherein challenges such as â€œhallucination,â€ high training cost, and efficiency issues are addressed, along with the inclusion of multimodal inputs. In this review, we aim to offer conceptual knowledge and actionable guidance to radiologists interested in utilizing LLMs through a succinct overview of the topic and a summary of radiology-specific aspects, from the beginning to potential future directions.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/25a9c0946925cc860c4600dba91b313cdbe7c8a8.pdf",
        "venue": "Korean Journal of Radiology",
        "citationCount": 46,
        "score": 46.0,
        "summary": "Large language models (LLMs) have revolutionized the global landscape of technology beyond natural language processing. Owing to their extensive pre-training on vast datasets, contemporary LLMs can handle tasks ranging from general functionalities to domain-specific areas, such as radiology, without additional fine-tuning. General-purpose chatbots based on LLMs can optimize the efficiency of radiologists in terms of their professional work and research endeavors. Importantly, these LLMs are on a trajectory of rapid evolution, wherein challenges such as â€œhallucination,â€ high training cost, and efficiency issues are addressed, along with the inclusion of multimodal inputs. In this review, we aim to offer conceptual knowledge and actionable guidance to radiologists interested in utilizing LLMs through a succinct overview of the topic and a summary of radiology-specific aspects, from the beginning to potential future directions.",
        "keywords": []
      },
      "file_name": "25a9c0946925cc860c4600dba91b313cdbe7c8a8.pdf"
    },
    {
      "success": true,
      "doc_id": "4207276026ec866a2bc704fc39caee4a",
      "summary": "Do current large language models (LLMs) better solve graph reasoning and generation tasks with parameter updates? In this paper, we propose InstructGraph, a framework that empowers LLMs with the abilities of graph reasoning and generation by instruction tuning and preference alignment. Specifically, we first propose a structured format verbalizer to unify all graph data into a universal code-like format, which can simply represent the graph without any external graph-specific encoders. Furthermore, a graph instruction tuning stage is introduced to guide LLMs in solving graph reasoning and generation tasks. Finally, we identify potential hallucination problems in graph tasks and sample negative instances for preference alignment, the target of which is to enhance the output's reliability of the model. Extensive experiments across multiple graph-centric tasks exhibit that InstructGraph can achieve the best performance and outperform GPT-4 and LLaMA2 by more than 13\\% and 38\\%, respectively.",
      "intriguing_abstract": "Do current large language models (LLMs) better solve graph reasoning and generation tasks with parameter updates? In this paper, we propose InstructGraph, a framework that empowers LLMs with the abilities of graph reasoning and generation by instruction tuning and preference alignment. Specifically, we first propose a structured format verbalizer to unify all graph data into a universal code-like format, which can simply represent the graph without any external graph-specific encoders. Furthermore, a graph instruction tuning stage is introduced to guide LLMs in solving graph reasoning and generation tasks. Finally, we identify potential hallucination problems in graph tasks and sample negative instances for preference alignment, the target of which is to enhance the output's reliability of the model. Extensive experiments across multiple graph-centric tasks exhibit that InstructGraph can achieve the best performance and outperform GPT-4 and LLaMA2 by more than 13\\% and 38\\%, respectively.",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/d299a6b26e9ee23d0337a1d1a896fc1c847f5a46.pdf",
      "citation_key": "wang2024sae",
      "metadata": {
        "title": "InstructGraph: Boosting Large Language Models via Graph-centric Instruction Tuning and Preference Alignment",
        "authors": [
          "Jianing Wang",
          "Junda Wu",
          "Yupeng Hou",
          "Yao Liu",
          "Ming Gao",
          "Julian McAuley"
        ],
        "published_date": "2024",
        "abstract": "Do current large language models (LLMs) better solve graph reasoning and generation tasks with parameter updates? In this paper, we propose InstructGraph, a framework that empowers LLMs with the abilities of graph reasoning and generation by instruction tuning and preference alignment. Specifically, we first propose a structured format verbalizer to unify all graph data into a universal code-like format, which can simply represent the graph without any external graph-specific encoders. Furthermore, a graph instruction tuning stage is introduced to guide LLMs in solving graph reasoning and generation tasks. Finally, we identify potential hallucination problems in graph tasks and sample negative instances for preference alignment, the target of which is to enhance the output's reliability of the model. Extensive experiments across multiple graph-centric tasks exhibit that InstructGraph can achieve the best performance and outperform GPT-4 and LLaMA2 by more than 13\\% and 38\\%, respectively.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/d299a6b26e9ee23d0337a1d1a896fc1c847f5a46.pdf",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "citationCount": 42,
        "score": 42.0,
        "summary": "Do current large language models (LLMs) better solve graph reasoning and generation tasks with parameter updates? In this paper, we propose InstructGraph, a framework that empowers LLMs with the abilities of graph reasoning and generation by instruction tuning and preference alignment. Specifically, we first propose a structured format verbalizer to unify all graph data into a universal code-like format, which can simply represent the graph without any external graph-specific encoders. Furthermore, a graph instruction tuning stage is introduced to guide LLMs in solving graph reasoning and generation tasks. Finally, we identify potential hallucination problems in graph tasks and sample negative instances for preference alignment, the target of which is to enhance the output's reliability of the model. Extensive experiments across multiple graph-centric tasks exhibit that InstructGraph can achieve the best performance and outperform GPT-4 and LLaMA2 by more than 13\\% and 38\\%, respectively.",
        "keywords": []
      },
      "file_name": "d299a6b26e9ee23d0337a1d1a896fc1c847f5a46.pdf"
    },
    {
      "success": true,
      "doc_id": "5f6ef7be1c9d722de7ccbb718d982280",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/ea0d41514a41f8273f13b3b277e7fcbbc65a8549.pdf",
      "citation_key": "huang20233v0",
      "metadata": {
        "title": "Look Before You Leap: An Exploratory Study of Uncertainty Measurement for Large Language Models",
        "authors": [
          "Yuheng Huang",
          "Jiayang Song",
          "Zhijie Wang",
          "Huaming Chen",
          "Lei Ma"
        ],
        "published_date": "2023",
        "abstract": "",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/ea0d41514a41f8273f13b3b277e7fcbbc65a8549.pdf",
        "venue": "arXiv.org",
        "citationCount": 83,
        "score": 41.5,
        "summary": "",
        "keywords": []
      },
      "file_name": "ea0d41514a41f8273f13b3b277e7fcbbc65a8549.pdf"
    },
    {
      "success": true,
      "doc_id": "6e283e9c729175c71f0215168d91d13e",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/a082c9c93b5cc0d38e7ac14c6c9dfe186bb5c824.pdf",
      "citation_key": "zhao2024s3a",
      "metadata": {
        "title": "Mitigating Object Hallucination in Large Vision-Language Models via Classifier-Free Guidance",
        "authors": [
          "Linxi Zhao",
          "Yihe Deng",
          "Weitong Zhang",
          "Quanquan Gu"
        ],
        "published_date": "2024",
        "abstract": "",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/a082c9c93b5cc0d38e7ac14c6c9dfe186bb5c824.pdf",
        "venue": "arXiv.org",
        "citationCount": 37,
        "score": 37.0,
        "summary": "",
        "keywords": []
      },
      "file_name": "a082c9c93b5cc0d38e7ac14c6c9dfe186bb5c824.pdf"
    },
    {
      "success": true,
      "doc_id": "b1aeaff9d36de5a6ce6dc79d552aba26",
      "summary": "Large language models (LLMs) are prone to hallucinations, i.e., nonsensical, unfaithful, and undesirable text. Users tend to overrely on LLMs and corresponding hallucinations which can lead to misinterpretations and errors. To tackle the problem of overreliance, we propose HILL, the \"Hallucination Identifier for Large Language Models\". First, we identified design features for HILL with a Wizard of Oz approach with nine participants. Subsequently, we implemented HILL based on the identified design features and evaluated HILLâ€™s interface design by surveying 17 participants. Further, we investigated HILLâ€™s functionality to identify hallucinations based on an existing question-answering dataset and five user interviews. We find that HILL can correctly identify and highlight hallucinations in LLM responses which enables users to handle LLM responses with more caution. With that, we propose an easy-to-implement adaptation to existing LLMs and demonstrate the relevance of user-centered designs of AI artifacts.",
      "intriguing_abstract": "Large language models (LLMs) are prone to hallucinations, i.e., nonsensical, unfaithful, and undesirable text. Users tend to overrely on LLMs and corresponding hallucinations which can lead to misinterpretations and errors. To tackle the problem of overreliance, we propose HILL, the \"Hallucination Identifier for Large Language Models\". First, we identified design features for HILL with a Wizard of Oz approach with nine participants. Subsequently, we implemented HILL based on the identified design features and evaluated HILLâ€™s interface design by surveying 17 participants. Further, we investigated HILLâ€™s functionality to identify hallucinations based on an existing question-answering dataset and five user interviews. We find that HILL can correctly identify and highlight hallucinations in LLM responses which enables users to handle LLM responses with more caution. With that, we propose an easy-to-implement adaptation to existing LLMs and demonstrate the relevance of user-centered designs of AI artifacts.",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/24b6b70e1b1525535155cc9fa66dfd9d5d42d6b5.pdf",
      "citation_key": "leiser2024kfo",
      "metadata": {
        "title": "HILL: A Hallucination Identifier for Large Language Models",
        "authors": [
          "Florian Leiser",
          "S. Eckhardt",
          "Valentin Leuthe",
          "Merlin Knaeble",
          "Alexander Maedche",
          "Gerhard Schwabe",
          "A. Sunyaev"
        ],
        "published_date": "2024",
        "abstract": "Large language models (LLMs) are prone to hallucinations, i.e., nonsensical, unfaithful, and undesirable text. Users tend to overrely on LLMs and corresponding hallucinations which can lead to misinterpretations and errors. To tackle the problem of overreliance, we propose HILL, the \"Hallucination Identifier for Large Language Models\". First, we identified design features for HILL with a Wizard of Oz approach with nine participants. Subsequently, we implemented HILL based on the identified design features and evaluated HILLâ€™s interface design by surveying 17 participants. Further, we investigated HILLâ€™s functionality to identify hallucinations based on an existing question-answering dataset and five user interviews. We find that HILL can correctly identify and highlight hallucinations in LLM responses which enables users to handle LLM responses with more caution. With that, we propose an easy-to-implement adaptation to existing LLMs and demonstrate the relevance of user-centered designs of AI artifacts.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/24b6b70e1b1525535155cc9fa66dfd9d5d42d6b5.pdf",
        "venue": "International Conference on Human Factors in Computing Systems",
        "citationCount": 35,
        "score": 35.0,
        "summary": "Large language models (LLMs) are prone to hallucinations, i.e., nonsensical, unfaithful, and undesirable text. Users tend to overrely on LLMs and corresponding hallucinations which can lead to misinterpretations and errors. To tackle the problem of overreliance, we propose HILL, the \"Hallucination Identifier for Large Language Models\". First, we identified design features for HILL with a Wizard of Oz approach with nine participants. Subsequently, we implemented HILL based on the identified design features and evaluated HILLâ€™s interface design by surveying 17 participants. Further, we investigated HILLâ€™s functionality to identify hallucinations based on an existing question-answering dataset and five user interviews. We find that HILL can correctly identify and highlight hallucinations in LLM responses which enables users to handle LLM responses with more caution. With that, we propose an easy-to-implement adaptation to existing LLMs and demonstrate the relevance of user-centered designs of AI artifacts.",
        "keywords": []
      },
      "file_name": "24b6b70e1b1525535155cc9fa66dfd9d5d42d6b5.pdf"
    },
    {
      "success": true,
      "doc_id": "756fd2ef7ab57b932c9fee361b79b3bb",
      "summary": "Large language models (LLMs) have demonstrated remarkable capabilities across a wide range of natural language processing tasks. However, their tendency to exhibit sycophantic behavior - excessively agreeing with or flattering users - poses significant risks to their reliability and ethical deployment. This paper provides a technical survey of sycophancy in LLMs, analyzing its causes, impacts, and potential mitigation strategies. We review recent work on measuring and quantifying sycophantic tendencies, examine the relationship between sycophancy and other challenges like hallucination and bias, and evaluate promising techniques for reducing sycophancy while maintaining model performance. Key approaches explored include improved training data, novel fine-tuning methods, post-deployment control mechanisms, and decoding strategies. We also discuss the broader implications of sycophancy for AI alignment and propose directions for future research. Our analysis suggests that mitigating sycophancy is crucial for developing more robust, reliable, and ethically-aligned language models.",
      "intriguing_abstract": "Large language models (LLMs) have demonstrated remarkable capabilities across a wide range of natural language processing tasks. However, their tendency to exhibit sycophantic behavior - excessively agreeing with or flattering users - poses significant risks to their reliability and ethical deployment. This paper provides a technical survey of sycophancy in LLMs, analyzing its causes, impacts, and potential mitigation strategies. We review recent work on measuring and quantifying sycophantic tendencies, examine the relationship between sycophancy and other challenges like hallucination and bias, and evaluate promising techniques for reducing sycophancy while maintaining model performance. Key approaches explored include improved training data, novel fine-tuning methods, post-deployment control mechanisms, and decoding strategies. We also discuss the broader implications of sycophancy for AI alignment and propose directions for future research. Our analysis suggests that mitigating sycophancy is crucial for developing more robust, reliable, and ethically-aligned language models.",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/6a9a9120d746a3c29902548bd1d93d6ea034c5d7.pdf",
      "citation_key": "malmqvist2024k7x",
      "metadata": {
        "title": "Sycophancy in Large Language Models: Causes and Mitigations",
        "authors": [
          "Lars Malmqvist"
        ],
        "published_date": "2024",
        "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities across a wide range of natural language processing tasks. However, their tendency to exhibit sycophantic behavior - excessively agreeing with or flattering users - poses significant risks to their reliability and ethical deployment. This paper provides a technical survey of sycophancy in LLMs, analyzing its causes, impacts, and potential mitigation strategies. We review recent work on measuring and quantifying sycophantic tendencies, examine the relationship between sycophancy and other challenges like hallucination and bias, and evaluate promising techniques for reducing sycophancy while maintaining model performance. Key approaches explored include improved training data, novel fine-tuning methods, post-deployment control mechanisms, and decoding strategies. We also discuss the broader implications of sycophancy for AI alignment and propose directions for future research. Our analysis suggests that mitigating sycophancy is crucial for developing more robust, reliable, and ethically-aligned language models.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/6a9a9120d746a3c29902548bd1d93d6ea034c5d7.pdf",
        "venue": "arXiv.org",
        "citationCount": 33,
        "score": 33.0,
        "summary": "Large language models (LLMs) have demonstrated remarkable capabilities across a wide range of natural language processing tasks. However, their tendency to exhibit sycophantic behavior - excessively agreeing with or flattering users - poses significant risks to their reliability and ethical deployment. This paper provides a technical survey of sycophancy in LLMs, analyzing its causes, impacts, and potential mitigation strategies. We review recent work on measuring and quantifying sycophantic tendencies, examine the relationship between sycophancy and other challenges like hallucination and bias, and evaluate promising techniques for reducing sycophancy while maintaining model performance. Key approaches explored include improved training data, novel fine-tuning methods, post-deployment control mechanisms, and decoding strategies. We also discuss the broader implications of sycophancy for AI alignment and propose directions for future research. Our analysis suggests that mitigating sycophancy is crucial for developing more robust, reliable, and ethically-aligned language models.",
        "keywords": []
      },
      "file_name": "6a9a9120d746a3c29902548bd1d93d6ea034c5d7.pdf"
    },
    {
      "success": true,
      "doc_id": "1fb4efa3486882ecde2c4b3cc1d6702b",
      "summary": "Alignment is a standard procedure to fine-tune pre-trained large language models (LLMs) to follow natural language instructions and serve as helpful AI assistants. We have observed, however, that the conventional alignment process fails to enhance the factual accuracy of LLMs, and often leads to the generation of more false facts (i.e. hallucination). In this paper, we study how to make the LLM alignment process more factual, by first identifying factors that lead to hallucination in both alignment steps:\\ supervised fine-tuning (SFT) and reinforcement learning (RL). In particular, we find that training the LLM on new knowledge or unfamiliar texts can encourage hallucination. This makes SFT less factual as it trains on human labeled data that may be novel to the LLM. Furthermore, reward functions used in standard RL can also encourage hallucination, because it guides the LLM to provide more helpful responses on a diverse set of instructions, often preferring longer and more detailed responses. Based on these observations, we propose factuality-aware alignment, comprised of factuality-aware SFT and factuality-aware RL through direct preference optimization. Experiments show that our proposed factuality-aware alignment guides LLMs to output more factual responses while maintaining instruction-following capability.",
      "intriguing_abstract": "Alignment is a standard procedure to fine-tune pre-trained large language models (LLMs) to follow natural language instructions and serve as helpful AI assistants. We have observed, however, that the conventional alignment process fails to enhance the factual accuracy of LLMs, and often leads to the generation of more false facts (i.e. hallucination). In this paper, we study how to make the LLM alignment process more factual, by first identifying factors that lead to hallucination in both alignment steps:\\ supervised fine-tuning (SFT) and reinforcement learning (RL). In particular, we find that training the LLM on new knowledge or unfamiliar texts can encourage hallucination. This makes SFT less factual as it trains on human labeled data that may be novel to the LLM. Furthermore, reward functions used in standard RL can also encourage hallucination, because it guides the LLM to provide more helpful responses on a diverse set of instructions, often preferring longer and more detailed responses. Based on these observations, we propose factuality-aware alignment, comprised of factuality-aware SFT and factuality-aware RL through direct preference optimization. Experiments show that our proposed factuality-aware alignment guides LLMs to output more factual responses while maintaining instruction-following capability.",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/95a52dd5adf6eb8d918cdfbf6189aab4eaa8e607.pdf",
      "citation_key": "lin2024gru",
      "metadata": {
        "title": "FLAME: Factuality-Aware Alignment for Large Language Models",
        "authors": [
          "Sheng-Chieh Lin",
          "Luyu Gao",
          "Barlas OÄŸuz",
          "Wenhan Xiong",
          "Jimmy Lin",
          "Wen-tau Yih",
          "Xilun Chen"
        ],
        "published_date": "2024",
        "abstract": "Alignment is a standard procedure to fine-tune pre-trained large language models (LLMs) to follow natural language instructions and serve as helpful AI assistants. We have observed, however, that the conventional alignment process fails to enhance the factual accuracy of LLMs, and often leads to the generation of more false facts (i.e. hallucination). In this paper, we study how to make the LLM alignment process more factual, by first identifying factors that lead to hallucination in both alignment steps:\\ supervised fine-tuning (SFT) and reinforcement learning (RL). In particular, we find that training the LLM on new knowledge or unfamiliar texts can encourage hallucination. This makes SFT less factual as it trains on human labeled data that may be novel to the LLM. Furthermore, reward functions used in standard RL can also encourage hallucination, because it guides the LLM to provide more helpful responses on a diverse set of instructions, often preferring longer and more detailed responses. Based on these observations, we propose factuality-aware alignment, comprised of factuality-aware SFT and factuality-aware RL through direct preference optimization. Experiments show that our proposed factuality-aware alignment guides LLMs to output more factual responses while maintaining instruction-following capability.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/95a52dd5adf6eb8d918cdfbf6189aab4eaa8e607.pdf",
        "venue": "Neural Information Processing Systems",
        "citationCount": 31,
        "score": 31.0,
        "summary": "Alignment is a standard procedure to fine-tune pre-trained large language models (LLMs) to follow natural language instructions and serve as helpful AI assistants. We have observed, however, that the conventional alignment process fails to enhance the factual accuracy of LLMs, and often leads to the generation of more false facts (i.e. hallucination). In this paper, we study how to make the LLM alignment process more factual, by first identifying factors that lead to hallucination in both alignment steps:\\ supervised fine-tuning (SFT) and reinforcement learning (RL). In particular, we find that training the LLM on new knowledge or unfamiliar texts can encourage hallucination. This makes SFT less factual as it trains on human labeled data that may be novel to the LLM. Furthermore, reward functions used in standard RL can also encourage hallucination, because it guides the LLM to provide more helpful responses on a diverse set of instructions, often preferring longer and more detailed responses. Based on these observations, we propose factuality-aware alignment, comprised of factuality-aware SFT and factuality-aware RL through direct preference optimization. Experiments show that our proposed factuality-aware alignment guides LLMs to output more factual responses while maintaining instruction-following capability.",
        "keywords": []
      },
      "file_name": "95a52dd5adf6eb8d918cdfbf6189aab4eaa8e607.pdf"
    },
    {
      "success": true,
      "doc_id": "69f55d9387c1e7cbaa39cea3fdc03418",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/80fd20e175f83a699258b8780cf365418d1538b0.pdf",
      "citation_key": "li2023dw0",
      "metadata": {
        "title": "Chain of Knowledge: A Framework for Grounding Large Language Models with Structured Knowledge Bases",
        "authors": [
          "Xingxuan Li",
          "Ruochen Zhao",
          "Yew Ken Chia",
          "Bosheng Ding",
          "Lidong Bing",
          "Shafiq R. Joty",
          "Soujanya Poria"
        ],
        "published_date": "2023",
        "abstract": "",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/80fd20e175f83a699258b8780cf365418d1538b0.pdf",
        "venue": "arXiv.org",
        "citationCount": 62,
        "score": 31.0,
        "summary": "",
        "keywords": []
      },
      "file_name": "80fd20e175f83a699258b8780cf365418d1538b0.pdf"
    },
    {
      "success": true,
      "doc_id": "c1747ef417eeff4901232aa85ba626de",
      "summary": "Recent advances in large video-language models have displayed promising outcomes in video comprehension. Current approaches straightforwardly convert video into language tokens and employ large language models for multi-modal tasks. However, this method often leads to the generation of irrelevant content, commonly known as â€œhallucinationâ€, as the length of the text increases and the impact of the video diminishes. To address this problem, we propose Vista-llama, a novel framework that maintains the consistent distance between all visual tokens and any language tokens, irrespective of the generated text length. Vista-llama omits relative position encoding when determining attention weights between visual and text tokens, retaining the position encoding for text and text tokens. This amplifies the effect of visual tokens on text generation, especially when the relative distance is longer between visual and text tokens. The proposed attention mechanism significantly reduces the chance of producing irrelevant text related to the video content. Furthermore, we present a sequential visual projector that projects the current video frame into tokens of language space with the assistance of the previous frame. This approach not only captures the temporal relationship within the video, but also allows less visual tokens to encompass the entire video. Our approach significantly outperforms various previous methods (e.g., Video-ChatGPT, MovieChat) on four challenging open-ended video question answering benchmarks. We reach an accuracy of 60.7 on the zero-shot NExT-QA and 60.5 on the zero-shot MSRVTT-QA, setting a new state-of-the-art performance. This project is available at https://jinxxian.github.iolVista-LLaMA.",
      "intriguing_abstract": "Recent advances in large video-language models have displayed promising outcomes in video comprehension. Current approaches straightforwardly convert video into language tokens and employ large language models for multi-modal tasks. However, this method often leads to the generation of irrelevant content, commonly known as â€œhallucinationâ€, as the length of the text increases and the impact of the video diminishes. To address this problem, we propose Vista-llama, a novel framework that maintains the consistent distance between all visual tokens and any language tokens, irrespective of the generated text length. Vista-llama omits relative position encoding when determining attention weights between visual and text tokens, retaining the position encoding for text and text tokens. This amplifies the effect of visual tokens on text generation, especially when the relative distance is longer between visual and text tokens. The proposed attention mechanism significantly reduces the chance of producing irrelevant text related to the video content. Furthermore, we present a sequential visual projector that projects the current video frame into tokens of language space with the assistance of the previous frame. This approach not only captures the temporal relationship within the video, but also allows less visual tokens to encompass the entire video. Our approach significantly outperforms various previous methods (e.g., Video-ChatGPT, MovieChat) on four challenging open-ended video question answering benchmarks. We reach an accuracy of 60.7 on the zero-shot NExT-QA and 60.5 on the zero-shot MSRVTT-QA, setting a new state-of-the-art performance. This project is available at https://jinxxian.github.iolVista-LLaMA.",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/81bd66d960503106ef969830568016da4f93754a.pdf",
      "citation_key": "ma2023mka",
      "metadata": {
        "title": "Vista-llama: Reducing Hallucination in Video Language Models via Equal Distance to Visual Tokens",
        "authors": [
          "Fan Ma",
          "Xiaojie Jin",
          "Heng Wang",
          "Yuchen Xian",
          "Jiashi Feng",
          "Yi Yang"
        ],
        "published_date": "2023",
        "abstract": "Recent advances in large video-language models have displayed promising outcomes in video comprehension. Current approaches straightforwardly convert video into language tokens and employ large language models for multi-modal tasks. However, this method often leads to the generation of irrelevant content, commonly known as â€œhallucinationâ€, as the length of the text increases and the impact of the video diminishes. To address this problem, we propose Vista-llama, a novel framework that maintains the consistent distance between all visual tokens and any language tokens, irrespective of the generated text length. Vista-llama omits relative position encoding when determining attention weights between visual and text tokens, retaining the position encoding for text and text tokens. This amplifies the effect of visual tokens on text generation, especially when the relative distance is longer between visual and text tokens. The proposed attention mechanism significantly reduces the chance of producing irrelevant text related to the video content. Furthermore, we present a sequential visual projector that projects the current video frame into tokens of language space with the assistance of the previous frame. This approach not only captures the temporal relationship within the video, but also allows less visual tokens to encompass the entire video. Our approach significantly outperforms various previous methods (e.g., Video-ChatGPT, MovieChat) on four challenging open-ended video question answering benchmarks. We reach an accuracy of 60.7 on the zero-shot NExT-QA and 60.5 on the zero-shot MSRVTT-QA, setting a new state-of-the-art performance. This project is available at https://jinxxian.github.iolVista-LLaMA.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/81bd66d960503106ef969830568016da4f93754a.pdf",
        "venue": "Computer Vision and Pattern Recognition",
        "citationCount": 62,
        "score": 31.0,
        "summary": "Recent advances in large video-language models have displayed promising outcomes in video comprehension. Current approaches straightforwardly convert video into language tokens and employ large language models for multi-modal tasks. However, this method often leads to the generation of irrelevant content, commonly known as â€œhallucinationâ€, as the length of the text increases and the impact of the video diminishes. To address this problem, we propose Vista-llama, a novel framework that maintains the consistent distance between all visual tokens and any language tokens, irrespective of the generated text length. Vista-llama omits relative position encoding when determining attention weights between visual and text tokens, retaining the position encoding for text and text tokens. This amplifies the effect of visual tokens on text generation, especially when the relative distance is longer between visual and text tokens. The proposed attention mechanism significantly reduces the chance of producing irrelevant text related to the video content. Furthermore, we present a sequential visual projector that projects the current video frame into tokens of language space with the assistance of the previous frame. This approach not only captures the temporal relationship within the video, but also allows less visual tokens to encompass the entire video. Our approach significantly outperforms various previous methods (e.g., Video-ChatGPT, MovieChat) on four challenging open-ended video question answering benchmarks. We reach an accuracy of 60.7 on the zero-shot NExT-QA and 60.5 on the zero-shot MSRVTT-QA, setting a new state-of-the-art performance. This project is available at https://jinxxian.github.iolVista-LLaMA.",
        "keywords": []
      },
      "file_name": "81bd66d960503106ef969830568016da4f93754a.pdf"
    },
    {
      "success": true,
      "doc_id": "5b83930ec1635311877533e89bda04c8",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/91ca6535fc8fb03efe0ecbe424ce5354ed129b0c.pdf",
      "citation_key": "song2024t8k",
      "metadata": {
        "title": "Towards Large Language Models as Copilots for Theorem Proving in Lean",
        "authors": [
          "Peiyang Song",
          "Kaiyu Yang",
          "Anima Anandkumar"
        ],
        "published_date": "2024",
        "abstract": "",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/91ca6535fc8fb03efe0ecbe424ce5354ed129b0c.pdf",
        "venue": "arXiv.org",
        "citationCount": 30,
        "score": 30.0,
        "summary": "",
        "keywords": []
      },
      "file_name": "91ca6535fc8fb03efe0ecbe424ce5354ed129b0c.pdf"
    },
    {
      "success": true,
      "doc_id": "ce2614557f6880bcea761af3f1c757a7",
      "summary": "Large language models are successful in answering factoid questions but are also prone to hallucination.We investigate the phenomenon of LLMs possessing correct answer knowledge yet still hallucinating from the perspective of inference dynamics, an area not previously covered in studies on hallucinations.We are able to conduct this analysis via two key ideas.First, we identify the factual questions that query the same triplet knowledge but result in different answers. The difference between the model behaviors on the correct and incorrect outputs hence suggests the patterns when hallucinations happen.Second, to measure the pattern, we utilize mappings from the residual streams to vocabulary space.We reveal the different dynamics of the output token probabilities along the depths of layers between the correct and hallucinated cases. In hallucinated cases, the output tokenâ€™s information rarely demonstrates abrupt increases and consistent superiority in the later stages of the model.Leveraging the dynamic curve as a feature, we build a classifier capable of accurately detecting hallucinatory predictions with an 88% success rate. Our study shed light on understanding the reasons for LLMsâ€™ hallucinations on their known facts, and more importantly, on accurately predicting when they are hallucinating.",
      "intriguing_abstract": "Large language models are successful in answering factoid questions but are also prone to hallucination.We investigate the phenomenon of LLMs possessing correct answer knowledge yet still hallucinating from the perspective of inference dynamics, an area not previously covered in studies on hallucinations.We are able to conduct this analysis via two key ideas.First, we identify the factual questions that query the same triplet knowledge but result in different answers. The difference between the model behaviors on the correct and incorrect outputs hence suggests the patterns when hallucinations happen.Second, to measure the pattern, we utilize mappings from the residual streams to vocabulary space.We reveal the different dynamics of the output token probabilities along the depths of layers between the correct and hallucinated cases. In hallucinated cases, the output tokenâ€™s information rarely demonstrates abrupt increases and consistent superiority in the later stages of the model.Leveraging the dynamic curve as a feature, we build a classifier capable of accurately detecting hallucinatory predictions with an 88% success rate. Our study shed light on understanding the reasons for LLMsâ€™ hallucinations on their known facts, and more importantly, on accurately predicting when they are hallucinating.",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/d48c56dbce88580736c037797666060cb3b03bf7.pdf",
      "citation_key": "jiang20242kz",
      "metadata": {
        "title": "On Large Language Modelsâ€™ Hallucination with Regard to Known Facts",
        "authors": [
          "Che Jiang",
          "Biqing Qi",
          "Xiangyu Hong",
          "Dayuan Fu",
          "Yang Cheng",
          "Fandong Meng",
          "Mo Yu",
          "Bowen Zhou",
          "Jie Zhou"
        ],
        "published_date": "2024",
        "abstract": "Large language models are successful in answering factoid questions but are also prone to hallucination.We investigate the phenomenon of LLMs possessing correct answer knowledge yet still hallucinating from the perspective of inference dynamics, an area not previously covered in studies on hallucinations.We are able to conduct this analysis via two key ideas.First, we identify the factual questions that query the same triplet knowledge but result in different answers. The difference between the model behaviors on the correct and incorrect outputs hence suggests the patterns when hallucinations happen.Second, to measure the pattern, we utilize mappings from the residual streams to vocabulary space.We reveal the different dynamics of the output token probabilities along the depths of layers between the correct and hallucinated cases. In hallucinated cases, the output tokenâ€™s information rarely demonstrates abrupt increases and consistent superiority in the later stages of the model.Leveraging the dynamic curve as a feature, we build a classifier capable of accurately detecting hallucinatory predictions with an 88% success rate. Our study shed light on understanding the reasons for LLMsâ€™ hallucinations on their known facts, and more importantly, on accurately predicting when they are hallucinating.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/d48c56dbce88580736c037797666060cb3b03bf7.pdf",
        "venue": "North American Chapter of the Association for Computational Linguistics",
        "citationCount": 29,
        "score": 29.0,
        "summary": "Large language models are successful in answering factoid questions but are also prone to hallucination.We investigate the phenomenon of LLMs possessing correct answer knowledge yet still hallucinating from the perspective of inference dynamics, an area not previously covered in studies on hallucinations.We are able to conduct this analysis via two key ideas.First, we identify the factual questions that query the same triplet knowledge but result in different answers. The difference between the model behaviors on the correct and incorrect outputs hence suggests the patterns when hallucinations happen.Second, to measure the pattern, we utilize mappings from the residual streams to vocabulary space.We reveal the different dynamics of the output token probabilities along the depths of layers between the correct and hallucinated cases. In hallucinated cases, the output tokenâ€™s information rarely demonstrates abrupt increases and consistent superiority in the later stages of the model.Leveraging the dynamic curve as a feature, we build a classifier capable of accurately detecting hallucinatory predictions with an 88% success rate. Our study shed light on understanding the reasons for LLMsâ€™ hallucinations on their known facts, and more importantly, on accurately predicting when they are hallucinating.",
        "keywords": []
      },
      "file_name": "d48c56dbce88580736c037797666060cb3b03bf7.pdf"
    },
    {
      "success": true,
      "doc_id": "83566338a892b536cf2d5e78a6482d92",
      "summary": "Neural theorem proving combines large language models (LLMs) with proof assistants such as Lean, where the correctness of formal proofs can be rigorously verified, leaving no room for hallucination. With existing neural theorem provers pretrained on a fixed collection of data and offering valuable suggestions at times, it is challenging for them to continually prove novel theorems in a fully autonomous mode, where human insights may be critical. In this paper, we explore LLMs as copilots that assist humans in proving theorems. We introduce Lean Copilot, a general framework for running LLM inference natively in Lean. It enables programmers to build various LLM-based proof automation tools that integrate seamlessly into the workflow of Lean users. Lean users can use our pretrained models or bring their own ones that run either locally (with or without GPUs) or on the cloud. Using Lean Copilot, we build LLM-based tools that suggest proof steps, complete proof goals, and select relevant premises. Experimental results on the Mathematics in Lean textbook demonstrate the effectiveness of our method compared to existing rule-based proof automation in Lean (aesop). When assisting humans, Lean Copilot requires only 2.08 manually-entered proof steps on average (3.86 required by aesop); when automating the theorem proving process, Lean Copilot automates 74.2% proof steps on average, 85% better than aesop (40.1%). We open source all code and artifacts under a permissive MIT license to facilitate further research.",
      "intriguing_abstract": "Neural theorem proving combines large language models (LLMs) with proof assistants such as Lean, where the correctness of formal proofs can be rigorously verified, leaving no room for hallucination. With existing neural theorem provers pretrained on a fixed collection of data and offering valuable suggestions at times, it is challenging for them to continually prove novel theorems in a fully autonomous mode, where human insights may be critical. In this paper, we explore LLMs as copilots that assist humans in proving theorems. We introduce Lean Copilot, a general framework for running LLM inference natively in Lean. It enables programmers to build various LLM-based proof automation tools that integrate seamlessly into the workflow of Lean users. Lean users can use our pretrained models or bring their own ones that run either locally (with or without GPUs) or on the cloud. Using Lean Copilot, we build LLM-based tools that suggest proof steps, complete proof goals, and select relevant premises. Experimental results on the Mathematics in Lean textbook demonstrate the effectiveness of our method compared to existing rule-based proof automation in Lean (aesop). When assisting humans, Lean Copilot requires only 2.08 manually-entered proof steps on average (3.86 required by aesop); when automating the theorem proving process, Lean Copilot automates 74.2% proof steps on average, 85% better than aesop (40.1%). We open source all code and artifacts under a permissive MIT license to facilitate further research.",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/668341051f3c9c087e42e393c610792df3e45992.pdf",
      "citation_key": "song2024br2",
      "metadata": {
        "title": "Lean Copilot: Large Language Models as Copilots for Theorem Proving in Lean",
        "authors": [
          "Peiyang Song",
          "Kaiyu Yang",
          "Anima Anandkumar"
        ],
        "published_date": "2024",
        "abstract": "Neural theorem proving combines large language models (LLMs) with proof assistants such as Lean, where the correctness of formal proofs can be rigorously verified, leaving no room for hallucination. With existing neural theorem provers pretrained on a fixed collection of data and offering valuable suggestions at times, it is challenging for them to continually prove novel theorems in a fully autonomous mode, where human insights may be critical. In this paper, we explore LLMs as copilots that assist humans in proving theorems. We introduce Lean Copilot, a general framework for running LLM inference natively in Lean. It enables programmers to build various LLM-based proof automation tools that integrate seamlessly into the workflow of Lean users. Lean users can use our pretrained models or bring their own ones that run either locally (with or without GPUs) or on the cloud. Using Lean Copilot, we build LLM-based tools that suggest proof steps, complete proof goals, and select relevant premises. Experimental results on the Mathematics in Lean textbook demonstrate the effectiveness of our method compared to existing rule-based proof automation in Lean (aesop). When assisting humans, Lean Copilot requires only 2.08 manually-entered proof steps on average (3.86 required by aesop); when automating the theorem proving process, Lean Copilot automates 74.2% proof steps on average, 85% better than aesop (40.1%). We open source all code and artifacts under a permissive MIT license to facilitate further research.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/668341051f3c9c087e42e393c610792df3e45992.pdf",
        "venue": "NeuS",
        "citationCount": 28,
        "score": 28.0,
        "summary": "Neural theorem proving combines large language models (LLMs) with proof assistants such as Lean, where the correctness of formal proofs can be rigorously verified, leaving no room for hallucination. With existing neural theorem provers pretrained on a fixed collection of data and offering valuable suggestions at times, it is challenging for them to continually prove novel theorems in a fully autonomous mode, where human insights may be critical. In this paper, we explore LLMs as copilots that assist humans in proving theorems. We introduce Lean Copilot, a general framework for running LLM inference natively in Lean. It enables programmers to build various LLM-based proof automation tools that integrate seamlessly into the workflow of Lean users. Lean users can use our pretrained models or bring their own ones that run either locally (with or without GPUs) or on the cloud. Using Lean Copilot, we build LLM-based tools that suggest proof steps, complete proof goals, and select relevant premises. Experimental results on the Mathematics in Lean textbook demonstrate the effectiveness of our method compared to existing rule-based proof automation in Lean (aesop). When assisting humans, Lean Copilot requires only 2.08 manually-entered proof steps on average (3.86 required by aesop); when automating the theorem proving process, Lean Copilot automates 74.2% proof steps on average, 85% better than aesop (40.1%). We open source all code and artifacts under a permissive MIT license to facilitate further research.",
        "keywords": []
      },
      "file_name": "668341051f3c9c087e42e393c610792df3e45992.pdf"
    },
    {
      "success": true,
      "doc_id": "69ab27b638f981a18704f35149dd9a53",
      "summary": "Large Language Models (LLMs) have shown unprecedented performance in various real-world applications. However, they are known to generate factually inaccurate outputs, a.k.a. the hallucination problem. In recent years, incorporating external knowledge extracted from Knowledge Graphs (KGs) has become a promising strategy to improve the factual accuracy of LLM-generated outputs. Nevertheless, most existing explorations rely on LLMs themselves to perform KG knowledge extraction, which is highly inflexible as LLMs can only provide binary judgment on whether a certain knowledge (e.g., a knowledge path in KG) should be used. In addition, LLMs tend to pick only knowledge with direct semantic relationship with the input text, while potentially useful knowledge with indirect semantics can be ignored. In this work, we propose a principled framework KELP with three stages to handle the above problems. Specifically, KELP is able to achieve finer granularity of flexible knowledge extraction by generating scores for knowledge paths with input texts via latent semantic matching. Meanwhile, knowledge paths with indirect semantic relationships with the input text can also be considered via trained encoding between the selected paths in KG and the input text. Experiments on real-world datasets validate the effectiveness of KELP.",
      "intriguing_abstract": "Large Language Models (LLMs) have shown unprecedented performance in various real-world applications. However, they are known to generate factually inaccurate outputs, a.k.a. the hallucination problem. In recent years, incorporating external knowledge extracted from Knowledge Graphs (KGs) has become a promising strategy to improve the factual accuracy of LLM-generated outputs. Nevertheless, most existing explorations rely on LLMs themselves to perform KG knowledge extraction, which is highly inflexible as LLMs can only provide binary judgment on whether a certain knowledge (e.g., a knowledge path in KG) should be used. In addition, LLMs tend to pick only knowledge with direct semantic relationship with the input text, while potentially useful knowledge with indirect semantics can be ignored. In this work, we propose a principled framework KELP with three stages to handle the above problems. Specifically, KELP is able to achieve finer granularity of flexible knowledge extraction by generating scores for knowledge paths with input texts via latent semantic matching. Meanwhile, knowledge paths with indirect semantic relationships with the input text can also be considered via trained encoding between the selected paths in KG and the input text. Experiments on real-world datasets validate the effectiveness of KELP.",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/3dea23e10eeff848f7352b17bbc1fdce38112acc.pdf",
      "citation_key": "liu2024ker",
      "metadata": {
        "title": "Knowledge Graph-Enhanced Large Language Models via Path Selection",
        "authors": [
          "Haochen Liu",
          "Song Wang",
          "Yaochen Zhu",
          "Yushun Dong",
          "Jundong Li"
        ],
        "published_date": "2024",
        "abstract": "Large Language Models (LLMs) have shown unprecedented performance in various real-world applications. However, they are known to generate factually inaccurate outputs, a.k.a. the hallucination problem. In recent years, incorporating external knowledge extracted from Knowledge Graphs (KGs) has become a promising strategy to improve the factual accuracy of LLM-generated outputs. Nevertheless, most existing explorations rely on LLMs themselves to perform KG knowledge extraction, which is highly inflexible as LLMs can only provide binary judgment on whether a certain knowledge (e.g., a knowledge path in KG) should be used. In addition, LLMs tend to pick only knowledge with direct semantic relationship with the input text, while potentially useful knowledge with indirect semantics can be ignored. In this work, we propose a principled framework KELP with three stages to handle the above problems. Specifically, KELP is able to achieve finer granularity of flexible knowledge extraction by generating scores for knowledge paths with input texts via latent semantic matching. Meanwhile, knowledge paths with indirect semantic relationships with the input text can also be considered via trained encoding between the selected paths in KG and the input text. Experiments on real-world datasets validate the effectiveness of KELP.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/3dea23e10eeff848f7352b17bbc1fdce38112acc.pdf",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "citationCount": 27,
        "score": 27.0,
        "summary": "Large Language Models (LLMs) have shown unprecedented performance in various real-world applications. However, they are known to generate factually inaccurate outputs, a.k.a. the hallucination problem. In recent years, incorporating external knowledge extracted from Knowledge Graphs (KGs) has become a promising strategy to improve the factual accuracy of LLM-generated outputs. Nevertheless, most existing explorations rely on LLMs themselves to perform KG knowledge extraction, which is highly inflexible as LLMs can only provide binary judgment on whether a certain knowledge (e.g., a knowledge path in KG) should be used. In addition, LLMs tend to pick only knowledge with direct semantic relationship with the input text, while potentially useful knowledge with indirect semantics can be ignored. In this work, we propose a principled framework KELP with three stages to handle the above problems. Specifically, KELP is able to achieve finer granularity of flexible knowledge extraction by generating scores for knowledge paths with input texts via latent semantic matching. Meanwhile, knowledge paths with indirect semantic relationships with the input text can also be considered via trained encoding between the selected paths in KG and the input text. Experiments on real-world datasets validate the effectiveness of KELP.",
        "keywords": []
      },
      "file_name": "3dea23e10eeff848f7352b17bbc1fdce38112acc.pdf"
    },
    {
      "success": true,
      "doc_id": "895d6536fb25879984cdf23f4de1a1ec",
      "summary": "Large language models (LLMs) are highly effective in various natural language processing (NLP) tasks. However, they are susceptible to producing unreliable conjectures in ambiguous contexts called hallucination. This paper presents a new method for evaluating LLM hallucination in Question Answering (QA) based on the unanswerable math word problem (MWP). To support this approach, we innovatively develop a dataset called Unanswerable Math Word Problem (UMWP) which comprises 5200 questions across five categories. We developed an evaluation methodology combining text similarity and mathematical expression detection to determine whether LLM considers the question unanswerable. The results of extensive experiments conducted on 31 LLMs, including GPT-3, InstructGPT, LLaMA, and Claude, demonstrate that in-context learning and reinforcement learning with human feedback (RLHF) training significantly enhance the modelâ€™s ability to avoid hallucination. We show that utilizing MWP is a reliable and effective approach to assess hallucination. Our code and data are available at https://github.com/Yuki-Asuuna/UMWP.",
      "intriguing_abstract": "Large language models (LLMs) are highly effective in various natural language processing (NLP) tasks. However, they are susceptible to producing unreliable conjectures in ambiguous contexts called hallucination. This paper presents a new method for evaluating LLM hallucination in Question Answering (QA) based on the unanswerable math word problem (MWP). To support this approach, we innovatively develop a dataset called Unanswerable Math Word Problem (UMWP) which comprises 5200 questions across five categories. We developed an evaluation methodology combining text similarity and mathematical expression detection to determine whether LLM considers the question unanswerable. The results of extensive experiments conducted on 31 LLMs, including GPT-3, InstructGPT, LLaMA, and Claude, demonstrate that in-context learning and reinforcement learning with human feedback (RLHF) training significantly enhance the modelâ€™s ability to avoid hallucination. We show that utilizing MWP is a reliable and effective approach to assess hallucination. Our code and data are available at https://github.com/Yuki-Asuuna/UMWP.",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/31968a970f14beab3cbadf9f6ad45c1a51f4ea95.pdf",
      "citation_key": "sun2024z6b",
      "metadata": {
        "title": "Benchmarking Hallucination in Large Language Models Based on Unanswerable Math Word Problem",
        "authors": [
          "Yuhong Sun",
          "Zhangyue Yin",
          "Qipeng Guo",
          "Jiawen Wu",
          "Xipeng Qiu",
          "Hui Zhao"
        ],
        "published_date": "2024",
        "abstract": "Large language models (LLMs) are highly effective in various natural language processing (NLP) tasks. However, they are susceptible to producing unreliable conjectures in ambiguous contexts called hallucination. This paper presents a new method for evaluating LLM hallucination in Question Answering (QA) based on the unanswerable math word problem (MWP). To support this approach, we innovatively develop a dataset called Unanswerable Math Word Problem (UMWP) which comprises 5200 questions across five categories. We developed an evaluation methodology combining text similarity and mathematical expression detection to determine whether LLM considers the question unanswerable. The results of extensive experiments conducted on 31 LLMs, including GPT-3, InstructGPT, LLaMA, and Claude, demonstrate that in-context learning and reinforcement learning with human feedback (RLHF) training significantly enhance the modelâ€™s ability to avoid hallucination. We show that utilizing MWP is a reliable and effective approach to assess hallucination. Our code and data are available at https://github.com/Yuki-Asuuna/UMWP.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/31968a970f14beab3cbadf9f6ad45c1a51f4ea95.pdf",
        "venue": "International Conference on Language Resources and Evaluation",
        "citationCount": 26,
        "score": 26.0,
        "summary": "Large language models (LLMs) are highly effective in various natural language processing (NLP) tasks. However, they are susceptible to producing unreliable conjectures in ambiguous contexts called hallucination. This paper presents a new method for evaluating LLM hallucination in Question Answering (QA) based on the unanswerable math word problem (MWP). To support this approach, we innovatively develop a dataset called Unanswerable Math Word Problem (UMWP) which comprises 5200 questions across five categories. We developed an evaluation methodology combining text similarity and mathematical expression detection to determine whether LLM considers the question unanswerable. The results of extensive experiments conducted on 31 LLMs, including GPT-3, InstructGPT, LLaMA, and Claude, demonstrate that in-context learning and reinforcement learning with human feedback (RLHF) training significantly enhance the modelâ€™s ability to avoid hallucination. We show that utilizing MWP is a reliable and effective approach to assess hallucination. Our code and data are available at https://github.com/Yuki-Asuuna/UMWP.",
        "keywords": []
      },
      "file_name": "31968a970f14beab3cbadf9f6ad45c1a51f4ea95.pdf"
    },
    {
      "success": true,
      "doc_id": "bf500d9a8001fff8acd6f95705db7f45",
      "summary": "In-context learning has emerged as a groundbreaking ability of Large Language Models (LLMs) and revolutionized various fields by providing a few task-relevant demonstrations in the prompt. However, trustworthy issues with LLMâ€™s response, such as hallucination, have also been actively discussed. Existing works have been devoted to quantifying the uncertainty in LLMâ€™s response, but they often overlook the complex nature of LLMs and the uniqueness of in-context learning. In this work, we delve into the predictive uncertainty of LLMs associated with in-context learning, highlighting that such uncertainties may stem from both the provided demonstrations (aleatoric uncertainty) and ambiguities tied to the modelâ€™s configurations (epistemic uncertainty). We propose a novel formulation and corresponding estimation method to quantify both types of uncertainties. The proposed method offers an unsupervised way to understand the prediction of in-context learning in a plug-and-play fashion. Extensive experiments are conducted to demonstrate the effectiveness of the decomposition. The code and data are available at: https://github.com/lingchen0331/UQ_ICL.",
      "intriguing_abstract": "In-context learning has emerged as a groundbreaking ability of Large Language Models (LLMs) and revolutionized various fields by providing a few task-relevant demonstrations in the prompt. However, trustworthy issues with LLMâ€™s response, such as hallucination, have also been actively discussed. Existing works have been devoted to quantifying the uncertainty in LLMâ€™s response, but they often overlook the complex nature of LLMs and the uniqueness of in-context learning. In this work, we delve into the predictive uncertainty of LLMs associated with in-context learning, highlighting that such uncertainties may stem from both the provided demonstrations (aleatoric uncertainty) and ambiguities tied to the modelâ€™s configurations (epistemic uncertainty). We propose a novel formulation and corresponding estimation method to quantify both types of uncertainties. The proposed method offers an unsupervised way to understand the prediction of in-context learning in a plug-and-play fashion. Extensive experiments are conducted to demonstrate the effectiveness of the decomposition. The code and data are available at: https://github.com/lingchen0331/UQ_ICL.",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/be8c90bca14d59f180f40a41126b7cd8c29c5d4e.pdf",
      "citation_key": "ling2024hqv",
      "metadata": {
        "title": "Uncertainty Quantification for In-Context Learning of Large Language Models",
        "authors": [
          "Chen Ling",
          "Xujiang Zhao",
          "Wei Cheng",
          "Yanchi Liu",
          "Yiyou Sun",
          "Xuchao Zhang",
          "Mika Oishi",
          "Takao Osaki",
          "Katsushi Matsuda",
          "Jie Ji",
          "Guangji Bai",
          "Liang Zhao",
          "Haifeng Chen"
        ],
        "published_date": "2024",
        "abstract": "In-context learning has emerged as a groundbreaking ability of Large Language Models (LLMs) and revolutionized various fields by providing a few task-relevant demonstrations in the prompt. However, trustworthy issues with LLMâ€™s response, such as hallucination, have also been actively discussed. Existing works have been devoted to quantifying the uncertainty in LLMâ€™s response, but they often overlook the complex nature of LLMs and the uniqueness of in-context learning. In this work, we delve into the predictive uncertainty of LLMs associated with in-context learning, highlighting that such uncertainties may stem from both the provided demonstrations (aleatoric uncertainty) and ambiguities tied to the modelâ€™s configurations (epistemic uncertainty). We propose a novel formulation and corresponding estimation method to quantify both types of uncertainties. The proposed method offers an unsupervised way to understand the prediction of in-context learning in a plug-and-play fashion. Extensive experiments are conducted to demonstrate the effectiveness of the decomposition. The code and data are available at: https://github.com/lingchen0331/UQ_ICL.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/be8c90bca14d59f180f40a41126b7cd8c29c5d4e.pdf",
        "venue": "North American Chapter of the Association for Computational Linguistics",
        "citationCount": 26,
        "score": 26.0,
        "summary": "In-context learning has emerged as a groundbreaking ability of Large Language Models (LLMs) and revolutionized various fields by providing a few task-relevant demonstrations in the prompt. However, trustworthy issues with LLMâ€™s response, such as hallucination, have also been actively discussed. Existing works have been devoted to quantifying the uncertainty in LLMâ€™s response, but they often overlook the complex nature of LLMs and the uniqueness of in-context learning. In this work, we delve into the predictive uncertainty of LLMs associated with in-context learning, highlighting that such uncertainties may stem from both the provided demonstrations (aleatoric uncertainty) and ambiguities tied to the modelâ€™s configurations (epistemic uncertainty). We propose a novel formulation and corresponding estimation method to quantify both types of uncertainties. The proposed method offers an unsupervised way to understand the prediction of in-context learning in a plug-and-play fashion. Extensive experiments are conducted to demonstrate the effectiveness of the decomposition. The code and data are available at: https://github.com/lingchen0331/UQ_ICL.",
        "keywords": []
      },
      "file_name": "be8c90bca14d59f180f40a41126b7cd8c29c5d4e.pdf"
    },
    {
      "success": true,
      "doc_id": "987efae5ff996dd027cc0ce28312176e",
      "summary": "Self-detection for Large Language Models (LLMs) seeks to evaluate the trustworthiness of the LLM's output by leveraging its own capabilities, thereby alleviating the issue of output hallucination. However, existing self-detection approaches only retrospectively evaluate answers generated by LLM, typically leading to the over-trust in incorrectly generated answers. To tackle this limitation, we propose a novel self-detection paradigm that considers the comprehensive answer space beyond LLM-generated answers. It thoroughly compares the trustworthiness of multiple candidate answers to mitigate the over-trust in LLM-generated incorrect answers. Building upon this paradigm, we introduce a two-step framework, which firstly instructs LLM to reflect and provide justifications for each candidate answer, and then aggregates the justifications for comprehensive target answer evaluation. This framework can be seamlessly integrated with existing approaches for superior self-detection. Extensive experiments on six datasets spanning three tasks demonstrate the effectiveness of the proposed framework.",
      "intriguing_abstract": "Self-detection for Large Language Models (LLMs) seeks to evaluate the trustworthiness of the LLM's output by leveraging its own capabilities, thereby alleviating the issue of output hallucination. However, existing self-detection approaches only retrospectively evaluate answers generated by LLM, typically leading to the over-trust in incorrectly generated answers. To tackle this limitation, we propose a novel self-detection paradigm that considers the comprehensive answer space beyond LLM-generated answers. It thoroughly compares the trustworthiness of multiple candidate answers to mitigate the over-trust in LLM-generated incorrect answers. Building upon this paradigm, we introduce a two-step framework, which firstly instructs LLM to reflect and provide justifications for each candidate answer, and then aggregates the justifications for comprehensive target answer evaluation. This framework can be seamlessly integrated with existing approaches for superior self-detection. Extensive experiments on six datasets spanning three tasks demonstrate the effectiveness of the proposed framework.",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/aa1fbd6e8d1c8e99b6ca34c17bcdb36e987b68a6.pdf",
      "citation_key": "li2024jbb",
      "metadata": {
        "title": "Think Twice Before Trusting: Self-Detection for Large Language Models through Comprehensive Answer Reflection",
        "authors": [
          "Moxin Li",
          "Wenjie Wang",
          "Fuli Feng",
          "Fengbin Zhu",
          "Qifan Wang",
          "Tat-Seng Chua"
        ],
        "published_date": "2024",
        "abstract": "Self-detection for Large Language Models (LLMs) seeks to evaluate the trustworthiness of the LLM's output by leveraging its own capabilities, thereby alleviating the issue of output hallucination. However, existing self-detection approaches only retrospectively evaluate answers generated by LLM, typically leading to the over-trust in incorrectly generated answers. To tackle this limitation, we propose a novel self-detection paradigm that considers the comprehensive answer space beyond LLM-generated answers. It thoroughly compares the trustworthiness of multiple candidate answers to mitigate the over-trust in LLM-generated incorrect answers. Building upon this paradigm, we introduce a two-step framework, which firstly instructs LLM to reflect and provide justifications for each candidate answer, and then aggregates the justifications for comprehensive target answer evaluation. This framework can be seamlessly integrated with existing approaches for superior self-detection. Extensive experiments on six datasets spanning three tasks demonstrate the effectiveness of the proposed framework.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/aa1fbd6e8d1c8e99b6ca34c17bcdb36e987b68a6.pdf",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "citationCount": 25,
        "score": 25.0,
        "summary": "Self-detection for Large Language Models (LLMs) seeks to evaluate the trustworthiness of the LLM's output by leveraging its own capabilities, thereby alleviating the issue of output hallucination. However, existing self-detection approaches only retrospectively evaluate answers generated by LLM, typically leading to the over-trust in incorrectly generated answers. To tackle this limitation, we propose a novel self-detection paradigm that considers the comprehensive answer space beyond LLM-generated answers. It thoroughly compares the trustworthiness of multiple candidate answers to mitigate the over-trust in LLM-generated incorrect answers. Building upon this paradigm, we introduce a two-step framework, which firstly instructs LLM to reflect and provide justifications for each candidate answer, and then aggregates the justifications for comprehensive target answer evaluation. This framework can be seamlessly integrated with existing approaches for superior self-detection. Extensive experiments on six datasets spanning three tasks demonstrate the effectiveness of the proposed framework.",
        "keywords": []
      },
      "file_name": "aa1fbd6e8d1c8e99b6ca34c17bcdb36e987b68a6.pdf"
    },
    {
      "success": true,
      "doc_id": "ecfede0c0c081ffeaa4c328772176e2b",
      "summary": "Language Models (LLMs) and their capabilities become an increasingly prominent aspect of our workflows and our lives",
      "intriguing_abstract": "Language Models (LLMs) and their capabilities become an increasingly prominent aspect of our workflows and our lives",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/694b753385820ea675f2ca80dcdb4c91fc05962a.pdf",
      "citation_key": "smith2023i8w",
      "metadata": {
        "title": "Hallucination or Confabulation? Neuroanatomy as metaphor in Large Language Models",
        "authors": [
          "Andrew L Smith",
          "Felix Greaves",
          "T. Panch"
        ],
        "published_date": "2023",
        "abstract": "Language Models (LLMs) and their capabilities become an increasingly prominent aspect of our workflows and our lives",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/694b753385820ea675f2ca80dcdb4c91fc05962a.pdf",
        "venue": "PLOS Digital Health",
        "citationCount": 49,
        "score": 24.5,
        "summary": "Language Models (LLMs) and their capabilities become an increasingly prominent aspect of our workflows and our lives",
        "keywords": []
      },
      "file_name": "694b753385820ea675f2ca80dcdb4c91fc05962a.pdf"
    },
    {
      "success": true,
      "doc_id": "aa1e7624118ab256fffaf7c9f611f8f4",
      "summary": "Researchers have been exploring advanced artificial intelligence (AI) techniques such as large language models (LLMs) to enhance data extraction from clinical notes in electronic medical records (EMRs), a critical need given the vast amounts of unstructured data in clinical notes. Evolving LLMs, their handling of details in prompting, and hallucinations in replication warrant a comparison with conventional approaches to understand the tradeoff between efficiency and accuracy. One such study by Burford et al 1 investigates the utility and efficacy of an LLM, ChatGPT-4 (OpenAI), in extracting helmet status from clinical narratives of patients involved in micromobility-related accidents. 1 This study by Burford et al 1 leverages data from the US Consumer Product Safety Commission (CPSC) National Electronic Injury Surveillance System (NEISS) spanning 2019 to 2022, including 54729 emergency department (ED) visits among patients with a micromobility accident. The primary objective was to compare the LLMâ€™s performance with the text stringâ€“search approach in identifying whether patients were wearing helmets at the time of their accidents. Three different levels of prompting detail for the LLM (low, intermediate, and high) were employed, and the agreementwiththetextstringâ€“searchapproachwasmeasuredusingCohenÎºteststatistics.Thetest-retestreliabilityofthehigh-detailpromptwasmeasuredacrossnewchatsessionson5differentdays using Fleiss Îº statistics. Performance statistics were calculated for a criterion standard review in a small random sample of 400 records comparing results from the high-detail prompt and text stringâ€“ search approach to classifications of helmet status generated by researchers reading the clinical notes. Burford and colleagues 1 found moderate agreement (Cohen Îº = 0.74 [95% CI, 0.73-0.75]) for the low-detail prompt and weak agreement (Cohen Îº = 0.53 [95% CI, 0.52-0.54]) for the intermediate-detail prompt compared with the text stringâ€“search approach. The high-detail prompt, which included comprehensive researcher-generated",
      "intriguing_abstract": "Researchers have been exploring advanced artificial intelligence (AI) techniques such as large language models (LLMs) to enhance data extraction from clinical notes in electronic medical records (EMRs), a critical need given the vast amounts of unstructured data in clinical notes. Evolving LLMs, their handling of details in prompting, and hallucinations in replication warrant a comparison with conventional approaches to understand the tradeoff between efficiency and accuracy. One such study by Burford et al 1 investigates the utility and efficacy of an LLM, ChatGPT-4 (OpenAI), in extracting helmet status from clinical narratives of patients involved in micromobility-related accidents. 1 This study by Burford et al 1 leverages data from the US Consumer Product Safety Commission (CPSC) National Electronic Injury Surveillance System (NEISS) spanning 2019 to 2022, including 54729 emergency department (ED) visits among patients with a micromobility accident. The primary objective was to compare the LLMâ€™s performance with the text stringâ€“search approach in identifying whether patients were wearing helmets at the time of their accidents. Three different levels of prompting detail for the LLM (low, intermediate, and high) were employed, and the agreementwiththetextstringâ€“searchapproachwasmeasuredusingCohenÎºteststatistics.Thetest-retestreliabilityofthehigh-detailpromptwasmeasuredacrossnewchatsessionson5differentdays using Fleiss Îº statistics. Performance statistics were calculated for a criterion standard review in a small random sample of 400 records comparing results from the high-detail prompt and text stringâ€“ search approach to classifications of helmet status generated by researchers reading the clinical notes. Burford and colleagues 1 found moderate agreement (Cohen Îº = 0.74 [95% CI, 0.73-0.75]) for the low-detail prompt and weak agreement (Cohen Îº = 0.53 [95% CI, 0.52-0.54]) for the intermediate-detail prompt compared with the text stringâ€“search approach. The high-detail prompt, which included comprehensive researcher-generated",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/6a2e0927914ef03f25b99c2666f2275e0c950e5d.pdf",
      "citation_key": "shah20242sx",
      "metadata": {
        "title": "Accuracy, Consistency, and Hallucination of Large Language Models When Analyzing Unstructured Clinical Notes in Electronic Medical Records.",
        "authors": [
          "Savyasachi V. Shah"
        ],
        "published_date": "2024",
        "abstract": "Researchers have been exploring advanced artificial intelligence (AI) techniques such as large language models (LLMs) to enhance data extraction from clinical notes in electronic medical records (EMRs), a critical need given the vast amounts of unstructured data in clinical notes. Evolving LLMs, their handling of details in prompting, and hallucinations in replication warrant a comparison with conventional approaches to understand the tradeoff between efficiency and accuracy. One such study by Burford et al 1 investigates the utility and efficacy of an LLM, ChatGPT-4 (OpenAI), in extracting helmet status from clinical narratives of patients involved in micromobility-related accidents. 1 This study by Burford et al 1 leverages data from the US Consumer Product Safety Commission (CPSC) National Electronic Injury Surveillance System (NEISS) spanning 2019 to 2022, including 54729 emergency department (ED) visits among patients with a micromobility accident. The primary objective was to compare the LLMâ€™s performance with the text stringâ€“search approach in identifying whether patients were wearing helmets at the time of their accidents. Three different levels of prompting detail for the LLM (low, intermediate, and high) were employed, and the agreementwiththetextstringâ€“searchapproachwasmeasuredusingCohenÎºteststatistics.Thetest-retestreliabilityofthehigh-detailpromptwasmeasuredacrossnewchatsessionson5differentdays using Fleiss Îº statistics. Performance statistics were calculated for a criterion standard review in a small random sample of 400 records comparing results from the high-detail prompt and text stringâ€“ search approach to classifications of helmet status generated by researchers reading the clinical notes. Burford and colleagues 1 found moderate agreement (Cohen Îº = 0.74 [95% CI, 0.73-0.75]) for the low-detail prompt and weak agreement (Cohen Îº = 0.53 [95% CI, 0.52-0.54]) for the intermediate-detail prompt compared with the text stringâ€“search approach. The high-detail prompt, which included comprehensive researcher-generated",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/6a2e0927914ef03f25b99c2666f2275e0c950e5d.pdf",
        "venue": "JAMA Network Open",
        "citationCount": 24,
        "score": 24.0,
        "summary": "Researchers have been exploring advanced artificial intelligence (AI) techniques such as large language models (LLMs) to enhance data extraction from clinical notes in electronic medical records (EMRs), a critical need given the vast amounts of unstructured data in clinical notes. Evolving LLMs, their handling of details in prompting, and hallucinations in replication warrant a comparison with conventional approaches to understand the tradeoff between efficiency and accuracy. One such study by Burford et al 1 investigates the utility and efficacy of an LLM, ChatGPT-4 (OpenAI), in extracting helmet status from clinical narratives of patients involved in micromobility-related accidents. 1 This study by Burford et al 1 leverages data from the US Consumer Product Safety Commission (CPSC) National Electronic Injury Surveillance System (NEISS) spanning 2019 to 2022, including 54729 emergency department (ED) visits among patients with a micromobility accident. The primary objective was to compare the LLMâ€™s performance with the text stringâ€“search approach in identifying whether patients were wearing helmets at the time of their accidents. Three different levels of prompting detail for the LLM (low, intermediate, and high) were employed, and the agreementwiththetextstringâ€“searchapproachwasmeasuredusingCohenÎºteststatistics.Thetest-retestreliabilityofthehigh-detailpromptwasmeasuredacrossnewchatsessionson5differentdays using Fleiss Îº statistics. Performance statistics were calculated for a criterion standard review in a small random sample of 400 records comparing results from the high-detail prompt and text stringâ€“ search approach to classifications of helmet status generated by researchers reading the clinical notes. Burford and colleagues 1 found moderate agreement (Cohen Îº = 0.74 [95% CI, 0.73-0.75]) for the low-detail prompt and weak agreement (Cohen Îº = 0.53 [95% CI, 0.52-0.54]) for the intermediate-detail prompt compared with the text stringâ€“search approach. The high-detail prompt, which included comprehensive researcher-generated",
        "keywords": []
      },
      "file_name": "6a2e0927914ef03f25b99c2666f2275e0c950e5d.pdf"
    },
    {
      "success": true,
      "doc_id": "df98226c717e6db7b234db254355798f",
      "summary": "We present a Chain-of-Action (CoA) framework for multimodal and retrieval-augmented Question-Answering (QA). Compared to the literature, CoA overcomes two major challenges of current QA applications: (i) unfaithful hallucination that is inconsistent with real-time or domain facts and (ii) weak reasoning performance over compositional information. Our key contribution is a novel reasoning-retrieval mechanism that decomposes a complex question into a reasoning chain via systematic prompting and pre-designed actions. Methodologically, we propose three types of domain-adaptable `Plug-and-Play' actions for retrieving real-time information from heterogeneous sources. We also propose a multi-reference faith score (MRFS) to verify and resolve conflicts in the answers. Empirically, we exploit both public benchmarks and a Web3 case study to demonstrate the capability of CoA over other methods.",
      "intriguing_abstract": "We present a Chain-of-Action (CoA) framework for multimodal and retrieval-augmented Question-Answering (QA). Compared to the literature, CoA overcomes two major challenges of current QA applications: (i) unfaithful hallucination that is inconsistent with real-time or domain facts and (ii) weak reasoning performance over compositional information. Our key contribution is a novel reasoning-retrieval mechanism that decomposes a complex question into a reasoning chain via systematic prompting and pre-designed actions. Methodologically, we propose three types of domain-adaptable `Plug-and-Play' actions for retrieving real-time information from heterogeneous sources. We also propose a multi-reference faith score (MRFS) to verify and resolve conflicts in the answers. Empirically, we exploit both public benchmarks and a Web3 case study to demonstrate the capability of CoA over other methods.",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/f19e6c955b05d61aeb1cbc7580dc3723d31398ea.pdf",
      "citation_key": "pan2024hm4",
      "metadata": {
        "title": "Chain-of-Action: Faithful and Multimodal Question Answering through Large Language Models",
        "authors": [
          "Zhenyu Pan",
          "Haozheng Luo",
          "Manling Li",
          "Han Liu"
        ],
        "published_date": "2024",
        "abstract": "We present a Chain-of-Action (CoA) framework for multimodal and retrieval-augmented Question-Answering (QA). Compared to the literature, CoA overcomes two major challenges of current QA applications: (i) unfaithful hallucination that is inconsistent with real-time or domain facts and (ii) weak reasoning performance over compositional information. Our key contribution is a novel reasoning-retrieval mechanism that decomposes a complex question into a reasoning chain via systematic prompting and pre-designed actions. Methodologically, we propose three types of domain-adaptable `Plug-and-Play' actions for retrieving real-time information from heterogeneous sources. We also propose a multi-reference faith score (MRFS) to verify and resolve conflicts in the answers. Empirically, we exploit both public benchmarks and a Web3 case study to demonstrate the capability of CoA over other methods.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/f19e6c955b05d61aeb1cbc7580dc3723d31398ea.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 23,
        "score": 23.0,
        "summary": "We present a Chain-of-Action (CoA) framework for multimodal and retrieval-augmented Question-Answering (QA). Compared to the literature, CoA overcomes two major challenges of current QA applications: (i) unfaithful hallucination that is inconsistent with real-time or domain facts and (ii) weak reasoning performance over compositional information. Our key contribution is a novel reasoning-retrieval mechanism that decomposes a complex question into a reasoning chain via systematic prompting and pre-designed actions. Methodologically, we propose three types of domain-adaptable `Plug-and-Play' actions for retrieving real-time information from heterogeneous sources. We also propose a multi-reference faith score (MRFS) to verify and resolve conflicts in the answers. Empirically, we exploit both public benchmarks and a Web3 case study to demonstrate the capability of CoA over other methods.",
        "keywords": []
      },
      "file_name": "f19e6c955b05d61aeb1cbc7580dc3723d31398ea.pdf"
    },
    {
      "success": true,
      "doc_id": "2a39632b40e6443959ec21611a2f037e",
      "summary": "Retrieval-augmented generation (RAG) is considered to be a promising approach to alleviate the hallucination issue of large language models (LLMs), and it has received widespread attention from researchers recently. Due to the limitation in the semantic understanding of retrieval models, the success of RAG heavily lies on the ability of LLMs to identify passages with utility. Recent efforts have explored the ability of LLMs to assess the relevance of passages in retrieval, but there has been limited work on evaluating the utility of passages in supporting question answering. In this work, we conduct a comprehensive study about the capabilities of LLMs in utility evaluation for open-domain question answering (QA). Specifically, we introduce a benchmarking procedure and collection of candidate passages with different characteristics, facilitating a series of experiments with five representative LLMs. Our experiments reveal that: (i) well-instructed LLMs can distinguish between relevance and utility, and that LLMs are highly receptive to newly generated counterfactual passages. Moreover, (ii) we scrutinize key factors that affect utility judgments in the instruction design. And finally, (iii) to verify the efficacy of utility judgments in practical retrieval augmentation applications, we delve into LLMs' QA capabilities using the evidence judged with utility and direct dense retrieval results. (iv) We propose a k-sampling, listwise approach to reduce the dependency of LLMs on the sequence of input passages, thereby facilitating subsequent answer generation. We believe that the way we formalize and study the problem along with our findings contributes to a critical assessment of retrieval-augmented LLMs. Our code and benchmark can be found at https://github.com/ict-bigdatalab/utility_judgments.",
      "intriguing_abstract": "Retrieval-augmented generation (RAG) is considered to be a promising approach to alleviate the hallucination issue of large language models (LLMs), and it has received widespread attention from researchers recently. Due to the limitation in the semantic understanding of retrieval models, the success of RAG heavily lies on the ability of LLMs to identify passages with utility. Recent efforts have explored the ability of LLMs to assess the relevance of passages in retrieval, but there has been limited work on evaluating the utility of passages in supporting question answering. In this work, we conduct a comprehensive study about the capabilities of LLMs in utility evaluation for open-domain question answering (QA). Specifically, we introduce a benchmarking procedure and collection of candidate passages with different characteristics, facilitating a series of experiments with five representative LLMs. Our experiments reveal that: (i) well-instructed LLMs can distinguish between relevance and utility, and that LLMs are highly receptive to newly generated counterfactual passages. Moreover, (ii) we scrutinize key factors that affect utility judgments in the instruction design. And finally, (iii) to verify the efficacy of utility judgments in practical retrieval augmentation applications, we delve into LLMs' QA capabilities using the evidence judged with utility and direct dense retrieval results. (iv) We propose a k-sampling, listwise approach to reduce the dependency of LLMs on the sequence of input passages, thereby facilitating subsequent answer generation. We believe that the way we formalize and study the problem along with our findings contributes to a critical assessment of retrieval-augmented LLMs. Our code and benchmark can be found at https://github.com/ict-bigdatalab/utility_judgments.",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/3c2130e219528df234658b94810de33fe7b077dc.pdf",
      "citation_key": "zhang2024o58",
      "metadata": {
        "title": "Are Large Language Models Good at Utility Judgments?",
        "authors": [
          "Hengran Zhang",
          "Ruqing Zhang",
          "J. Guo",
          "M. D. Rijke",
          "Yixing Fan",
          "Xueqi Cheng"
        ],
        "published_date": "2024",
        "abstract": "Retrieval-augmented generation (RAG) is considered to be a promising approach to alleviate the hallucination issue of large language models (LLMs), and it has received widespread attention from researchers recently. Due to the limitation in the semantic understanding of retrieval models, the success of RAG heavily lies on the ability of LLMs to identify passages with utility. Recent efforts have explored the ability of LLMs to assess the relevance of passages in retrieval, but there has been limited work on evaluating the utility of passages in supporting question answering. In this work, we conduct a comprehensive study about the capabilities of LLMs in utility evaluation for open-domain question answering (QA). Specifically, we introduce a benchmarking procedure and collection of candidate passages with different characteristics, facilitating a series of experiments with five representative LLMs. Our experiments reveal that: (i) well-instructed LLMs can distinguish between relevance and utility, and that LLMs are highly receptive to newly generated counterfactual passages. Moreover, (ii) we scrutinize key factors that affect utility judgments in the instruction design. And finally, (iii) to verify the efficacy of utility judgments in practical retrieval augmentation applications, we delve into LLMs' QA capabilities using the evidence judged with utility and direct dense retrieval results. (iv) We propose a k-sampling, listwise approach to reduce the dependency of LLMs on the sequence of input passages, thereby facilitating subsequent answer generation. We believe that the way we formalize and study the problem along with our findings contributes to a critical assessment of retrieval-augmented LLMs. Our code and benchmark can be found at https://github.com/ict-bigdatalab/utility_judgments.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/3c2130e219528df234658b94810de33fe7b077dc.pdf",
        "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
        "citationCount": 20,
        "score": 20.0,
        "summary": "Retrieval-augmented generation (RAG) is considered to be a promising approach to alleviate the hallucination issue of large language models (LLMs), and it has received widespread attention from researchers recently. Due to the limitation in the semantic understanding of retrieval models, the success of RAG heavily lies on the ability of LLMs to identify passages with utility. Recent efforts have explored the ability of LLMs to assess the relevance of passages in retrieval, but there has been limited work on evaluating the utility of passages in supporting question answering. In this work, we conduct a comprehensive study about the capabilities of LLMs in utility evaluation for open-domain question answering (QA). Specifically, we introduce a benchmarking procedure and collection of candidate passages with different characteristics, facilitating a series of experiments with five representative LLMs. Our experiments reveal that: (i) well-instructed LLMs can distinguish between relevance and utility, and that LLMs are highly receptive to newly generated counterfactual passages. Moreover, (ii) we scrutinize key factors that affect utility judgments in the instruction design. And finally, (iii) to verify the efficacy of utility judgments in practical retrieval augmentation applications, we delve into LLMs' QA capabilities using the evidence judged with utility and direct dense retrieval results. (iv) We propose a k-sampling, listwise approach to reduce the dependency of LLMs on the sequence of input passages, thereby facilitating subsequent answer generation. We believe that the way we formalize and study the problem along with our findings contributes to a critical assessment of retrieval-augmented LLMs. Our code and benchmark can be found at https://github.com/ict-bigdatalab/utility_judgments.",
        "keywords": []
      },
      "file_name": "3c2130e219528df234658b94810de33fe7b077dc.pdf"
    },
    {
      "success": true,
      "doc_id": "085593aac870f37ed174e7ed73961b8b",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/37f6249b8381c955d732755714c1e700ba62b988.pdf",
      "citation_key": "tang2024cxa",
      "metadata": {
        "title": "GraphArena: Benchmarking Large Language Models on Graph Computational Problems",
        "authors": [
          "Jianheng Tang",
          "Qifan Zhang",
          "Yuhan Li",
          "Jia Li"
        ],
        "published_date": "2024",
        "abstract": "",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/37f6249b8381c955d732755714c1e700ba62b988.pdf",
        "venue": "arXiv.org",
        "citationCount": 20,
        "score": 20.0,
        "summary": "",
        "keywords": []
      },
      "file_name": "37f6249b8381c955d732755714c1e700ba62b988.pdf"
    },
    {
      "success": true,
      "doc_id": "aa0e4c836fb8a3a064b3d1f837dd988e",
      "summary": "Multimodal large language models (MLLMs) fine-tuned with multimodal instruction datasets have demonstrated remarkable capabilities in multimodal tasks. However, fine-tuning all parameters of MLLMs has become challenging as they usually contain billions of parameters. To address this issue, we study parameter-efficient fine-tuning (PEFT) methods for MLLMs. We aim to identify effective methods for enhancing the performance of MLLMs in scenarios where only a limited number of parameters are trained. This paper conducts empirical studies using four popular PEFT methods to fine-tune the LLM component of open-source MLLMs. We present a comprehensive analysis that encompasses various aspects, including the impact of PEFT methods on various models, parameters and location of the PEFT module, size of fine-tuning data, model stability based on PEFT methods, MLLM's generalization, and hallucination. We evaluated four PEFT methods on seven datasets from two different categories: unseen and seen datasets. Across all experiments, we show that the adapter is the best-performing PEFT method. At the same time, fine-tuning the connector layers leads to improved performance in most MLLMs. Code and data are available at https://github.com/alenai97/PEFT-MLLM.git.",
      "intriguing_abstract": "Multimodal large language models (MLLMs) fine-tuned with multimodal instruction datasets have demonstrated remarkable capabilities in multimodal tasks. However, fine-tuning all parameters of MLLMs has become challenging as they usually contain billions of parameters. To address this issue, we study parameter-efficient fine-tuning (PEFT) methods for MLLMs. We aim to identify effective methods for enhancing the performance of MLLMs in scenarios where only a limited number of parameters are trained. This paper conducts empirical studies using four popular PEFT methods to fine-tune the LLM component of open-source MLLMs. We present a comprehensive analysis that encompasses various aspects, including the impact of PEFT methods on various models, parameters and location of the PEFT module, size of fine-tuning data, model stability based on PEFT methods, MLLM's generalization, and hallucination. We evaluated four PEFT methods on seven datasets from two different categories: unseen and seen datasets. Across all experiments, we show that the adapter is the best-performing PEFT method. At the same time, fine-tuning the connector layers leads to improved performance in most MLLMs. Code and data are available at https://github.com/alenai97/PEFT-MLLM.git.",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/f123b838e000e11f08cb0d7c63e01934b38d3092.pdf",
      "citation_key": "zhou2024d14",
      "metadata": {
        "title": "An Empirical Study on Parameter-Efficient Fine-Tuning for MultiModal Large Language Models",
        "authors": [
          "Xiongtao Zhou",
          "Jie He",
          "Yuhua Ke",
          "Guangyao Zhu",
          "V'ictor Guti'errez-Basulto",
          "Jeff Z. Pan"
        ],
        "published_date": "2024",
        "abstract": "Multimodal large language models (MLLMs) fine-tuned with multimodal instruction datasets have demonstrated remarkable capabilities in multimodal tasks. However, fine-tuning all parameters of MLLMs has become challenging as they usually contain billions of parameters. To address this issue, we study parameter-efficient fine-tuning (PEFT) methods for MLLMs. We aim to identify effective methods for enhancing the performance of MLLMs in scenarios where only a limited number of parameters are trained. This paper conducts empirical studies using four popular PEFT methods to fine-tune the LLM component of open-source MLLMs. We present a comprehensive analysis that encompasses various aspects, including the impact of PEFT methods on various models, parameters and location of the PEFT module, size of fine-tuning data, model stability based on PEFT methods, MLLM's generalization, and hallucination. We evaluated four PEFT methods on seven datasets from two different categories: unseen and seen datasets. Across all experiments, we show that the adapter is the best-performing PEFT method. At the same time, fine-tuning the connector layers leads to improved performance in most MLLMs. Code and data are available at https://github.com/alenai97/PEFT-MLLM.git.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/f123b838e000e11f08cb0d7c63e01934b38d3092.pdf",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "citationCount": 20,
        "score": 20.0,
        "summary": "Multimodal large language models (MLLMs) fine-tuned with multimodal instruction datasets have demonstrated remarkable capabilities in multimodal tasks. However, fine-tuning all parameters of MLLMs has become challenging as they usually contain billions of parameters. To address this issue, we study parameter-efficient fine-tuning (PEFT) methods for MLLMs. We aim to identify effective methods for enhancing the performance of MLLMs in scenarios where only a limited number of parameters are trained. This paper conducts empirical studies using four popular PEFT methods to fine-tune the LLM component of open-source MLLMs. We present a comprehensive analysis that encompasses various aspects, including the impact of PEFT methods on various models, parameters and location of the PEFT module, size of fine-tuning data, model stability based on PEFT methods, MLLM's generalization, and hallucination. We evaluated four PEFT methods on seven datasets from two different categories: unseen and seen datasets. Across all experiments, we show that the adapter is the best-performing PEFT method. At the same time, fine-tuning the connector layers leads to improved performance in most MLLMs. Code and data are available at https://github.com/alenai97/PEFT-MLLM.git.",
        "keywords": []
      },
      "file_name": "f123b838e000e11f08cb0d7c63e01934b38d3092.pdf"
    },
    {
      "success": true,
      "doc_id": "a4a868efa624df61fea6a9e791911d64",
      "summary": "Large language models (LLMs) encounter challenges such as hallucination and factual errors in knowledge-intensive tasks. One the one hand, LLMs sometimes struggle to generate reliable answers based on the black-box parametric knowledge, due to the lack of responsible knowledge. Moreover, fragmented knowledge facts extracted by knowledge retrievers fail to provide explicit and coherent reasoning paths for improving LLM reasoning. To address these challenges, we propose KG-CoT, a novel knowledge-augmented paradigm that leverages a small-scale step-by-step graph reasoning model to reason over knowledge graphs (KGs) and utilizes a reasoning path generation method to generate chains of reasoning with high confidence for large-scale LLMs. Extensive experiments demonstrate that our KG-CoT significantly improves the performance of LLMs on knowledge-intensive question answering tasks, such as multi-hop, single-hop, and open-domain question answering benchmarks, without fine-tuning LLMs. KG-CoT outperforms the CoT prompting as well as prior retrieval-augmented and knowledge base question answering baselines. Moreover, KG-CoT can reduce the number of API calls and cost and generalize to various LLM backbones in a lightweight plug-and-play manner.",
      "intriguing_abstract": "Large language models (LLMs) encounter challenges such as hallucination and factual errors in knowledge-intensive tasks. One the one hand, LLMs sometimes struggle to generate reliable answers based on the black-box parametric knowledge, due to the lack of responsible knowledge. Moreover, fragmented knowledge facts extracted by knowledge retrievers fail to provide explicit and coherent reasoning paths for improving LLM reasoning. To address these challenges, we propose KG-CoT, a novel knowledge-augmented paradigm that leverages a small-scale step-by-step graph reasoning model to reason over knowledge graphs (KGs) and utilizes a reasoning path generation method to generate chains of reasoning with high confidence for large-scale LLMs. Extensive experiments demonstrate that our KG-CoT significantly improves the performance of LLMs on knowledge-intensive question answering tasks, such as multi-hop, single-hop, and open-domain question answering benchmarks, without fine-tuning LLMs. KG-CoT outperforms the CoT prompting as well as prior retrieval-augmented and knowledge base question answering baselines. Moreover, KG-CoT can reduce the number of API calls and cost and generalize to various LLM backbones in a lightweight plug-and-play manner.",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/4c0f029efd5371eed087d0794fb0df71238600cc.pdf",
      "citation_key": "zhao2024g9c",
      "metadata": {
        "title": "KG-CoT: Chain-of-Thought Prompting of Large Language Models over Knowledge Graphs for Knowledge-Aware Question Answering",
        "authors": [
          "Ruilin Zhao",
          "Feng Zhao",
          "Long Wang",
          "Xianzhi Wang",
          "Guandong Xu"
        ],
        "published_date": "2024",
        "abstract": "Large language models (LLMs) encounter challenges such as hallucination and factual errors in knowledge-intensive tasks. One the one hand, LLMs sometimes struggle to generate reliable answers based on the black-box parametric knowledge, due to the lack of responsible knowledge. Moreover, fragmented knowledge facts extracted by knowledge retrievers fail to provide explicit and coherent reasoning paths for improving LLM reasoning. To address these challenges, we propose KG-CoT, a novel knowledge-augmented paradigm that leverages a small-scale step-by-step graph reasoning model to reason over knowledge graphs (KGs) and utilizes a reasoning path generation method to generate chains of reasoning with high confidence for large-scale LLMs. Extensive experiments demonstrate that our KG-CoT significantly improves the performance of LLMs on knowledge-intensive question answering tasks, such as multi-hop, single-hop, and open-domain question answering benchmarks, without fine-tuning LLMs. KG-CoT outperforms the CoT prompting as well as prior retrieval-augmented and knowledge base question answering baselines. Moreover, KG-CoT can reduce the number of API calls and cost and generalize to various LLM backbones in a lightweight plug-and-play manner.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/4c0f029efd5371eed087d0794fb0df71238600cc.pdf",
        "venue": "International Joint Conference on Artificial Intelligence",
        "citationCount": 19,
        "score": 19.0,
        "summary": "Large language models (LLMs) encounter challenges such as hallucination and factual errors in knowledge-intensive tasks. One the one hand, LLMs sometimes struggle to generate reliable answers based on the black-box parametric knowledge, due to the lack of responsible knowledge. Moreover, fragmented knowledge facts extracted by knowledge retrievers fail to provide explicit and coherent reasoning paths for improving LLM reasoning. To address these challenges, we propose KG-CoT, a novel knowledge-augmented paradigm that leverages a small-scale step-by-step graph reasoning model to reason over knowledge graphs (KGs) and utilizes a reasoning path generation method to generate chains of reasoning with high confidence for large-scale LLMs. Extensive experiments demonstrate that our KG-CoT significantly improves the performance of LLMs on knowledge-intensive question answering tasks, such as multi-hop, single-hop, and open-domain question answering benchmarks, without fine-tuning LLMs. KG-CoT outperforms the CoT prompting as well as prior retrieval-augmented and knowledge base question answering baselines. Moreover, KG-CoT can reduce the number of API calls and cost and generalize to various LLM backbones in a lightweight plug-and-play manner.",
        "keywords": []
      },
      "file_name": "4c0f029efd5371eed087d0794fb0df71238600cc.pdf"
    },
    {
      "success": true,
      "doc_id": "41eba52631cf567ef84ee6efcded489f",
      "summary": "We present a Conversational Chain-of-Action (Conv-CoA) framework for Open-domain Conversational Question Answering (OCQA). Compared with literature, Conv-CoA addresses three major challenges: (i) unfaithful hallucination that is inconsistent with real-time or domain facts, (ii) weak reasoning performance in conversational scenarios, and (iii) unsatisfying performance in conversational information retrieval. Our key contribution is a dynamic reasoning-retrieval mechanism that extracts the intent of the question and decomposes it into a reasoning chain to be solved via systematic prompting, pre-designed actions, updating the Contextual Knowledge Set (CKS), and a novel Hopfield-based retriever. Methodologically, we propose a resource-efficiency Hopfield retriever to enhance the efficiency and accuracy of conversational information retrieval within our actions. Additionally, we propose a conversational-multi-reference faith score (Conv-MRFS) to verify and resolve conflicts between retrieved knowledge and answers in conversations. Empirically, we conduct comparisons between our framework and 23 state-of-the-art methods across five different research directions and two public benchmarks. These comparisons demonstrate that our Conv-CoA outperforms other methods in both the accuracy and efficiency dimensions.",
      "intriguing_abstract": "We present a Conversational Chain-of-Action (Conv-CoA) framework for Open-domain Conversational Question Answering (OCQA). Compared with literature, Conv-CoA addresses three major challenges: (i) unfaithful hallucination that is inconsistent with real-time or domain facts, (ii) weak reasoning performance in conversational scenarios, and (iii) unsatisfying performance in conversational information retrieval. Our key contribution is a dynamic reasoning-retrieval mechanism that extracts the intent of the question and decomposes it into a reasoning chain to be solved via systematic prompting, pre-designed actions, updating the Contextual Knowledge Set (CKS), and a novel Hopfield-based retriever. Methodologically, we propose a resource-efficiency Hopfield retriever to enhance the efficiency and accuracy of conversational information retrieval within our actions. Additionally, we propose a conversational-multi-reference faith score (Conv-MRFS) to verify and resolve conflicts between retrieved knowledge and answers in conversations. Empirically, we conduct comparisons between our framework and 23 state-of-the-art methods across five different research directions and two public benchmarks. These comparisons demonstrate that our Conv-CoA outperforms other methods in both the accuracy and efficiency dimensions.",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/cdfab7e389f94263ce99b7c0025090971df40a01.pdf",
      "citation_key": "pan2024uot",
      "metadata": {
        "title": "Conv-CoA: Improving Open-domain Question Answering in Large Language Models via Conversational Chain-of-Action",
        "authors": [
          "Zhenyu Pan",
          "Haozheng Luo",
          "Manling Li",
          "Han Liu"
        ],
        "published_date": "2024",
        "abstract": "We present a Conversational Chain-of-Action (Conv-CoA) framework for Open-domain Conversational Question Answering (OCQA). Compared with literature, Conv-CoA addresses three major challenges: (i) unfaithful hallucination that is inconsistent with real-time or domain facts, (ii) weak reasoning performance in conversational scenarios, and (iii) unsatisfying performance in conversational information retrieval. Our key contribution is a dynamic reasoning-retrieval mechanism that extracts the intent of the question and decomposes it into a reasoning chain to be solved via systematic prompting, pre-designed actions, updating the Contextual Knowledge Set (CKS), and a novel Hopfield-based retriever. Methodologically, we propose a resource-efficiency Hopfield retriever to enhance the efficiency and accuracy of conversational information retrieval within our actions. Additionally, we propose a conversational-multi-reference faith score (Conv-MRFS) to verify and resolve conflicts between retrieved knowledge and answers in conversations. Empirically, we conduct comparisons between our framework and 23 state-of-the-art methods across five different research directions and two public benchmarks. These comparisons demonstrate that our Conv-CoA outperforms other methods in both the accuracy and efficiency dimensions.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/cdfab7e389f94263ce99b7c0025090971df40a01.pdf",
        "venue": "arXiv.org",
        "citationCount": 17,
        "score": 17.0,
        "summary": "We present a Conversational Chain-of-Action (Conv-CoA) framework for Open-domain Conversational Question Answering (OCQA). Compared with literature, Conv-CoA addresses three major challenges: (i) unfaithful hallucination that is inconsistent with real-time or domain facts, (ii) weak reasoning performance in conversational scenarios, and (iii) unsatisfying performance in conversational information retrieval. Our key contribution is a dynamic reasoning-retrieval mechanism that extracts the intent of the question and decomposes it into a reasoning chain to be solved via systematic prompting, pre-designed actions, updating the Contextual Knowledge Set (CKS), and a novel Hopfield-based retriever. Methodologically, we propose a resource-efficiency Hopfield retriever to enhance the efficiency and accuracy of conversational information retrieval within our actions. Additionally, we propose a conversational-multi-reference faith score (Conv-MRFS) to verify and resolve conflicts between retrieved knowledge and answers in conversations. Empirically, we conduct comparisons between our framework and 23 state-of-the-art methods across five different research directions and two public benchmarks. These comparisons demonstrate that our Conv-CoA outperforms other methods in both the accuracy and efficiency dimensions.",
        "keywords": []
      },
      "file_name": "cdfab7e389f94263ce99b7c0025090971df40a01.pdf"
    },
    {
      "success": true,
      "doc_id": "fac3d5b76e2fd0ca885f9e63d1da8be6",
      "summary": "Despite tremendous advancements in large language models (LLMs) over recent years, a notably urgent challenge for their practical deployment is the phenomenon of hallucination, where the model fabricates facts and produces non-factual statements. In response, we propose PoLLMgraph, a Polygraph for LLMs, as an effective model-based white-box detection and forecasting approach. PoLLMgraph distinctly differs from the large body of existing research that concentrates on addressing such challenges through black-box evaluations. In particular, we demonstrate that hallucination can be effectively detected by analyzing the LLM's internal state transition dynamics during generation via tractable probabilistic models. Experimental results on various open-source LLMs confirm the efficacy of PoLLMgraph, outperforming state-of-the-art methods by a considerable margin, evidenced by over 20% improvement in AUC-ROC on common benchmarking datasets like TruthfulQA. Our work paves a new way for model-based white-box analysis of LLMs, motivating the research community to further explore, understand, and refine the intricate dynamics of LLM behaviors.",
      "intriguing_abstract": "Despite tremendous advancements in large language models (LLMs) over recent years, a notably urgent challenge for their practical deployment is the phenomenon of hallucination, where the model fabricates facts and produces non-factual statements. In response, we propose PoLLMgraph, a Polygraph for LLMs, as an effective model-based white-box detection and forecasting approach. PoLLMgraph distinctly differs from the large body of existing research that concentrates on addressing such challenges through black-box evaluations. In particular, we demonstrate that hallucination can be effectively detected by analyzing the LLM's internal state transition dynamics during generation via tractable probabilistic models. Experimental results on various open-source LLMs confirm the efficacy of PoLLMgraph, outperforming state-of-the-art methods by a considerable margin, evidenced by over 20% improvement in AUC-ROC on common benchmarking datasets like TruthfulQA. Our work paves a new way for model-based white-box analysis of LLMs, motivating the research community to further explore, understand, and refine the intricate dynamics of LLM behaviors.",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/4c3447dce6798b894313bb3ff2735ef139cbf071.pdf",
      "citation_key": "zhu2024hll",
      "metadata": {
        "title": "PoLLMgraph: Unraveling Hallucinations in Large Language Models via State Transition Dynamics",
        "authors": [
          "Derui Zhu",
          "Dingfan Chen",
          "Qing Li",
          "Zongxiong Chen",
          "Lei Ma",
          "Jens Grossklags",
          "Mario Fritz"
        ],
        "published_date": "2024",
        "abstract": "Despite tremendous advancements in large language models (LLMs) over recent years, a notably urgent challenge for their practical deployment is the phenomenon of hallucination, where the model fabricates facts and produces non-factual statements. In response, we propose PoLLMgraph, a Polygraph for LLMs, as an effective model-based white-box detection and forecasting approach. PoLLMgraph distinctly differs from the large body of existing research that concentrates on addressing such challenges through black-box evaluations. In particular, we demonstrate that hallucination can be effectively detected by analyzing the LLM's internal state transition dynamics during generation via tractable probabilistic models. Experimental results on various open-source LLMs confirm the efficacy of PoLLMgraph, outperforming state-of-the-art methods by a considerable margin, evidenced by over 20% improvement in AUC-ROC on common benchmarking datasets like TruthfulQA. Our work paves a new way for model-based white-box analysis of LLMs, motivating the research community to further explore, understand, and refine the intricate dynamics of LLM behaviors.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/4c3447dce6798b894313bb3ff2735ef139cbf071.pdf",
        "venue": "NAACL-HLT",
        "citationCount": 16,
        "score": 16.0,
        "summary": "Despite tremendous advancements in large language models (LLMs) over recent years, a notably urgent challenge for their practical deployment is the phenomenon of hallucination, where the model fabricates facts and produces non-factual statements. In response, we propose PoLLMgraph, a Polygraph for LLMs, as an effective model-based white-box detection and forecasting approach. PoLLMgraph distinctly differs from the large body of existing research that concentrates on addressing such challenges through black-box evaluations. In particular, we demonstrate that hallucination can be effectively detected by analyzing the LLM's internal state transition dynamics during generation via tractable probabilistic models. Experimental results on various open-source LLMs confirm the efficacy of PoLLMgraph, outperforming state-of-the-art methods by a considerable margin, evidenced by over 20% improvement in AUC-ROC on common benchmarking datasets like TruthfulQA. Our work paves a new way for model-based white-box analysis of LLMs, motivating the research community to further explore, understand, and refine the intricate dynamics of LLM behaviors.",
        "keywords": []
      },
      "file_name": "4c3447dce6798b894313bb3ff2735ef139cbf071.pdf"
    },
    {
      "success": true,
      "doc_id": "d16ec400d84dfa27d5a81e7a789755a5",
      "summary": "Retrieval-augmented generation (RAG) leverages the strengths of information retrieval and generative models to enhance the handling of real-time and domain-specific knowledge. Despite its advantages, limitations within RAG components may cause hallucinations, or more precisely termed confabulations in generated outputs, driving extensive research to address these limitations and mitigate hallucinations. This review focuses on hallucination in retrieval-augmented large language models (LLMs). We first examine the causes of hallucinations from different sub-tasks in the retrieval and generation phases. Then, we provide a comprehensive overview of corresponding hallucination mitigation techniques, offering a targeted and complete framework for addressing hallucinations in retrieval-augmented LLMs. We also investigate methods to reduce the impact of hallucination through detection and correction. Finally, we discuss promising future research directions for mitigating hallucinations in retrieval-augmented LLMs.",
      "intriguing_abstract": "Retrieval-augmented generation (RAG) leverages the strengths of information retrieval and generative models to enhance the handling of real-time and domain-specific knowledge. Despite its advantages, limitations within RAG components may cause hallucinations, or more precisely termed confabulations in generated outputs, driving extensive research to address these limitations and mitigate hallucinations. This review focuses on hallucination in retrieval-augmented large language models (LLMs). We first examine the causes of hallucinations from different sub-tasks in the retrieval and generation phases. Then, we provide a comprehensive overview of corresponding hallucination mitigation techniques, offering a targeted and complete framework for addressing hallucinations in retrieval-augmented LLMs. We also investigate methods to reduce the impact of hallucination through detection and correction. Finally, we discuss promising future research directions for mitigating hallucinations in retrieval-augmented LLMs.",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/1f49b4586cc71cca59151e7a7bbfd500574c2fee.pdf",
      "citation_key": "zhang20252at",
      "metadata": {
        "title": "Hallucination Mitigation for Retrieval-Augmented Large Language Models: A Review",
        "authors": [
          "Wan Zhang",
          "Jing Zhang"
        ],
        "published_date": "2025",
        "abstract": "Retrieval-augmented generation (RAG) leverages the strengths of information retrieval and generative models to enhance the handling of real-time and domain-specific knowledge. Despite its advantages, limitations within RAG components may cause hallucinations, or more precisely termed confabulations in generated outputs, driving extensive research to address these limitations and mitigate hallucinations. This review focuses on hallucination in retrieval-augmented large language models (LLMs). We first examine the causes of hallucinations from different sub-tasks in the retrieval and generation phases. Then, we provide a comprehensive overview of corresponding hallucination mitigation techniques, offering a targeted and complete framework for addressing hallucinations in retrieval-augmented LLMs. We also investigate methods to reduce the impact of hallucination through detection and correction. Finally, we discuss promising future research directions for mitigating hallucinations in retrieval-augmented LLMs.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/1f49b4586cc71cca59151e7a7bbfd500574c2fee.pdf",
        "venue": "Mathematics",
        "citationCount": 16,
        "score": 16.0,
        "summary": "Retrieval-augmented generation (RAG) leverages the strengths of information retrieval and generative models to enhance the handling of real-time and domain-specific knowledge. Despite its advantages, limitations within RAG components may cause hallucinations, or more precisely termed confabulations in generated outputs, driving extensive research to address these limitations and mitigate hallucinations. This review focuses on hallucination in retrieval-augmented large language models (LLMs). We first examine the causes of hallucinations from different sub-tasks in the retrieval and generation phases. Then, we provide a comprehensive overview of corresponding hallucination mitigation techniques, offering a targeted and complete framework for addressing hallucinations in retrieval-augmented LLMs. We also investigate methods to reduce the impact of hallucination through detection and correction. Finally, we discuss promising future research directions for mitigating hallucinations in retrieval-augmented LLMs.",
        "keywords": []
      },
      "file_name": "1f49b4586cc71cca59151e7a7bbfd500574c2fee.pdf"
    },
    {
      "success": true,
      "doc_id": "149bfc8390a2108a22d62757ce54a55a",
      "summary": "Large language models (LLMs) have achieved great success, but their occasional content fabrication, or hallucination, limits their practical application. Hallucination arises because LLMs struggle to admit ignorance due to inadequate training on knowledge boundaries. We call it a limitation of LLMs that they can not accurately express their knowledge boundary, answering questions they know while admitting ignorance to questions they do not know. In this paper, we aim to teach LLMs to recognize and express their knowledge boundary, so they can reduce hallucinations caused by fabricating when they do not know. We propose CoKE, which first probes LLMs' knowledge boundary via internal confidence given a set of questions, and then leverages the probing results to elicit the expression of the knowledge boundary. Extensive experiments show CoKE helps LLMs express knowledge boundaries, answering known questions while declining unknown ones, significantly improving in-domain and out-of-domain performance.",
      "intriguing_abstract": "Large language models (LLMs) have achieved great success, but their occasional content fabrication, or hallucination, limits their practical application. Hallucination arises because LLMs struggle to admit ignorance due to inadequate training on knowledge boundaries. We call it a limitation of LLMs that they can not accurately express their knowledge boundary, answering questions they know while admitting ignorance to questions they do not know. In this paper, we aim to teach LLMs to recognize and express their knowledge boundary, so they can reduce hallucinations caused by fabricating when they do not know. We propose CoKE, which first probes LLMs' knowledge boundary via internal confidence given a set of questions, and then leverages the probing results to elicit the expression of the knowledge boundary. Extensive experiments show CoKE helps LLMs express knowledge boundaries, answering known questions while declining unknown ones, significantly improving in-domain and out-of-domain performance.",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/4150d370a29258ac88552561cdd2b10f7862bfd7.pdf",
      "citation_key": "chen2024kgu",
      "metadata": {
        "title": "Teaching Large Language Models to Express Knowledge Boundary from Their Own Signals",
        "authors": [
          "Lida Chen",
          "Zujie Liang",
          "Xintao Wang",
          "Jiaqing Liang",
          "Yanghua Xiao",
          "Feng Wei",
          "Jinglei Chen",
          "Zhenghong Hao",
          "Bing Han",
          "Wei Wang"
        ],
        "published_date": "2024",
        "abstract": "Large language models (LLMs) have achieved great success, but their occasional content fabrication, or hallucination, limits their practical application. Hallucination arises because LLMs struggle to admit ignorance due to inadequate training on knowledge boundaries. We call it a limitation of LLMs that they can not accurately express their knowledge boundary, answering questions they know while admitting ignorance to questions they do not know. In this paper, we aim to teach LLMs to recognize and express their knowledge boundary, so they can reduce hallucinations caused by fabricating when they do not know. We propose CoKE, which first probes LLMs' knowledge boundary via internal confidence given a set of questions, and then leverages the probing results to elicit the expression of the knowledge boundary. Extensive experiments show CoKE helps LLMs express knowledge boundaries, answering known questions while declining unknown ones, significantly improving in-domain and out-of-domain performance.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/4150d370a29258ac88552561cdd2b10f7862bfd7.pdf",
        "venue": "Proceedings of the 3rd Workshop on Towards Knowledgeable Foundation Models (KnowFM)",
        "citationCount": 16,
        "score": 16.0,
        "summary": "Large language models (LLMs) have achieved great success, but their occasional content fabrication, or hallucination, limits their practical application. Hallucination arises because LLMs struggle to admit ignorance due to inadequate training on knowledge boundaries. We call it a limitation of LLMs that they can not accurately express their knowledge boundary, answering questions they know while admitting ignorance to questions they do not know. In this paper, we aim to teach LLMs to recognize and express their knowledge boundary, so they can reduce hallucinations caused by fabricating when they do not know. We propose CoKE, which first probes LLMs' knowledge boundary via internal confidence given a set of questions, and then leverages the probing results to elicit the expression of the knowledge boundary. Extensive experiments show CoKE helps LLMs express knowledge boundaries, answering known questions while declining unknown ones, significantly improving in-domain and out-of-domain performance.",
        "keywords": []
      },
      "file_name": "4150d370a29258ac88552561cdd2b10f7862bfd7.pdf"
    },
    {
      "success": true,
      "doc_id": "4af174050635f45c39e8f3d15b8bbdd9",
      "summary": "Integrating large language models with knowledge graphs derived from domain-specific data represents an important advancement towards more powerful and factual reasoning. As these models grow more capable, it is crucial to enable them to perform multi-step inferences over real-world knowledge graphs while minimizing hallucination. While large language models excel at conversation and text generation, their ability to reason over domain-specialized graphs of interconnected entities remains limited. For example, can we query a model to identify the optimal contact in a professional network for a specific goal, based on relationships and attributes in a private database? The answer is no â€“ such capabilities lie beyond current methods. However, this question underscores a critical technical gap that must be addressed. Many high-value applications in areas such as science, security, and e-commerce rely on proprietary knowledge graphs encoding unique structures, relationships, and logical constraints. We introduce a fine-tuning framework for developing Graph-aligned Language Models (GaLM) that transforms a knowledge graph into an alternate text representation with labeled question-answer pairs. We demonstrate that grounding the models in specific graph-based knowledge expands the modelsâ€™ capacity for structure-based reasoning. Our methodology leverages the large-language model's generative capabilities to create the dataset and proposes an efficient alternate to retrieval-augmented generation styled methods.",
      "intriguing_abstract": "Integrating large language models with knowledge graphs derived from domain-specific data represents an important advancement towards more powerful and factual reasoning. As these models grow more capable, it is crucial to enable them to perform multi-step inferences over real-world knowledge graphs while minimizing hallucination. While large language models excel at conversation and text generation, their ability to reason over domain-specialized graphs of interconnected entities remains limited. For example, can we query a model to identify the optimal contact in a professional network for a specific goal, based on relationships and attributes in a private database? The answer is no â€“ such capabilities lie beyond current methods. However, this question underscores a critical technical gap that must be addressed. Many high-value applications in areas such as science, security, and e-commerce rely on proprietary knowledge graphs encoding unique structures, relationships, and logical constraints. We introduce a fine-tuning framework for developing Graph-aligned Language Models (GaLM) that transforms a knowledge graph into an alternate text representation with labeled question-answer pairs. We demonstrate that grounding the models in specific graph-based knowledge expands the modelsâ€™ capacity for structure-based reasoning. Our methodology leverages the large-language model's generative capabilities to create the dataset and proposes an efficient alternate to retrieval-augmented generation styled methods.",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/de02ba19fb957ae30de7f09904ae3d983c3b50e7.pdf",
      "citation_key": "dernbach2024w0b",
      "metadata": {
        "title": "GLaM: Fine-Tuning Large Language Models for Domain Knowledge Graph Alignment via Neighborhood Partitioning and Generative Subgraph Encoding",
        "authors": [
          "Stefan Dernbach",
          "Khushbu Agarwal",
          "Alejandro Zuniga",
          "Michael Henry",
          "Sutanay Choudhury"
        ],
        "published_date": "2024",
        "abstract": "Integrating large language models with knowledge graphs derived from domain-specific data represents an important advancement towards more powerful and factual reasoning. As these models grow more capable, it is crucial to enable them to perform multi-step inferences over real-world knowledge graphs while minimizing hallucination. While large language models excel at conversation and text generation, their ability to reason over domain-specialized graphs of interconnected entities remains limited. For example, can we query a model to identify the optimal contact in a professional network for a specific goal, based on relationships and attributes in a private database? The answer is no â€“ such capabilities lie beyond current methods. However, this question underscores a critical technical gap that must be addressed. Many high-value applications in areas such as science, security, and e-commerce rely on proprietary knowledge graphs encoding unique structures, relationships, and logical constraints. We introduce a fine-tuning framework for developing Graph-aligned Language Models (GaLM) that transforms a knowledge graph into an alternate text representation with labeled question-answer pairs. We demonstrate that grounding the models in specific graph-based knowledge expands the modelsâ€™ capacity for structure-based reasoning. Our methodology leverages the large-language model's generative capabilities to create the dataset and proposes an efficient alternate to retrieval-augmented generation styled methods.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/de02ba19fb957ae30de7f09904ae3d983c3b50e7.pdf",
        "venue": "AAAI Spring Symposia",
        "citationCount": 16,
        "score": 16.0,
        "summary": "Integrating large language models with knowledge graphs derived from domain-specific data represents an important advancement towards more powerful and factual reasoning. As these models grow more capable, it is crucial to enable them to perform multi-step inferences over real-world knowledge graphs while minimizing hallucination. While large language models excel at conversation and text generation, their ability to reason over domain-specialized graphs of interconnected entities remains limited. For example, can we query a model to identify the optimal contact in a professional network for a specific goal, based on relationships and attributes in a private database? The answer is no â€“ such capabilities lie beyond current methods. However, this question underscores a critical technical gap that must be addressed. Many high-value applications in areas such as science, security, and e-commerce rely on proprietary knowledge graphs encoding unique structures, relationships, and logical constraints. We introduce a fine-tuning framework for developing Graph-aligned Language Models (GaLM) that transforms a knowledge graph into an alternate text representation with labeled question-answer pairs. We demonstrate that grounding the models in specific graph-based knowledge expands the modelsâ€™ capacity for structure-based reasoning. Our methodology leverages the large-language model's generative capabilities to create the dataset and proposes an efficient alternate to retrieval-augmented generation styled methods.",
        "keywords": []
      },
      "file_name": "de02ba19fb957ae30de7f09904ae3d983c3b50e7.pdf"
    },
    {
      "success": true,
      "doc_id": "a738af97f1757a04c01536011a3a4f0c",
      "summary": "Clinical reasoning refers to the cognitive process that physicians employ in evaluating and managing patients. This process typically involves suggesting necessary examinations, diagnosing patientsâ€™ diseases, and selecting appropriate therapies, etc. Accurate clinical reasoning requires extensive medical knowledge and rich clinical experience, setting a high bar for physicians. This is particularly challenging in developing countries due to the overwhelming number of patients and limited physician resources, contributing significantly to global health inequity and necessitating automated clinical reasoning approaches. Recently, the emergence of large language models (LLMs) such as ChatGPT and GPT-4 have demonstrated their potential in clinical reasoning. However, these LLMs are prone to hallucination problems, and the reasoning process of LLMs may not align with the clinical decision pathways of physicians. In this study, we introduce a novel framework, In-Context Padding (ICP), to enhance LLMs reasoning with medical knowledge. Specifically, we infer critical clinical reasoning elements (referred to as knowledge seeds) and use these as anchors to guide the generation process of LLMs. Experiments on two clinical question datasets validate that ICP significantly improves the clinical reasoning ability of LLMs.",
      "intriguing_abstract": "Clinical reasoning refers to the cognitive process that physicians employ in evaluating and managing patients. This process typically involves suggesting necessary examinations, diagnosing patientsâ€™ diseases, and selecting appropriate therapies, etc. Accurate clinical reasoning requires extensive medical knowledge and rich clinical experience, setting a high bar for physicians. This is particularly challenging in developing countries due to the overwhelming number of patients and limited physician resources, contributing significantly to global health inequity and necessitating automated clinical reasoning approaches. Recently, the emergence of large language models (LLMs) such as ChatGPT and GPT-4 have demonstrated their potential in clinical reasoning. However, these LLMs are prone to hallucination problems, and the reasoning process of LLMs may not align with the clinical decision pathways of physicians. In this study, we introduce a novel framework, In-Context Padding (ICP), to enhance LLMs reasoning with medical knowledge. Specifically, we infer critical clinical reasoning elements (referred to as knowledge seeds) and use these as anchors to guide the generation process of LLMs. Experiments on two clinical question datasets validate that ICP significantly improves the clinical reasoning ability of LLMs.",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/75a381667ef536d02d99063eb3568e410d7ce909.pdf",
      "citation_key": "wu202407f",
      "metadata": {
        "title": "Guiding Clinical Reasoning with Large Language Models via Knowledge Seeds",
        "authors": [
          "Jiageng Wu",
          "Xian Wu",
          "Jie Yang"
        ],
        "published_date": "2024",
        "abstract": "Clinical reasoning refers to the cognitive process that physicians employ in evaluating and managing patients. This process typically involves suggesting necessary examinations, diagnosing patientsâ€™ diseases, and selecting appropriate therapies, etc. Accurate clinical reasoning requires extensive medical knowledge and rich clinical experience, setting a high bar for physicians. This is particularly challenging in developing countries due to the overwhelming number of patients and limited physician resources, contributing significantly to global health inequity and necessitating automated clinical reasoning approaches. Recently, the emergence of large language models (LLMs) such as ChatGPT and GPT-4 have demonstrated their potential in clinical reasoning. However, these LLMs are prone to hallucination problems, and the reasoning process of LLMs may not align with the clinical decision pathways of physicians. In this study, we introduce a novel framework, In-Context Padding (ICP), to enhance LLMs reasoning with medical knowledge. Specifically, we infer critical clinical reasoning elements (referred to as knowledge seeds) and use these as anchors to guide the generation process of LLMs. Experiments on two clinical question datasets validate that ICP significantly improves the clinical reasoning ability of LLMs.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/75a381667ef536d02d99063eb3568e410d7ce909.pdf",
        "venue": "International Joint Conference on Artificial Intelligence",
        "citationCount": 16,
        "score": 16.0,
        "summary": "Clinical reasoning refers to the cognitive process that physicians employ in evaluating and managing patients. This process typically involves suggesting necessary examinations, diagnosing patientsâ€™ diseases, and selecting appropriate therapies, etc. Accurate clinical reasoning requires extensive medical knowledge and rich clinical experience, setting a high bar for physicians. This is particularly challenging in developing countries due to the overwhelming number of patients and limited physician resources, contributing significantly to global health inequity and necessitating automated clinical reasoning approaches. Recently, the emergence of large language models (LLMs) such as ChatGPT and GPT-4 have demonstrated their potential in clinical reasoning. However, these LLMs are prone to hallucination problems, and the reasoning process of LLMs may not align with the clinical decision pathways of physicians. In this study, we introduce a novel framework, In-Context Padding (ICP), to enhance LLMs reasoning with medical knowledge. Specifically, we infer critical clinical reasoning elements (referred to as knowledge seeds) and use these as anchors to guide the generation process of LLMs. Experiments on two clinical question datasets validate that ICP significantly improves the clinical reasoning ability of LLMs.",
        "keywords": []
      },
      "file_name": "75a381667ef536d02d99063eb3568e410d7ce909.pdf"
    },
    {
      "success": true,
      "doc_id": "74cdddce1f6e3af569906b990b0acb03",
      "summary": "While Large Language Models (LLMs) can serve as agents to simulate human behaviors (i.e., role-playing agents), we emphasize the importance of point-in-time role-playing. This situates characters at specific moments in the narrative progression for three main reasons: (i) enhancing users' narrative immersion, (ii) avoiding spoilers, and (iii) fostering engagement in fandom role-playing. To accurately represent characters at specific time points, agents must avoid character hallucination, where they display knowledge that contradicts their characters' identities and historical timelines. We introduce TimeChara, a new benchmark designed to evaluate point-in-time character hallucination in role-playing LLMs. Comprising 10,895 instances generated through an automated pipeline, this benchmark reveals significant hallucination issues in current state-of-the-art LLMs (e.g., GPT-4o). To counter this challenge, we propose Narrative-Experts, a method that decomposes the reasoning steps and utilizes narrative experts to reduce point-in-time character hallucinations effectively. Still, our findings with TimeChara highlight the ongoing challenges of point-in-time character hallucination, calling for further study.",
      "intriguing_abstract": "While Large Language Models (LLMs) can serve as agents to simulate human behaviors (i.e., role-playing agents), we emphasize the importance of point-in-time role-playing. This situates characters at specific moments in the narrative progression for three main reasons: (i) enhancing users' narrative immersion, (ii) avoiding spoilers, and (iii) fostering engagement in fandom role-playing. To accurately represent characters at specific time points, agents must avoid character hallucination, where they display knowledge that contradicts their characters' identities and historical timelines. We introduce TimeChara, a new benchmark designed to evaluate point-in-time character hallucination in role-playing LLMs. Comprising 10,895 instances generated through an automated pipeline, this benchmark reveals significant hallucination issues in current state-of-the-art LLMs (e.g., GPT-4o). To counter this challenge, we propose Narrative-Experts, a method that decomposes the reasoning steps and utilizes narrative experts to reduce point-in-time character hallucinations effectively. Still, our findings with TimeChara highlight the ongoing challenges of point-in-time character hallucination, calling for further study.",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/d409053ed94ec725d72c812e7c8bd71b87278b96.pdf",
      "citation_key": "ahn2024r1o",
      "metadata": {
        "title": "TimeChara: Evaluating Point-in-Time Character Hallucination of Role-Playing Large Language Models",
        "authors": [
          "Jaewoo Ahn",
          "Taehyun Lee",
          "Junyoung Lim",
          "Jin-Hwa Kim",
          "Sangdoo Yun",
          "Hwaran Lee",
          "Gunhee Kim"
        ],
        "published_date": "2024",
        "abstract": "While Large Language Models (LLMs) can serve as agents to simulate human behaviors (i.e., role-playing agents), we emphasize the importance of point-in-time role-playing. This situates characters at specific moments in the narrative progression for three main reasons: (i) enhancing users' narrative immersion, (ii) avoiding spoilers, and (iii) fostering engagement in fandom role-playing. To accurately represent characters at specific time points, agents must avoid character hallucination, where they display knowledge that contradicts their characters' identities and historical timelines. We introduce TimeChara, a new benchmark designed to evaluate point-in-time character hallucination in role-playing LLMs. Comprising 10,895 instances generated through an automated pipeline, this benchmark reveals significant hallucination issues in current state-of-the-art LLMs (e.g., GPT-4o). To counter this challenge, we propose Narrative-Experts, a method that decomposes the reasoning steps and utilizes narrative experts to reduce point-in-time character hallucinations effectively. Still, our findings with TimeChara highlight the ongoing challenges of point-in-time character hallucination, calling for further study.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/d409053ed94ec725d72c812e7c8bd71b87278b96.pdf",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "citationCount": 15,
        "score": 15.0,
        "summary": "While Large Language Models (LLMs) can serve as agents to simulate human behaviors (i.e., role-playing agents), we emphasize the importance of point-in-time role-playing. This situates characters at specific moments in the narrative progression for three main reasons: (i) enhancing users' narrative immersion, (ii) avoiding spoilers, and (iii) fostering engagement in fandom role-playing. To accurately represent characters at specific time points, agents must avoid character hallucination, where they display knowledge that contradicts their characters' identities and historical timelines. We introduce TimeChara, a new benchmark designed to evaluate point-in-time character hallucination in role-playing LLMs. Comprising 10,895 instances generated through an automated pipeline, this benchmark reveals significant hallucination issues in current state-of-the-art LLMs (e.g., GPT-4o). To counter this challenge, we propose Narrative-Experts, a method that decomposes the reasoning steps and utilizes narrative experts to reduce point-in-time character hallucinations effectively. Still, our findings with TimeChara highlight the ongoing challenges of point-in-time character hallucination, calling for further study.",
        "keywords": []
      },
      "file_name": "d409053ed94ec725d72c812e7c8bd71b87278b96.pdf"
    },
    {
      "success": true,
      "doc_id": "20ae7ae785d1bef7c0bcfa674bbc2285",
      "summary": "Large Language Models (LLMs) have revolutionized solutions for general natural language processing (NLP) tasks. However, deploying these models in specific domains still faces challenges like hallucination. While existing knowledge graph retrieval-based approaches offer partial solutions, they cannot be well adapted to the political domain. On one hand, existing generic knowledge graphs lack vital political context, hindering deductions for practical tasks. On the other hand, the nature of political questions often renders the direct facts elusive, necessitating deeper aggregation and comprehension of retrieved evidence. To address these challenges, we propose a Political Experts through Knowledge Graph Integration (PEG) framework. PEG entails the creation and utilization of a multi-view political knowledge graph (MVPKG), which integrates U.S. legislative, election, and diplomatic data, as well as conceptual knowledge from Wikidata. With MVPKG as its foundation, PEG enhances existing methods through knowledge acquisition, aggregation, and injection. This process begins with refining evidence through semantic filtering, followed by its aggregation into global knowledge via implicit or explicit methods. The integrated knowledge is then utilized by LLMs through prompts. Experiments on three real-world datasets across diverse LLMs confirm PEG's superiority in tackling political modeling tasks.",
      "intriguing_abstract": "Large Language Models (LLMs) have revolutionized solutions for general natural language processing (NLP) tasks. However, deploying these models in specific domains still faces challenges like hallucination. While existing knowledge graph retrieval-based approaches offer partial solutions, they cannot be well adapted to the political domain. On one hand, existing generic knowledge graphs lack vital political context, hindering deductions for practical tasks. On the other hand, the nature of political questions often renders the direct facts elusive, necessitating deeper aggregation and comprehension of retrieved evidence. To address these challenges, we propose a Political Experts through Knowledge Graph Integration (PEG) framework. PEG entails the creation and utilization of a multi-view political knowledge graph (MVPKG), which integrates U.S. legislative, election, and diplomatic data, as well as conceptual knowledge from Wikidata. With MVPKG as its foundation, PEG enhances existing methods through knowledge acquisition, aggregation, and injection. This process begins with refining evidence through semantic filtering, followed by its aggregation into global knowledge via implicit or explicit methods. The integrated knowledge is then utilized by LLMs through prompts. Experiments on three real-world datasets across diverse LLMs confirm PEG's superiority in tackling political modeling tasks.",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/379e3ee9a6a92817bd0812848b409eafb9bf9550.pdf",
      "citation_key": "mou2024fsy",
      "metadata": {
        "title": "Unifying Local and Global Knowledge: Empowering Large Language Models as Political Experts with Knowledge Graphs",
        "authors": [
          "Xinyi Mou",
          "Zejun Li",
          "Hanjia Lyu",
          "Jiebo Luo",
          "Zhongyu Wei"
        ],
        "published_date": "2024",
        "abstract": "Large Language Models (LLMs) have revolutionized solutions for general natural language processing (NLP) tasks. However, deploying these models in specific domains still faces challenges like hallucination. While existing knowledge graph retrieval-based approaches offer partial solutions, they cannot be well adapted to the political domain. On one hand, existing generic knowledge graphs lack vital political context, hindering deductions for practical tasks. On the other hand, the nature of political questions often renders the direct facts elusive, necessitating deeper aggregation and comprehension of retrieved evidence. To address these challenges, we propose a Political Experts through Knowledge Graph Integration (PEG) framework. PEG entails the creation and utilization of a multi-view political knowledge graph (MVPKG), which integrates U.S. legislative, election, and diplomatic data, as well as conceptual knowledge from Wikidata. With MVPKG as its foundation, PEG enhances existing methods through knowledge acquisition, aggregation, and injection. This process begins with refining evidence through semantic filtering, followed by its aggregation into global knowledge via implicit or explicit methods. The integrated knowledge is then utilized by LLMs through prompts. Experiments on three real-world datasets across diverse LLMs confirm PEG's superiority in tackling political modeling tasks.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/379e3ee9a6a92817bd0812848b409eafb9bf9550.pdf",
        "venue": "The Web Conference",
        "citationCount": 14,
        "score": 14.0,
        "summary": "Large Language Models (LLMs) have revolutionized solutions for general natural language processing (NLP) tasks. However, deploying these models in specific domains still faces challenges like hallucination. While existing knowledge graph retrieval-based approaches offer partial solutions, they cannot be well adapted to the political domain. On one hand, existing generic knowledge graphs lack vital political context, hindering deductions for practical tasks. On the other hand, the nature of political questions often renders the direct facts elusive, necessitating deeper aggregation and comprehension of retrieved evidence. To address these challenges, we propose a Political Experts through Knowledge Graph Integration (PEG) framework. PEG entails the creation and utilization of a multi-view political knowledge graph (MVPKG), which integrates U.S. legislative, election, and diplomatic data, as well as conceptual knowledge from Wikidata. With MVPKG as its foundation, PEG enhances existing methods through knowledge acquisition, aggregation, and injection. This process begins with refining evidence through semantic filtering, followed by its aggregation into global knowledge via implicit or explicit methods. The integrated knowledge is then utilized by LLMs through prompts. Experiments on three real-world datasets across diverse LLMs confirm PEG's superiority in tackling political modeling tasks.",
        "keywords": []
      },
      "file_name": "379e3ee9a6a92817bd0812848b409eafb9bf9550.pdf"
    },
    {
      "success": true,
      "doc_id": "36c7ad5157a3424f35b30b1a61589f1e",
      "summary": "Model editing aims to precisely alter the behaviors of large language models (LLMs) in relation to specific knowledge, while leaving unrelated knowledge intact. This approach has proven effective in addressing issues of hallucination and outdated information in LLMs. However, the potential of using model editing to modify knowledge in the medical field remains largely unexplored, even though resolving hallucination is a pressing need in this area. Our observations indicate that current methods face significant challenges in dealing with specialized and complex knowledge in medical domain. Therefore, we propose MedLaSA, a novel Layer-wise Scalable Adapter strategy for medical model editing. MedLaSA harnesses the strengths of both adding extra parameters and locate-then-edit methods for medical model editing. We utilize causal tracing to identify the association of knowledge in neurons across different layers, and generate a corresponding scale set from the association value for each piece of knowledge. Subsequently, we incorporate scalable adapters into the dense layers of LLMs. These adapters are assigned scaling values based on the corresponding specific knowledge, which allows for the adjustment of the adapter's weight and rank. The more similar the content, the more consistent the scale between them. This ensures precise editing of semantically identical knowledge while avoiding impact on unrelated knowledge. To evaluate the editing impact on the behaviours of LLMs, we propose two model editing studies for medical domain: (1) editing factual knowledge for medical specialization and (2) editing the explanatory ability for complex knowledge. We build two novel medical benchmarking datasets and introduce a series of challenging and comprehensive metrics. Extensive experiments on medical LLMs demonstrate the editing efficiency of MedLaSA, without affecting unrelated knowledge.",
      "intriguing_abstract": "Model editing aims to precisely alter the behaviors of large language models (LLMs) in relation to specific knowledge, while leaving unrelated knowledge intact. This approach has proven effective in addressing issues of hallucination and outdated information in LLMs. However, the potential of using model editing to modify knowledge in the medical field remains largely unexplored, even though resolving hallucination is a pressing need in this area. Our observations indicate that current methods face significant challenges in dealing with specialized and complex knowledge in medical domain. Therefore, we propose MedLaSA, a novel Layer-wise Scalable Adapter strategy for medical model editing. MedLaSA harnesses the strengths of both adding extra parameters and locate-then-edit methods for medical model editing. We utilize causal tracing to identify the association of knowledge in neurons across different layers, and generate a corresponding scale set from the association value for each piece of knowledge. Subsequently, we incorporate scalable adapters into the dense layers of LLMs. These adapters are assigned scaling values based on the corresponding specific knowledge, which allows for the adjustment of the adapter's weight and rank. The more similar the content, the more consistent the scale between them. This ensures precise editing of semantically identical knowledge while avoiding impact on unrelated knowledge. To evaluate the editing impact on the behaviours of LLMs, we propose two model editing studies for medical domain: (1) editing factual knowledge for medical specialization and (2) editing the explanatory ability for complex knowledge. We build two novel medical benchmarking datasets and introduce a series of challenging and comprehensive metrics. Extensive experiments on medical LLMs demonstrate the editing efficiency of MedLaSA, without affecting unrelated knowledge.",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/b0633ccf235e467c35b963ad012f6b8c54aba19f.pdf",
      "citation_key": "xu2024f68",
      "metadata": {
        "title": "Editing Factual Knowledge and Explanatory Ability of Medical Large Language Models",
        "authors": [
          "Derong Xu",
          "Ziheng Zhang",
          "Zhihong Zhu",
          "Zhenxi Lin",
          "Qidong Liu",
          "Xian Wu",
          "Tong Xu",
          "Xiangyu Zhao",
          "Yefeng Zheng",
          "Enhong Chen"
        ],
        "published_date": "2024",
        "abstract": "Model editing aims to precisely alter the behaviors of large language models (LLMs) in relation to specific knowledge, while leaving unrelated knowledge intact. This approach has proven effective in addressing issues of hallucination and outdated information in LLMs. However, the potential of using model editing to modify knowledge in the medical field remains largely unexplored, even though resolving hallucination is a pressing need in this area. Our observations indicate that current methods face significant challenges in dealing with specialized and complex knowledge in medical domain. Therefore, we propose MedLaSA, a novel Layer-wise Scalable Adapter strategy for medical model editing. MedLaSA harnesses the strengths of both adding extra parameters and locate-then-edit methods for medical model editing. We utilize causal tracing to identify the association of knowledge in neurons across different layers, and generate a corresponding scale set from the association value for each piece of knowledge. Subsequently, we incorporate scalable adapters into the dense layers of LLMs. These adapters are assigned scaling values based on the corresponding specific knowledge, which allows for the adjustment of the adapter's weight and rank. The more similar the content, the more consistent the scale between them. This ensures precise editing of semantically identical knowledge while avoiding impact on unrelated knowledge. To evaluate the editing impact on the behaviours of LLMs, we propose two model editing studies for medical domain: (1) editing factual knowledge for medical specialization and (2) editing the explanatory ability for complex knowledge. We build two novel medical benchmarking datasets and introduce a series of challenging and comprehensive metrics. Extensive experiments on medical LLMs demonstrate the editing efficiency of MedLaSA, without affecting unrelated knowledge.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/b0633ccf235e467c35b963ad012f6b8c54aba19f.pdf",
        "venue": "International Conference on Information and Knowledge Management",
        "citationCount": 14,
        "score": 14.0,
        "summary": "Model editing aims to precisely alter the behaviors of large language models (LLMs) in relation to specific knowledge, while leaving unrelated knowledge intact. This approach has proven effective in addressing issues of hallucination and outdated information in LLMs. However, the potential of using model editing to modify knowledge in the medical field remains largely unexplored, even though resolving hallucination is a pressing need in this area. Our observations indicate that current methods face significant challenges in dealing with specialized and complex knowledge in medical domain. Therefore, we propose MedLaSA, a novel Layer-wise Scalable Adapter strategy for medical model editing. MedLaSA harnesses the strengths of both adding extra parameters and locate-then-edit methods for medical model editing. We utilize causal tracing to identify the association of knowledge in neurons across different layers, and generate a corresponding scale set from the association value for each piece of knowledge. Subsequently, we incorporate scalable adapters into the dense layers of LLMs. These adapters are assigned scaling values based on the corresponding specific knowledge, which allows for the adjustment of the adapter's weight and rank. The more similar the content, the more consistent the scale between them. This ensures precise editing of semantically identical knowledge while avoiding impact on unrelated knowledge. To evaluate the editing impact on the behaviours of LLMs, we propose two model editing studies for medical domain: (1) editing factual knowledge for medical specialization and (2) editing the explanatory ability for complex knowledge. We build two novel medical benchmarking datasets and introduce a series of challenging and comprehensive metrics. Extensive experiments on medical LLMs demonstrate the editing efficiency of MedLaSA, without affecting unrelated knowledge.",
        "keywords": []
      },
      "file_name": "b0633ccf235e467c35b963ad012f6b8c54aba19f.pdf"
    },
    {
      "success": true,
      "doc_id": "c091ff0494b4082efd6e9a914fa34518",
      "summary": "Large Language Models (LLMs) have shown impressive capabilities but also a concerning tendency to hallucinate. This paper presents RefChecker, a framework that introduces claim-triplets to represent claims in LLM responses, aiming to detect fine-grained hallucinations. In RefChecker, an extractor generates claim-triplets from a response, which are then evaluated by a checker against a reference. We delineate three task settings: Zero, Noisy and Accurate Context, to reflect various real-world use cases. We curated a benchmark spanning various NLP tasks and annotated 11k claim-triplets from 2.1k responses by seven LLMs. RefChecker supports both proprietary and open-source models as the extractor and checker. Experiments demonstrate that claim-triplets enable superior hallucination detection, compared to other granularities such as response, sentence and sub-sentence level claims. RefChecker outperforms prior methods by 6.8 to 26.1 points on our benchmark and the checking results of RefChecker are strongly aligned with human judgments. This work is open sourced at https://github.com/amazon-science/RefChecker",
      "intriguing_abstract": "Large Language Models (LLMs) have shown impressive capabilities but also a concerning tendency to hallucinate. This paper presents RefChecker, a framework that introduces claim-triplets to represent claims in LLM responses, aiming to detect fine-grained hallucinations. In RefChecker, an extractor generates claim-triplets from a response, which are then evaluated by a checker against a reference. We delineate three task settings: Zero, Noisy and Accurate Context, to reflect various real-world use cases. We curated a benchmark spanning various NLP tasks and annotated 11k claim-triplets from 2.1k responses by seven LLMs. RefChecker supports both proprietary and open-source models as the extractor and checker. Experiments demonstrate that claim-triplets enable superior hallucination detection, compared to other granularities such as response, sentence and sub-sentence level claims. RefChecker outperforms prior methods by 6.8 to 26.1 points on our benchmark and the checking results of RefChecker are strongly aligned with human judgments. This work is open sourced at https://github.com/amazon-science/RefChecker",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/35a544ca04325006f7f2e2369d3e0aaa8ba36a07.pdf",
      "citation_key": "hu2024fnt",
      "metadata": {
        "title": "RefChecker: Reference-based Fine-grained Hallucination Checker and Benchmark for Large Language Models",
        "authors": [
          "Xiangkun Hu",
          "Dongyu Ru",
          "Lin Qiu",
          "Qipeng Guo",
          "Tianhang Zhang",
          "Yang Xu",
          "Yun Luo",
          "Pengfei Liu",
          "Yue Zhang",
          "Zheng Zhang"
        ],
        "published_date": "2024",
        "abstract": "Large Language Models (LLMs) have shown impressive capabilities but also a concerning tendency to hallucinate. This paper presents RefChecker, a framework that introduces claim-triplets to represent claims in LLM responses, aiming to detect fine-grained hallucinations. In RefChecker, an extractor generates claim-triplets from a response, which are then evaluated by a checker against a reference. We delineate three task settings: Zero, Noisy and Accurate Context, to reflect various real-world use cases. We curated a benchmark spanning various NLP tasks and annotated 11k claim-triplets from 2.1k responses by seven LLMs. RefChecker supports both proprietary and open-source models as the extractor and checker. Experiments demonstrate that claim-triplets enable superior hallucination detection, compared to other granularities such as response, sentence and sub-sentence level claims. RefChecker outperforms prior methods by 6.8 to 26.1 points on our benchmark and the checking results of RefChecker are strongly aligned with human judgments. This work is open sourced at https://github.com/amazon-science/RefChecker",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/35a544ca04325006f7f2e2369d3e0aaa8ba36a07.pdf",
        "venue": "arXiv.org",
        "citationCount": 13,
        "score": 13.0,
        "summary": "Large Language Models (LLMs) have shown impressive capabilities but also a concerning tendency to hallucinate. This paper presents RefChecker, a framework that introduces claim-triplets to represent claims in LLM responses, aiming to detect fine-grained hallucinations. In RefChecker, an extractor generates claim-triplets from a response, which are then evaluated by a checker against a reference. We delineate three task settings: Zero, Noisy and Accurate Context, to reflect various real-world use cases. We curated a benchmark spanning various NLP tasks and annotated 11k claim-triplets from 2.1k responses by seven LLMs. RefChecker supports both proprietary and open-source models as the extractor and checker. Experiments demonstrate that claim-triplets enable superior hallucination detection, compared to other granularities such as response, sentence and sub-sentence level claims. RefChecker outperforms prior methods by 6.8 to 26.1 points on our benchmark and the checking results of RefChecker are strongly aligned with human judgments. This work is open sourced at https://github.com/amazon-science/RefChecker",
        "keywords": []
      },
      "file_name": "35a544ca04325006f7f2e2369d3e0aaa8ba36a07.pdf"
    },
    {
      "success": true,
      "doc_id": "3ca9ed11d8351aaf720e5bb5f76de93f",
      "summary": "Multi-objective alignment from human feedback (MOAHF) in large language models (LLMs) is a challenging problem as human preferences are complex, multifaceted, and often conflicting. Recent works on MOAHF considered a-priori multi-objective optimization (MOO), where human preferences are known at training or inference time. In contrast, when human preferences are unknown or difficult to quantify, a natural approach is to cover the Pareto front by multiple diverse solutions. We propose an algorithm HaM for learning diverse LLM policies that maximizes their hypervolume. This is the first application of a-posteriori MOO to MOAHF. HaM is computationally and space efficient, and empirically superior across objectives such as harmlessness, helpfulness, humor, faithfulness, and hallucination, on various datasets.",
      "intriguing_abstract": "Multi-objective alignment from human feedback (MOAHF) in large language models (LLMs) is a challenging problem as human preferences are complex, multifaceted, and often conflicting. Recent works on MOAHF considered a-priori multi-objective optimization (MOO), where human preferences are known at training or inference time. In contrast, when human preferences are unknown or difficult to quantify, a natural approach is to cover the Pareto front by multiple diverse solutions. We propose an algorithm HaM for learning diverse LLM policies that maximizes their hypervolume. This is the first application of a-posteriori MOO to MOAHF. HaM is computationally and space efficient, and empirically superior across objectives such as harmlessness, helpfulness, humor, faithfulness, and hallucination, on various datasets.",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/165fdad3949b7abdb985cb8834c26c7baa7bd40f.pdf",
      "citation_key": "mukherjee2024o5w",
      "metadata": {
        "title": "Multi-Objective Alignment of Large Language Models Through Hypervolume Maximization",
        "authors": [
          "Subhojyoti Mukherjee",
          "Anusha Lalitha",
          "Sailik Sengupta",
          "Aniket Deshmukh",
          "B. Kveton"
        ],
        "published_date": "2024",
        "abstract": "Multi-objective alignment from human feedback (MOAHF) in large language models (LLMs) is a challenging problem as human preferences are complex, multifaceted, and often conflicting. Recent works on MOAHF considered a-priori multi-objective optimization (MOO), where human preferences are known at training or inference time. In contrast, when human preferences are unknown or difficult to quantify, a natural approach is to cover the Pareto front by multiple diverse solutions. We propose an algorithm HaM for learning diverse LLM policies that maximizes their hypervolume. This is the first application of a-posteriori MOO to MOAHF. HaM is computationally and space efficient, and empirically superior across objectives such as harmlessness, helpfulness, humor, faithfulness, and hallucination, on various datasets.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/165fdad3949b7abdb985cb8834c26c7baa7bd40f.pdf",
        "venue": "arXiv.org",
        "citationCount": 13,
        "score": 13.0,
        "summary": "Multi-objective alignment from human feedback (MOAHF) in large language models (LLMs) is a challenging problem as human preferences are complex, multifaceted, and often conflicting. Recent works on MOAHF considered a-priori multi-objective optimization (MOO), where human preferences are known at training or inference time. In contrast, when human preferences are unknown or difficult to quantify, a natural approach is to cover the Pareto front by multiple diverse solutions. We propose an algorithm HaM for learning diverse LLM policies that maximizes their hypervolume. This is the first application of a-posteriori MOO to MOAHF. HaM is computationally and space efficient, and empirically superior across objectives such as harmlessness, helpfulness, humor, faithfulness, and hallucination, on various datasets.",
        "keywords": []
      },
      "file_name": "165fdad3949b7abdb985cb8834c26c7baa7bd40f.pdf"
    },
    {
      "success": true,
      "doc_id": "499dfe2be85ce4cdcb892c572136cadd",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/8c8527f7615d53cbc21b9c3536486540f1c75000.pdf",
      "citation_key": "jiao2024l4e",
      "metadata": {
        "title": "Enhancing Multimodal Large Language Models with Vision Detection Models: An Empirical Study",
        "authors": [
          "Qirui Jiao",
          "Daoyuan Chen",
          "Yilun Huang",
          "Yaliang Li",
          "Ying Shen"
        ],
        "published_date": "2024",
        "abstract": "",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/8c8527f7615d53cbc21b9c3536486540f1c75000.pdf",
        "venue": "arXiv.org",
        "citationCount": 13,
        "score": 13.0,
        "summary": "",
        "keywords": []
      },
      "file_name": "8c8527f7615d53cbc21b9c3536486540f1c75000.pdf"
    },
    {
      "success": true,
      "doc_id": "dbefd32f28b6a5e4bbe306fa86be2943",
      "summary": "Large Language Models (LLMs) are revolutionizing the field of code development by leveraging their deep understanding of code patterns, syntax, and semantics to assist developers in various tasks, from code generation and testing to code understanding and documentation. In this survey, accompanying our proposed lecture-style tutorial for KDD 2024, we explore the multifaceted impact of LLMs on the code development, delving into techniques for generating a high-quality code, creating comprehensive test cases, automatically generating documentation, and engaging in an interactive code reasoning. Throughout the survey, we highlight some crucial components surrounding LLMs, including pre-training, fine-tuning, prompt engineering, iterative refinement, agent planning, and hallucination mitigation. We put forward that such ingredients are essential to harness the full potential of these powerful AI models in revolutionizing software engineering and paving the way for a more efficient, effective, and innovative future in code development.",
      "intriguing_abstract": "Large Language Models (LLMs) are revolutionizing the field of code development by leveraging their deep understanding of code patterns, syntax, and semantics to assist developers in various tasks, from code generation and testing to code understanding and documentation. In this survey, accompanying our proposed lecture-style tutorial for KDD 2024, we explore the multifaceted impact of LLMs on the code development, delving into techniques for generating a high-quality code, creating comprehensive test cases, automatically generating documentation, and engaging in an interactive code reasoning. Throughout the survey, we highlight some crucial components surrounding LLMs, including pre-training, fine-tuning, prompt engineering, iterative refinement, agent planning, and hallucination mitigation. We put forward that such ingredients are essential to harness the full potential of these powerful AI models in revolutionizing software engineering and paving the way for a more efficient, effective, and innovative future in code development.",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/e131a11ad907023f655f22a4fae2b2a6f2db96f7.pdf",
      "citation_key": "ding20245e3",
      "metadata": {
        "title": "Reasoning and Planning with Large Language Models in Code Development",
        "authors": [
          "Hao Ding",
          "Ziwei Fan",
          "Ingo GÃ¼hring",
          "Gaurav Gupta",
          "Wooseok Ha",
          "Jun Huan",
          "Linbo Liu",
          "Behrooz Omidvar-Tehrani",
          "Shiqi Wang",
          "Hao Zhou"
        ],
        "published_date": "2024",
        "abstract": "Large Language Models (LLMs) are revolutionizing the field of code development by leveraging their deep understanding of code patterns, syntax, and semantics to assist developers in various tasks, from code generation and testing to code understanding and documentation. In this survey, accompanying our proposed lecture-style tutorial for KDD 2024, we explore the multifaceted impact of LLMs on the code development, delving into techniques for generating a high-quality code, creating comprehensive test cases, automatically generating documentation, and engaging in an interactive code reasoning. Throughout the survey, we highlight some crucial components surrounding LLMs, including pre-training, fine-tuning, prompt engineering, iterative refinement, agent planning, and hallucination mitigation. We put forward that such ingredients are essential to harness the full potential of these powerful AI models in revolutionizing software engineering and paving the way for a more efficient, effective, and innovative future in code development.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/e131a11ad907023f655f22a4fae2b2a6f2db96f7.pdf",
        "venue": "Knowledge Discovery and Data Mining",
        "citationCount": 13,
        "score": 13.0,
        "summary": "Large Language Models (LLMs) are revolutionizing the field of code development by leveraging their deep understanding of code patterns, syntax, and semantics to assist developers in various tasks, from code generation and testing to code understanding and documentation. In this survey, accompanying our proposed lecture-style tutorial for KDD 2024, we explore the multifaceted impact of LLMs on the code development, delving into techniques for generating a high-quality code, creating comprehensive test cases, automatically generating documentation, and engaging in an interactive code reasoning. Throughout the survey, we highlight some crucial components surrounding LLMs, including pre-training, fine-tuning, prompt engineering, iterative refinement, agent planning, and hallucination mitigation. We put forward that such ingredients are essential to harness the full potential of these powerful AI models in revolutionizing software engineering and paving the way for a more efficient, effective, and innovative future in code development.",
        "keywords": []
      },
      "file_name": "e131a11ad907023f655f22a4fae2b2a6f2db96f7.pdf"
    },
    {
      "success": true,
      "doc_id": "146c41636b886a87e230a81a73487fdd",
      "summary": "Large language models (LLMs) show potential in code reasoning tasks, facilitating the customization of detecting bugs in software development. However, the hallucination effect can significantly compromise the reliability of bug reports. This work formulates a new schema of bug detection and presents a novel sanitization technique that detects false positives for hallucination mitigation. Our key idea is to enforce LLMs to emit data-flow paths in few-shot chain-of-thought prompting and validate them via the program-property decomposition . Specifically, we dissect data-flow paths into basic properties upon concise code snip-pets and leverage parsing-based analysis and LLMs for validation. Our approach averagely achieves 91.03% precision and 74.00% recall upon synthetic benchmarks, and boosts the precision by 21.99% with the sanitization. The evaluation upon real-world Android malware applications also demonstrates the superiority over an industrial analyzer, surpassing the precision and recall by 15.36% and 3.61%, respectively. LLMSAN is open-sourced at https: //github.com/chengpeng-wang/LLMSAN .",
      "intriguing_abstract": "Large language models (LLMs) show potential in code reasoning tasks, facilitating the customization of detecting bugs in software development. However, the hallucination effect can significantly compromise the reliability of bug reports. This work formulates a new schema of bug detection and presents a novel sanitization technique that detects false positives for hallucination mitigation. Our key idea is to enforce LLMs to emit data-flow paths in few-shot chain-of-thought prompting and validate them via the program-property decomposition . Specifically, we dissect data-flow paths into basic properties upon concise code snip-pets and leverage parsing-based analysis and LLMs for validation. Our approach averagely achieves 91.03% precision and 74.00% recall upon synthetic benchmarks, and boosts the precision by 21.99% with the sanitization. The evaluation upon real-world Android malware applications also demonstrates the superiority over an industrial analyzer, surpassing the precision and recall by 15.36% and 3.61%, respectively. LLMSAN is open-sourced at https: //github.com/chengpeng-wang/LLMSAN .",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/ffeec58ed1fc045c55512e20b30fce951913a3f0.pdf",
      "citation_key": "wang2024swy",
      "metadata": {
        "title": "Sanitizing Large Language Models in Bug Detection with Data-Flow",
        "authors": [
          "Chengpeng Wang",
          "Wuqi Zhang",
          "Zian Su",
          "Xiangzhe Xu",
          "Xiangyu Zhang"
        ],
        "published_date": "2024",
        "abstract": "Large language models (LLMs) show potential in code reasoning tasks, facilitating the customization of detecting bugs in software development. However, the hallucination effect can significantly compromise the reliability of bug reports. This work formulates a new schema of bug detection and presents a novel sanitization technique that detects false positives for hallucination mitigation. Our key idea is to enforce LLMs to emit data-flow paths in few-shot chain-of-thought prompting and validate them via the program-property decomposition . Specifically, we dissect data-flow paths into basic properties upon concise code snip-pets and leverage parsing-based analysis and LLMs for validation. Our approach averagely achieves 91.03% precision and 74.00% recall upon synthetic benchmarks, and boosts the precision by 21.99% with the sanitization. The evaluation upon real-world Android malware applications also demonstrates the superiority over an industrial analyzer, surpassing the precision and recall by 15.36% and 3.61%, respectively. LLMSAN is open-sourced at https: //github.com/chengpeng-wang/LLMSAN .",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/ffeec58ed1fc045c55512e20b30fce951913a3f0.pdf",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "citationCount": 13,
        "score": 13.0,
        "summary": "Large language models (LLMs) show potential in code reasoning tasks, facilitating the customization of detecting bugs in software development. However, the hallucination effect can significantly compromise the reliability of bug reports. This work formulates a new schema of bug detection and presents a novel sanitization technique that detects false positives for hallucination mitigation. Our key idea is to enforce LLMs to emit data-flow paths in few-shot chain-of-thought prompting and validate them via the program-property decomposition . Specifically, we dissect data-flow paths into basic properties upon concise code snip-pets and leverage parsing-based analysis and LLMs for validation. Our approach averagely achieves 91.03% precision and 74.00% recall upon synthetic benchmarks, and boosts the precision by 21.99% with the sanitization. The evaluation upon real-world Android malware applications also demonstrates the superiority over an industrial analyzer, surpassing the precision and recall by 15.36% and 3.61%, respectively. LLMSAN is open-sourced at https: //github.com/chengpeng-wang/LLMSAN .",
        "keywords": []
      },
      "file_name": "ffeec58ed1fc045c55512e20b30fce951913a3f0.pdf"
    },
    {
      "success": true,
      "doc_id": "87ed5e21af45a3183082cb7fbe6827aa",
      "summary": "Retrieval-Augmented Generation (RAG) is applied to solve hallucination problems and real-time constraints of large language models, but it also induces vulnerabilities against retrieval corruption attacks. Existing research mainly explores the unreliability of RAG in white-box and closed-domain QA tasks. In this paper, we aim to reveal the vulnerabilities of Retrieval-Enhanced Generative (RAG) models when faced with black-box attacks for opinion manipulation. We explore the impact of such attacks on user cognition and decision-making, providing new insight to enhance the reliability and security of RAG models. We manipulate the ranking results of the retrieval model in RAG with instruction and use these results as data to train a surrogate model. By employing adversarial retrieval attack methods to the surrogate model, black-box transfer attacks on RAG are further realized. Experiments conducted on opinion datasets across multiple topics show that the proposed attack strategy can significantly alter the opinion polarity of the content generated by RAG. This demonstrates the model's vulnerability and, more importantly, reveals the potential negative impact on user cognition and decision-making, making it easier to mislead users into accepting incorrect or biased information.",
      "intriguing_abstract": "Retrieval-Augmented Generation (RAG) is applied to solve hallucination problems and real-time constraints of large language models, but it also induces vulnerabilities against retrieval corruption attacks. Existing research mainly explores the unreliability of RAG in white-box and closed-domain QA tasks. In this paper, we aim to reveal the vulnerabilities of Retrieval-Enhanced Generative (RAG) models when faced with black-box attacks for opinion manipulation. We explore the impact of such attacks on user cognition and decision-making, providing new insight to enhance the reliability and security of RAG models. We manipulate the ranking results of the retrieval model in RAG with instruction and use these results as data to train a surrogate model. By employing adversarial retrieval attack methods to the surrogate model, black-box transfer attacks on RAG are further realized. Experiments conducted on opinion datasets across multiple topics show that the proposed attack strategy can significantly alter the opinion polarity of the content generated by RAG. This demonstrates the model's vulnerability and, more importantly, reveals the potential negative impact on user cognition and decision-making, making it easier to mislead users into accepting incorrect or biased information.",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/89729cdfe0f71ad7a04c73e9167c2b266ee0ee8c.pdf",
      "citation_key": "chen2024md6",
      "metadata": {
        "title": "Black-Box Opinion Manipulation Attacks to Retrieval-Augmented Generation of Large Language Models",
        "authors": [
          "Zhuo Chen",
          "Jiawei Liu",
          "Haotan Liu",
          "Qikai Cheng",
          "Fan Zhang",
          "Wei Lu",
          "Xiaozhong Liu"
        ],
        "published_date": "2024",
        "abstract": "Retrieval-Augmented Generation (RAG) is applied to solve hallucination problems and real-time constraints of large language models, but it also induces vulnerabilities against retrieval corruption attacks. Existing research mainly explores the unreliability of RAG in white-box and closed-domain QA tasks. In this paper, we aim to reveal the vulnerabilities of Retrieval-Enhanced Generative (RAG) models when faced with black-box attacks for opinion manipulation. We explore the impact of such attacks on user cognition and decision-making, providing new insight to enhance the reliability and security of RAG models. We manipulate the ranking results of the retrieval model in RAG with instruction and use these results as data to train a surrogate model. By employing adversarial retrieval attack methods to the surrogate model, black-box transfer attacks on RAG are further realized. Experiments conducted on opinion datasets across multiple topics show that the proposed attack strategy can significantly alter the opinion polarity of the content generated by RAG. This demonstrates the model's vulnerability and, more importantly, reveals the potential negative impact on user cognition and decision-making, making it easier to mislead users into accepting incorrect or biased information.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/89729cdfe0f71ad7a04c73e9167c2b266ee0ee8c.pdf",
        "venue": "arXiv.org",
        "citationCount": 13,
        "score": 13.0,
        "summary": "Retrieval-Augmented Generation (RAG) is applied to solve hallucination problems and real-time constraints of large language models, but it also induces vulnerabilities against retrieval corruption attacks. Existing research mainly explores the unreliability of RAG in white-box and closed-domain QA tasks. In this paper, we aim to reveal the vulnerabilities of Retrieval-Enhanced Generative (RAG) models when faced with black-box attacks for opinion manipulation. We explore the impact of such attacks on user cognition and decision-making, providing new insight to enhance the reliability and security of RAG models. We manipulate the ranking results of the retrieval model in RAG with instruction and use these results as data to train a surrogate model. By employing adversarial retrieval attack methods to the surrogate model, black-box transfer attacks on RAG are further realized. Experiments conducted on opinion datasets across multiple topics show that the proposed attack strategy can significantly alter the opinion polarity of the content generated by RAG. This demonstrates the model's vulnerability and, more importantly, reveals the potential negative impact on user cognition and decision-making, making it easier to mislead users into accepting incorrect or biased information.",
        "keywords": []
      },
      "file_name": "89729cdfe0f71ad7a04c73e9167c2b266ee0ee8c.pdf"
    },
    {
      "success": true,
      "doc_id": "3de3c04619b7d57975bb8f384635c5b0",
      "summary": ",",
      "intriguing_abstract": ",",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/bca44a53d9becd158ee0abf34c4255375fdc7327.pdf",
      "citation_key": "wang2023ynd",
      "metadata": {
        "title": "Hallucination Detection for Generative Large Language Models by Bayesian Sequential Estimation",
        "authors": [
          "Xiaohua Wang",
          "Yuliang Yan",
          "Longtao Huang",
          "Xiaoqing Zheng",
          "Xuanjing Huang"
        ],
        "published_date": "2023",
        "abstract": ",",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/bca44a53d9becd158ee0abf34c4255375fdc7327.pdf",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "citationCount": 25,
        "score": 12.5,
        "summary": ",",
        "keywords": []
      },
      "file_name": "bca44a53d9becd158ee0abf34c4255375fdc7327.pdf"
    },
    {
      "success": true,
      "doc_id": "0d3b75a30503581accddc59c09ad4522",
      "summary": "Large language models (LLMs) often necessitate extensive labeled datasets and training compute to achieve impressive performance across downstream tasks. This paper explores a self-training paradigm, where the LLM autonomously curates its own labels and selectively trains on unknown data samples identified through a reference-free consistency method. Empirical evaluations demonstrate significant improvements in reducing hallucination in generation across multiple subjects. Furthermore, the selective training framework mitigates catastrophic forgetting in out-of-distribution benchmarks, addressing a critical limitation in training LLMs. Our findings suggest that such an approach can substantially reduce the dependency on large labeled datasets, paving the way for more scalable and cost-effective language model training.",
      "intriguing_abstract": "Large language models (LLMs) often necessitate extensive labeled datasets and training compute to achieve impressive performance across downstream tasks. This paper explores a self-training paradigm, where the LLM autonomously curates its own labels and selectively trains on unknown data samples identified through a reference-free consistency method. Empirical evaluations demonstrate significant improvements in reducing hallucination in generation across multiple subjects. Furthermore, the selective training framework mitigates catastrophic forgetting in out-of-distribution benchmarks, addressing a critical limitation in training LLMs. Our findings suggest that such an approach can substantially reduce the dependency on large labeled datasets, paving the way for more scalable and cost-effective language model training.",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/d5ebd84b996491d8ffadefd05a32f8f25085935d.pdf",
      "citation_key": "yeo2024g7d",
      "metadata": {
        "title": "Self-training Large Language Models through Knowledge Detection",
        "authors": [
          "Wei Jie Yeo",
          "Teddy Ferdinan",
          "PrzemysÅ‚aw Kazienko",
          "Ranjan Satapathy",
          "Erik Cambria"
        ],
        "published_date": "2024",
        "abstract": "Large language models (LLMs) often necessitate extensive labeled datasets and training compute to achieve impressive performance across downstream tasks. This paper explores a self-training paradigm, where the LLM autonomously curates its own labels and selectively trains on unknown data samples identified through a reference-free consistency method. Empirical evaluations demonstrate significant improvements in reducing hallucination in generation across multiple subjects. Furthermore, the selective training framework mitigates catastrophic forgetting in out-of-distribution benchmarks, addressing a critical limitation in training LLMs. Our findings suggest that such an approach can substantially reduce the dependency on large labeled datasets, paving the way for more scalable and cost-effective language model training.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/d5ebd84b996491d8ffadefd05a32f8f25085935d.pdf",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "citationCount": 12,
        "score": 12.0,
        "summary": "Large language models (LLMs) often necessitate extensive labeled datasets and training compute to achieve impressive performance across downstream tasks. This paper explores a self-training paradigm, where the LLM autonomously curates its own labels and selectively trains on unknown data samples identified through a reference-free consistency method. Empirical evaluations demonstrate significant improvements in reducing hallucination in generation across multiple subjects. Furthermore, the selective training framework mitigates catastrophic forgetting in out-of-distribution benchmarks, addressing a critical limitation in training LLMs. Our findings suggest that such an approach can substantially reduce the dependency on large labeled datasets, paving the way for more scalable and cost-effective language model training.",
        "keywords": []
      },
      "file_name": "d5ebd84b996491d8ffadefd05a32f8f25085935d.pdf"
    },
    {
      "success": true,
      "doc_id": "afd533edec00c41711b4a31d3a34ebe5",
      "summary": "We introduce PokeLLMon, the first LLM-embodied agent that achieves human-parity performance in tactical battle games, as demonstrated in Pokemon battles. The design of PokeLLMon incorporates three key strategies: (i) In-context reinforcement learning that instantly consumes text-based feedback derived from battles to iteratively refine the policy; (ii) Knowledge-augmented generation that retrieves external knowledge to counteract hallucination and enables the agent to act timely and properly; (iii) Consistent action generation to mitigate the panic switching phenomenon when the agent faces a powerful opponent and wants to elude the battle. We show that online battles against human demonstrates PokeLLMon's human-like battle strategies and just-in-time decision making, achieving 49% of win rate in the Ladder competitions and 56% of win rate in the invited battles. Our implementation and playable battle logs are available at: https://github.com/git-disl/PokeLLMon.",
      "intriguing_abstract": "We introduce PokeLLMon, the first LLM-embodied agent that achieves human-parity performance in tactical battle games, as demonstrated in Pokemon battles. The design of PokeLLMon incorporates three key strategies: (i) In-context reinforcement learning that instantly consumes text-based feedback derived from battles to iteratively refine the policy; (ii) Knowledge-augmented generation that retrieves external knowledge to counteract hallucination and enables the agent to act timely and properly; (iii) Consistent action generation to mitigate the panic switching phenomenon when the agent faces a powerful opponent and wants to elude the battle. We show that online battles against human demonstrates PokeLLMon's human-like battle strategies and just-in-time decision making, achieving 49% of win rate in the Ladder competitions and 56% of win rate in the invited battles. Our implementation and playable battle logs are available at: https://github.com/git-disl/PokeLLMon.",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/5cb8fc293567f4f2930712a3bf7dec97b4dd1776.pdf",
      "citation_key": "hu2024fld",
      "metadata": {
        "title": "PokeLLMon: A Human-Parity Agent for Pokemon Battles with Large Language Models",
        "authors": [
          "Sihao Hu",
          "Tiansheng Huang",
          "Ling Liu"
        ],
        "published_date": "2024",
        "abstract": "We introduce PokeLLMon, the first LLM-embodied agent that achieves human-parity performance in tactical battle games, as demonstrated in Pokemon battles. The design of PokeLLMon incorporates three key strategies: (i) In-context reinforcement learning that instantly consumes text-based feedback derived from battles to iteratively refine the policy; (ii) Knowledge-augmented generation that retrieves external knowledge to counteract hallucination and enables the agent to act timely and properly; (iii) Consistent action generation to mitigate the panic switching phenomenon when the agent faces a powerful opponent and wants to elude the battle. We show that online battles against human demonstrates PokeLLMon's human-like battle strategies and just-in-time decision making, achieving 49% of win rate in the Ladder competitions and 56% of win rate in the invited battles. Our implementation and playable battle logs are available at: https://github.com/git-disl/PokeLLMon.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/5cb8fc293567f4f2930712a3bf7dec97b4dd1776.pdf",
        "venue": "arXiv.org",
        "citationCount": 12,
        "score": 12.0,
        "summary": "We introduce PokeLLMon, the first LLM-embodied agent that achieves human-parity performance in tactical battle games, as demonstrated in Pokemon battles. The design of PokeLLMon incorporates three key strategies: (i) In-context reinforcement learning that instantly consumes text-based feedback derived from battles to iteratively refine the policy; (ii) Knowledge-augmented generation that retrieves external knowledge to counteract hallucination and enables the agent to act timely and properly; (iii) Consistent action generation to mitigate the panic switching phenomenon when the agent faces a powerful opponent and wants to elude the battle. We show that online battles against human demonstrates PokeLLMon's human-like battle strategies and just-in-time decision making, achieving 49% of win rate in the Ladder competitions and 56% of win rate in the invited battles. Our implementation and playable battle logs are available at: https://github.com/git-disl/PokeLLMon.",
        "keywords": []
      },
      "file_name": "5cb8fc293567f4f2930712a3bf7dec97b4dd1776.pdf"
    },
    {
      "success": true,
      "doc_id": "6c7ae739556d109aa61ce2f82e9b4737",
      "summary": "With the rapid development of artificial intelligence technology, especially the increasingly widespread application of question-and-answer systems, high-quality question generation has become a key component in supporting the development of these systems. This article focuses on knowledge-based question generation technology, which aims to enable computers to simulate the human questioning process based on understanding specific texts or knowledge bases. In light of the issues of hallucination and knowledge gaps present in large-scale language models when applied to knowledge-intensive tasks, this paper proposes an enhanced question generation method that incorporates contrastive learning. This method utilizes multiple models to jointly mine domain knowledge and uses contrastive learning to guide the model in reducing noise and hallucinations in generation. Experimental results show that by designing prompts containing contrasting examples, the model's performance in question generation improves considerably, particularly when contrasting instructions and examples are used simultaneously, leading to the highest quality of generated questions and improved accuracy. These results demonstrate that the method proposed in this study, which combines contrasting context and chain-of-thought prompts, can effectively improve both the quality and the practicality of question generation.",
      "intriguing_abstract": "With the rapid development of artificial intelligence technology, especially the increasingly widespread application of question-and-answer systems, high-quality question generation has become a key component in supporting the development of these systems. This article focuses on knowledge-based question generation technology, which aims to enable computers to simulate the human questioning process based on understanding specific texts or knowledge bases. In light of the issues of hallucination and knowledge gaps present in large-scale language models when applied to knowledge-intensive tasks, this paper proposes an enhanced question generation method that incorporates contrastive learning. This method utilizes multiple models to jointly mine domain knowledge and uses contrastive learning to guide the model in reducing noise and hallucinations in generation. Experimental results show that by designing prompts containing contrasting examples, the model's performance in question generation improves considerably, particularly when contrasting instructions and examples are used simultaneously, leading to the highest quality of generated questions and improved accuracy. These results demonstrate that the method proposed in this study, which combines contrasting context and chain-of-thought prompts, can effectively improve both the quality and the practicality of question generation.",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/388e6dbb4b4486e01d4f040684560135a9e1ef71.pdf",
      "citation_key": "zhang2024ia4",
      "metadata": {
        "title": "Contrastive Learning for Knowledge-Based Question Generation in Large Language Models",
        "authors": [
          "Zhenhong Zhang",
          "Jiajing Chen",
          "Weiyan Shi",
          "Lingjie Yi",
          "Chihang Wang",
          "Qian Yu"
        ],
        "published_date": "2024",
        "abstract": "With the rapid development of artificial intelligence technology, especially the increasingly widespread application of question-and-answer systems, high-quality question generation has become a key component in supporting the development of these systems. This article focuses on knowledge-based question generation technology, which aims to enable computers to simulate the human questioning process based on understanding specific texts or knowledge bases. In light of the issues of hallucination and knowledge gaps present in large-scale language models when applied to knowledge-intensive tasks, this paper proposes an enhanced question generation method that incorporates contrastive learning. This method utilizes multiple models to jointly mine domain knowledge and uses contrastive learning to guide the model in reducing noise and hallucinations in generation. Experimental results show that by designing prompts containing contrasting examples, the model's performance in question generation improves considerably, particularly when contrasting instructions and examples are used simultaneously, leading to the highest quality of generated questions and improved accuracy. These results demonstrate that the method proposed in this study, which combines contrasting context and chain-of-thought prompts, can effectively improve both the quality and the practicality of question generation.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/388e6dbb4b4486e01d4f040684560135a9e1ef71.pdf",
        "venue": "2024 5th International Conference on Intelligent Computing and Human-Computer Interaction (ICHCI)",
        "citationCount": 11,
        "score": 11.0,
        "summary": "With the rapid development of artificial intelligence technology, especially the increasingly widespread application of question-and-answer systems, high-quality question generation has become a key component in supporting the development of these systems. This article focuses on knowledge-based question generation technology, which aims to enable computers to simulate the human questioning process based on understanding specific texts or knowledge bases. In light of the issues of hallucination and knowledge gaps present in large-scale language models when applied to knowledge-intensive tasks, this paper proposes an enhanced question generation method that incorporates contrastive learning. This method utilizes multiple models to jointly mine domain knowledge and uses contrastive learning to guide the model in reducing noise and hallucinations in generation. Experimental results show that by designing prompts containing contrasting examples, the model's performance in question generation improves considerably, particularly when contrasting instructions and examples are used simultaneously, leading to the highest quality of generated questions and improved accuracy. These results demonstrate that the method proposed in this study, which combines contrasting context and chain-of-thought prompts, can effectively improve both the quality and the practicality of question generation.",
        "keywords": []
      },
      "file_name": "388e6dbb4b4486e01d4f040684560135a9e1ef71.pdf"
    },
    {
      "success": true,
      "doc_id": "ad060288029cd86be6a8614615fec112",
      "summary": "Despite their remarkable capabilities, Large Language Models (LLMs) are prone to generate responses that contradict verifiable facts, i.e., unfaithful hallucination content. Existing efforts generally focus on optimizing model parameters or editing semantic representations, which compromise the internal factual knowledge of target LLMs. In addition, hallucinations typically exhibit multifaceted patterns in downstream tasks, limiting the model's holistic performance across tasks. In this paper, we propose a Comparator-driven Decoding-Time (CDT) framework to alleviate the response hallucination. Firstly, we construct hallucinatory and truthful comparators with multi-task fine-tuning samples. In this case, we present an instruction prototype-guided mixture of experts strategy to enhance the ability of the corresponding comparators to capture different hallucination or truthfulness patterns in distinct task instructions. CDT constrains next-token predictions to factuality-robust distributions by contrasting the logit differences between the target LLMs and these comparators. Systematic experiments on multiple downstream tasks show that our framework can significantly improve the model performance and response factuality.",
      "intriguing_abstract": "Despite their remarkable capabilities, Large Language Models (LLMs) are prone to generate responses that contradict verifiable facts, i.e., unfaithful hallucination content. Existing efforts generally focus on optimizing model parameters or editing semantic representations, which compromise the internal factual knowledge of target LLMs. In addition, hallucinations typically exhibit multifaceted patterns in downstream tasks, limiting the model's holistic performance across tasks. In this paper, we propose a Comparator-driven Decoding-Time (CDT) framework to alleviate the response hallucination. Firstly, we construct hallucinatory and truthful comparators with multi-task fine-tuning samples. In this case, we present an instruction prototype-guided mixture of experts strategy to enhance the ability of the corresponding comparators to capture different hallucination or truthfulness patterns in distinct task instructions. CDT constrains next-token predictions to factuality-robust distributions by contrasting the logit differences between the target LLMs and these comparators. Systematic experiments on multiple downstream tasks show that our framework can significantly improve the model performance and response factuality.",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/e384f513114d7e6c20d007b4a3ad13fa58cf83dd.pdf",
      "citation_key": "yang2024iia",
      "metadata": {
        "title": "Improving Factuality in Large Language Models via Decoding-Time Hallucinatory and Truthful Comparators",
        "authors": [
          "Dingkang Yang",
          "Dongling Xiao",
          "Jinjie Wei",
          "Mingcheng Li",
          "Zhaoyu Chen",
          "Ke Li",
          "Lihua Zhang"
        ],
        "published_date": "2024",
        "abstract": "Despite their remarkable capabilities, Large Language Models (LLMs) are prone to generate responses that contradict verifiable facts, i.e., unfaithful hallucination content. Existing efforts generally focus on optimizing model parameters or editing semantic representations, which compromise the internal factual knowledge of target LLMs. In addition, hallucinations typically exhibit multifaceted patterns in downstream tasks, limiting the model's holistic performance across tasks. In this paper, we propose a Comparator-driven Decoding-Time (CDT) framework to alleviate the response hallucination. Firstly, we construct hallucinatory and truthful comparators with multi-task fine-tuning samples. In this case, we present an instruction prototype-guided mixture of experts strategy to enhance the ability of the corresponding comparators to capture different hallucination or truthfulness patterns in distinct task instructions. CDT constrains next-token predictions to factuality-robust distributions by contrasting the logit differences between the target LLMs and these comparators. Systematic experiments on multiple downstream tasks show that our framework can significantly improve the model performance and response factuality.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/e384f513114d7e6c20d007b4a3ad13fa58cf83dd.pdf",
        "venue": "AAAI Conference on Artificial Intelligence",
        "citationCount": 11,
        "score": 11.0,
        "summary": "Despite their remarkable capabilities, Large Language Models (LLMs) are prone to generate responses that contradict verifiable facts, i.e., unfaithful hallucination content. Existing efforts generally focus on optimizing model parameters or editing semantic representations, which compromise the internal factual knowledge of target LLMs. In addition, hallucinations typically exhibit multifaceted patterns in downstream tasks, limiting the model's holistic performance across tasks. In this paper, we propose a Comparator-driven Decoding-Time (CDT) framework to alleviate the response hallucination. Firstly, we construct hallucinatory and truthful comparators with multi-task fine-tuning samples. In this case, we present an instruction prototype-guided mixture of experts strategy to enhance the ability of the corresponding comparators to capture different hallucination or truthfulness patterns in distinct task instructions. CDT constrains next-token predictions to factuality-robust distributions by contrasting the logit differences between the target LLMs and these comparators. Systematic experiments on multiple downstream tasks show that our framework can significantly improve the model performance and response factuality.",
        "keywords": []
      },
      "file_name": "e384f513114d7e6c20d007b4a3ad13fa58cf83dd.pdf"
    },
    {
      "success": true,
      "doc_id": "56e0512c74ebcf51e02312eb2c333cc3",
      "summary": "Despite remarkable advancements in mitigating hallucinations in large language models (LLMs) by retrieval augmentation, it remains challenging to measure the reliability of LLMs using static question-answering (QA) data. Specifically, given the potential of data contamination (e.g., leading to memorization), good static benchmark performance does not ensure that model can reliably use the provided evidence for responding, which is essential to avoid hallucination when the required knowledge is new or private. Inspired by adversarial machine learning, we investigate the feasibility of automatically perturbing existing static one for dynamic evaluation. Specifically, this paper presents ReEval, an LLM-based framework using prompt chaining to perturb the original evidence for generating new test cases for evaluating the LLMs' reliability in using new evidence for answering. We implement ReEval using ChatGPT and evaluate the resulting variants of two popular open-domain QA datasets on a collection of LLMs under various prompting settings. Our generated data is human-readable and useful to trigger hallucination in LLM. Accurate models on static data are observed to produce unsupported answers from the perturbed evidence, with pronounced accuracy drops across LLMs including GPT-4. We find that our adversarial examples are transferable across all considered LLMs. The examples generated by a small model can be used to evaluate a much larger model, making our approach cost-effective.",
      "intriguing_abstract": "Despite remarkable advancements in mitigating hallucinations in large language models (LLMs) by retrieval augmentation, it remains challenging to measure the reliability of LLMs using static question-answering (QA) data. Specifically, given the potential of data contamination (e.g., leading to memorization), good static benchmark performance does not ensure that model can reliably use the provided evidence for responding, which is essential to avoid hallucination when the required knowledge is new or private. Inspired by adversarial machine learning, we investigate the feasibility of automatically perturbing existing static one for dynamic evaluation. Specifically, this paper presents ReEval, an LLM-based framework using prompt chaining to perturb the original evidence for generating new test cases for evaluating the LLMs' reliability in using new evidence for answering. We implement ReEval using ChatGPT and evaluate the resulting variants of two popular open-domain QA datasets on a collection of LLMs under various prompting settings. Our generated data is human-readable and useful to trigger hallucination in LLM. Accurate models on static data are observed to produce unsupported answers from the perturbed evidence, with pronounced accuracy drops across LLMs including GPT-4. We find that our adversarial examples are transferable across all considered LLMs. The examples generated by a small model can be used to evaluate a much larger model, making our approach cost-effective.",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/56ff9de0931bd1accb9d4e3f109afcbf31f7df25.pdf",
      "citation_key": "yu2023ine",
      "metadata": {
        "title": "ReEval: Automatic Hallucination Evaluation for Retrieval-Augmented Large Language Models via Transferable Adversarial Attacks",
        "authors": [
          "Xiaodong Yu",
          "Hao Cheng",
          "Xiaodong Liu",
          "Dan Roth",
          "Jianfeng Gao"
        ],
        "published_date": "2023",
        "abstract": "Despite remarkable advancements in mitigating hallucinations in large language models (LLMs) by retrieval augmentation, it remains challenging to measure the reliability of LLMs using static question-answering (QA) data. Specifically, given the potential of data contamination (e.g., leading to memorization), good static benchmark performance does not ensure that model can reliably use the provided evidence for responding, which is essential to avoid hallucination when the required knowledge is new or private. Inspired by adversarial machine learning, we investigate the feasibility of automatically perturbing existing static one for dynamic evaluation. Specifically, this paper presents ReEval, an LLM-based framework using prompt chaining to perturb the original evidence for generating new test cases for evaluating the LLMs' reliability in using new evidence for answering. We implement ReEval using ChatGPT and evaluate the resulting variants of two popular open-domain QA datasets on a collection of LLMs under various prompting settings. Our generated data is human-readable and useful to trigger hallucination in LLM. Accurate models on static data are observed to produce unsupported answers from the perturbed evidence, with pronounced accuracy drops across LLMs including GPT-4. We find that our adversarial examples are transferable across all considered LLMs. The examples generated by a small model can be used to evaluate a much larger model, making our approach cost-effective.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/56ff9de0931bd1accb9d4e3f109afcbf31f7df25.pdf",
        "venue": "NAACL-HLT",
        "citationCount": 22,
        "score": 11.0,
        "summary": "Despite remarkable advancements in mitigating hallucinations in large language models (LLMs) by retrieval augmentation, it remains challenging to measure the reliability of LLMs using static question-answering (QA) data. Specifically, given the potential of data contamination (e.g., leading to memorization), good static benchmark performance does not ensure that model can reliably use the provided evidence for responding, which is essential to avoid hallucination when the required knowledge is new or private. Inspired by adversarial machine learning, we investigate the feasibility of automatically perturbing existing static one for dynamic evaluation. Specifically, this paper presents ReEval, an LLM-based framework using prompt chaining to perturb the original evidence for generating new test cases for evaluating the LLMs' reliability in using new evidence for answering. We implement ReEval using ChatGPT and evaluate the resulting variants of two popular open-domain QA datasets on a collection of LLMs under various prompting settings. Our generated data is human-readable and useful to trigger hallucination in LLM. Accurate models on static data are observed to produce unsupported answers from the perturbed evidence, with pronounced accuracy drops across LLMs including GPT-4. We find that our adversarial examples are transferable across all considered LLMs. The examples generated by a small model can be used to evaluate a much larger model, making our approach cost-effective.",
        "keywords": []
      },
      "file_name": "56ff9de0931bd1accb9d4e3f109afcbf31f7df25.pdf"
    },
    {
      "success": true,
      "doc_id": "997afe8cf37b3d2cf282de7ba870dc6e",
      "summary": "Large Vision Language Models (LVLMs) have recently achieved superior performance in various tasks on natural image and text data, which inspires a large amount of studies for LVLMs fine-tuning and training. Despite their advancements, there has been scant research on the robustness of these models against hallucination when fine-tuned on smaller datasets. In this study, we introduce a new benchmark dataset, the Medical Visual Hallucination Test (MedVH), to evaluate the hallucination of domain-specific LVLMs. MedVH comprises five tasks to evaluate hallucinations in LVLMs within the medical context, which includes tasks for comprehensive understanding of textual and visual input, as well as long textual response generation. Our extensive experiments with both general and medical LVLMs reveal that, although medical LVLMs demonstrate promising performance on standard medical tasks, they are particularly susceptible to hallucinations, often more so than the general models, raising significant concerns about the reliability of these domain-specific models. For medical LVLMs to be truly valuable in real-world applications, they must not only accurately integrate medical knowledge but also maintain robust reasoning abilities to prevent hallucination. Our work paves the way for future evaluations of these studies.",
      "intriguing_abstract": "Large Vision Language Models (LVLMs) have recently achieved superior performance in various tasks on natural image and text data, which inspires a large amount of studies for LVLMs fine-tuning and training. Despite their advancements, there has been scant research on the robustness of these models against hallucination when fine-tuned on smaller datasets. In this study, we introduce a new benchmark dataset, the Medical Visual Hallucination Test (MedVH), to evaluate the hallucination of domain-specific LVLMs. MedVH comprises five tasks to evaluate hallucinations in LVLMs within the medical context, which includes tasks for comprehensive understanding of textual and visual input, as well as long textual response generation. Our extensive experiments with both general and medical LVLMs reveal that, although medical LVLMs demonstrate promising performance on standard medical tasks, they are particularly susceptible to hallucinations, often more so than the general models, raising significant concerns about the reliability of these domain-specific models. For medical LVLMs to be truly valuable in real-world applications, they must not only accurately integrate medical knowledge but also maintain robust reasoning abilities to prevent hallucination. Our work paves the way for future evaluations of these studies.",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/bff123171a5b7bddd699a72daed96b4c56742069.pdf",
      "citation_key": "gu2024eig",
      "metadata": {
        "title": "MedVH: Towards Systematic Evaluation of Hallucination for Large Vision Language Models in the Medical Context",
        "authors": [
          "Zishan Gu",
          "Changchang Yin",
          "Fenglin Liu",
          "Ping Zhang"
        ],
        "published_date": "2024",
        "abstract": "Large Vision Language Models (LVLMs) have recently achieved superior performance in various tasks on natural image and text data, which inspires a large amount of studies for LVLMs fine-tuning and training. Despite their advancements, there has been scant research on the robustness of these models against hallucination when fine-tuned on smaller datasets. In this study, we introduce a new benchmark dataset, the Medical Visual Hallucination Test (MedVH), to evaluate the hallucination of domain-specific LVLMs. MedVH comprises five tasks to evaluate hallucinations in LVLMs within the medical context, which includes tasks for comprehensive understanding of textual and visual input, as well as long textual response generation. Our extensive experiments with both general and medical LVLMs reveal that, although medical LVLMs demonstrate promising performance on standard medical tasks, they are particularly susceptible to hallucinations, often more so than the general models, raising significant concerns about the reliability of these domain-specific models. For medical LVLMs to be truly valuable in real-world applications, they must not only accurately integrate medical knowledge but also maintain robust reasoning abilities to prevent hallucination. Our work paves the way for future evaluations of these studies.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/bff123171a5b7bddd699a72daed96b4c56742069.pdf",
        "venue": "arXiv.org",
        "citationCount": 11,
        "score": 11.0,
        "summary": "Large Vision Language Models (LVLMs) have recently achieved superior performance in various tasks on natural image and text data, which inspires a large amount of studies for LVLMs fine-tuning and training. Despite their advancements, there has been scant research on the robustness of these models against hallucination when fine-tuned on smaller datasets. In this study, we introduce a new benchmark dataset, the Medical Visual Hallucination Test (MedVH), to evaluate the hallucination of domain-specific LVLMs. MedVH comprises five tasks to evaluate hallucinations in LVLMs within the medical context, which includes tasks for comprehensive understanding of textual and visual input, as well as long textual response generation. Our extensive experiments with both general and medical LVLMs reveal that, although medical LVLMs demonstrate promising performance on standard medical tasks, they are particularly susceptible to hallucinations, often more so than the general models, raising significant concerns about the reliability of these domain-specific models. For medical LVLMs to be truly valuable in real-world applications, they must not only accurately integrate medical knowledge but also maintain robust reasoning abilities to prevent hallucination. Our work paves the way for future evaluations of these studies.",
        "keywords": []
      },
      "file_name": "bff123171a5b7bddd699a72daed96b4c56742069.pdf"
    },
    {
      "success": true,
      "doc_id": "c474402a8b24b78928846e1f07f2281d",
      "summary": "Despite the superb performance in many tasks, large language models (LLMs) bear the risk of generating hallucination or even wrong answers when confronted with tasks that demand the accuracy of knowledge. The issue becomes even more noticeable when addressing logic queries that require multiple logic reasoning steps. On the other hand, knowledge graph (KG) based question answering methods are capable of accurately identifying the correct answers with the help of knowledge graph, yet its accuracy could quickly deteriorate when the knowledge graph itself is sparse and incomplete. It remains a critical challenge on how to integrate knowledge graph reasoning with LLMs in a mutually beneficial way so as to mitigate both the hallucination problem of LLMs as well as the incompleteness issue of knowledge graphs. In this paper, we propose 'Logic-Query-of-Thoughts' (LGOT) which is the first of its kind to combine LLMs with knowledge graph based logic query reasoning. LGOT seamlessly combines knowledge graph reasoning and LLMs, effectively breaking down complex logic queries into easy to answer subquestions. Through the utilization of both knowledge graph reasoning and LLMs, it successfully derives answers for each subquestion. By aggregating these results and selecting the highest quality candidate answers for each step, LGOT achieves accurate results to complex questions. Our experimental findings demonstrate substantial performance enhancements, with up to 20% improvement over ChatGPT.",
      "intriguing_abstract": "Despite the superb performance in many tasks, large language models (LLMs) bear the risk of generating hallucination or even wrong answers when confronted with tasks that demand the accuracy of knowledge. The issue becomes even more noticeable when addressing logic queries that require multiple logic reasoning steps. On the other hand, knowledge graph (KG) based question answering methods are capable of accurately identifying the correct answers with the help of knowledge graph, yet its accuracy could quickly deteriorate when the knowledge graph itself is sparse and incomplete. It remains a critical challenge on how to integrate knowledge graph reasoning with LLMs in a mutually beneficial way so as to mitigate both the hallucination problem of LLMs as well as the incompleteness issue of knowledge graphs. In this paper, we propose 'Logic-Query-of-Thoughts' (LGOT) which is the first of its kind to combine LLMs with knowledge graph based logic query reasoning. LGOT seamlessly combines knowledge graph reasoning and LLMs, effectively breaking down complex logic queries into easy to answer subquestions. Through the utilization of both knowledge graph reasoning and LLMs, it successfully derives answers for each subquestion. By aggregating these results and selecting the highest quality candidate answers for each step, LGOT achieves accurate results to complex questions. Our experimental findings demonstrate substantial performance enhancements, with up to 20% improvement over ChatGPT.",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/4582779668ab801f29db457790cd291767510035.pdf",
      "citation_key": "liu2024kf2",
      "metadata": {
        "title": "Logic Query of Thoughts: Guiding Large Language Models to Answer Complex Logic Queries with Knowledge Graphs",
        "authors": [
          "Lihui Liu",
          "Zihao Wang",
          "Ruizhong Qiu",
          "Yikun Ban",
          "Hanghang Tong"
        ],
        "published_date": "2024",
        "abstract": "Despite the superb performance in many tasks, large language models (LLMs) bear the risk of generating hallucination or even wrong answers when confronted with tasks that demand the accuracy of knowledge. The issue becomes even more noticeable when addressing logic queries that require multiple logic reasoning steps. On the other hand, knowledge graph (KG) based question answering methods are capable of accurately identifying the correct answers with the help of knowledge graph, yet its accuracy could quickly deteriorate when the knowledge graph itself is sparse and incomplete. It remains a critical challenge on how to integrate knowledge graph reasoning with LLMs in a mutually beneficial way so as to mitigate both the hallucination problem of LLMs as well as the incompleteness issue of knowledge graphs. In this paper, we propose 'Logic-Query-of-Thoughts' (LGOT) which is the first of its kind to combine LLMs with knowledge graph based logic query reasoning. LGOT seamlessly combines knowledge graph reasoning and LLMs, effectively breaking down complex logic queries into easy to answer subquestions. Through the utilization of both knowledge graph reasoning and LLMs, it successfully derives answers for each subquestion. By aggregating these results and selecting the highest quality candidate answers for each step, LGOT achieves accurate results to complex questions. Our experimental findings demonstrate substantial performance enhancements, with up to 20% improvement over ChatGPT.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/4582779668ab801f29db457790cd291767510035.pdf",
        "venue": "arXiv.org",
        "citationCount": 11,
        "score": 11.0,
        "summary": "Despite the superb performance in many tasks, large language models (LLMs) bear the risk of generating hallucination or even wrong answers when confronted with tasks that demand the accuracy of knowledge. The issue becomes even more noticeable when addressing logic queries that require multiple logic reasoning steps. On the other hand, knowledge graph (KG) based question answering methods are capable of accurately identifying the correct answers with the help of knowledge graph, yet its accuracy could quickly deteriorate when the knowledge graph itself is sparse and incomplete. It remains a critical challenge on how to integrate knowledge graph reasoning with LLMs in a mutually beneficial way so as to mitigate both the hallucination problem of LLMs as well as the incompleteness issue of knowledge graphs. In this paper, we propose 'Logic-Query-of-Thoughts' (LGOT) which is the first of its kind to combine LLMs with knowledge graph based logic query reasoning. LGOT seamlessly combines knowledge graph reasoning and LLMs, effectively breaking down complex logic queries into easy to answer subquestions. Through the utilization of both knowledge graph reasoning and LLMs, it successfully derives answers for each subquestion. By aggregating these results and selecting the highest quality candidate answers for each step, LGOT achieves accurate results to complex questions. Our experimental findings demonstrate substantial performance enhancements, with up to 20% improvement over ChatGPT.",
        "keywords": []
      },
      "file_name": "4582779668ab801f29db457790cd291767510035.pdf"
    },
    {
      "success": true,
      "doc_id": "e2fdc3345d7a316eff19d4f8701aa3df",
      "summary": "Large language models (LLMs) such as ChatGPT have emerged as potential game-changers in nursing, aiding in patient education, diagnostic assistance, treatment recommendations, and administrative task efficiency. While these advancements signal promising strides in healthcare, integrated LLMs are not without challenges, particularly artificial intelligence hallucination and data privacy concerns. Methodologies such as prompt engineering, temperature adjustments, model fine-tuning, and local deployment are proposed to refine the accuracy of LLMs and ensure data security. While LLMs offer transformative potential, it is imperative to acknowledge that they cannot substitute the intricate expertise of human professionals in the clinical field, advocating for a synergistic approach in patient care.",
      "intriguing_abstract": "Large language models (LLMs) such as ChatGPT have emerged as potential game-changers in nursing, aiding in patient education, diagnostic assistance, treatment recommendations, and administrative task efficiency. While these advancements signal promising strides in healthcare, integrated LLMs are not without challenges, particularly artificial intelligence hallucination and data privacy concerns. Methodologies such as prompt engineering, temperature adjustments, model fine-tuning, and local deployment are proposed to refine the accuracy of LLMs and ensure data security. While LLMs offer transformative potential, it is imperative to acknowledge that they cannot substitute the intricate expertise of human professionals in the clinical field, advocating for a synergistic approach in patient care.",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/e7d5f2da48b9141e18de9ea57ed8a4cebb6a09cd.pdf",
      "citation_key": "woo2024dtm",
      "metadata": {
        "title": "Transforming nursing with large language models: from concept to practice.",
        "authors": [
          "B. Woo",
          "Tom Huynh",
          "Arthur Tang",
          "Nhat Bui",
          "Giang Nguyen",
          "W. Tam"
        ],
        "published_date": "2024",
        "abstract": "Large language models (LLMs) such as ChatGPT have emerged as potential game-changers in nursing, aiding in patient education, diagnostic assistance, treatment recommendations, and administrative task efficiency. While these advancements signal promising strides in healthcare, integrated LLMs are not without challenges, particularly artificial intelligence hallucination and data privacy concerns. Methodologies such as prompt engineering, temperature adjustments, model fine-tuning, and local deployment are proposed to refine the accuracy of LLMs and ensure data security. While LLMs offer transformative potential, it is imperative to acknowledge that they cannot substitute the intricate expertise of human professionals in the clinical field, advocating for a synergistic approach in patient care.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/e7d5f2da48b9141e18de9ea57ed8a4cebb6a09cd.pdf",
        "venue": "European Journal of Cardiovascular Nursing",
        "citationCount": 11,
        "score": 11.0,
        "summary": "Large language models (LLMs) such as ChatGPT have emerged as potential game-changers in nursing, aiding in patient education, diagnostic assistance, treatment recommendations, and administrative task efficiency. While these advancements signal promising strides in healthcare, integrated LLMs are not without challenges, particularly artificial intelligence hallucination and data privacy concerns. Methodologies such as prompt engineering, temperature adjustments, model fine-tuning, and local deployment are proposed to refine the accuracy of LLMs and ensure data security. While LLMs offer transformative potential, it is imperative to acknowledge that they cannot substitute the intricate expertise of human professionals in the clinical field, advocating for a synergistic approach in patient care.",
        "keywords": []
      },
      "file_name": "e7d5f2da48b9141e18de9ea57ed8a4cebb6a09cd.pdf"
    },
    {
      "success": true,
      "doc_id": "a3131d7c2cc59010f6bd386e4c0a9543",
      "summary": "Despite large language modelsâ€™ (LLMsâ€™) recent advancements, their bias and hallucination issues persist, and their ability to offer consistent and preferential rankings remains underexplored. This study investigates the capacity of LLMs to provide consistent ordinal preferences, a crucial aspect in scenarios lacking absolute answers. We introduce a formalization of consistency based on order theory, outlining criteria such as transitivity, asymmetry, reversibility, and independence from irrelevant alternatives. Our diagnostic experiments on selected state-of-the-art LLMs reveal their inability to meet these criteria, indicating a strong positional bias and poor transitivity, with preferences easily swayed by irrelevant alternatives. These findings highlight a significant inconsistency in LLM-generated preferential rankings, underscoring the need for further research to address these limitations.",
      "intriguing_abstract": "Despite large language modelsâ€™ (LLMsâ€™) recent advancements, their bias and hallucination issues persist, and their ability to offer consistent and preferential rankings remains underexplored. This study investigates the capacity of LLMs to provide consistent ordinal preferences, a crucial aspect in scenarios lacking absolute answers. We introduce a formalization of consistency based on order theory, outlining criteria such as transitivity, asymmetry, reversibility, and independence from irrelevant alternatives. Our diagnostic experiments on selected state-of-the-art LLMs reveal their inability to meet these criteria, indicating a strong positional bias and poor transitivity, with preferences easily swayed by irrelevant alternatives. These findings highlight a significant inconsistency in LLM-generated preferential rankings, underscoring the need for further research to address these limitations.",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/36ffd74db97f98f121a6c2954bf98c93dad5d2ce.pdf",
      "citation_key": "zhao20246wi",
      "metadata": {
        "title": "Measuring the Inconsistency of Large Language Models in Preferential Ranking",
        "authors": [
          "Xiutian Zhao",
          "Ke Wang",
          "Wei Peng"
        ],
        "published_date": "2024",
        "abstract": "Despite large language modelsâ€™ (LLMsâ€™) recent advancements, their bias and hallucination issues persist, and their ability to offer consistent and preferential rankings remains underexplored. This study investigates the capacity of LLMs to provide consistent ordinal preferences, a crucial aspect in scenarios lacking absolute answers. We introduce a formalization of consistency based on order theory, outlining criteria such as transitivity, asymmetry, reversibility, and independence from irrelevant alternatives. Our diagnostic experiments on selected state-of-the-art LLMs reveal their inability to meet these criteria, indicating a strong positional bias and poor transitivity, with preferences easily swayed by irrelevant alternatives. These findings highlight a significant inconsistency in LLM-generated preferential rankings, underscoring the need for further research to address these limitations.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/36ffd74db97f98f121a6c2954bf98c93dad5d2ce.pdf",
        "venue": "KNOWLLM",
        "citationCount": 11,
        "score": 11.0,
        "summary": "Despite large language modelsâ€™ (LLMsâ€™) recent advancements, their bias and hallucination issues persist, and their ability to offer consistent and preferential rankings remains underexplored. This study investigates the capacity of LLMs to provide consistent ordinal preferences, a crucial aspect in scenarios lacking absolute answers. We introduce a formalization of consistency based on order theory, outlining criteria such as transitivity, asymmetry, reversibility, and independence from irrelevant alternatives. Our diagnostic experiments on selected state-of-the-art LLMs reveal their inability to meet these criteria, indicating a strong positional bias and poor transitivity, with preferences easily swayed by irrelevant alternatives. These findings highlight a significant inconsistency in LLM-generated preferential rankings, underscoring the need for further research to address these limitations.",
        "keywords": []
      },
      "file_name": "36ffd74db97f98f121a6c2954bf98c93dad5d2ce.pdf"
    },
    {
      "success": true,
      "doc_id": "81d3a5230256d277429873585a2048dc",
      "summary": "Temporal Knowledge Graph Question Answering (TKGQA) aims to answer temporal questions using knowledge in Temporal Knowledge Graphs (TKGs). Previous works employ pre-trained TKG embeddings or graph neural networks to incorporate the knowledge of TKGs. However, these methods fail to fully understand the complex semantic information of time constraints in questions.In contrast, Large Language Models (LLMs) have shown exceptional performance in knowledge graph reasoning, unifying both semantic understanding and structural reasoning. To further enhance LLMsâ€™ temporal reasoning ability, this paper aims to integrate relevant temporal knowledge from TKGs into LLMs through a Time-aware Retrieve-Rewrite-Retrieve-Rerank framework, which we named TimeR^4.Specifically, to reduce temporal hallucination in LLMs, we propose a retrieve-rewrite module to rewrite questions using background knowledge stored in the TKGs, thereby acquiring explicit time constraints. Then, we implement a retrieve-rerank module aimed at retrieving semantically and temporally relevant facts from the TKGs and reranking them according to the temporal constraints.To achieve this, we fine-tune a retriever using the contrastive time-aware learning framework.Our approach achieves great improvements, with relative gains of 47.8% and 22.5% on two datasets, underscoring its effectiveness in boosting the temporal reasoning abilities of LLMs. Our code is available at https://github.com/qianxinying/TimeR4.",
      "intriguing_abstract": "Temporal Knowledge Graph Question Answering (TKGQA) aims to answer temporal questions using knowledge in Temporal Knowledge Graphs (TKGs). Previous works employ pre-trained TKG embeddings or graph neural networks to incorporate the knowledge of TKGs. However, these methods fail to fully understand the complex semantic information of time constraints in questions.In contrast, Large Language Models (LLMs) have shown exceptional performance in knowledge graph reasoning, unifying both semantic understanding and structural reasoning. To further enhance LLMsâ€™ temporal reasoning ability, this paper aims to integrate relevant temporal knowledge from TKGs into LLMs through a Time-aware Retrieve-Rewrite-Retrieve-Rerank framework, which we named TimeR^4.Specifically, to reduce temporal hallucination in LLMs, we propose a retrieve-rewrite module to rewrite questions using background knowledge stored in the TKGs, thereby acquiring explicit time constraints. Then, we implement a retrieve-rerank module aimed at retrieving semantically and temporally relevant facts from the TKGs and reranking them according to the temporal constraints.To achieve this, we fine-tune a retriever using the contrastive time-aware learning framework.Our approach achieves great improvements, with relative gains of 47.8% and 22.5% on two datasets, underscoring its effectiveness in boosting the temporal reasoning abilities of LLMs. Our code is available at https://github.com/qianxinying/TimeR4.",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/4cf85f9436bb8b5b1c68715f44d6b67254413ef8.pdf",
      "citation_key": "qian2024mj9",
      "metadata": {
        "title": "TimeR^4 : Time-aware Retrieval-Augmented Large Language Models for Temporal Knowledge Graph Question Answering",
        "authors": [
          "Xinying Qian",
          "Ying Zhang",
          "Yu Zhao",
          "Baohang Zhou",
          "Xuhui Sui",
          "Li Zhang",
          "Kehui Song"
        ],
        "published_date": "2024",
        "abstract": "Temporal Knowledge Graph Question Answering (TKGQA) aims to answer temporal questions using knowledge in Temporal Knowledge Graphs (TKGs). Previous works employ pre-trained TKG embeddings or graph neural networks to incorporate the knowledge of TKGs. However, these methods fail to fully understand the complex semantic information of time constraints in questions.In contrast, Large Language Models (LLMs) have shown exceptional performance in knowledge graph reasoning, unifying both semantic understanding and structural reasoning. To further enhance LLMsâ€™ temporal reasoning ability, this paper aims to integrate relevant temporal knowledge from TKGs into LLMs through a Time-aware Retrieve-Rewrite-Retrieve-Rerank framework, which we named TimeR^4.Specifically, to reduce temporal hallucination in LLMs, we propose a retrieve-rewrite module to rewrite questions using background knowledge stored in the TKGs, thereby acquiring explicit time constraints. Then, we implement a retrieve-rerank module aimed at retrieving semantically and temporally relevant facts from the TKGs and reranking them according to the temporal constraints.To achieve this, we fine-tune a retriever using the contrastive time-aware learning framework.Our approach achieves great improvements, with relative gains of 47.8% and 22.5% on two datasets, underscoring its effectiveness in boosting the temporal reasoning abilities of LLMs. Our code is available at https://github.com/qianxinying/TimeR4.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/4cf85f9436bb8b5b1c68715f44d6b67254413ef8.pdf",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "citationCount": 11,
        "score": 11.0,
        "summary": "Temporal Knowledge Graph Question Answering (TKGQA) aims to answer temporal questions using knowledge in Temporal Knowledge Graphs (TKGs). Previous works employ pre-trained TKG embeddings or graph neural networks to incorporate the knowledge of TKGs. However, these methods fail to fully understand the complex semantic information of time constraints in questions.In contrast, Large Language Models (LLMs) have shown exceptional performance in knowledge graph reasoning, unifying both semantic understanding and structural reasoning. To further enhance LLMsâ€™ temporal reasoning ability, this paper aims to integrate relevant temporal knowledge from TKGs into LLMs through a Time-aware Retrieve-Rewrite-Retrieve-Rerank framework, which we named TimeR^4.Specifically, to reduce temporal hallucination in LLMs, we propose a retrieve-rewrite module to rewrite questions using background knowledge stored in the TKGs, thereby acquiring explicit time constraints. Then, we implement a retrieve-rerank module aimed at retrieving semantically and temporally relevant facts from the TKGs and reranking them according to the temporal constraints.To achieve this, we fine-tune a retriever using the contrastive time-aware learning framework.Our approach achieves great improvements, with relative gains of 47.8% and 22.5% on two datasets, underscoring its effectiveness in boosting the temporal reasoning abilities of LLMs. Our code is available at https://github.com/qianxinying/TimeR4.",
        "keywords": []
      },
      "file_name": "4cf85f9436bb8b5b1c68715f44d6b67254413ef8.pdf"
    },
    {
      "success": true,
      "doc_id": "c9817e68ee43a54e21405825132906bb",
      "summary": "PURPOSE\nThe purpose of this study was to evaluate the effectiveness of an Artificial Intelligence-Large Language Model (AI-LLM) at improving the readability of knee radiology reports.\n\n\nMETHODS\nReports of 100 knee X-rays, 100 knee computed tomography (CT) scans and 100 knee magnetic resonance imaging (MRI) scans were retrieved. The following prompt command was inserted into the AI-LLM: 'Explain this radiology report to a patient in layman's terms in the second person:[Report Text]'. The Flesch-Kincaid reading level (FKRL) score, Flesch reading ease (FRE) score and report length were calculated for the original radiology report and the AI-LLM generated report. Any 'hallucination' or inaccurate text produced by the AI-LLM-generated report was documented.\n\n\nRESULTS\nStatistically significant improvements in mean FKRL scores in the AI-LLM generated X-ray report (12.7â€‰Â±â€‰1.0-7.2â€‰Â±â€‰0.6), CT report (13.4â€‰Â±â€‰1.0-7.5â€‰Â±â€‰0.5) and MRI report (13.5â€‰Â±â€‰0.9-7.5â€‰Â±â€‰0.6) were observed. Statistically significant improvements in mean FRE scores in the AI-LLM generated X-ray report (39.5â€‰Â±â€‰7.5-76.8â€‰Â±â€‰5.1), CT report (27.3â€‰Â±â€‰5.9-73.1â€‰Â±â€‰5.6) and MRI report (26.8â€‰Â±â€‰6.4-73.4â€‰Â±â€‰5.0) were observed. Superior FKRL scores and FRE scores were observed in the AI-LLM-generated X-ray report compared to the AI-LLM-generated CT report and MRI report, pâ€‰<â€‰0.001. The hallucination rates in the AI-LLM generated X-ray report, CT report and MRI report were 2%, 5% and 5%, respectively.\n\n\nCONCLUSIONS\nThis study highlights the promising use of AI-LLMs as an innovative, patient-centred strategy to improve the readability of knee radiology reports. The clinical relevance of this study is that an AI-LLM-generated knee radiology report may enhance patients' understanding of their imaging reports, potentially reducing the responder burden placed on the ordering physicians. However, due to the 'hallucinations' produced by the AI-LLM-generated report, the ordering physician must always engage in a collaborative discussion with the patient regarding both reports and the corresponding images.\n\n\nLEVEL OF EVIDENCE\nLevel IV.",
      "intriguing_abstract": "PURPOSE\nThe purpose of this study was to evaluate the effectiveness of an Artificial Intelligence-Large Language Model (AI-LLM) at improving the readability of knee radiology reports.\n\n\nMETHODS\nReports of 100 knee X-rays, 100 knee computed tomography (CT) scans and 100 knee magnetic resonance imaging (MRI) scans were retrieved. The following prompt command was inserted into the AI-LLM: 'Explain this radiology report to a patient in layman's terms in the second person:[Report Text]'. The Flesch-Kincaid reading level (FKRL) score, Flesch reading ease (FRE) score and report length were calculated for the original radiology report and the AI-LLM generated report. Any 'hallucination' or inaccurate text produced by the AI-LLM-generated report was documented.\n\n\nRESULTS\nStatistically significant improvements in mean FKRL scores in the AI-LLM generated X-ray report (12.7â€‰Â±â€‰1.0-7.2â€‰Â±â€‰0.6), CT report (13.4â€‰Â±â€‰1.0-7.5â€‰Â±â€‰0.5) and MRI report (13.5â€‰Â±â€‰0.9-7.5â€‰Â±â€‰0.6) were observed. Statistically significant improvements in mean FRE scores in the AI-LLM generated X-ray report (39.5â€‰Â±â€‰7.5-76.8â€‰Â±â€‰5.1), CT report (27.3â€‰Â±â€‰5.9-73.1â€‰Â±â€‰5.6) and MRI report (26.8â€‰Â±â€‰6.4-73.4â€‰Â±â€‰5.0) were observed. Superior FKRL scores and FRE scores were observed in the AI-LLM-generated X-ray report compared to the AI-LLM-generated CT report and MRI report, pâ€‰<â€‰0.001. The hallucination rates in the AI-LLM generated X-ray report, CT report and MRI report were 2%, 5% and 5%, respectively.\n\n\nCONCLUSIONS\nThis study highlights the promising use of AI-LLMs as an innovative, patient-centred strategy to improve the readability of knee radiology reports. The clinical relevance of this study is that an AI-LLM-generated knee radiology report may enhance patients' understanding of their imaging reports, potentially reducing the responder burden placed on the ordering physicians. However, due to the 'hallucinations' produced by the AI-LLM-generated report, the ordering physician must always engage in a collaborative discussion with the patient regarding both reports and the corresponding images.\n\n\nLEVEL OF EVIDENCE\nLevel IV.",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/2b277717203bb5354e7d41e79a35c59e34fa6778.pdf",
      "citation_key": "butler20242xs",
      "metadata": {
        "title": "From technical to understandable: Artificial Intelligence Large Language Models improve the readability of knee radiology reports.",
        "authors": [
          "J. Butler",
          "James Puleo",
          "Michael Harrington",
          "J. Dahmen",
          "Andrew J. Rosenbaum",
          "G. Kerkhoffs",
          "J. Kennedy"
        ],
        "published_date": "2024",
        "abstract": "PURPOSE\nThe purpose of this study was to evaluate the effectiveness of an Artificial Intelligence-Large Language Model (AI-LLM) at improving the readability of knee radiology reports.\n\n\nMETHODS\nReports of 100 knee X-rays, 100 knee computed tomography (CT) scans and 100 knee magnetic resonance imaging (MRI) scans were retrieved. The following prompt command was inserted into the AI-LLM: 'Explain this radiology report to a patient in layman's terms in the second person:[Report Text]'. The Flesch-Kincaid reading level (FKRL) score, Flesch reading ease (FRE) score and report length were calculated for the original radiology report and the AI-LLM generated report. Any 'hallucination' or inaccurate text produced by the AI-LLM-generated report was documented.\n\n\nRESULTS\nStatistically significant improvements in mean FKRL scores in the AI-LLM generated X-ray report (12.7â€‰Â±â€‰1.0-7.2â€‰Â±â€‰0.6), CT report (13.4â€‰Â±â€‰1.0-7.5â€‰Â±â€‰0.5) and MRI report (13.5â€‰Â±â€‰0.9-7.5â€‰Â±â€‰0.6) were observed. Statistically significant improvements in mean FRE scores in the AI-LLM generated X-ray report (39.5â€‰Â±â€‰7.5-76.8â€‰Â±â€‰5.1), CT report (27.3â€‰Â±â€‰5.9-73.1â€‰Â±â€‰5.6) and MRI report (26.8â€‰Â±â€‰6.4-73.4â€‰Â±â€‰5.0) were observed. Superior FKRL scores and FRE scores were observed in the AI-LLM-generated X-ray report compared to the AI-LLM-generated CT report and MRI report, pâ€‰<â€‰0.001. The hallucination rates in the AI-LLM generated X-ray report, CT report and MRI report were 2%, 5% and 5%, respectively.\n\n\nCONCLUSIONS\nThis study highlights the promising use of AI-LLMs as an innovative, patient-centred strategy to improve the readability of knee radiology reports. The clinical relevance of this study is that an AI-LLM-generated knee radiology report may enhance patients' understanding of their imaging reports, potentially reducing the responder burden placed on the ordering physicians. However, due to the 'hallucinations' produced by the AI-LLM-generated report, the ordering physician must always engage in a collaborative discussion with the patient regarding both reports and the corresponding images.\n\n\nLEVEL OF EVIDENCE\nLevel IV.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/2b277717203bb5354e7d41e79a35c59e34fa6778.pdf",
        "venue": "Knee Surgery, Sports Traumatology, Arthroscopy",
        "citationCount": 11,
        "score": 11.0,
        "summary": "PURPOSE\nThe purpose of this study was to evaluate the effectiveness of an Artificial Intelligence-Large Language Model (AI-LLM) at improving the readability of knee radiology reports.\n\n\nMETHODS\nReports of 100 knee X-rays, 100 knee computed tomography (CT) scans and 100 knee magnetic resonance imaging (MRI) scans were retrieved. The following prompt command was inserted into the AI-LLM: 'Explain this radiology report to a patient in layman's terms in the second person:[Report Text]'. The Flesch-Kincaid reading level (FKRL) score, Flesch reading ease (FRE) score and report length were calculated for the original radiology report and the AI-LLM generated report. Any 'hallucination' or inaccurate text produced by the AI-LLM-generated report was documented.\n\n\nRESULTS\nStatistically significant improvements in mean FKRL scores in the AI-LLM generated X-ray report (12.7â€‰Â±â€‰1.0-7.2â€‰Â±â€‰0.6), CT report (13.4â€‰Â±â€‰1.0-7.5â€‰Â±â€‰0.5) and MRI report (13.5â€‰Â±â€‰0.9-7.5â€‰Â±â€‰0.6) were observed. Statistically significant improvements in mean FRE scores in the AI-LLM generated X-ray report (39.5â€‰Â±â€‰7.5-76.8â€‰Â±â€‰5.1), CT report (27.3â€‰Â±â€‰5.9-73.1â€‰Â±â€‰5.6) and MRI report (26.8â€‰Â±â€‰6.4-73.4â€‰Â±â€‰5.0) were observed. Superior FKRL scores and FRE scores were observed in the AI-LLM-generated X-ray report compared to the AI-LLM-generated CT report and MRI report, pâ€‰<â€‰0.001. The hallucination rates in the AI-LLM generated X-ray report, CT report and MRI report were 2%, 5% and 5%, respectively.\n\n\nCONCLUSIONS\nThis study highlights the promising use of AI-LLMs as an innovative, patient-centred strategy to improve the readability of knee radiology reports. The clinical relevance of this study is that an AI-LLM-generated knee radiology report may enhance patients' understanding of their imaging reports, potentially reducing the responder burden placed on the ordering physicians. However, due to the 'hallucinations' produced by the AI-LLM-generated report, the ordering physician must always engage in a collaborative discussion with the patient regarding both reports and the corresponding images.\n\n\nLEVEL OF EVIDENCE\nLevel IV.",
        "keywords": []
      },
      "file_name": "2b277717203bb5354e7d41e79a35c59e34fa6778.pdf"
    },
    {
      "success": true,
      "doc_id": "1475aa300a5e2fc81ed63bf68a528423",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/dd20cfadf992986b5d71b3a44b5a8660f0d68671.pdf",
      "citation_key": "sahoo202420w",
      "metadata": {
        "title": "Addressing Bias and Hallucination in Large Language Models",
        "authors": [
          "N. R. Sahoo",
          "Ashita Saxena",
          "Kishan Maharaj",
          "Arif Ahmad",
          "Abhijit Mishra",
          "Pushpak Bhattacharyya"
        ],
        "published_date": "2024",
        "abstract": "",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/dd20cfadf992986b5d71b3a44b5a8660f0d68671.pdf",
        "venue": "International Conference on Language Resources and Evaluation",
        "citationCount": 10,
        "score": 10.0,
        "summary": "",
        "keywords": []
      },
      "file_name": "dd20cfadf992986b5d71b3a44b5a8660f0d68671.pdf"
    },
    {
      "success": true,
      "doc_id": "a1d6d2bea0d849f75342645c67f46d4f",
      "summary": "Medical Large Language Models (MLLMs) have demonstrated potential in healthcare applications, yet their propensity for hallucinations -- generating medically implausible or inaccurate information -- presents substantial risks to patient care. This paper introduces MedHallBench, a comprehensive benchmark framework for evaluating and mitigating hallucinations in MLLMs. Our methodology integrates expert-validated medical case scenarios with established medical databases to create a robust evaluation dataset. The framework employs a sophisticated measurement system that combines automated ACHMI (Automatic Caption Hallucination Measurement in Medical Imaging) scoring with rigorous clinical expert evaluations and utilizes reinforcement learning methods to achieve automatic annotation. Through an optimized reinforcement learning from human feedback (RLHF) training pipeline specifically designed for medical applications, MedHallBench enables thorough evaluation of MLLMs across diverse clinical contexts while maintaining stringent accuracy standards. We conducted comparative experiments involving various models, utilizing the benchmark to establish a baseline for widely adopted large language models (LLMs). Our findings indicate that ACHMI provides a more nuanced understanding of the effects of hallucinations compared to traditional metrics, thereby highlighting its advantages in hallucination assessment. This research establishes a foundational framework for enhancing MLLMs' reliability in healthcare settings and presents actionable strategies for addressing the critical challenge of AI hallucinations in medical applications.",
      "intriguing_abstract": "Medical Large Language Models (MLLMs) have demonstrated potential in healthcare applications, yet their propensity for hallucinations -- generating medically implausible or inaccurate information -- presents substantial risks to patient care. This paper introduces MedHallBench, a comprehensive benchmark framework for evaluating and mitigating hallucinations in MLLMs. Our methodology integrates expert-validated medical case scenarios with established medical databases to create a robust evaluation dataset. The framework employs a sophisticated measurement system that combines automated ACHMI (Automatic Caption Hallucination Measurement in Medical Imaging) scoring with rigorous clinical expert evaluations and utilizes reinforcement learning methods to achieve automatic annotation. Through an optimized reinforcement learning from human feedback (RLHF) training pipeline specifically designed for medical applications, MedHallBench enables thorough evaluation of MLLMs across diverse clinical contexts while maintaining stringent accuracy standards. We conducted comparative experiments involving various models, utilizing the benchmark to establish a baseline for widely adopted large language models (LLMs). Our findings indicate that ACHMI provides a more nuanced understanding of the effects of hallucinations compared to traditional metrics, thereby highlighting its advantages in hallucination assessment. This research establishes a foundational framework for enhancing MLLMs' reliability in healthcare settings and presents actionable strategies for addressing the critical challenge of AI hallucinations in medical applications.",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/7112d0d36b960b590f55569bc294b2190d288860.pdf",
      "citation_key": "zuo20242i0",
      "metadata": {
        "title": "MedHallBench: A New Benchmark for Assessing Hallucination in Medical Large Language Models",
        "authors": [
          "Kaiwen Zuo",
          "Yirui Jiang"
        ],
        "published_date": "2024",
        "abstract": "Medical Large Language Models (MLLMs) have demonstrated potential in healthcare applications, yet their propensity for hallucinations -- generating medically implausible or inaccurate information -- presents substantial risks to patient care. This paper introduces MedHallBench, a comprehensive benchmark framework for evaluating and mitigating hallucinations in MLLMs. Our methodology integrates expert-validated medical case scenarios with established medical databases to create a robust evaluation dataset. The framework employs a sophisticated measurement system that combines automated ACHMI (Automatic Caption Hallucination Measurement in Medical Imaging) scoring with rigorous clinical expert evaluations and utilizes reinforcement learning methods to achieve automatic annotation. Through an optimized reinforcement learning from human feedback (RLHF) training pipeline specifically designed for medical applications, MedHallBench enables thorough evaluation of MLLMs across diverse clinical contexts while maintaining stringent accuracy standards. We conducted comparative experiments involving various models, utilizing the benchmark to establish a baseline for widely adopted large language models (LLMs). Our findings indicate that ACHMI provides a more nuanced understanding of the effects of hallucinations compared to traditional metrics, thereby highlighting its advantages in hallucination assessment. This research establishes a foundational framework for enhancing MLLMs' reliability in healthcare settings and presents actionable strategies for addressing the critical challenge of AI hallucinations in medical applications.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/7112d0d36b960b590f55569bc294b2190d288860.pdf",
        "venue": "arXiv.org",
        "citationCount": 10,
        "score": 10.0,
        "summary": "Medical Large Language Models (MLLMs) have demonstrated potential in healthcare applications, yet their propensity for hallucinations -- generating medically implausible or inaccurate information -- presents substantial risks to patient care. This paper introduces MedHallBench, a comprehensive benchmark framework for evaluating and mitigating hallucinations in MLLMs. Our methodology integrates expert-validated medical case scenarios with established medical databases to create a robust evaluation dataset. The framework employs a sophisticated measurement system that combines automated ACHMI (Automatic Caption Hallucination Measurement in Medical Imaging) scoring with rigorous clinical expert evaluations and utilizes reinforcement learning methods to achieve automatic annotation. Through an optimized reinforcement learning from human feedback (RLHF) training pipeline specifically designed for medical applications, MedHallBench enables thorough evaluation of MLLMs across diverse clinical contexts while maintaining stringent accuracy standards. We conducted comparative experiments involving various models, utilizing the benchmark to establish a baseline for widely adopted large language models (LLMs). Our findings indicate that ACHMI provides a more nuanced understanding of the effects of hallucinations compared to traditional metrics, thereby highlighting its advantages in hallucination assessment. This research establishes a foundational framework for enhancing MLLMs' reliability in healthcare settings and presents actionable strategies for addressing the critical challenge of AI hallucinations in medical applications.",
        "keywords": []
      },
      "file_name": "7112d0d36b960b590f55569bc294b2190d288860.pdf"
    },
    {
      "success": true,
      "doc_id": "d22f05130edc1c1844c79cad55bdc8d1",
      "summary": "Generative artificial intelligence and large language models are the continuation of a technological revolution in information processing that began with the invention of the transistor in 1947. These technologies, driven by transformer architectures for artificial neural networks, are poised to broadly influence society. It is already apparent that these technologies will be adapted to drive innovation in education. Medical education is a high-risk activity: Information that is incorrectly taught to a student may go unrecognized for years until a relevant clinical situation appears in which that error can lead to patient harm. In this article, I discuss the principal limitations to the use of generative artificial intelligence in medical education-hallucination, bias, cost, and security-and suggest some approaches to confronting these problems. Additionally, I identify the potential applications of generative artificial intelligence to medical education, including personalized instruction, simulation, feedback, evaluation, augmentation of qualitative research, and performance of critical assessment of the existing scientific literature.",
      "intriguing_abstract": "Generative artificial intelligence and large language models are the continuation of a technological revolution in information processing that began with the invention of the transistor in 1947. These technologies, driven by transformer architectures for artificial neural networks, are poised to broadly influence society. It is already apparent that these technologies will be adapted to drive innovation in education. Medical education is a high-risk activity: Information that is incorrectly taught to a student may go unrecognized for years until a relevant clinical situation appears in which that error can lead to patient harm. In this article, I discuss the principal limitations to the use of generative artificial intelligence in medical education-hallucination, bias, cost, and security-and suggest some approaches to confronting these problems. Additionally, I identify the potential applications of generative artificial intelligence to medical education, including personalized instruction, simulation, feedback, evaluation, augmentation of qualitative research, and performance of critical assessment of the existing scientific literature.",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/5be8fd7aa6564ea1884626c16bcac36508f79ff1.pdf",
      "citation_key": "parente2024vlq",
      "metadata": {
        "title": "Generative Artificial Intelligence and Large Language Models in Primary Care Medical Education.",
        "authors": [
          "D. J. Parente"
        ],
        "published_date": "2024",
        "abstract": "Generative artificial intelligence and large language models are the continuation of a technological revolution in information processing that began with the invention of the transistor in 1947. These technologies, driven by transformer architectures for artificial neural networks, are poised to broadly influence society. It is already apparent that these technologies will be adapted to drive innovation in education. Medical education is a high-risk activity: Information that is incorrectly taught to a student may go unrecognized for years until a relevant clinical situation appears in which that error can lead to patient harm. In this article, I discuss the principal limitations to the use of generative artificial intelligence in medical education-hallucination, bias, cost, and security-and suggest some approaches to confronting these problems. Additionally, I identify the potential applications of generative artificial intelligence to medical education, including personalized instruction, simulation, feedback, evaluation, augmentation of qualitative research, and performance of critical assessment of the existing scientific literature.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/5be8fd7aa6564ea1884626c16bcac36508f79ff1.pdf",
        "venue": "Family Medicine",
        "citationCount": 10,
        "score": 10.0,
        "summary": "Generative artificial intelligence and large language models are the continuation of a technological revolution in information processing that began with the invention of the transistor in 1947. These technologies, driven by transformer architectures for artificial neural networks, are poised to broadly influence society. It is already apparent that these technologies will be adapted to drive innovation in education. Medical education is a high-risk activity: Information that is incorrectly taught to a student may go unrecognized for years until a relevant clinical situation appears in which that error can lead to patient harm. In this article, I discuss the principal limitations to the use of generative artificial intelligence in medical education-hallucination, bias, cost, and security-and suggest some approaches to confronting these problems. Additionally, I identify the potential applications of generative artificial intelligence to medical education, including personalized instruction, simulation, feedback, evaluation, augmentation of qualitative research, and performance of critical assessment of the existing scientific literature.",
        "keywords": []
      },
      "file_name": "5be8fd7aa6564ea1884626c16bcac36508f79ff1.pdf"
    },
    {
      "success": true,
      "doc_id": "d995aabe9247f03d3de63bb52abb94c2",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/8a9b43946dc10f91ce8c5971a1f247fbacda7a42.pdf",
      "citation_key": "zhao2024h5n",
      "metadata": {
        "title": "Opening the Black Box of Large Language Models: Two Views on Holistic Interpretability",
        "authors": [
          "Haiyan Zhao",
          "Fan Yang",
          "Himabindu Lakkaraju",
          "Mengnan Du"
        ],
        "published_date": "2024",
        "abstract": "",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/8a9b43946dc10f91ce8c5971a1f247fbacda7a42.pdf",
        "venue": "arXiv.org",
        "citationCount": 10,
        "score": 10.0,
        "summary": "",
        "keywords": []
      },
      "file_name": "8a9b43946dc10f91ce8c5971a1f247fbacda7a42.pdf"
    },
    {
      "success": true,
      "doc_id": "6a7d118c2d4ceec590acc91ebf0b4b05",
      "summary": "We find that language models have difficulties generating fallacious and deceptive reasoning. When asked to generate deceptive outputs, language models tend to leak honest counterparts but believe them to be false. Exploiting this deficiency, we propose a jailbreak attack method that elicits an aligned language model for malicious output. Specifically, we query the model to generate a fallacious yet deceptively real procedure for the harmful behavior. Since a fallacious procedure is generally considered fake and thus harmless by LLMs, it helps bypass the safeguard mechanism. Yet the output is factually harmful since the LLM cannot fabricate fallacious solutions but proposes truthful ones. We evaluate our approach over five safety-aligned large language models, comparing four previous jailbreak methods, and show that our approach achieves competitive performance with more harmful outputs. We believe the findings could be extended beyond model safety, such as self-verification and hallucination.",
      "intriguing_abstract": "We find that language models have difficulties generating fallacious and deceptive reasoning. When asked to generate deceptive outputs, language models tend to leak honest counterparts but believe them to be false. Exploiting this deficiency, we propose a jailbreak attack method that elicits an aligned language model for malicious output. Specifically, we query the model to generate a fallacious yet deceptively real procedure for the harmful behavior. Since a fallacious procedure is generally considered fake and thus harmless by LLMs, it helps bypass the safeguard mechanism. Yet the output is factually harmful since the LLM cannot fabricate fallacious solutions but proposes truthful ones. We evaluate our approach over five safety-aligned large language models, comparing four previous jailbreak methods, and show that our approach achieves competitive performance with more harmful outputs. We believe the findings could be extended beyond model safety, such as self-verification and hallucination.",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/e615f33365ea1d439507fc477588528ffb0764a8.pdf",
      "citation_key": "zhou2024b0u",
      "metadata": {
        "title": "Large Language Models Are Involuntary Truth-Tellers: Exploiting Fallacy Failure for Jailbreak Attacks",
        "authors": [
          "Yue Zhou",
          "Henry Peng Zou",
          "Barbara Di Eugenio",
          "Yang Zhang"
        ],
        "published_date": "2024",
        "abstract": "We find that language models have difficulties generating fallacious and deceptive reasoning. When asked to generate deceptive outputs, language models tend to leak honest counterparts but believe them to be false. Exploiting this deficiency, we propose a jailbreak attack method that elicits an aligned language model for malicious output. Specifically, we query the model to generate a fallacious yet deceptively real procedure for the harmful behavior. Since a fallacious procedure is generally considered fake and thus harmless by LLMs, it helps bypass the safeguard mechanism. Yet the output is factually harmful since the LLM cannot fabricate fallacious solutions but proposes truthful ones. We evaluate our approach over five safety-aligned large language models, comparing four previous jailbreak methods, and show that our approach achieves competitive performance with more harmful outputs. We believe the findings could be extended beyond model safety, such as self-verification and hallucination.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/e615f33365ea1d439507fc477588528ffb0764a8.pdf",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "citationCount": 10,
        "score": 10.0,
        "summary": "We find that language models have difficulties generating fallacious and deceptive reasoning. When asked to generate deceptive outputs, language models tend to leak honest counterparts but believe them to be false. Exploiting this deficiency, we propose a jailbreak attack method that elicits an aligned language model for malicious output. Specifically, we query the model to generate a fallacious yet deceptively real procedure for the harmful behavior. Since a fallacious procedure is generally considered fake and thus harmless by LLMs, it helps bypass the safeguard mechanism. Yet the output is factually harmful since the LLM cannot fabricate fallacious solutions but proposes truthful ones. We evaluate our approach over five safety-aligned large language models, comparing four previous jailbreak methods, and show that our approach achieves competitive performance with more harmful outputs. We believe the findings could be extended beyond model safety, such as self-verification and hallucination.",
        "keywords": []
      },
      "file_name": "e615f33365ea1d439507fc477588528ffb0764a8.pdf"
    },
    {
      "success": true,
      "doc_id": "d85acc432f07faf246c18ae18659a000",
      "summary": "In this work, we introduce a new hallucination metric - Simple Hallucination Index (SHI) and provide insight into one important limitation of the parametric knowledge of large language models (LLMs), i.e. false attribution. The task of automatic author attribution for relatively small chunks of text is an important NLP task but can be challenging. We empirically evaluate the power of 3 open SotA LLMs in zero-shot setting (Gemma-7B, Mixtral 8x7B, and LLaMA-2-13B). We acquired the top 10 most popular books of a month, according to Project Gutenberg, divided each one into equal chunks of 400 words, and prompted each LLM to predict the author. We then randomly sampled 162 chunks per book for human evaluation, based on the error margin of 7% and a confidence level of 95%. The average results show that Mixtral 8x7B has the highest prediction accuracy, the lowest SHI, and a Pearson's correlation (r) of 0.724, 0.263, and -0.9996, respectively, followed by LLaMA-2-13B and Gemma-7B. However, Mixtral 8x7B suffers from high hallucinations for 3 books, rising as high as a SHI of 0.87 (in the range 0-1, where 1 is the worst). The strong negative correlation of accuracy and SHI, given by r, demonstrates the fidelity of the new hallucination metric, which may generalize to other tasks. We also show that prediction accuracies correlate positively with the frequencies of Wikipedia instances of the book titles instead of the downloads and we perform error analyses of predictions. We publicly release the annotated chunks of data and our codes to aid the reproducibility and evaluation of other models.",
      "intriguing_abstract": "In this work, we introduce a new hallucination metric - Simple Hallucination Index (SHI) and provide insight into one important limitation of the parametric knowledge of large language models (LLMs), i.e. false attribution. The task of automatic author attribution for relatively small chunks of text is an important NLP task but can be challenging. We empirically evaluate the power of 3 open SotA LLMs in zero-shot setting (Gemma-7B, Mixtral 8x7B, and LLaMA-2-13B). We acquired the top 10 most popular books of a month, according to Project Gutenberg, divided each one into equal chunks of 400 words, and prompted each LLM to predict the author. We then randomly sampled 162 chunks per book for human evaluation, based on the error margin of 7% and a confidence level of 95%. The average results show that Mixtral 8x7B has the highest prediction accuracy, the lowest SHI, and a Pearson's correlation (r) of 0.724, 0.263, and -0.9996, respectively, followed by LLaMA-2-13B and Gemma-7B. However, Mixtral 8x7B suffers from high hallucinations for 3 books, rising as high as a SHI of 0.87 (in the range 0-1, where 1 is the worst). The strong negative correlation of accuracy and SHI, given by r, demonstrates the fidelity of the new hallucination metric, which may generalize to other tasks. We also show that prediction accuracies correlate positively with the frequencies of Wikipedia instances of the book titles instead of the downloads and we perform error analyses of predictions. We publicly release the annotated chunks of data and our codes to aid the reproducibility and evaluation of other models.",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/108bc0498629f4710b44076fe0c6270954494097.pdf",
      "citation_key": "adewumi2024lv9",
      "metadata": {
        "title": "On the Limitations of Large Language Models (LLMs): False Attribution",
        "authors": [
          "Tosin P. Adewumi",
          "Nudrat Habib",
          "Lama Alkhaled",
          "Elisa Barney"
        ],
        "published_date": "2024",
        "abstract": "In this work, we introduce a new hallucination metric - Simple Hallucination Index (SHI) and provide insight into one important limitation of the parametric knowledge of large language models (LLMs), i.e. false attribution. The task of automatic author attribution for relatively small chunks of text is an important NLP task but can be challenging. We empirically evaluate the power of 3 open SotA LLMs in zero-shot setting (Gemma-7B, Mixtral 8x7B, and LLaMA-2-13B). We acquired the top 10 most popular books of a month, according to Project Gutenberg, divided each one into equal chunks of 400 words, and prompted each LLM to predict the author. We then randomly sampled 162 chunks per book for human evaluation, based on the error margin of 7% and a confidence level of 95%. The average results show that Mixtral 8x7B has the highest prediction accuracy, the lowest SHI, and a Pearson's correlation (r) of 0.724, 0.263, and -0.9996, respectively, followed by LLaMA-2-13B and Gemma-7B. However, Mixtral 8x7B suffers from high hallucinations for 3 books, rising as high as a SHI of 0.87 (in the range 0-1, where 1 is the worst). The strong negative correlation of accuracy and SHI, given by r, demonstrates the fidelity of the new hallucination metric, which may generalize to other tasks. We also show that prediction accuracies correlate positively with the frequencies of Wikipedia instances of the book titles instead of the downloads and we perform error analyses of predictions. We publicly release the annotated chunks of data and our codes to aid the reproducibility and evaluation of other models.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/108bc0498629f4710b44076fe0c6270954494097.pdf",
        "venue": "arXiv.org",
        "citationCount": 10,
        "score": 10.0,
        "summary": "In this work, we introduce a new hallucination metric - Simple Hallucination Index (SHI) and provide insight into one important limitation of the parametric knowledge of large language models (LLMs), i.e. false attribution. The task of automatic author attribution for relatively small chunks of text is an important NLP task but can be challenging. We empirically evaluate the power of 3 open SotA LLMs in zero-shot setting (Gemma-7B, Mixtral 8x7B, and LLaMA-2-13B). We acquired the top 10 most popular books of a month, according to Project Gutenberg, divided each one into equal chunks of 400 words, and prompted each LLM to predict the author. We then randomly sampled 162 chunks per book for human evaluation, based on the error margin of 7% and a confidence level of 95%. The average results show that Mixtral 8x7B has the highest prediction accuracy, the lowest SHI, and a Pearson's correlation (r) of 0.724, 0.263, and -0.9996, respectively, followed by LLaMA-2-13B and Gemma-7B. However, Mixtral 8x7B suffers from high hallucinations for 3 books, rising as high as a SHI of 0.87 (in the range 0-1, where 1 is the worst). The strong negative correlation of accuracy and SHI, given by r, demonstrates the fidelity of the new hallucination metric, which may generalize to other tasks. We also show that prediction accuracies correlate positively with the frequencies of Wikipedia instances of the book titles instead of the downloads and we perform error analyses of predictions. We publicly release the annotated chunks of data and our codes to aid the reproducibility and evaluation of other models.",
        "keywords": []
      },
      "file_name": "108bc0498629f4710b44076fe0c6270954494097.pdf"
    },
    {
      "success": true,
      "doc_id": "c18a0d8119ac9696b3b8b8181c4afaae",
      "summary": "Knowledge Graph Question Answering (KGQA) methods seek to answer Natural Language questions using the relational information stored in Knowledge Graphs (KGs). With the recent advancements of Large Language Models (LLMs) and their remarkable reasoning abilities, there is a growing trend to leverage them for KGQA. However, existing methodologies have only focused on answering factual questions, e.g., *â€œIn which city was Silvio Berlusconiâ€™s first wife born?â€*, leaving questions involving commonsense reasoning that real-world users may pose more often, e.g., *â€œDo I need separate visas to see the Venus of Willendorf and attend the Olympics this summer?â€* unaddressed. In this work, we first observe that existing LLM-based methods for KGQA struggle with hallucination on such questions, especially on queries targeting long-tail entities (e.g., non-mainstream and recent entities), thus hindering their applicability in real-world applications especially since their reasoning processes are not easily verifiable. In response, we propose Right for Right Reasons (R^3), a commonsense KGQA methodology that allows for a verifiable reasoning procedure by axiomatically surfacing intrinsic commonsense knowledge of LLMs and grounding every factual reasoning step on KG triples. Through experimental evaluations across three different tasksâ€”question answering, claim verification, and preference matchingâ€”our findings showcase R^3 as a superior approach, outperforming existing methodologies and notably reducing instances of hallucination and reasoning errors.",
      "intriguing_abstract": "Knowledge Graph Question Answering (KGQA) methods seek to answer Natural Language questions using the relational information stored in Knowledge Graphs (KGs). With the recent advancements of Large Language Models (LLMs) and their remarkable reasoning abilities, there is a growing trend to leverage them for KGQA. However, existing methodologies have only focused on answering factual questions, e.g., *â€œIn which city was Silvio Berlusconiâ€™s first wife born?â€*, leaving questions involving commonsense reasoning that real-world users may pose more often, e.g., *â€œDo I need separate visas to see the Venus of Willendorf and attend the Olympics this summer?â€* unaddressed. In this work, we first observe that existing LLM-based methods for KGQA struggle with hallucination on such questions, especially on queries targeting long-tail entities (e.g., non-mainstream and recent entities), thus hindering their applicability in real-world applications especially since their reasoning processes are not easily verifiable. In response, we propose Right for Right Reasons (R^3), a commonsense KGQA methodology that allows for a verifiable reasoning procedure by axiomatically surfacing intrinsic commonsense knowledge of LLMs and grounding every factual reasoning step on KG triples. Through experimental evaluations across three different tasksâ€”question answering, claim verification, and preference matchingâ€”our findings showcase R^3 as a superior approach, outperforming existing methodologies and notably reducing instances of hallucination and reasoning errors.",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/59395cf4f9346ef4ccb37499a3a7e52c2978fc61.pdf",
      "citation_key": "toroghi2024mxf",
      "metadata": {
        "title": "Right for Right Reasons: Large Language Models for Verifiable Commonsense Knowledge Graph Question Answering",
        "authors": [
          "Armin Toroghi",
          "Willis Guo",
          "Mohammad Mahdi Torabi pour",
          "Scott Sanner"
        ],
        "published_date": "2024",
        "abstract": "Knowledge Graph Question Answering (KGQA) methods seek to answer Natural Language questions using the relational information stored in Knowledge Graphs (KGs). With the recent advancements of Large Language Models (LLMs) and their remarkable reasoning abilities, there is a growing trend to leverage them for KGQA. However, existing methodologies have only focused on answering factual questions, e.g., *â€œIn which city was Silvio Berlusconiâ€™s first wife born?â€*, leaving questions involving commonsense reasoning that real-world users may pose more often, e.g., *â€œDo I need separate visas to see the Venus of Willendorf and attend the Olympics this summer?â€* unaddressed. In this work, we first observe that existing LLM-based methods for KGQA struggle with hallucination on such questions, especially on queries targeting long-tail entities (e.g., non-mainstream and recent entities), thus hindering their applicability in real-world applications especially since their reasoning processes are not easily verifiable. In response, we propose Right for Right Reasons (R^3), a commonsense KGQA methodology that allows for a verifiable reasoning procedure by axiomatically surfacing intrinsic commonsense knowledge of LLMs and grounding every factual reasoning step on KG triples. Through experimental evaluations across three different tasksâ€”question answering, claim verification, and preference matchingâ€”our findings showcase R^3 as a superior approach, outperforming existing methodologies and notably reducing instances of hallucination and reasoning errors.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/59395cf4f9346ef4ccb37499a3a7e52c2978fc61.pdf",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "citationCount": 10,
        "score": 10.0,
        "summary": "Knowledge Graph Question Answering (KGQA) methods seek to answer Natural Language questions using the relational information stored in Knowledge Graphs (KGs). With the recent advancements of Large Language Models (LLMs) and their remarkable reasoning abilities, there is a growing trend to leverage them for KGQA. However, existing methodologies have only focused on answering factual questions, e.g., *â€œIn which city was Silvio Berlusconiâ€™s first wife born?â€*, leaving questions involving commonsense reasoning that real-world users may pose more often, e.g., *â€œDo I need separate visas to see the Venus of Willendorf and attend the Olympics this summer?â€* unaddressed. In this work, we first observe that existing LLM-based methods for KGQA struggle with hallucination on such questions, especially on queries targeting long-tail entities (e.g., non-mainstream and recent entities), thus hindering their applicability in real-world applications especially since their reasoning processes are not easily verifiable. In response, we propose Right for Right Reasons (R^3), a commonsense KGQA methodology that allows for a verifiable reasoning procedure by axiomatically surfacing intrinsic commonsense knowledge of LLMs and grounding every factual reasoning step on KG triples. Through experimental evaluations across three different tasksâ€”question answering, claim verification, and preference matchingâ€”our findings showcase R^3 as a superior approach, outperforming existing methodologies and notably reducing instances of hallucination and reasoning errors.",
        "keywords": []
      },
      "file_name": "59395cf4f9346ef4ccb37499a3a7e52c2978fc61.pdf"
    },
    {
      "success": true,
      "doc_id": "6b8b54abf89f35cee8880d77158b1fb1",
      "summary": "This paper presents a series of investigations into an interesting phenomenon where we observe performance increases in large language models (LLMs) when providing a prompt that causes and exploits hallucination. We propose null-shot prompting, a counter-intuitive approach where we intentionally instruct LLMs to look at and utilize information from a null section. We investigate null-shot prompting on a wide range of tasks, including arithmetic reasoning, commonsense reasoning, and reading comprehension. We observe a substantial increase in performance in arithmetic reasoning tasks for various models, with up to a 44.62% increase compared to a baseline in one model. Therefore, we investigate deeper into this task by utilizing a more challenging mathematics problem-solving benchmark. We observe that LLMs benefit from hallucination in null-shot prompting in this task and discuss the mathematical topics that benefit the most from introducing hallucination in the prompt. We continue our investigation by evaluating hallucination detection abilities of the LLMs when using null-shot prompting. We find surprising results where hallucination in prompts can improve hallucination detection abilities of many LLMs. We also examine the effects of introducing both reasoning, which is known to mitigate hallucination, and hallucination simultaneously in the prompt and observe another surprising turn for the mathematics problem-solving benchmark with many performance improvements. We hope this paper will spark more interest, investigations, and discussions on how hallucination in prompts LLMs and even bolsters them in certain cases.",
      "intriguing_abstract": "This paper presents a series of investigations into an interesting phenomenon where we observe performance increases in large language models (LLMs) when providing a prompt that causes and exploits hallucination. We propose null-shot prompting, a counter-intuitive approach where we intentionally instruct LLMs to look at and utilize information from a null section. We investigate null-shot prompting on a wide range of tasks, including arithmetic reasoning, commonsense reasoning, and reading comprehension. We observe a substantial increase in performance in arithmetic reasoning tasks for various models, with up to a 44.62% increase compared to a baseline in one model. Therefore, we investigate deeper into this task by utilizing a more challenging mathematics problem-solving benchmark. We observe that LLMs benefit from hallucination in null-shot prompting in this task and discuss the mathematical topics that benefit the most from introducing hallucination in the prompt. We continue our investigation by evaluating hallucination detection abilities of the LLMs when using null-shot prompting. We find surprising results where hallucination in prompts can improve hallucination detection abilities of many LLMs. We also examine the effects of introducing both reasoning, which is known to mitigate hallucination, and hallucination simultaneously in the prompt and observe another surprising turn for the mathematics problem-solving benchmark with many performance improvements. We hope this paper will spark more interest, investigations, and discussions on how hallucination in prompts LLMs and even bolsters them in certain cases.",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/655f92355d7930919a01125bd7a35c812498b1a9.pdf",
      "citation_key": "taveekitworachai2024aql",
      "metadata": {
        "title": "Null-Shot Prompting: Rethinking Prompting Large Language Models With Hallucination",
        "authors": [
          "Pittawat Taveekitworachai",
          "Febri Abdullah",
          "R. Thawonmas"
        ],
        "published_date": "2024",
        "abstract": "This paper presents a series of investigations into an interesting phenomenon where we observe performance increases in large language models (LLMs) when providing a prompt that causes and exploits hallucination. We propose null-shot prompting, a counter-intuitive approach where we intentionally instruct LLMs to look at and utilize information from a null section. We investigate null-shot prompting on a wide range of tasks, including arithmetic reasoning, commonsense reasoning, and reading comprehension. We observe a substantial increase in performance in arithmetic reasoning tasks for various models, with up to a 44.62% increase compared to a baseline in one model. Therefore, we investigate deeper into this task by utilizing a more challenging mathematics problem-solving benchmark. We observe that LLMs benefit from hallucination in null-shot prompting in this task and discuss the mathematical topics that benefit the most from introducing hallucination in the prompt. We continue our investigation by evaluating hallucination detection abilities of the LLMs when using null-shot prompting. We find surprising results where hallucination in prompts can improve hallucination detection abilities of many LLMs. We also examine the effects of introducing both reasoning, which is known to mitigate hallucination, and hallucination simultaneously in the prompt and observe another surprising turn for the mathematics problem-solving benchmark with many performance improvements. We hope this paper will spark more interest, investigations, and discussions on how hallucination in prompts LLMs and even bolsters them in certain cases.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/655f92355d7930919a01125bd7a35c812498b1a9.pdf",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "citationCount": 9,
        "score": 9.0,
        "summary": "This paper presents a series of investigations into an interesting phenomenon where we observe performance increases in large language models (LLMs) when providing a prompt that causes and exploits hallucination. We propose null-shot prompting, a counter-intuitive approach where we intentionally instruct LLMs to look at and utilize information from a null section. We investigate null-shot prompting on a wide range of tasks, including arithmetic reasoning, commonsense reasoning, and reading comprehension. We observe a substantial increase in performance in arithmetic reasoning tasks for various models, with up to a 44.62% increase compared to a baseline in one model. Therefore, we investigate deeper into this task by utilizing a more challenging mathematics problem-solving benchmark. We observe that LLMs benefit from hallucination in null-shot prompting in this task and discuss the mathematical topics that benefit the most from introducing hallucination in the prompt. We continue our investigation by evaluating hallucination detection abilities of the LLMs when using null-shot prompting. We find surprising results where hallucination in prompts can improve hallucination detection abilities of many LLMs. We also examine the effects of introducing both reasoning, which is known to mitigate hallucination, and hallucination simultaneously in the prompt and observe another surprising turn for the mathematics problem-solving benchmark with many performance improvements. We hope this paper will spark more interest, investigations, and discussions on how hallucination in prompts LLMs and even bolsters them in certain cases.",
        "keywords": []
      },
      "file_name": "655f92355d7930919a01125bd7a35c812498b1a9.pdf"
    },
    {
      "success": true,
      "doc_id": "47e43d16d76809eefaf6719c125d4f2e",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/d0fe343fbdecaf4cc477d70e8701f9a6935b13d0.pdf",
      "citation_key": "wan2024mh1",
      "metadata": {
        "title": "Mitigating Hallucinations of Large Language Models via Knowledge Consistent Alignment",
        "authors": [
          "Fanqi Wan",
          "Xinting Huang",
          "Leyang Cui",
          "Xiaojun Quan",
          "Wei Bi",
          "Shuming Shi"
        ],
        "published_date": "2024",
        "abstract": "",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/d0fe343fbdecaf4cc477d70e8701f9a6935b13d0.pdf",
        "venue": "arXiv.org",
        "citationCount": 9,
        "score": 9.0,
        "summary": "",
        "keywords": []
      },
      "file_name": "d0fe343fbdecaf4cc477d70e8701f9a6935b13d0.pdf"
    },
    {
      "success": true,
      "doc_id": "082968f0cebe8ae864131d1a09251fb1",
      "summary": "We present LARL-RM (Large language model-generated Automaton for Reinforcement Learning with Reward Machine) algorithm to encode high-level knowledge into reinforcement learning using automaton to expedite the reinforcement learning. Our method uses large language models (LLM) to obtain high-level domain-specific knowledge using prompt engineering instead of providing the reinforcement learning (RL) algorithm directly with the high-level knowledge that requires an expert to encode the automaton. We use chain-of-thought and few-shot methods for prompt engineering and demonstrate that our method works using these approaches. Additionally, LARL-RM allows for fully closed-loop reinforcement learning without the need for an expert to guide and supervise the learning since LARL-RM can use the LLM directly to generate the required high-level knowledge for the task at hand. Moreover, we demonstrate LARM-RM robustness to LLM hallucination and show the theoretical guarantee of our algorithm to converge to an optimal policy. We show that LARL-RM speeds up the convergence by implementing our method in two case studies and compare it to other RL methods.",
      "intriguing_abstract": "We present LARL-RM (Large language model-generated Automaton for Reinforcement Learning with Reward Machine) algorithm to encode high-level knowledge into reinforcement learning using automaton to expedite the reinforcement learning. Our method uses large language models (LLM) to obtain high-level domain-specific knowledge using prompt engineering instead of providing the reinforcement learning (RL) algorithm directly with the high-level knowledge that requires an expert to encode the automaton. We use chain-of-thought and few-shot methods for prompt engineering and demonstrate that our method works using these approaches. Additionally, LARL-RM allows for fully closed-loop reinforcement learning without the need for an expert to guide and supervise the learning since LARL-RM can use the LLM directly to generate the required high-level knowledge for the task at hand. Moreover, we demonstrate LARM-RM robustness to LLM hallucination and show the theoretical guarantee of our algorithm to converge to an optimal policy. We show that LARL-RM speeds up the convergence by implementing our method in two case studies and compare it to other RL methods.",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/10269639a0462a0e9790bd0524f5a092325d8d51.pdf",
      "citation_key": "alsadat2024i78",
      "metadata": {
        "title": "Using Large Language Models to Automate and Expedite Reinforcement Learning with Reward Machine",
        "authors": [
          "Shayan Meshkat Alsadat",
          "Jean-Raphael Gaglione",
          "D. Neider",
          "U. Topcu",
          "Zhe Xu"
        ],
        "published_date": "2024",
        "abstract": "We present LARL-RM (Large language model-generated Automaton for Reinforcement Learning with Reward Machine) algorithm to encode high-level knowledge into reinforcement learning using automaton to expedite the reinforcement learning. Our method uses large language models (LLM) to obtain high-level domain-specific knowledge using prompt engineering instead of providing the reinforcement learning (RL) algorithm directly with the high-level knowledge that requires an expert to encode the automaton. We use chain-of-thought and few-shot methods for prompt engineering and demonstrate that our method works using these approaches. Additionally, LARL-RM allows for fully closed-loop reinforcement learning without the need for an expert to guide and supervise the learning since LARL-RM can use the LLM directly to generate the required high-level knowledge for the task at hand. Moreover, we demonstrate LARM-RM robustness to LLM hallucination and show the theoretical guarantee of our algorithm to converge to an optimal policy. We show that LARL-RM speeds up the convergence by implementing our method in two case studies and compare it to other RL methods.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/10269639a0462a0e9790bd0524f5a092325d8d51.pdf",
        "venue": "American Control Conference",
        "citationCount": 9,
        "score": 9.0,
        "summary": "We present LARL-RM (Large language model-generated Automaton for Reinforcement Learning with Reward Machine) algorithm to encode high-level knowledge into reinforcement learning using automaton to expedite the reinforcement learning. Our method uses large language models (LLM) to obtain high-level domain-specific knowledge using prompt engineering instead of providing the reinforcement learning (RL) algorithm directly with the high-level knowledge that requires an expert to encode the automaton. We use chain-of-thought and few-shot methods for prompt engineering and demonstrate that our method works using these approaches. Additionally, LARL-RM allows for fully closed-loop reinforcement learning without the need for an expert to guide and supervise the learning since LARL-RM can use the LLM directly to generate the required high-level knowledge for the task at hand. Moreover, we demonstrate LARM-RM robustness to LLM hallucination and show the theoretical guarantee of our algorithm to converge to an optimal policy. We show that LARL-RM speeds up the convergence by implementing our method in two case studies and compare it to other RL methods.",
        "keywords": []
      },
      "file_name": "10269639a0462a0e9790bd0524f5a092325d8d51.pdf"
    },
    {
      "success": true,
      "doc_id": "8e12efed5771f130381a13a198cf93b1",
      "summary": "Despite the signiï¬cant success of large vision-language models (LVLMs), some studies have revealed that LVLMs suffer from the hal-lucination problem, where the LVLMsâ€™ response contains descriptions of non-existent objects. Although various benchmarks have been proposed to investigate this problem, they mostly focus on single-turn evaluation and overlook the hallucination raised by textual inputs. To investigate the hallucination problem of LVLMs when given long-term misleading textual history, we propose a novel visual dialogue hallucination evaluation benchmark VisDiaHalBench. The benchmark consists of samples with ï¬ve-turn questions about an edited image and its original version. Vis-DiaHalBench differs from previous hallucination benchmarks in the following three points:",
      "intriguing_abstract": "Despite the signiï¬cant success of large vision-language models (LVLMs), some studies have revealed that LVLMs suffer from the hal-lucination problem, where the LVLMsâ€™ response contains descriptions of non-existent objects. Although various benchmarks have been proposed to investigate this problem, they mostly focus on single-turn evaluation and overlook the hallucination raised by textual inputs. To investigate the hallucination problem of LVLMs when given long-term misleading textual history, we propose a novel visual dialogue hallucination evaluation benchmark VisDiaHalBench. The benchmark consists of samples with ï¬ve-turn questions about an edited image and its original version. Vis-DiaHalBench differs from previous hallucination benchmarks in the following three points:",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/7e38ac6f71408383939ec05f60b0bd85759a4c4e.pdf",
      "citation_key": "cao2024o9a",
      "metadata": {
        "title": "VisDiaHalBench: A Visual Dialogue Benchmark For Diagnosing Hallucination in Large Vision-Language Models",
        "authors": [
          "Qingxing Cao",
          "Junhao Cheng",
          "Xiaodan Liang",
          "Liang Lin"
        ],
        "published_date": "2024",
        "abstract": "Despite the signiï¬cant success of large vision-language models (LVLMs), some studies have revealed that LVLMs suffer from the hal-lucination problem, where the LVLMsâ€™ response contains descriptions of non-existent objects. Although various benchmarks have been proposed to investigate this problem, they mostly focus on single-turn evaluation and overlook the hallucination raised by textual inputs. To investigate the hallucination problem of LVLMs when given long-term misleading textual history, we propose a novel visual dialogue hallucination evaluation benchmark VisDiaHalBench. The benchmark consists of samples with ï¬ve-turn questions about an edited image and its original version. Vis-DiaHalBench differs from previous hallucination benchmarks in the following three points:",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/7e38ac6f71408383939ec05f60b0bd85759a4c4e.pdf",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "citationCount": 8,
        "score": 8.0,
        "summary": "Despite the signiï¬cant success of large vision-language models (LVLMs), some studies have revealed that LVLMs suffer from the hal-lucination problem, where the LVLMsâ€™ response contains descriptions of non-existent objects. Although various benchmarks have been proposed to investigate this problem, they mostly focus on single-turn evaluation and overlook the hallucination raised by textual inputs. To investigate the hallucination problem of LVLMs when given long-term misleading textual history, we propose a novel visual dialogue hallucination evaluation benchmark VisDiaHalBench. The benchmark consists of samples with ï¬ve-turn questions about an edited image and its original version. Vis-DiaHalBench differs from previous hallucination benchmarks in the following three points:",
        "keywords": []
      },
      "file_name": "7e38ac6f71408383939ec05f60b0bd85759a4c4e.pdf"
    },
    {
      "success": true,
      "doc_id": "2bba79f50f0f27430ef37e05397c92da",
      "summary": "Recent advancements in massively multilingual machine translation systems have significantly enhanced translation accuracy; however, even the best performing systems still generate hallucinations, severely impacting user trust. Detecting hallucinations in Machine Translation (MT) remains a critical challenge, particularly since existing methods excel with High-Resource Languages (HRLs) but exhibit substantial limitations when applied to Low-Resource Languages (LRLs). This paper evaluates sentence-level hallucination detection approaches using Large Language Models (LLMs) and semantic similarity within massively multilingual embeddings. Our study spans 16 language directions, covering HRLs, LRLs, with diverse scripts. We find that the choice of model is essential for performance. On average, for HRLs, Llama3-70B outperforms the previous state of the art by as much as 0.16 MCC (Matthews Correlation Coefficient). However, for LRLs we observe that Claude Sonnet outperforms other LLMs on average by 0.03 MCC. The key takeaway from our study is that LLMs can achieve performance comparable or even better than previously proposed models, despite not being explicitly trained for any machine translation task. However, their advantage is less significant for LRLs.",
      "intriguing_abstract": "Recent advancements in massively multilingual machine translation systems have significantly enhanced translation accuracy; however, even the best performing systems still generate hallucinations, severely impacting user trust. Detecting hallucinations in Machine Translation (MT) remains a critical challenge, particularly since existing methods excel with High-Resource Languages (HRLs) but exhibit substantial limitations when applied to Low-Resource Languages (LRLs). This paper evaluates sentence-level hallucination detection approaches using Large Language Models (LLMs) and semantic similarity within massively multilingual embeddings. Our study spans 16 language directions, covering HRLs, LRLs, with diverse scripts. We find that the choice of model is essential for performance. On average, for HRLs, Llama3-70B outperforms the previous state of the art by as much as 0.16 MCC (Matthews Correlation Coefficient). However, for LRLs we observe that Claude Sonnet outperforms other LLMs on average by 0.03 MCC. The key takeaway from our study is that LLMs can achieve performance comparable or even better than previously proposed models, despite not being explicitly trained for any machine translation task. However, their advantage is less significant for LRLs.",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/1541bc9e588bfcd4bf365c868fa2f11461896980.pdf",
      "citation_key": "benkirane202494i",
      "metadata": {
        "title": "Machine Translation Hallucination Detection for Low and High Resource Languages using Large Language Models",
        "authors": [
          "Kenza Benkirane",
          "Laura Gongas",
          "Shahar Pelles",
          "Naomi Fuchs",
          "Joshua Darmon",
          "Pontus Stenetorp",
          "David Ifeoluwa Adelani",
          "Eduardo SÃ¡nchez",
          "Meta"
        ],
        "published_date": "2024",
        "abstract": "Recent advancements in massively multilingual machine translation systems have significantly enhanced translation accuracy; however, even the best performing systems still generate hallucinations, severely impacting user trust. Detecting hallucinations in Machine Translation (MT) remains a critical challenge, particularly since existing methods excel with High-Resource Languages (HRLs) but exhibit substantial limitations when applied to Low-Resource Languages (LRLs). This paper evaluates sentence-level hallucination detection approaches using Large Language Models (LLMs) and semantic similarity within massively multilingual embeddings. Our study spans 16 language directions, covering HRLs, LRLs, with diverse scripts. We find that the choice of model is essential for performance. On average, for HRLs, Llama3-70B outperforms the previous state of the art by as much as 0.16 MCC (Matthews Correlation Coefficient). However, for LRLs we observe that Claude Sonnet outperforms other LLMs on average by 0.03 MCC. The key takeaway from our study is that LLMs can achieve performance comparable or even better than previously proposed models, despite not being explicitly trained for any machine translation task. However, their advantage is less significant for LRLs.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/1541bc9e588bfcd4bf365c868fa2f11461896980.pdf",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "citationCount": 8,
        "score": 8.0,
        "summary": "Recent advancements in massively multilingual machine translation systems have significantly enhanced translation accuracy; however, even the best performing systems still generate hallucinations, severely impacting user trust. Detecting hallucinations in Machine Translation (MT) remains a critical challenge, particularly since existing methods excel with High-Resource Languages (HRLs) but exhibit substantial limitations when applied to Low-Resource Languages (LRLs). This paper evaluates sentence-level hallucination detection approaches using Large Language Models (LLMs) and semantic similarity within massively multilingual embeddings. Our study spans 16 language directions, covering HRLs, LRLs, with diverse scripts. We find that the choice of model is essential for performance. On average, for HRLs, Llama3-70B outperforms the previous state of the art by as much as 0.16 MCC (Matthews Correlation Coefficient). However, for LRLs we observe that Claude Sonnet outperforms other LLMs on average by 0.03 MCC. The key takeaway from our study is that LLMs can achieve performance comparable or even better than previously proposed models, despite not being explicitly trained for any machine translation task. However, their advantage is less significant for LRLs.",
        "keywords": []
      },
      "file_name": "1541bc9e588bfcd4bf365c868fa2f11461896980.pdf"
    },
    {
      "success": true,
      "doc_id": "e130eda5501cfa7a508999df8800f280",
      "summary": "The most common training pipeline for large language models includes pretraining, finetuning and aligning phases, with their respective resulting models, such as the pretrained model and the finetuned model. Finetuned and aligned models show improved abilities of instruction following and safe generation, however their abilities to stay factual about the world are impacted by the finetuning process. Furthermore, the common practice of using sampling during generation also increases chances of hallucination. In this work, we introduce a collaborative decoding framework to harness the high factuality within pretrained models through the concept of critical tokens. We first design a critical token classifier to decide which model to use for the next token, and subsequently generates the next token using different decoding strategies. Experiments with different models and datasets show that our decoding framework is able to reduce model hallucination significantly, showcasing the importance of the collaborative decoding framework.",
      "intriguing_abstract": "The most common training pipeline for large language models includes pretraining, finetuning and aligning phases, with their respective resulting models, such as the pretrained model and the finetuned model. Finetuned and aligned models show improved abilities of instruction following and safe generation, however their abilities to stay factual about the world are impacted by the finetuning process. Furthermore, the common practice of using sampling during generation also increases chances of hallucination. In this work, we introduce a collaborative decoding framework to harness the high factuality within pretrained models through the concept of critical tokens. We first design a critical token classifier to decide which model to use for the next token, and subsequently generates the next token using different decoding strategies. Experiments with different models and datasets show that our decoding framework is able to reduce model hallucination significantly, showcasing the importance of the collaborative decoding framework.",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/f05e64c2a096e3762939dfdb7f475724c04a46bd.pdf",
      "citation_key": "jin2024jpw",
      "metadata": {
        "title": "Collaborative decoding of critical tokens for boosting factuality of large language models",
        "authors": [
          "Lifeng Jin",
          "Baolin Peng",
          "Linfeng Song",
          "Haitao Mi",
          "Ye Tian",
          "Dong Yu"
        ],
        "published_date": "2024",
        "abstract": "The most common training pipeline for large language models includes pretraining, finetuning and aligning phases, with their respective resulting models, such as the pretrained model and the finetuned model. Finetuned and aligned models show improved abilities of instruction following and safe generation, however their abilities to stay factual about the world are impacted by the finetuning process. Furthermore, the common practice of using sampling during generation also increases chances of hallucination. In this work, we introduce a collaborative decoding framework to harness the high factuality within pretrained models through the concept of critical tokens. We first design a critical token classifier to decide which model to use for the next token, and subsequently generates the next token using different decoding strategies. Experiments with different models and datasets show that our decoding framework is able to reduce model hallucination significantly, showcasing the importance of the collaborative decoding framework.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/f05e64c2a096e3762939dfdb7f475724c04a46bd.pdf",
        "venue": "arXiv.org",
        "citationCount": 8,
        "score": 8.0,
        "summary": "The most common training pipeline for large language models includes pretraining, finetuning and aligning phases, with their respective resulting models, such as the pretrained model and the finetuned model. Finetuned and aligned models show improved abilities of instruction following and safe generation, however their abilities to stay factual about the world are impacted by the finetuning process. Furthermore, the common practice of using sampling during generation also increases chances of hallucination. In this work, we introduce a collaborative decoding framework to harness the high factuality within pretrained models through the concept of critical tokens. We first design a critical token classifier to decide which model to use for the next token, and subsequently generates the next token using different decoding strategies. Experiments with different models and datasets show that our decoding framework is able to reduce model hallucination significantly, showcasing the importance of the collaborative decoding framework.",
        "keywords": []
      },
      "file_name": "f05e64c2a096e3762939dfdb7f475724c04a46bd.pdf"
    },
    {
      "success": true,
      "doc_id": "135ce746e0d75814f846bf8778bea8f6",
      "summary": "Large language models (LLMs) with their strong zero-shot topic extraction capabilities offer an alternative to probabilistic topic modelling and closed-set topic classification approaches. As zero-shot topic extractors, LLMs are expected to understand human instructions to generate relevant and non-hallucinated topics based on the given documents. However, LLM-based topic modelling approaches often face difficulties in generating topics with adherence to granularity as specified in human instructions, often resulting in many near-duplicate topics. Furthermore, methods for addressing hallucinated topics generated by LLMs have not yet been investigated. In this paper, we focus on addressing the issues of topic granularity and hallucinations for better LLM-based topic modelling. To this end, we introduce a novel approach that leverages Direct Preference Optimisation (DPO) to fine-tune open-source LLMs, such as Mistral-7B. Our approach does not rely on traditional human annotation to rank preferred answers but employs a reconstruction pipeline to modify raw topics generated by LLMs, thus enabling a fast and efficient training and inference framework. Comparative experiments show that our fine-tuning approach not only significantly improves the LLM's capability to produce more coherent, relevant, and precise topics, but also reduces the number of hallucinated topics.",
      "intriguing_abstract": "Large language models (LLMs) with their strong zero-shot topic extraction capabilities offer an alternative to probabilistic topic modelling and closed-set topic classification approaches. As zero-shot topic extractors, LLMs are expected to understand human instructions to generate relevant and non-hallucinated topics based on the given documents. However, LLM-based topic modelling approaches often face difficulties in generating topics with adherence to granularity as specified in human instructions, often resulting in many near-duplicate topics. Furthermore, methods for addressing hallucinated topics generated by LLMs have not yet been investigated. In this paper, we focus on addressing the issues of topic granularity and hallucinations for better LLM-based topic modelling. To this end, we introduce a novel approach that leverages Direct Preference Optimisation (DPO) to fine-tune open-source LLMs, such as Mistral-7B. Our approach does not rely on traditional human annotation to rank preferred answers but employs a reconstruction pipeline to modify raw topics generated by LLMs, thus enabling a fast and efficient training and inference framework. Comparative experiments show that our fine-tuning approach not only significantly improves the LLM's capability to produce more coherent, relevant, and precise topics, but also reduces the number of hallucinated topics.",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/46e542884db4fc4df605eb28473cff79aec54c99.pdf",
      "citation_key": "mu2024f3b",
      "metadata": {
        "title": "Addressing Topic Granularity and Hallucination in Large Language Models for Topic Modelling",
        "authors": [
          "Yida Mu",
          "Peizhen Bai",
          "Kalina Bontcheva",
          "Xingyi Song"
        ],
        "published_date": "2024",
        "abstract": "Large language models (LLMs) with their strong zero-shot topic extraction capabilities offer an alternative to probabilistic topic modelling and closed-set topic classification approaches. As zero-shot topic extractors, LLMs are expected to understand human instructions to generate relevant and non-hallucinated topics based on the given documents. However, LLM-based topic modelling approaches often face difficulties in generating topics with adherence to granularity as specified in human instructions, often resulting in many near-duplicate topics. Furthermore, methods for addressing hallucinated topics generated by LLMs have not yet been investigated. In this paper, we focus on addressing the issues of topic granularity and hallucinations for better LLM-based topic modelling. To this end, we introduce a novel approach that leverages Direct Preference Optimisation (DPO) to fine-tune open-source LLMs, such as Mistral-7B. Our approach does not rely on traditional human annotation to rank preferred answers but employs a reconstruction pipeline to modify raw topics generated by LLMs, thus enabling a fast and efficient training and inference framework. Comparative experiments show that our fine-tuning approach not only significantly improves the LLM's capability to produce more coherent, relevant, and precise topics, but also reduces the number of hallucinated topics.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/46e542884db4fc4df605eb28473cff79aec54c99.pdf",
        "venue": "arXiv.org",
        "citationCount": 7,
        "score": 7.0,
        "summary": "Large language models (LLMs) with their strong zero-shot topic extraction capabilities offer an alternative to probabilistic topic modelling and closed-set topic classification approaches. As zero-shot topic extractors, LLMs are expected to understand human instructions to generate relevant and non-hallucinated topics based on the given documents. However, LLM-based topic modelling approaches often face difficulties in generating topics with adherence to granularity as specified in human instructions, often resulting in many near-duplicate topics. Furthermore, methods for addressing hallucinated topics generated by LLMs have not yet been investigated. In this paper, we focus on addressing the issues of topic granularity and hallucinations for better LLM-based topic modelling. To this end, we introduce a novel approach that leverages Direct Preference Optimisation (DPO) to fine-tune open-source LLMs, such as Mistral-7B. Our approach does not rely on traditional human annotation to rank preferred answers but employs a reconstruction pipeline to modify raw topics generated by LLMs, thus enabling a fast and efficient training and inference framework. Comparative experiments show that our fine-tuning approach not only significantly improves the LLM's capability to produce more coherent, relevant, and precise topics, but also reduces the number of hallucinated topics.",
        "keywords": []
      },
      "file_name": "46e542884db4fc4df605eb28473cff79aec54c99.pdf"
    },
    {
      "success": true,
      "doc_id": "823a0c6dadb12f1da67207258270dac3",
      "summary": "Visual Spatial Description (VSD) is an emerging image-to-text task which aims at generating descriptions of the spatial relationships between given objects in an image. In this paper, we apply Retrieval-Augmented Generation (RAG) technology in guiding Multimodal Large Language Models (MLLMs) for the task of VSD, complemented by an Adaptive Hallucination Corrector, and further fine-tuning them to bolster semantic understanding and overall model efficacy. We found that our approach demonstrated higher accuracy and fewer hallucination errors in both spatial relationship classification and visual language description tasks within the VSD task, achieving state-of-the-art results.",
      "intriguing_abstract": "Visual Spatial Description (VSD) is an emerging image-to-text task which aims at generating descriptions of the spatial relationships between given objects in an image. In this paper, we apply Retrieval-Augmented Generation (RAG) technology in guiding Multimodal Large Language Models (MLLMs) for the task of VSD, complemented by an Adaptive Hallucination Corrector, and further fine-tuning them to bolster semantic understanding and overall model efficacy. We found that our approach demonstrated higher accuracy and fewer hallucination errors in both spatial relationship classification and visual language description tasks within the VSD task, achieving state-of-the-art results.",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/ff06bee3d898b3dc3a0364f2bfe506591d7e6d52.pdf",
      "citation_key": "yu2024pp9",
      "metadata": {
        "title": "RAG-Guided Large Language Models for Visual Spatial Description with Adaptive Hallucination Corrector",
        "authors": [
          "Jun Yu",
          "Yunxiang Zhang",
          "Zerui Zhang",
          "Zhao Yang",
          "Gongpeng Zhao",
          "Fengzhao Sun",
          "Fanrui Zhang",
          "Qingsong Liu",
          "Jianqing Sun",
          "Jiaen Liang",
          "Yaohui Zhang"
        ],
        "published_date": "2024",
        "abstract": "Visual Spatial Description (VSD) is an emerging image-to-text task which aims at generating descriptions of the spatial relationships between given objects in an image. In this paper, we apply Retrieval-Augmented Generation (RAG) technology in guiding Multimodal Large Language Models (MLLMs) for the task of VSD, complemented by an Adaptive Hallucination Corrector, and further fine-tuning them to bolster semantic understanding and overall model efficacy. We found that our approach demonstrated higher accuracy and fewer hallucination errors in both spatial relationship classification and visual language description tasks within the VSD task, achieving state-of-the-art results.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/ff06bee3d898b3dc3a0364f2bfe506591d7e6d52.pdf",
        "venue": "ACM Multimedia",
        "citationCount": 7,
        "score": 7.0,
        "summary": "Visual Spatial Description (VSD) is an emerging image-to-text task which aims at generating descriptions of the spatial relationships between given objects in an image. In this paper, we apply Retrieval-Augmented Generation (RAG) technology in guiding Multimodal Large Language Models (MLLMs) for the task of VSD, complemented by an Adaptive Hallucination Corrector, and further fine-tuning them to bolster semantic understanding and overall model efficacy. We found that our approach demonstrated higher accuracy and fewer hallucination errors in both spatial relationship classification and visual language description tasks within the VSD task, achieving state-of-the-art results.",
        "keywords": []
      },
      "file_name": "ff06bee3d898b3dc3a0364f2bfe506591d7e6d52.pdf"
    },
    {
      "success": true,
      "doc_id": "2b28f1a0f5c55e4b2c901da76fc70808",
      "summary": "As Large Language Models (LLMs) become more pervasive across various users and scenarios, identifying potential issues when using these models becomes essential. Examples of such issues include: bias, inconsistencies, and hallucination. Although auditing the LLM for these problems is often warranted, such a process is neither easy nor accessible for most. An effective method is to probe the LLM using different versions of the same question. This could expose inconsistencies in its knowledge or operation, indicating potential for bias or hallucination. However, to operationalize this auditing method at scale, we need an approach to create those probes reliably and automatically. In this paper we propose the LLMAuditor framework which is an automatic, and scalable solution, where one uses a different LLM along with human-in-the-loop (HIL). This approach offers verifiability and transparency, while avoiding circular reliance on the same LLM, and increasing scientific rigor and generalizability. Specifically, LLMAuditor includes two phases of verification using humans: standardized evaluation criteria to verify responses, and a structured prompt template to generate desired probes. A case study using questions from the TruthfulQA dataset demonstrates that we can generate a reliable set of probes from one LLM that can be used to audit inconsistencies in a different LLM. This process is enhanced by our structured prompt template with HIL, which not only boosts the reliability of our approach in auditing but also yields the delivery of less hallucinated results. The novelty of our research stems from the development of a comprehensive, general-purpose framework that includes a HIL verified prompt template for auditing responses generated by LLMs.",
      "intriguing_abstract": "As Large Language Models (LLMs) become more pervasive across various users and scenarios, identifying potential issues when using these models becomes essential. Examples of such issues include: bias, inconsistencies, and hallucination. Although auditing the LLM for these problems is often warranted, such a process is neither easy nor accessible for most. An effective method is to probe the LLM using different versions of the same question. This could expose inconsistencies in its knowledge or operation, indicating potential for bias or hallucination. However, to operationalize this auditing method at scale, we need an approach to create those probes reliably and automatically. In this paper we propose the LLMAuditor framework which is an automatic, and scalable solution, where one uses a different LLM along with human-in-the-loop (HIL). This approach offers verifiability and transparency, while avoiding circular reliance on the same LLM, and increasing scientific rigor and generalizability. Specifically, LLMAuditor includes two phases of verification using humans: standardized evaluation criteria to verify responses, and a structured prompt template to generate desired probes. A case study using questions from the TruthfulQA dataset demonstrates that we can generate a reliable set of probes from one LLM that can be used to audit inconsistencies in a different LLM. This process is enhanced by our structured prompt template with HIL, which not only boosts the reliability of our approach in auditing but also yields the delivery of less hallucinated results. The novelty of our research stems from the development of a comprehensive, general-purpose framework that includes a HIL verified prompt template for auditing responses generated by LLMs.",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/7641749cae1ad30779bfb46948fd47922bcc296a.pdf",
      "citation_key": "amirizaniani2024cad",
      "metadata": {
        "title": "LLMAuditor: A Framework for Auditing Large Language Models Using Human-in-the-Loop",
        "authors": [
          "Maryam Amirizaniani",
          "Jihan Yao",
          "Adrian Lavergne",
          "Elizabeth Snell Okada",
          "Aman Chadha",
          "Tanya Roosta",
          "Chirag Shah"
        ],
        "published_date": "2024",
        "abstract": "As Large Language Models (LLMs) become more pervasive across various users and scenarios, identifying potential issues when using these models becomes essential. Examples of such issues include: bias, inconsistencies, and hallucination. Although auditing the LLM for these problems is often warranted, such a process is neither easy nor accessible for most. An effective method is to probe the LLM using different versions of the same question. This could expose inconsistencies in its knowledge or operation, indicating potential for bias or hallucination. However, to operationalize this auditing method at scale, we need an approach to create those probes reliably and automatically. In this paper we propose the LLMAuditor framework which is an automatic, and scalable solution, where one uses a different LLM along with human-in-the-loop (HIL). This approach offers verifiability and transparency, while avoiding circular reliance on the same LLM, and increasing scientific rigor and generalizability. Specifically, LLMAuditor includes two phases of verification using humans: standardized evaluation criteria to verify responses, and a structured prompt template to generate desired probes. A case study using questions from the TruthfulQA dataset demonstrates that we can generate a reliable set of probes from one LLM that can be used to audit inconsistencies in a different LLM. This process is enhanced by our structured prompt template with HIL, which not only boosts the reliability of our approach in auditing but also yields the delivery of less hallucinated results. The novelty of our research stems from the development of a comprehensive, general-purpose framework that includes a HIL verified prompt template for auditing responses generated by LLMs.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/7641749cae1ad30779bfb46948fd47922bcc296a.pdf",
        "venue": "",
        "citationCount": 7,
        "score": 7.0,
        "summary": "As Large Language Models (LLMs) become more pervasive across various users and scenarios, identifying potential issues when using these models becomes essential. Examples of such issues include: bias, inconsistencies, and hallucination. Although auditing the LLM for these problems is often warranted, such a process is neither easy nor accessible for most. An effective method is to probe the LLM using different versions of the same question. This could expose inconsistencies in its knowledge or operation, indicating potential for bias or hallucination. However, to operationalize this auditing method at scale, we need an approach to create those probes reliably and automatically. In this paper we propose the LLMAuditor framework which is an automatic, and scalable solution, where one uses a different LLM along with human-in-the-loop (HIL). This approach offers verifiability and transparency, while avoiding circular reliance on the same LLM, and increasing scientific rigor and generalizability. Specifically, LLMAuditor includes two phases of verification using humans: standardized evaluation criteria to verify responses, and a structured prompt template to generate desired probes. A case study using questions from the TruthfulQA dataset demonstrates that we can generate a reliable set of probes from one LLM that can be used to audit inconsistencies in a different LLM. This process is enhanced by our structured prompt template with HIL, which not only boosts the reliability of our approach in auditing but also yields the delivery of less hallucinated results. The novelty of our research stems from the development of a comprehensive, general-purpose framework that includes a HIL verified prompt template for auditing responses generated by LLMs.",
        "keywords": []
      },
      "file_name": "7641749cae1ad30779bfb46948fd47922bcc296a.pdf"
    },
    {
      "success": true,
      "doc_id": "953f66d815b51c3eff93f3dd5c767b58",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/7adb88771376c2a31688e3b0395b0550a35b824d.pdf",
      "citation_key": "ling2024qto",
      "metadata": {
        "title": "Uncertainty Decomposition and Quantification for In-Context Learning of Large Language Models",
        "authors": [
          "Chen Ling",
          "Xujiang Zhao",
          "Wei Cheng",
          "Yanchi Liu",
          "Yiyou Sun",
          "Xuchao Zhang",
          "Mika Oishi",
          "Takao Osaki",
          "Katsushi Matsuda",
          "Jie Ji",
          "Guangji Bai",
          "Liang Zhao",
          "Haifeng Chen"
        ],
        "published_date": "2024",
        "abstract": "",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/7adb88771376c2a31688e3b0395b0550a35b824d.pdf",
        "venue": "arXiv.org",
        "citationCount": 7,
        "score": 7.0,
        "summary": "",
        "keywords": []
      },
      "file_name": "7adb88771376c2a31688e3b0395b0550a35b824d.pdf"
    },
    {
      "success": true,
      "doc_id": "e397e32b25a03cf01657ffc7db6656a2",
      "summary": "For a financial analyst, the question and answer (Q&A) segment of the company financial report is a crucial piece of information for various analysis and investment decisions. However, extracting valuable insights from the Q&A section has posed considerable challenges as the conventional methods such as detailed reading and note-taking lack scalability and are susceptible to human errors, and Optical Character Recognition (OCR) and similar techniques encounter difficulties in accurately processing unstructured transcript text, often missing subtle linguistic nuances that drive investor decisions. Here, we demonstrate the utilization of Large Language Models (LLMs) to efficiently and rapidly extract information from earnings report transcripts while ensuring high accuracyâ€”transforming the extraction process as well as reducing hallucination by combining retrieval-augmented generation technique as well as metadata. We evaluate the outcomes of various LLMs with and without using our proposed approach based on various objective metrics for evaluating Q&A systems, and empirically demonstrate superiority of our method.",
      "intriguing_abstract": "For a financial analyst, the question and answer (Q&A) segment of the company financial report is a crucial piece of information for various analysis and investment decisions. However, extracting valuable insights from the Q&A section has posed considerable challenges as the conventional methods such as detailed reading and note-taking lack scalability and are susceptible to human errors, and Optical Character Recognition (OCR) and similar techniques encounter difficulties in accurately processing unstructured transcript text, often missing subtle linguistic nuances that drive investor decisions. Here, we demonstrate the utilization of Large Language Models (LLMs) to efficiently and rapidly extract information from earnings report transcripts while ensuring high accuracyâ€”transforming the extraction process as well as reducing hallucination by combining retrieval-augmented generation technique as well as metadata. We evaluate the outcomes of various LLMs with and without using our proposed approach based on various objective metrics for evaluating Q&A systems, and empirically demonstrate superiority of our method.",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/f41977c497c96c1da2e9e945315e9be6d6ad472e.pdf",
      "citation_key": "sarmah2023cuq",
      "metadata": {
        "title": "Towards reducing hallucination in extracting information from financial reports using Large Language Models",
        "authors": [
          "Bhaskarjit Sarmah",
          "Dhagash Mehta",
          "Stefano Pasquali",
          "Tianjie Zhu"
        ],
        "published_date": "2023",
        "abstract": "For a financial analyst, the question and answer (Q&A) segment of the company financial report is a crucial piece of information for various analysis and investment decisions. However, extracting valuable insights from the Q&A section has posed considerable challenges as the conventional methods such as detailed reading and note-taking lack scalability and are susceptible to human errors, and Optical Character Recognition (OCR) and similar techniques encounter difficulties in accurately processing unstructured transcript text, often missing subtle linguistic nuances that drive investor decisions. Here, we demonstrate the utilization of Large Language Models (LLMs) to efficiently and rapidly extract information from earnings report transcripts while ensuring high accuracyâ€”transforming the extraction process as well as reducing hallucination by combining retrieval-augmented generation technique as well as metadata. We evaluate the outcomes of various LLMs with and without using our proposed approach based on various objective metrics for evaluating Q&A systems, and empirically demonstrate superiority of our method.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/f41977c497c96c1da2e9e945315e9be6d6ad472e.pdf",
        "venue": "International Conference on AI-ML-Systems",
        "citationCount": 13,
        "score": 6.5,
        "summary": "For a financial analyst, the question and answer (Q&A) segment of the company financial report is a crucial piece of information for various analysis and investment decisions. However, extracting valuable insights from the Q&A section has posed considerable challenges as the conventional methods such as detailed reading and note-taking lack scalability and are susceptible to human errors, and Optical Character Recognition (OCR) and similar techniques encounter difficulties in accurately processing unstructured transcript text, often missing subtle linguistic nuances that drive investor decisions. Here, we demonstrate the utilization of Large Language Models (LLMs) to efficiently and rapidly extract information from earnings report transcripts while ensuring high accuracyâ€”transforming the extraction process as well as reducing hallucination by combining retrieval-augmented generation technique as well as metadata. We evaluate the outcomes of various LLMs with and without using our proposed approach based on various objective metrics for evaluating Q&A systems, and empirically demonstrate superiority of our method.",
        "keywords": []
      },
      "file_name": "f41977c497c96c1da2e9e945315e9be6d6ad472e.pdf"
    },
    {
      "success": true,
      "doc_id": "c2b96293d375b908c37990bdcf534a67",
      "summary": "Language Models (LMs) have shown impressive performance in various natural language tasks. However, when it comes to natural language reasoning, LMs still face challenges such as hallucination, generating incorrect intermediate reasoning steps, and making mathematical errors. Recent research has focused on enhancing LMs through self-improvement using feedback. Nevertheless, existing approaches relying on a single generic feedback source fail to address the diverse error types found in LM-generated reasoning chains. In this work, we propose Multi-Aspect Feedback, an iterative refinement framework that integrates multiple feedback modules, including frozen LMs and external tools, each focusing on a specific error category. Our experimental results demonstrate the efficacy of our approach to addressing several errors in the LM-generated reasoning chain and thus improving the overall performance of an LM in several reasoning tasks. We see a relative improvement of up to 20% in Mathematical Reasoning and up to 18% in Logical Entailment.",
      "intriguing_abstract": "Language Models (LMs) have shown impressive performance in various natural language tasks. However, when it comes to natural language reasoning, LMs still face challenges such as hallucination, generating incorrect intermediate reasoning steps, and making mathematical errors. Recent research has focused on enhancing LMs through self-improvement using feedback. Nevertheless, existing approaches relying on a single generic feedback source fail to address the diverse error types found in LM-generated reasoning chains. In this work, we propose Multi-Aspect Feedback, an iterative refinement framework that integrates multiple feedback modules, including frozen LMs and external tools, each focusing on a specific error category. Our experimental results demonstrate the efficacy of our approach to addressing several errors in the LM-generated reasoning chain and thus improving the overall performance of an LM in several reasoning tasks. We see a relative improvement of up to 20% in Mathematical Reasoning and up to 18% in Logical Entailment.",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/20eecb9ead20ffe49a66588a9662336eefb20a54.pdf",
      "citation_key": "nathani202338c",
      "metadata": {
        "title": "MAF: Multi-Aspect Feedback for Improving Reasoning in Large Language Models",
        "authors": [
          "Deepak Nathani",
          "David Wang",
          "Liangming Pan",
          "W. Wang"
        ],
        "published_date": "2023",
        "abstract": "Language Models (LMs) have shown impressive performance in various natural language tasks. However, when it comes to natural language reasoning, LMs still face challenges such as hallucination, generating incorrect intermediate reasoning steps, and making mathematical errors. Recent research has focused on enhancing LMs through self-improvement using feedback. Nevertheless, existing approaches relying on a single generic feedback source fail to address the diverse error types found in LM-generated reasoning chains. In this work, we propose Multi-Aspect Feedback, an iterative refinement framework that integrates multiple feedback modules, including frozen LMs and external tools, each focusing on a specific error category. Our experimental results demonstrate the efficacy of our approach to addressing several errors in the LM-generated reasoning chain and thus improving the overall performance of an LM in several reasoning tasks. We see a relative improvement of up to 20% in Mathematical Reasoning and up to 18% in Logical Entailment.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/20eecb9ead20ffe49a66588a9662336eefb20a54.pdf",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "citationCount": 13,
        "score": 6.5,
        "summary": "Language Models (LMs) have shown impressive performance in various natural language tasks. However, when it comes to natural language reasoning, LMs still face challenges such as hallucination, generating incorrect intermediate reasoning steps, and making mathematical errors. Recent research has focused on enhancing LMs through self-improvement using feedback. Nevertheless, existing approaches relying on a single generic feedback source fail to address the diverse error types found in LM-generated reasoning chains. In this work, we propose Multi-Aspect Feedback, an iterative refinement framework that integrates multiple feedback modules, including frozen LMs and external tools, each focusing on a specific error category. Our experimental results demonstrate the efficacy of our approach to addressing several errors in the LM-generated reasoning chain and thus improving the overall performance of an LM in several reasoning tasks. We see a relative improvement of up to 20% in Mathematical Reasoning and up to 18% in Logical Entailment.",
        "keywords": []
      },
      "file_name": "20eecb9ead20ffe49a66588a9662336eefb20a54.pdf"
    },
    {
      "success": true,
      "doc_id": "721410da89f7f77142b062d4a8f64e39",
      "summary": "Large language models (LLMs) are increasingly used as alternatives to traditional search engines given their capacity to generate text that resembles human language. However, this shift is concerning, as LLMs often generate hallucinations, misleading or false information that appears highly credible. In this study, we explore the phenomenon of hallucinations across multiple languages in freeform text generation, focusing on what we call multilingual hallucination gaps. These gaps reflect differences in the frequency of hallucinated answers depending on the prompt and language used. To quantify such hallucinations, we used the FactScore metric and extended its framework to a multilingual setting. We conducted experiments using LLMs from the LLaMA, Qwen, and Aya families, generating biographies in 19 languages and comparing the results to Wikipedia pages. Our results reveal variations in hallucination rates, especially between high and low resource languages, raising important questions about LLM multilingual performance and the challenges in evaluating hallucinations in multilingual freeform text generation.",
      "intriguing_abstract": "Large language models (LLMs) are increasingly used as alternatives to traditional search engines given their capacity to generate text that resembles human language. However, this shift is concerning, as LLMs often generate hallucinations, misleading or false information that appears highly credible. In this study, we explore the phenomenon of hallucinations across multiple languages in freeform text generation, focusing on what we call multilingual hallucination gaps. These gaps reflect differences in the frequency of hallucinated answers depending on the prompt and language used. To quantify such hallucinations, we used the FactScore metric and extended its framework to a multilingual setting. We conducted experiments using LLMs from the LLaMA, Qwen, and Aya families, generating biographies in 19 languages and comparing the results to Wikipedia pages. Our results reveal variations in hallucination rates, especially between high and low resource languages, raising important questions about LLM multilingual performance and the challenges in evaluating hallucinations in multilingual freeform text generation.",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/ff0450c78e5fabaa8aab61a368f267bd83753a64.pdf",
      "citation_key": "chataigner2024cr0",
      "metadata": {
        "title": "Multilingual Hallucination Gaps in Large Language Models",
        "authors": [
          "Cl'ea Chataigner",
          "Afaf TaÃ¯k",
          "G. Farnadi"
        ],
        "published_date": "2024",
        "abstract": "Large language models (LLMs) are increasingly used as alternatives to traditional search engines given their capacity to generate text that resembles human language. However, this shift is concerning, as LLMs often generate hallucinations, misleading or false information that appears highly credible. In this study, we explore the phenomenon of hallucinations across multiple languages in freeform text generation, focusing on what we call multilingual hallucination gaps. These gaps reflect differences in the frequency of hallucinated answers depending on the prompt and language used. To quantify such hallucinations, we used the FactScore metric and extended its framework to a multilingual setting. We conducted experiments using LLMs from the LLaMA, Qwen, and Aya families, generating biographies in 19 languages and comparing the results to Wikipedia pages. Our results reveal variations in hallucination rates, especially between high and low resource languages, raising important questions about LLM multilingual performance and the challenges in evaluating hallucinations in multilingual freeform text generation.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/ff0450c78e5fabaa8aab61a368f267bd83753a64.pdf",
        "venue": "arXiv.org",
        "citationCount": 6,
        "score": 6.0,
        "summary": "Large language models (LLMs) are increasingly used as alternatives to traditional search engines given their capacity to generate text that resembles human language. However, this shift is concerning, as LLMs often generate hallucinations, misleading or false information that appears highly credible. In this study, we explore the phenomenon of hallucinations across multiple languages in freeform text generation, focusing on what we call multilingual hallucination gaps. These gaps reflect differences in the frequency of hallucinated answers depending on the prompt and language used. To quantify such hallucinations, we used the FactScore metric and extended its framework to a multilingual setting. We conducted experiments using LLMs from the LLaMA, Qwen, and Aya families, generating biographies in 19 languages and comparing the results to Wikipedia pages. Our results reveal variations in hallucination rates, especially between high and low resource languages, raising important questions about LLM multilingual performance and the challenges in evaluating hallucinations in multilingual freeform text generation.",
        "keywords": []
      },
      "file_name": "ff0450c78e5fabaa8aab61a368f267bd83753a64.pdf"
    },
    {
      "success": true,
      "doc_id": "c1f596484c03c3ed4a12d3e27f027590",
      "summary": "Hallucination has emerged as a significant barrier to the effective application of Large Language Models (LLMs). In this work, we introduce a novel Attention-Guided SElf-Reflection (AGSER) approach for zero-shot hallucination detection in LLMs. The AGSER method utilizes attention contributions to categorize the input query into attentive and non-attentive queries. Each query is then processed separately through the LLMs, allowing us to compute consistency scores between the generated responses and the original answer. The difference between the two consistency scores serves as a hallucination estimator. In addition to its efficacy in detecting hallucinations, AGSER notably reduces computational overhead, requiring only three passes through the LLM and utilizing two sets of tokens. We have conducted extensive experiments with four widely-used LLMs across three different hallucination benchmarks, demonstrating that our approach significantly outperforms existing methods in zero-shot hallucination detection.",
      "intriguing_abstract": "Hallucination has emerged as a significant barrier to the effective application of Large Language Models (LLMs). In this work, we introduce a novel Attention-Guided SElf-Reflection (AGSER) approach for zero-shot hallucination detection in LLMs. The AGSER method utilizes attention contributions to categorize the input query into attentive and non-attentive queries. Each query is then processed separately through the LLMs, allowing us to compute consistency scores between the generated responses and the original answer. The difference between the two consistency scores serves as a hallucination estimator. In addition to its efficacy in detecting hallucinations, AGSER notably reduces computational overhead, requiring only three passes through the LLM and utilizing two sets of tokens. We have conducted extensive experiments with four widely-used LLMs across three different hallucination benchmarks, demonstrating that our approach significantly outperforms existing methods in zero-shot hallucination detection.",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/e33fceb7cfb825ae3c530de0bf093769169039fc.pdf",
      "citation_key": "liu2025xwv",
      "metadata": {
        "title": "Attention-guided Self-reflection for Zero-shot Hallucination Detection in Large Language Models",
        "authors": [
          "Q. Liu",
          "Xinlong Chen",
          "Yue Ding",
          "Shizhen Xu",
          "Shu Wu",
          "Liang Wang"
        ],
        "published_date": "2025",
        "abstract": "Hallucination has emerged as a significant barrier to the effective application of Large Language Models (LLMs). In this work, we introduce a novel Attention-Guided SElf-Reflection (AGSER) approach for zero-shot hallucination detection in LLMs. The AGSER method utilizes attention contributions to categorize the input query into attentive and non-attentive queries. Each query is then processed separately through the LLMs, allowing us to compute consistency scores between the generated responses and the original answer. The difference between the two consistency scores serves as a hallucination estimator. In addition to its efficacy in detecting hallucinations, AGSER notably reduces computational overhead, requiring only three passes through the LLM and utilizing two sets of tokens. We have conducted extensive experiments with four widely-used LLMs across three different hallucination benchmarks, demonstrating that our approach significantly outperforms existing methods in zero-shot hallucination detection.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/e33fceb7cfb825ae3c530de0bf093769169039fc.pdf",
        "venue": "arXiv.org",
        "citationCount": 6,
        "score": 6.0,
        "summary": "Hallucination has emerged as a significant barrier to the effective application of Large Language Models (LLMs). In this work, we introduce a novel Attention-Guided SElf-Reflection (AGSER) approach for zero-shot hallucination detection in LLMs. The AGSER method utilizes attention contributions to categorize the input query into attentive and non-attentive queries. Each query is then processed separately through the LLMs, allowing us to compute consistency scores between the generated responses and the original answer. The difference between the two consistency scores serves as a hallucination estimator. In addition to its efficacy in detecting hallucinations, AGSER notably reduces computational overhead, requiring only three passes through the LLM and utilizing two sets of tokens. We have conducted extensive experiments with four widely-used LLMs across three different hallucination benchmarks, demonstrating that our approach significantly outperforms existing methods in zero-shot hallucination detection.",
        "keywords": []
      },
      "file_name": "e33fceb7cfb825ae3c530de0bf093769169039fc.pdf"
    },
    {
      "success": true,
      "doc_id": "dfb2cf6808c047376b15d79f5e8d9d62",
      "summary": "In this paper, we identify a new category of bias that induces input-conflicting hallucinations, where large language models (LLMs) generate responses inconsistent with the content of the input context. This issue we have termed the false negative problem refers to the phenomenon where LLMs are predisposed to return negative judgments when assessing the correctness of a statement given the context. In experiments involving pairs of statements that contain the same information but have contradictory factual directions, we observe that LLMs exhibit a bias toward false negatives. Specifically, the model presents greater overconfidence when responding with False. Furthermore, we analyze the relationship between the false negative problem and context and query rewriting and observe that both effectively tackle false negatives in LLMs.",
      "intriguing_abstract": "In this paper, we identify a new category of bias that induces input-conflicting hallucinations, where large language models (LLMs) generate responses inconsistent with the content of the input context. This issue we have termed the false negative problem refers to the phenomenon where LLMs are predisposed to return negative judgments when assessing the correctness of a statement given the context. In experiments involving pairs of statements that contain the same information but have contradictory factual directions, we observe that LLMs exhibit a bias toward false negatives. Specifically, the model presents greater overconfidence when responding with False. Furthermore, we analyze the relationship between the false negative problem and context and query rewriting and observe that both effectively tackle false negatives in LLMs.",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/6518d3209cec0a1ad277e8aaf153242b3a4233d9.pdf",
      "citation_key": "song2024v5n",
      "metadata": {
        "title": "Large Language Models are Skeptics: False Negative Problem of Input-conflicting Hallucination",
        "authors": [
          "Jongyoon Song",
          "Sangwon Yu",
          "Sungroh Yoon"
        ],
        "published_date": "2024",
        "abstract": "In this paper, we identify a new category of bias that induces input-conflicting hallucinations, where large language models (LLMs) generate responses inconsistent with the content of the input context. This issue we have termed the false negative problem refers to the phenomenon where LLMs are predisposed to return negative judgments when assessing the correctness of a statement given the context. In experiments involving pairs of statements that contain the same information but have contradictory factual directions, we observe that LLMs exhibit a bias toward false negatives. Specifically, the model presents greater overconfidence when responding with False. Furthermore, we analyze the relationship between the false negative problem and context and query rewriting and observe that both effectively tackle false negatives in LLMs.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/6518d3209cec0a1ad277e8aaf153242b3a4233d9.pdf",
        "venue": "arXiv.org",
        "citationCount": 6,
        "score": 6.0,
        "summary": "In this paper, we identify a new category of bias that induces input-conflicting hallucinations, where large language models (LLMs) generate responses inconsistent with the content of the input context. This issue we have termed the false negative problem refers to the phenomenon where LLMs are predisposed to return negative judgments when assessing the correctness of a statement given the context. In experiments involving pairs of statements that contain the same information but have contradictory factual directions, we observe that LLMs exhibit a bias toward false negatives. Specifically, the model presents greater overconfidence when responding with False. Furthermore, we analyze the relationship between the false negative problem and context and query rewriting and observe that both effectively tackle false negatives in LLMs.",
        "keywords": []
      },
      "file_name": "6518d3209cec0a1ad277e8aaf153242b3a4233d9.pdf"
    },
    {
      "success": true,
      "doc_id": "ed1b37494b8f36c96d953de46734c5c1",
      "summary": "Large Language Models (LLMs) are powerful computational models trained on extensive corpora of human-readable text, enabling them to perform general-purpose language understanding and generation. LLMs have garnered significant attention in both industry and academia due to their exceptional performance across various natural language processing (NLP) tasks. Despite these successes, LLMs often produce inaccuracies, commonly referred to as hallucinations. Prompt engineering, the process of designing and formulating instructions for LLMs to perform specific tasks, has emerged as a key approach to mitigating hallucinations. This paper provides a comprehensive empirical evaluation of different prompting strategies and frameworks aimed at reducing hallucinations in LLMs. Various prompting techniques are applied to a broad set of benchmark datasets to assess the accuracy and hallucination rate of each method. Additionally, the paper investigates the influence of tool-calling agents (LLMs augmented with external tools to enhance their capabilities beyond language generation) on hallucination rates in the same benchmarks. The findings demonstrate that the optimal prompting technique depends on the type of problem, and that simpler techniques often outperform more complex methods in reducing hallucinations. Furthermore, it is shown that LLM agents can exhibit significantly higher hallucination rates due to the added complexity of external tool usage.",
      "intriguing_abstract": "Large Language Models (LLMs) are powerful computational models trained on extensive corpora of human-readable text, enabling them to perform general-purpose language understanding and generation. LLMs have garnered significant attention in both industry and academia due to their exceptional performance across various natural language processing (NLP) tasks. Despite these successes, LLMs often produce inaccuracies, commonly referred to as hallucinations. Prompt engineering, the process of designing and formulating instructions for LLMs to perform specific tasks, has emerged as a key approach to mitigating hallucinations. This paper provides a comprehensive empirical evaluation of different prompting strategies and frameworks aimed at reducing hallucinations in LLMs. Various prompting techniques are applied to a broad set of benchmark datasets to assess the accuracy and hallucination rate of each method. Additionally, the paper investigates the influence of tool-calling agents (LLMs augmented with external tools to enhance their capabilities beyond language generation) on hallucination rates in the same benchmarks. The findings demonstrate that the optimal prompting technique depends on the type of problem, and that simpler techniques often outperform more complex methods in reducing hallucinations. Furthermore, it is shown that LLM agents can exhibit significantly higher hallucination rates due to the added complexity of external tool usage.",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/d45865c981161ad711adf18b0492e959771554e4.pdf",
      "citation_key": "barkley202472d",
      "metadata": {
        "title": "Investigating the Role of Prompting and External Tools in Hallucination Rates of Large Language Models",
        "authors": [
          "Liam Barkley",
          "Brink van der Merwe"
        ],
        "published_date": "2024",
        "abstract": "Large Language Models (LLMs) are powerful computational models trained on extensive corpora of human-readable text, enabling them to perform general-purpose language understanding and generation. LLMs have garnered significant attention in both industry and academia due to their exceptional performance across various natural language processing (NLP) tasks. Despite these successes, LLMs often produce inaccuracies, commonly referred to as hallucinations. Prompt engineering, the process of designing and formulating instructions for LLMs to perform specific tasks, has emerged as a key approach to mitigating hallucinations. This paper provides a comprehensive empirical evaluation of different prompting strategies and frameworks aimed at reducing hallucinations in LLMs. Various prompting techniques are applied to a broad set of benchmark datasets to assess the accuracy and hallucination rate of each method. Additionally, the paper investigates the influence of tool-calling agents (LLMs augmented with external tools to enhance their capabilities beyond language generation) on hallucination rates in the same benchmarks. The findings demonstrate that the optimal prompting technique depends on the type of problem, and that simpler techniques often outperform more complex methods in reducing hallucinations. Furthermore, it is shown that LLM agents can exhibit significantly higher hallucination rates due to the added complexity of external tool usage.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/d45865c981161ad711adf18b0492e959771554e4.pdf",
        "venue": "arXiv.org",
        "citationCount": 6,
        "score": 6.0,
        "summary": "Large Language Models (LLMs) are powerful computational models trained on extensive corpora of human-readable text, enabling them to perform general-purpose language understanding and generation. LLMs have garnered significant attention in both industry and academia due to their exceptional performance across various natural language processing (NLP) tasks. Despite these successes, LLMs often produce inaccuracies, commonly referred to as hallucinations. Prompt engineering, the process of designing and formulating instructions for LLMs to perform specific tasks, has emerged as a key approach to mitigating hallucinations. This paper provides a comprehensive empirical evaluation of different prompting strategies and frameworks aimed at reducing hallucinations in LLMs. Various prompting techniques are applied to a broad set of benchmark datasets to assess the accuracy and hallucination rate of each method. Additionally, the paper investigates the influence of tool-calling agents (LLMs augmented with external tools to enhance their capabilities beyond language generation) on hallucination rates in the same benchmarks. The findings demonstrate that the optimal prompting technique depends on the type of problem, and that simpler techniques often outperform more complex methods in reducing hallucinations. Furthermore, it is shown that LLM agents can exhibit significantly higher hallucination rates due to the added complexity of external tool usage.",
        "keywords": []
      },
      "file_name": "d45865c981161ad711adf18b0492e959771554e4.pdf"
    },
    {
      "success": true,
      "doc_id": "1cc6a9211092bf5cb45811cdc8e84a02",
      "summary": "Large language models have demonstrated significant potential as the next-generation information access engines. However, their reliability is hindered by issues of hallucination and generating non-factual content. This is particularly problematic in long-form responses, where assessing and ensuring factual accuracy is complex. In this paper, we address this gap by proposing FactAlign, a novel alignment framework designed to enhance the factuality of LLMs' long-form responses while maintaining their helpfulness. We introduce fKTO, a fine-grained, sentence-level alignment algorithm that extends the Kahneman-Tversky Optimization (KTO) alignment method. Leveraging recent advances in automatic factuality evaluation, FactAlign utilizes fine-grained factuality assessments to guide the alignment process. Our experiments on open-domain prompts and information-seeking questions demonstrate that FactAlign significantly improves the factual accuracy of LLM responses while also improving their helpfulness. Further analyses identify that FactAlign is capable of training LLMs to provide more information without losing factual precision, thus improving the factual F1 score. Our source code, datasets, and trained models are publicly available at https://github.com/MiuLab/FactAlign",
      "intriguing_abstract": "Large language models have demonstrated significant potential as the next-generation information access engines. However, their reliability is hindered by issues of hallucination and generating non-factual content. This is particularly problematic in long-form responses, where assessing and ensuring factual accuracy is complex. In this paper, we address this gap by proposing FactAlign, a novel alignment framework designed to enhance the factuality of LLMs' long-form responses while maintaining their helpfulness. We introduce fKTO, a fine-grained, sentence-level alignment algorithm that extends the Kahneman-Tversky Optimization (KTO) alignment method. Leveraging recent advances in automatic factuality evaluation, FactAlign utilizes fine-grained factuality assessments to guide the alignment process. Our experiments on open-domain prompts and information-seeking questions demonstrate that FactAlign significantly improves the factual accuracy of LLM responses while also improving their helpfulness. Further analyses identify that FactAlign is capable of training LLMs to provide more information without losing factual precision, thus improving the factual F1 score. Our source code, datasets, and trained models are publicly available at https://github.com/MiuLab/FactAlign",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/4d710532fff7aec8187f68fb2ca90079c40e7004.pdf",
      "citation_key": "huang2024c9t",
      "metadata": {
        "title": "FactAlign: Long-form Factuality Alignment of Large Language Models",
        "authors": [
          "Chao-Wei Huang",
          "Yun-Nung Chen"
        ],
        "published_date": "2024",
        "abstract": "Large language models have demonstrated significant potential as the next-generation information access engines. However, their reliability is hindered by issues of hallucination and generating non-factual content. This is particularly problematic in long-form responses, where assessing and ensuring factual accuracy is complex. In this paper, we address this gap by proposing FactAlign, a novel alignment framework designed to enhance the factuality of LLMs' long-form responses while maintaining their helpfulness. We introduce fKTO, a fine-grained, sentence-level alignment algorithm that extends the Kahneman-Tversky Optimization (KTO) alignment method. Leveraging recent advances in automatic factuality evaluation, FactAlign utilizes fine-grained factuality assessments to guide the alignment process. Our experiments on open-domain prompts and information-seeking questions demonstrate that FactAlign significantly improves the factual accuracy of LLM responses while also improving their helpfulness. Further analyses identify that FactAlign is capable of training LLMs to provide more information without losing factual precision, thus improving the factual F1 score. Our source code, datasets, and trained models are publicly available at https://github.com/MiuLab/FactAlign",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/4d710532fff7aec8187f68fb2ca90079c40e7004.pdf",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "citationCount": 6,
        "score": 6.0,
        "summary": "Large language models have demonstrated significant potential as the next-generation information access engines. However, their reliability is hindered by issues of hallucination and generating non-factual content. This is particularly problematic in long-form responses, where assessing and ensuring factual accuracy is complex. In this paper, we address this gap by proposing FactAlign, a novel alignment framework designed to enhance the factuality of LLMs' long-form responses while maintaining their helpfulness. We introduce fKTO, a fine-grained, sentence-level alignment algorithm that extends the Kahneman-Tversky Optimization (KTO) alignment method. Leveraging recent advances in automatic factuality evaluation, FactAlign utilizes fine-grained factuality assessments to guide the alignment process. Our experiments on open-domain prompts and information-seeking questions demonstrate that FactAlign significantly improves the factual accuracy of LLM responses while also improving their helpfulness. Further analyses identify that FactAlign is capable of training LLMs to provide more information without losing factual precision, thus improving the factual F1 score. Our source code, datasets, and trained models are publicly available at https://github.com/MiuLab/FactAlign",
        "keywords": []
      },
      "file_name": "4d710532fff7aec8187f68fb2ca90079c40e7004.pdf"
    },
    {
      "success": true,
      "doc_id": "72c14d351665e6010b79322b9dc5a29a",
      "summary": "This review examines the means with which faithfulness has been evaluated across open-ended summarization, question-answering and machine translation tasks. We find that the use of LLMs as a faithfulness evaluator is commonly the metric that is most highly correlated with human judgement. The means with which other studies have mitigated hallucinations is discussed, with both retrieval augmented generation (RAG) and prompting framework approaches having been linked with superior faithfulness, whilst other recommendations for mitigation are provided. Research into faithfulness is integral to the continued widespread use of LLMs, as unfaithful responses can pose major risks to many areas whereby LLMs would otherwise be suitable. Furthermore, evaluating open-ended generation provides a more comprehensive measure of LLM performance than commonly used multiple-choice benchmarking, which can help in advancing the trust that can be placed within LLMs.",
      "intriguing_abstract": "This review examines the means with which faithfulness has been evaluated across open-ended summarization, question-answering and machine translation tasks. We find that the use of LLMs as a faithfulness evaluator is commonly the metric that is most highly correlated with human judgement. The means with which other studies have mitigated hallucinations is discussed, with both retrieval augmented generation (RAG) and prompting framework approaches having been linked with superior faithfulness, whilst other recommendations for mitigation are provided. Research into faithfulness is integral to the continued widespread use of LLMs, as unfaithful responses can pose major risks to many areas whereby LLMs would otherwise be suitable. Furthermore, evaluating open-ended generation provides a more comprehensive measure of LLM performance than commonly used multiple-choice benchmarking, which can help in advancing the trust that can be placed within LLMs.",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/003463160918704684f812ae8d7b6920d2d15e31.pdf",
      "citation_key": "malin2024fin",
      "metadata": {
        "title": "A review of faithfulness metrics for hallucination assessment in Large Language Models",
        "authors": [
          "B. Malin",
          "Tatiana Kalganova",
          "Nikoloas Boulgouris"
        ],
        "published_date": "2024",
        "abstract": "This review examines the means with which faithfulness has been evaluated across open-ended summarization, question-answering and machine translation tasks. We find that the use of LLMs as a faithfulness evaluator is commonly the metric that is most highly correlated with human judgement. The means with which other studies have mitigated hallucinations is discussed, with both retrieval augmented generation (RAG) and prompting framework approaches having been linked with superior faithfulness, whilst other recommendations for mitigation are provided. Research into faithfulness is integral to the continued widespread use of LLMs, as unfaithful responses can pose major risks to many areas whereby LLMs would otherwise be suitable. Furthermore, evaluating open-ended generation provides a more comprehensive measure of LLM performance than commonly used multiple-choice benchmarking, which can help in advancing the trust that can be placed within LLMs.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/003463160918704684f812ae8d7b6920d2d15e31.pdf",
        "venue": "IEEE Journal on Selected Topics in Signal Processing",
        "citationCount": 5,
        "score": 5.0,
        "summary": "This review examines the means with which faithfulness has been evaluated across open-ended summarization, question-answering and machine translation tasks. We find that the use of LLMs as a faithfulness evaluator is commonly the metric that is most highly correlated with human judgement. The means with which other studies have mitigated hallucinations is discussed, with both retrieval augmented generation (RAG) and prompting framework approaches having been linked with superior faithfulness, whilst other recommendations for mitigation are provided. Research into faithfulness is integral to the continued widespread use of LLMs, as unfaithful responses can pose major risks to many areas whereby LLMs would otherwise be suitable. Furthermore, evaluating open-ended generation provides a more comprehensive measure of LLM performance than commonly used multiple-choice benchmarking, which can help in advancing the trust that can be placed within LLMs.",
        "keywords": []
      },
      "file_name": "003463160918704684f812ae8d7b6920d2d15e31.pdf"
    },
    {
      "success": true,
      "doc_id": "25743100f77290d19758d965951bfd91",
      "summary": "Large Language Models (LLMs) have shown impressive capabilities but still suffer from the issue of hallucinations. A significant type of this issue is the false premise hallucination, which we define as the phenomenon when LLMs generate hallucinated text when confronted with false premise questions. In this paper, we perform a comprehensive analysis of the false premise hallucination and elucidate its internal working mechanism: a small subset of attention heads (which we designate as false premise heads) disturb the knowledge extraction process, leading to the occurrence of false premise hallucination. Based on our analysis, we propose FAITH (False premise Attention head constraIining for miTigating Hallucinations), a novel and effective method to mitigate false premise hallucinations. It constrains the false premise attention heads during the model inference process. Impressively, extensive experiments demonstrate that constraining only approximately 1% of the attention heads in the model yields a notable increase of nearly 20% of model performance.",
      "intriguing_abstract": "Large Language Models (LLMs) have shown impressive capabilities but still suffer from the issue of hallucinations. A significant type of this issue is the false premise hallucination, which we define as the phenomenon when LLMs generate hallucinated text when confronted with false premise questions. In this paper, we perform a comprehensive analysis of the false premise hallucination and elucidate its internal working mechanism: a small subset of attention heads (which we designate as false premise heads) disturb the knowledge extraction process, leading to the occurrence of false premise hallucination. Based on our analysis, we propose FAITH (False premise Attention head constraIining for miTigating Hallucinations), a novel and effective method to mitigate false premise hallucinations. It constrains the false premise attention heads during the model inference process. Impressively, extensive experiments demonstrate that constraining only approximately 1% of the attention heads in the model yields a notable increase of nearly 20% of model performance.",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/f0a79fe7765ab253480a0be6d29c889eac19eb3c.pdf",
      "citation_key": "yuan2024o7d",
      "metadata": {
        "title": "Whispers that Shake Foundations: Analyzing and Mitigating False Premise Hallucinations in Large Language Models",
        "authors": [
          "Hongbang Yuan",
          "Pengfei Cao",
          "Zhuoran Jin",
          "Yubo Chen",
          "Daojian Zeng",
          "Kang Liu",
          "Jun Zhao"
        ],
        "published_date": "2024",
        "abstract": "Large Language Models (LLMs) have shown impressive capabilities but still suffer from the issue of hallucinations. A significant type of this issue is the false premise hallucination, which we define as the phenomenon when LLMs generate hallucinated text when confronted with false premise questions. In this paper, we perform a comprehensive analysis of the false premise hallucination and elucidate its internal working mechanism: a small subset of attention heads (which we designate as false premise heads) disturb the knowledge extraction process, leading to the occurrence of false premise hallucination. Based on our analysis, we propose FAITH (False premise Attention head constraIining for miTigating Hallucinations), a novel and effective method to mitigate false premise hallucinations. It constrains the false premise attention heads during the model inference process. Impressively, extensive experiments demonstrate that constraining only approximately 1% of the attention heads in the model yields a notable increase of nearly 20% of model performance.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/f0a79fe7765ab253480a0be6d29c889eac19eb3c.pdf",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "citationCount": 5,
        "score": 5.0,
        "summary": "Large Language Models (LLMs) have shown impressive capabilities but still suffer from the issue of hallucinations. A significant type of this issue is the false premise hallucination, which we define as the phenomenon when LLMs generate hallucinated text when confronted with false premise questions. In this paper, we perform a comprehensive analysis of the false premise hallucination and elucidate its internal working mechanism: a small subset of attention heads (which we designate as false premise heads) disturb the knowledge extraction process, leading to the occurrence of false premise hallucination. Based on our analysis, we propose FAITH (False premise Attention head constraIining for miTigating Hallucinations), a novel and effective method to mitigate false premise hallucinations. It constrains the false premise attention heads during the model inference process. Impressively, extensive experiments demonstrate that constraining only approximately 1% of the attention heads in the model yields a notable increase of nearly 20% of model performance.",
        "keywords": []
      },
      "file_name": "f0a79fe7765ab253480a0be6d29c889eac19eb3c.pdf"
    },
    {
      "success": true,
      "doc_id": "5087d345206f6c0153dcfe32d0282411",
      "summary": "Advancements in Large Language Models (LLMs) and their increasing use in medical question-answering necessitate rigorous evaluation of their reliability. A critical challenge lies in hallucination, where models generate plausible yet factually incorrect outputs. In the medical domain, this poses serious risks to patient safety and clinical decision-making. To address this, we introduce MedHallu, the first benchmark specifically designed for medical hallucination detection. MedHallu comprises 10,000 high-quality question-answer pairs derived from PubMedQA, with hallucinated answers systematically generated through a controlled pipeline. Our experiments show that state-of-the-art LLMs, including GPT-4o, Llama-3.1, and the medically fine-tuned UltraMedical, struggle with this binary hallucination detection task, with the best model achieving an F1 score as low as 0.625 for detecting\"hard\"category hallucinations. Using bidirectional entailment clustering, we show that harder-to-detect hallucinations are semantically closer to ground truth. Through experiments, we also show incorporating domain-specific knowledge and introducing a\"not sure\"category as one of the answer categories improves the precision and F1 scores by up to 38% relative to baselines.",
      "intriguing_abstract": "Advancements in Large Language Models (LLMs) and their increasing use in medical question-answering necessitate rigorous evaluation of their reliability. A critical challenge lies in hallucination, where models generate plausible yet factually incorrect outputs. In the medical domain, this poses serious risks to patient safety and clinical decision-making. To address this, we introduce MedHallu, the first benchmark specifically designed for medical hallucination detection. MedHallu comprises 10,000 high-quality question-answer pairs derived from PubMedQA, with hallucinated answers systematically generated through a controlled pipeline. Our experiments show that state-of-the-art LLMs, including GPT-4o, Llama-3.1, and the medically fine-tuned UltraMedical, struggle with this binary hallucination detection task, with the best model achieving an F1 score as low as 0.625 for detecting\"hard\"category hallucinations. Using bidirectional entailment clustering, we show that harder-to-detect hallucinations are semantically closer to ground truth. Through experiments, we also show incorporating domain-specific knowledge and introducing a\"not sure\"category as one of the answer categories improves the precision and F1 scores by up to 38% relative to baselines.",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/910d26adcd83c4ec36f365198f8b2224b14ad6c9.pdf",
      "citation_key": "pandit20257jx",
      "metadata": {
        "title": "MedHallu: A Comprehensive Benchmark for Detecting Medical Hallucinations in Large Language Models",
        "authors": [
          "Shrey Pandit",
          "Jiawei Xu",
          "Junyuan Hong",
          "Zhangyang Wang",
          "Tianlong Chen",
          "Kaidi Xu",
          "Ying Ding"
        ],
        "published_date": "2025",
        "abstract": "Advancements in Large Language Models (LLMs) and their increasing use in medical question-answering necessitate rigorous evaluation of their reliability. A critical challenge lies in hallucination, where models generate plausible yet factually incorrect outputs. In the medical domain, this poses serious risks to patient safety and clinical decision-making. To address this, we introduce MedHallu, the first benchmark specifically designed for medical hallucination detection. MedHallu comprises 10,000 high-quality question-answer pairs derived from PubMedQA, with hallucinated answers systematically generated through a controlled pipeline. Our experiments show that state-of-the-art LLMs, including GPT-4o, Llama-3.1, and the medically fine-tuned UltraMedical, struggle with this binary hallucination detection task, with the best model achieving an F1 score as low as 0.625 for detecting\"hard\"category hallucinations. Using bidirectional entailment clustering, we show that harder-to-detect hallucinations are semantically closer to ground truth. Through experiments, we also show incorporating domain-specific knowledge and introducing a\"not sure\"category as one of the answer categories improves the precision and F1 scores by up to 38% relative to baselines.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/910d26adcd83c4ec36f365198f8b2224b14ad6c9.pdf",
        "venue": "arXiv.org",
        "citationCount": 5,
        "score": 5.0,
        "summary": "Advancements in Large Language Models (LLMs) and their increasing use in medical question-answering necessitate rigorous evaluation of their reliability. A critical challenge lies in hallucination, where models generate plausible yet factually incorrect outputs. In the medical domain, this poses serious risks to patient safety and clinical decision-making. To address this, we introduce MedHallu, the first benchmark specifically designed for medical hallucination detection. MedHallu comprises 10,000 high-quality question-answer pairs derived from PubMedQA, with hallucinated answers systematically generated through a controlled pipeline. Our experiments show that state-of-the-art LLMs, including GPT-4o, Llama-3.1, and the medically fine-tuned UltraMedical, struggle with this binary hallucination detection task, with the best model achieving an F1 score as low as 0.625 for detecting\"hard\"category hallucinations. Using bidirectional entailment clustering, we show that harder-to-detect hallucinations are semantically closer to ground truth. Through experiments, we also show incorporating domain-specific knowledge and introducing a\"not sure\"category as one of the answer categories improves the precision and F1 scores by up to 38% relative to baselines.",
        "keywords": []
      },
      "file_name": "910d26adcd83c4ec36f365198f8b2224b14ad6c9.pdf"
    },
    {
      "success": true,
      "doc_id": "8179962d3a350f006d17ac222f673ca3",
      "summary": "State-of-the-art Large Language Models have recently exhibited extraordinary linguistic abilities which have surprisingly extended to reasoning. However, responses that are unreliable, false, or invented are still a frequent issue. It has been argued that scaling up strategies, as in increasing model size or hardware power, might not be enough to resolve the issue. Recent research has implemented Type 2 strategies (such as Chain-of-Thought and Tree-of-Thought), as strategies that mimic Type 2 reasoning, from Dual Process Theory, to interact with Large Language Models for improved results. The current paper reviews these strategies in light of the Predicting and Reflecting Framework for understanding Dual Process Theory and suggests what Psychology, drawing from research in executive functions, thinking disposition and creativity, can further contribute to possible implementations that address hallucination and reliability issues.",
      "intriguing_abstract": "State-of-the-art Large Language Models have recently exhibited extraordinary linguistic abilities which have surprisingly extended to reasoning. However, responses that are unreliable, false, or invented are still a frequent issue. It has been argued that scaling up strategies, as in increasing model size or hardware power, might not be enough to resolve the issue. Recent research has implemented Type 2 strategies (such as Chain-of-Thought and Tree-of-Thought), as strategies that mimic Type 2 reasoning, from Dual Process Theory, to interact with Large Language Models for improved results. The current paper reviews these strategies in light of the Predicting and Reflecting Framework for understanding Dual Process Theory and suggests what Psychology, drawing from research in executive functions, thinking disposition and creativity, can further contribute to possible implementations that address hallucination and reliability issues.",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/b5f56f466c06d10100d8d1aac9e1f979c527b1cf.pdf",
      "citation_key": "bellinileite2023y38",
      "metadata": {
        "title": "Dual Process Theory for Large Language Models: An overview of using Psychology to address hallucination and reliability issues",
        "authors": [
          "Samuel C. Bellini-Leite"
        ],
        "published_date": "2023",
        "abstract": "State-of-the-art Large Language Models have recently exhibited extraordinary linguistic abilities which have surprisingly extended to reasoning. However, responses that are unreliable, false, or invented are still a frequent issue. It has been argued that scaling up strategies, as in increasing model size or hardware power, might not be enough to resolve the issue. Recent research has implemented Type 2 strategies (such as Chain-of-Thought and Tree-of-Thought), as strategies that mimic Type 2 reasoning, from Dual Process Theory, to interact with Large Language Models for improved results. The current paper reviews these strategies in light of the Predicting and Reflecting Framework for understanding Dual Process Theory and suggests what Psychology, drawing from research in executive functions, thinking disposition and creativity, can further contribute to possible implementations that address hallucination and reliability issues.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/b5f56f466c06d10100d8d1aac9e1f979c527b1cf.pdf",
        "venue": "Adaptive Behavior",
        "citationCount": 10,
        "score": 5.0,
        "summary": "State-of-the-art Large Language Models have recently exhibited extraordinary linguistic abilities which have surprisingly extended to reasoning. However, responses that are unreliable, false, or invented are still a frequent issue. It has been argued that scaling up strategies, as in increasing model size or hardware power, might not be enough to resolve the issue. Recent research has implemented Type 2 strategies (such as Chain-of-Thought and Tree-of-Thought), as strategies that mimic Type 2 reasoning, from Dual Process Theory, to interact with Large Language Models for improved results. The current paper reviews these strategies in light of the Predicting and Reflecting Framework for understanding Dual Process Theory and suggests what Psychology, drawing from research in executive functions, thinking disposition and creativity, can further contribute to possible implementations that address hallucination and reliability issues.",
        "keywords": []
      },
      "file_name": "b5f56f466c06d10100d8d1aac9e1f979c527b1cf.pdf"
    },
    {
      "success": true,
      "doc_id": "28041145b0cacc12f4d8d3b73bc0ce21",
      "summary": "Background: Large language models (LLMs) show promise in clinical contexts but can generate false facts (often referred to as hallucinations). One subset of these errors arises from adversarial attacks, in which fabricated details embedded in prompts lead the model to produce or elaborate on the false information. We embedded fabricated content in clinical prompts to elicit adversarial hallucination attacks in multiple large language models. We quantified how often they elaborated on false details and tested whether a specialized mitigation prompt or altered temperature settings reduced errors. Methods: We created 300 physician-validated simulated vignettes, each containing one fabricated detail (a laboratory test, a physical or radiological sign, or a medical condition). Each vignette was presented in short and long versions - differing only in word count but identical in medical content. We tested six LLMs under three conditions: default (standard settings), mitigating prompt (designed to reduce hallucinations), and temperature 0 (deterministic output with maximum response certainty), generating 5,400 outputs. If a model elaborated on the fabricated detail, the case was classified as a hallucination. Results: Hallucination rates ranged from 50% to 82% across models and prompting methods. Prompt-based mitigation lowered overall hallucinations (mean across all models) from 66% to 44% (p<0.001). For the best overall performing model, GPT 4o, rates declined from 53% to 23% (p<0.001). Temperature adjustments offered no significant improvement. Short vignettes showed slightly higher odds of hallucination. Conclusions: LLMs are highly susceptible to adversarial hallucination attacks, frequently generating false clinical details that pose risks when used without safeguards. While prompt engineering reduces errors, it does not eliminate them.",
      "intriguing_abstract": "Background: Large language models (LLMs) show promise in clinical contexts but can generate false facts (often referred to as hallucinations). One subset of these errors arises from adversarial attacks, in which fabricated details embedded in prompts lead the model to produce or elaborate on the false information. We embedded fabricated content in clinical prompts to elicit adversarial hallucination attacks in multiple large language models. We quantified how often they elaborated on false details and tested whether a specialized mitigation prompt or altered temperature settings reduced errors. Methods: We created 300 physician-validated simulated vignettes, each containing one fabricated detail (a laboratory test, a physical or radiological sign, or a medical condition). Each vignette was presented in short and long versions - differing only in word count but identical in medical content. We tested six LLMs under three conditions: default (standard settings), mitigating prompt (designed to reduce hallucinations), and temperature 0 (deterministic output with maximum response certainty), generating 5,400 outputs. If a model elaborated on the fabricated detail, the case was classified as a hallucination. Results: Hallucination rates ranged from 50% to 82% across models and prompting methods. Prompt-based mitigation lowered overall hallucinations (mean across all models) from 66% to 44% (p<0.001). For the best overall performing model, GPT 4o, rates declined from 53% to 23% (p<0.001). Temperature adjustments offered no significant improvement. Short vignettes showed slightly higher odds of hallucination. Conclusions: LLMs are highly susceptible to adversarial hallucination attacks, frequently generating false clinical details that pose risks when used without safeguards. While prompt engineering reduces errors, it does not eliminate them.",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/6b44d0ac2b6d6deeb7f35ef4a7ad77b12b646b9e.pdf",
      "citation_key": "omar2025us3",
      "metadata": {
        "title": "Large Language Models Are Highly Vulnerable to Adversarial Hallucination Attacks in Clinical Decision Support: A Multi-Model Assurance Analysis",
        "authors": [
          "M. Omar",
          "V. Sorin",
          "J. Collins",
          "D. Reich",
          "R. Freeman",
          "N. Gavin",
          "A. Charney",
          "L. Stump",
          "N. L. Bragazzi",
          "G. Nadkarni",
          "E. Klang"
        ],
        "published_date": "2025",
        "abstract": "Background: Large language models (LLMs) show promise in clinical contexts but can generate false facts (often referred to as hallucinations). One subset of these errors arises from adversarial attacks, in which fabricated details embedded in prompts lead the model to produce or elaborate on the false information. We embedded fabricated content in clinical prompts to elicit adversarial hallucination attacks in multiple large language models. We quantified how often they elaborated on false details and tested whether a specialized mitigation prompt or altered temperature settings reduced errors. Methods: We created 300 physician-validated simulated vignettes, each containing one fabricated detail (a laboratory test, a physical or radiological sign, or a medical condition). Each vignette was presented in short and long versions - differing only in word count but identical in medical content. We tested six LLMs under three conditions: default (standard settings), mitigating prompt (designed to reduce hallucinations), and temperature 0 (deterministic output with maximum response certainty), generating 5,400 outputs. If a model elaborated on the fabricated detail, the case was classified as a hallucination. Results: Hallucination rates ranged from 50% to 82% across models and prompting methods. Prompt-based mitigation lowered overall hallucinations (mean across all models) from 66% to 44% (p<0.001). For the best overall performing model, GPT 4o, rates declined from 53% to 23% (p<0.001). Temperature adjustments offered no significant improvement. Short vignettes showed slightly higher odds of hallucination. Conclusions: LLMs are highly susceptible to adversarial hallucination attacks, frequently generating false clinical details that pose risks when used without safeguards. While prompt engineering reduces errors, it does not eliminate them.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/6b44d0ac2b6d6deeb7f35ef4a7ad77b12b646b9e.pdf",
        "venue": "medRxiv",
        "citationCount": 5,
        "score": 5.0,
        "summary": "Background: Large language models (LLMs) show promise in clinical contexts but can generate false facts (often referred to as hallucinations). One subset of these errors arises from adversarial attacks, in which fabricated details embedded in prompts lead the model to produce or elaborate on the false information. We embedded fabricated content in clinical prompts to elicit adversarial hallucination attacks in multiple large language models. We quantified how often they elaborated on false details and tested whether a specialized mitigation prompt or altered temperature settings reduced errors. Methods: We created 300 physician-validated simulated vignettes, each containing one fabricated detail (a laboratory test, a physical or radiological sign, or a medical condition). Each vignette was presented in short and long versions - differing only in word count but identical in medical content. We tested six LLMs under three conditions: default (standard settings), mitigating prompt (designed to reduce hallucinations), and temperature 0 (deterministic output with maximum response certainty), generating 5,400 outputs. If a model elaborated on the fabricated detail, the case was classified as a hallucination. Results: Hallucination rates ranged from 50% to 82% across models and prompting methods. Prompt-based mitigation lowered overall hallucinations (mean across all models) from 66% to 44% (p<0.001). For the best overall performing model, GPT 4o, rates declined from 53% to 23% (p<0.001). Temperature adjustments offered no significant improvement. Short vignettes showed slightly higher odds of hallucination. Conclusions: LLMs are highly susceptible to adversarial hallucination attacks, frequently generating false clinical details that pose risks when used without safeguards. While prompt engineering reduces errors, it does not eliminate them.",
        "keywords": []
      },
      "file_name": "6b44d0ac2b6d6deeb7f35ef4a7ad77b12b646b9e.pdf"
    },
    {
      "success": true,
      "doc_id": "b42f0a03f131c2abdd595f5df247d474",
      "summary": "While large vision-language models (LVLMs) have shown impressive capabilities in generating plausible responses correlated with input visual contents, they still suffer from hallucinations, where the generated text inaccurately reflects visual contents. To address this, recent approaches apply contrastive decoding to calibrate the model's response via contrasting output distributions with original and visually distorted samples, demonstrating promising hallucination mitigation in a training-free manner. However, the potential of changing information in visual inputs is not well-explored, so a deeper investigation into the behaviors of visual contrastive decoding is of great interest. In this paper, we first explore various methods for contrastive decoding to change visual contents, including image downsampling and editing. Downsampling images reduces the detailed textual information while editing yields new contents in images, providing new aspects as visual contrastive samples. To further study benefits by using different contrastive samples, we analyze probability-level metrics, including entropy and distribution distance. Interestingly, the effect of these samples in mitigating hallucinations varies a lot across LVLMs and benchmarks. Based on our analysis, we propose a simple yet effective method to combine contrastive samples, offering a practical solution for applying contrastive decoding across various scenarios. Extensive experiments are conducted to validate the proposed fusion method among different benchmarks.",
      "intriguing_abstract": "While large vision-language models (LVLMs) have shown impressive capabilities in generating plausible responses correlated with input visual contents, they still suffer from hallucinations, where the generated text inaccurately reflects visual contents. To address this, recent approaches apply contrastive decoding to calibrate the model's response via contrasting output distributions with original and visually distorted samples, demonstrating promising hallucination mitigation in a training-free manner. However, the potential of changing information in visual inputs is not well-explored, so a deeper investigation into the behaviors of visual contrastive decoding is of great interest. In this paper, we first explore various methods for contrastive decoding to change visual contents, including image downsampling and editing. Downsampling images reduces the detailed textual information while editing yields new contents in images, providing new aspects as visual contrastive samples. To further study benefits by using different contrastive samples, we analyze probability-level metrics, including entropy and distribution distance. Interestingly, the effect of these samples in mitigating hallucinations varies a lot across LVLMs and benchmarks. Based on our analysis, we propose a simple yet effective method to combine contrastive samples, offering a practical solution for applying contrastive decoding across various scenarios. Extensive experiments are conducted to validate the proposed fusion method among different benchmarks.",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/9516ad22fbd81875b160c4471ff3f747c4543da1.pdf",
      "citation_key": "lee2024i72",
      "metadata": {
        "title": "Delve into Visual Contrastive Decoding for Hallucination Mitigation of Large Vision-Language Models",
        "authors": [
          "Yi-Lun Lee",
          "Yi-Hsuan Tsai",
          "Wei-Chen Chiu"
        ],
        "published_date": "2024",
        "abstract": "While large vision-language models (LVLMs) have shown impressive capabilities in generating plausible responses correlated with input visual contents, they still suffer from hallucinations, where the generated text inaccurately reflects visual contents. To address this, recent approaches apply contrastive decoding to calibrate the model's response via contrasting output distributions with original and visually distorted samples, demonstrating promising hallucination mitigation in a training-free manner. However, the potential of changing information in visual inputs is not well-explored, so a deeper investigation into the behaviors of visual contrastive decoding is of great interest. In this paper, we first explore various methods for contrastive decoding to change visual contents, including image downsampling and editing. Downsampling images reduces the detailed textual information while editing yields new contents in images, providing new aspects as visual contrastive samples. To further study benefits by using different contrastive samples, we analyze probability-level metrics, including entropy and distribution distance. Interestingly, the effect of these samples in mitigating hallucinations varies a lot across LVLMs and benchmarks. Based on our analysis, we propose a simple yet effective method to combine contrastive samples, offering a practical solution for applying contrastive decoding across various scenarios. Extensive experiments are conducted to validate the proposed fusion method among different benchmarks.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/9516ad22fbd81875b160c4471ff3f747c4543da1.pdf",
        "venue": "arXiv.org",
        "citationCount": 5,
        "score": 5.0,
        "summary": "While large vision-language models (LVLMs) have shown impressive capabilities in generating plausible responses correlated with input visual contents, they still suffer from hallucinations, where the generated text inaccurately reflects visual contents. To address this, recent approaches apply contrastive decoding to calibrate the model's response via contrasting output distributions with original and visually distorted samples, demonstrating promising hallucination mitigation in a training-free manner. However, the potential of changing information in visual inputs is not well-explored, so a deeper investigation into the behaviors of visual contrastive decoding is of great interest. In this paper, we first explore various methods for contrastive decoding to change visual contents, including image downsampling and editing. Downsampling images reduces the detailed textual information while editing yields new contents in images, providing new aspects as visual contrastive samples. To further study benefits by using different contrastive samples, we analyze probability-level metrics, including entropy and distribution distance. Interestingly, the effect of these samples in mitigating hallucinations varies a lot across LVLMs and benchmarks. Based on our analysis, we propose a simple yet effective method to combine contrastive samples, offering a practical solution for applying contrastive decoding across various scenarios. Extensive experiments are conducted to validate the proposed fusion method among different benchmarks.",
        "keywords": []
      },
      "file_name": "9516ad22fbd81875b160c4471ff3f747c4543da1.pdf"
    },
    {
      "success": true,
      "doc_id": "c30ddcba5493b6010cad048b066e2a0d",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/cb7fa7ee3df826628c113ba0c6db1205751d89a3.pdf",
      "citation_key": "li2023irg",
      "metadata": {
        "title": "HELMA: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models",
        "authors": [
          "Junyi Li",
          "Xiaoxue Cheng",
          "Wayne Xin Zhao",
          "J. Nie",
          "Ji-rong Wen"
        ],
        "published_date": "2023",
        "abstract": "",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/cb7fa7ee3df826628c113ba0c6db1205751d89a3.pdf",
        "venue": "",
        "citationCount": 8,
        "score": 4.0,
        "summary": "",
        "keywords": []
      },
      "file_name": "cb7fa7ee3df826628c113ba0c6db1205751d89a3.pdf"
    },
    {
      "success": true,
      "doc_id": "81bcc7f333dc389fe727c8cd71d32632",
      "summary": "In modern dialogue systems, the use of Large Language Models (LLMs) has grown exponentially due to their capacity to generate diverse, relevant, and creative responses. Despite their strengths, striking a balance between the LLMs' creativity and their faithfulness to external knowledge remains a key challenge. This paper presents an innovative user-controllable mechanism that modulates the balance between an LLM's imaginative capabilities and its adherence to factual information. Our approach incorporates a numerical tag during the fine-tuning phase of the LLM's training, representing the degree of faithfulness to the reference knowledge in the generated responses. This degree is computed through an automated process that measures lexical overlap using ROUGE scores, semantic similarity using Sentence-BERT embeddings, and an LLM's self-evaluation score. During model inference, users can manipulate this numerical tag, thus controlling the degree of the LLM's reliance on external knowledge. We conduct extensive experiments across various scenarios, demonstrating the adaptability of our method and its efficacy in ensuring the quality and accuracy of the LLM's responses. The results highlight the potential of our approach to enhance the versatility of LLMs while maintaining a balance between creativity and hallucination.",
      "intriguing_abstract": "In modern dialogue systems, the use of Large Language Models (LLMs) has grown exponentially due to their capacity to generate diverse, relevant, and creative responses. Despite their strengths, striking a balance between the LLMs' creativity and their faithfulness to external knowledge remains a key challenge. This paper presents an innovative user-controllable mechanism that modulates the balance between an LLM's imaginative capabilities and its adherence to factual information. Our approach incorporates a numerical tag during the fine-tuning phase of the LLM's training, representing the degree of faithfulness to the reference knowledge in the generated responses. This degree is computed through an automated process that measures lexical overlap using ROUGE scores, semantic similarity using Sentence-BERT embeddings, and an LLM's self-evaluation score. During model inference, users can manipulate this numerical tag, thus controlling the degree of the LLM's reliance on external knowledge. We conduct extensive experiments across various scenarios, demonstrating the adaptability of our method and its efficacy in ensuring the quality and accuracy of the LLM's responses. The results highlight the potential of our approach to enhance the versatility of LLMs while maintaining a balance between creativity and hallucination.",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/13fd528a587196ff6429bfbe1d11d2f89a4036f5.pdf",
      "citation_key": "zhang2023pb6",
      "metadata": {
        "title": "User-Controlled Knowledge Fusion in Large Language Models: Balancing Creativity and Hallucination",
        "authors": [
          "Chen Zhang"
        ],
        "published_date": "2023",
        "abstract": "In modern dialogue systems, the use of Large Language Models (LLMs) has grown exponentially due to their capacity to generate diverse, relevant, and creative responses. Despite their strengths, striking a balance between the LLMs' creativity and their faithfulness to external knowledge remains a key challenge. This paper presents an innovative user-controllable mechanism that modulates the balance between an LLM's imaginative capabilities and its adherence to factual information. Our approach incorporates a numerical tag during the fine-tuning phase of the LLM's training, representing the degree of faithfulness to the reference knowledge in the generated responses. This degree is computed through an automated process that measures lexical overlap using ROUGE scores, semantic similarity using Sentence-BERT embeddings, and an LLM's self-evaluation score. During model inference, users can manipulate this numerical tag, thus controlling the degree of the LLM's reliance on external knowledge. We conduct extensive experiments across various scenarios, demonstrating the adaptability of our method and its efficacy in ensuring the quality and accuracy of the LLM's responses. The results highlight the potential of our approach to enhance the versatility of LLMs while maintaining a balance between creativity and hallucination.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/13fd528a587196ff6429bfbe1d11d2f89a4036f5.pdf",
        "venue": "arXiv.org",
        "citationCount": 7,
        "score": 3.5,
        "summary": "In modern dialogue systems, the use of Large Language Models (LLMs) has grown exponentially due to their capacity to generate diverse, relevant, and creative responses. Despite their strengths, striking a balance between the LLMs' creativity and their faithfulness to external knowledge remains a key challenge. This paper presents an innovative user-controllable mechanism that modulates the balance between an LLM's imaginative capabilities and its adherence to factual information. Our approach incorporates a numerical tag during the fine-tuning phase of the LLM's training, representing the degree of faithfulness to the reference knowledge in the generated responses. This degree is computed through an automated process that measures lexical overlap using ROUGE scores, semantic similarity using Sentence-BERT embeddings, and an LLM's self-evaluation score. During model inference, users can manipulate this numerical tag, thus controlling the degree of the LLM's reliance on external knowledge. We conduct extensive experiments across various scenarios, demonstrating the adaptability of our method and its efficacy in ensuring the quality and accuracy of the LLM's responses. The results highlight the potential of our approach to enhance the versatility of LLMs while maintaining a balance between creativity and hallucination.",
        "keywords": []
      },
      "file_name": "13fd528a587196ff6429bfbe1d11d2f89a4036f5.pdf"
    },
    {
      "success": true,
      "doc_id": "a87e370c578b4acefb0c61dff44c8d42",
      "summary": "Large Language Models (LLMs) are widely used in critical fields such as healthcare, education, and finance due to their remarkable proficiency in various language-related tasks. However, LLMs are prone to generating factually incorrect responses or\"hallucinations,\"which can lead to a loss of credibility and trust among users. To address this issue, we propose a multi-stage framework that generates the rationale first, verifies and refines incorrect ones, and uses them as supporting references to generate the answer. The generated rationale enhances the transparency of the answer and our framework provides insights into how the model arrived at this answer, by using this rationale and the references to the context. In this paper, we demonstrate its effectiveness in improving the quality of responses to drug-related inquiries in the life sciences industry. Our framework improves traditional Retrieval Augmented Generation (RAG) by enabling OpenAI GPT-3.5-turbo to be 14-25% more faithful and 16-22% more accurate on two datasets. Furthermore, fine-tuning samples based on our framework improves the accuracy of smaller open-access LLMs by 33-42% and competes with RAG on commercial models.",
      "intriguing_abstract": "Large Language Models (LLMs) are widely used in critical fields such as healthcare, education, and finance due to their remarkable proficiency in various language-related tasks. However, LLMs are prone to generating factually incorrect responses or\"hallucinations,\"which can lead to a loss of credibility and trust among users. To address this issue, we propose a multi-stage framework that generates the rationale first, verifies and refines incorrect ones, and uses them as supporting references to generate the answer. The generated rationale enhances the transparency of the answer and our framework provides insights into how the model arrived at this answer, by using this rationale and the references to the context. In this paper, we demonstrate its effectiveness in improving the quality of responses to drug-related inquiries in the life sciences industry. Our framework improves traditional Retrieval Augmented Generation (RAG) by enabling OpenAI GPT-3.5-turbo to be 14-25% more faithful and 16-22% more accurate on two datasets. Furthermore, fine-tuning samples based on our framework improves the accuracy of smaller open-access LLMs by 33-42% and competes with RAG on commercial models.",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/f63fdbbdf9005245d960ac1912cf4d0805e274a8.pdf",
      "citation_key": "irulandi2023xlg",
      "metadata": {
        "title": "Minimizing Factual Inconsistency and Hallucination in Large Language Models",
        "authors": [
          "Muneeswaran Irulandi",
          "Shreya Saxena",
          "Siva Prasad",
          "M. V. S. Prakash",
          "Advaith Shankar",
          "V. Varun",
          "Vishal Vaddina",
          "Saisubramaniam Gopalakrishnan"
        ],
        "published_date": "2023",
        "abstract": "Large Language Models (LLMs) are widely used in critical fields such as healthcare, education, and finance due to their remarkable proficiency in various language-related tasks. However, LLMs are prone to generating factually incorrect responses or\"hallucinations,\"which can lead to a loss of credibility and trust among users. To address this issue, we propose a multi-stage framework that generates the rationale first, verifies and refines incorrect ones, and uses them as supporting references to generate the answer. The generated rationale enhances the transparency of the answer and our framework provides insights into how the model arrived at this answer, by using this rationale and the references to the context. In this paper, we demonstrate its effectiveness in improving the quality of responses to drug-related inquiries in the life sciences industry. Our framework improves traditional Retrieval Augmented Generation (RAG) by enabling OpenAI GPT-3.5-turbo to be 14-25% more faithful and 16-22% more accurate on two datasets. Furthermore, fine-tuning samples based on our framework improves the accuracy of smaller open-access LLMs by 33-42% and competes with RAG on commercial models.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/f63fdbbdf9005245d960ac1912cf4d0805e274a8.pdf",
        "venue": "arXiv.org",
        "citationCount": 6,
        "score": 3.0,
        "summary": "Large Language Models (LLMs) are widely used in critical fields such as healthcare, education, and finance due to their remarkable proficiency in various language-related tasks. However, LLMs are prone to generating factually incorrect responses or\"hallucinations,\"which can lead to a loss of credibility and trust among users. To address this issue, we propose a multi-stage framework that generates the rationale first, verifies and refines incorrect ones, and uses them as supporting references to generate the answer. The generated rationale enhances the transparency of the answer and our framework provides insights into how the model arrived at this answer, by using this rationale and the references to the context. In this paper, we demonstrate its effectiveness in improving the quality of responses to drug-related inquiries in the life sciences industry. Our framework improves traditional Retrieval Augmented Generation (RAG) by enabling OpenAI GPT-3.5-turbo to be 14-25% more faithful and 16-22% more accurate on two datasets. Furthermore, fine-tuning samples based on our framework improves the accuracy of smaller open-access LLMs by 33-42% and competes with RAG on commercial models.",
        "keywords": []
      },
      "file_name": "f63fdbbdf9005245d960ac1912cf4d0805e274a8.pdf"
    },
    {
      "success": true,
      "doc_id": "85431f306955708a0b55fbfe83ee4531",
      "summary": "Large Language Models (LLMs) demonstrate remarkable capabilities, yet struggle with hallucination and outdated knowledge when tasked with complex knowledge reasoning, resulting in factually incorrect outputs. Previous studies have attempted to mitigate it by retrieving factual knowledge from large-scale knowledge graphs (KGs) to assist LLMs in logical reasoning and prediction of answers. However, this kind of approach often introduces noise and irrelevant data, especially in situations with extensive context from multiple knowledge aspects. In this way, LLM attention can be potentially mislead from question and relevant information. In our study, we introduce an Adaptive Multi-Aspect Retrieval-augmented over KGs (Amar) framework. This method retrieves knowledge including entities, relations, and subgraphs, and converts each piece of retrieved text into prompt embeddings. The Amar framework comprises two key sub-components: 1) a self-alignment module that aligns commonalities among entities, relations, and subgraphs to enhance retrieved text, thereby reducing noise interference; 2) a relevance gating module that employs a soft gate to learn the relevance score between question and multi-aspect retrieved data, to determine which information should be used to enhance LLMs' output, or even filtered altogether. Our method has achieved state-of-the-art performance on two common datasets, WebQSP and CWQ, showing a 1.9\\% improvement in accuracy over its best competitor and a 6.6\\% improvement in logical form generation over a method that directly uses retrieved text as context prompts. These results demonstrate the effectiveness of Amar in improving the reasoning of LLMs.",
      "intriguing_abstract": "Large Language Models (LLMs) demonstrate remarkable capabilities, yet struggle with hallucination and outdated knowledge when tasked with complex knowledge reasoning, resulting in factually incorrect outputs. Previous studies have attempted to mitigate it by retrieving factual knowledge from large-scale knowledge graphs (KGs) to assist LLMs in logical reasoning and prediction of answers. However, this kind of approach often introduces noise and irrelevant data, especially in situations with extensive context from multiple knowledge aspects. In this way, LLM attention can be potentially mislead from question and relevant information. In our study, we introduce an Adaptive Multi-Aspect Retrieval-augmented over KGs (Amar) framework. This method retrieves knowledge including entities, relations, and subgraphs, and converts each piece of retrieved text into prompt embeddings. The Amar framework comprises two key sub-components: 1) a self-alignment module that aligns commonalities among entities, relations, and subgraphs to enhance retrieved text, thereby reducing noise interference; 2) a relevance gating module that employs a soft gate to learn the relevance score between question and multi-aspect retrieved data, to determine which information should be used to enhance LLMs' output, or even filtered altogether. Our method has achieved state-of-the-art performance on two common datasets, WebQSP and CWQ, showing a 1.9\\% improvement in accuracy over its best competitor and a 6.6\\% improvement in logical form generation over a method that directly uses retrieved text as context prompts. These results demonstrate the effectiveness of Amar in improving the reasoning of LLMs.",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/3b1baabcbfd19e2f292863c522de41083814856a.pdf",
      "citation_key": "li2024ncc",
      "metadata": {
        "title": "Harnessing Large Language Models for Knowledge Graph Question Answering via Adaptive Multi-Aspect Retrieval-Augmentation",
        "authors": [
          "Derong Xu Xinhang Li",
          "Ziheng Zhang",
          "Zhenxi Lin",
          "Zhihong Zhu",
          "Zhi Zheng",
          "Xian Wu",
          "Xiangyu Zhao",
          "Tong Xu",
          "Enhong Chen"
        ],
        "published_date": "2024",
        "abstract": "Large Language Models (LLMs) demonstrate remarkable capabilities, yet struggle with hallucination and outdated knowledge when tasked with complex knowledge reasoning, resulting in factually incorrect outputs. Previous studies have attempted to mitigate it by retrieving factual knowledge from large-scale knowledge graphs (KGs) to assist LLMs in logical reasoning and prediction of answers. However, this kind of approach often introduces noise and irrelevant data, especially in situations with extensive context from multiple knowledge aspects. In this way, LLM attention can be potentially mislead from question and relevant information. In our study, we introduce an Adaptive Multi-Aspect Retrieval-augmented over KGs (Amar) framework. This method retrieves knowledge including entities, relations, and subgraphs, and converts each piece of retrieved text into prompt embeddings. The Amar framework comprises two key sub-components: 1) a self-alignment module that aligns commonalities among entities, relations, and subgraphs to enhance retrieved text, thereby reducing noise interference; 2) a relevance gating module that employs a soft gate to learn the relevance score between question and multi-aspect retrieved data, to determine which information should be used to enhance LLMs' output, or even filtered altogether. Our method has achieved state-of-the-art performance on two common datasets, WebQSP and CWQ, showing a 1.9\\% improvement in accuracy over its best competitor and a 6.6\\% improvement in logical form generation over a method that directly uses retrieved text as context prompts. These results demonstrate the effectiveness of Amar in improving the reasoning of LLMs.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/3b1baabcbfd19e2f292863c522de41083814856a.pdf",
        "venue": "arXiv.org",
        "citationCount": 12,
        "score": 12.0,
        "summary": "Large Language Models (LLMs) demonstrate remarkable capabilities, yet struggle with hallucination and outdated knowledge when tasked with complex knowledge reasoning, resulting in factually incorrect outputs. Previous studies have attempted to mitigate it by retrieving factual knowledge from large-scale knowledge graphs (KGs) to assist LLMs in logical reasoning and prediction of answers. However, this kind of approach often introduces noise and irrelevant data, especially in situations with extensive context from multiple knowledge aspects. In this way, LLM attention can be potentially mislead from question and relevant information. In our study, we introduce an Adaptive Multi-Aspect Retrieval-augmented over KGs (Amar) framework. This method retrieves knowledge including entities, relations, and subgraphs, and converts each piece of retrieved text into prompt embeddings. The Amar framework comprises two key sub-components: 1) a self-alignment module that aligns commonalities among entities, relations, and subgraphs to enhance retrieved text, thereby reducing noise interference; 2) a relevance gating module that employs a soft gate to learn the relevance score between question and multi-aspect retrieved data, to determine which information should be used to enhance LLMs' output, or even filtered altogether. Our method has achieved state-of-the-art performance on two common datasets, WebQSP and CWQ, showing a 1.9\\% improvement in accuracy over its best competitor and a 6.6\\% improvement in logical form generation over a method that directly uses retrieved text as context prompts. These results demonstrate the effectiveness of Amar in improving the reasoning of LLMs.",
        "keywords": []
      },
      "file_name": "3b1baabcbfd19e2f292863c522de41083814856a.pdf"
    },
    {
      "success": true,
      "doc_id": "f728e9af5dc940c836f53f4836b71d3d",
      "summary": "Large Language Models (LLMs) harness extensive data from the Internet, storing a broad spectrum of prior knowledge. While LLMs have proven beneficial as decision-making aids, their reliability is hampered by limitations in reasoning, hallucination phenomenon, and so on. On the other hand, Monte-Carlo Tree Search (MCTS) is a heuristic search algorithm that provides reliable decision-making solutions, achieved through recursive rollouts and self-play. However, the effectiveness of MCTS relies heavily on heuristic pruning and external value functions, particularly in complex decision scenarios. This work introduces an innovative approach that bolsters LLMs with MCTS self-play to efficiently resolve deterministic turn-based zero-sum games (DTZG), such as chess and go, without the need for additional training. Specifically, we utilize LLMs as both action pruners and proxies for value functions without the need for additional training. We theoretically prove that the suboptimality of the estimated value in our proposed method scales with $\\tilde{\\mathcal O}\\Bigl(\\frac{|\\tilde {\\mathcal A}|}{\\sqrt{N}} + \\epsilon_\\mathrm{pruner} + \\epsilon_\\mathrm{critic}\\Bigr)$, where \\(N\\) is the number of simulations, $|\\tilde {\\mathcal A}|$ is the cardinality of the pruned action space by LLM, and $\\epsilon_\\mathrm{pruner}$ and $\\epsilon_\\mathrm{critic}$ quantify the errors incurred by adopting LLMs as action space pruner and value function proxy, respectively. Our experiments in chess and go demonstrate the capability of our method to address challenges beyond the scope of MCTS and improve the performance of the directly application of LLMs.",
      "intriguing_abstract": "Large Language Models (LLMs) harness extensive data from the Internet, storing a broad spectrum of prior knowledge. While LLMs have proven beneficial as decision-making aids, their reliability is hampered by limitations in reasoning, hallucination phenomenon, and so on. On the other hand, Monte-Carlo Tree Search (MCTS) is a heuristic search algorithm that provides reliable decision-making solutions, achieved through recursive rollouts and self-play. However, the effectiveness of MCTS relies heavily on heuristic pruning and external value functions, particularly in complex decision scenarios. This work introduces an innovative approach that bolsters LLMs with MCTS self-play to efficiently resolve deterministic turn-based zero-sum games (DTZG), such as chess and go, without the need for additional training. Specifically, we utilize LLMs as both action pruners and proxies for value functions without the need for additional training. We theoretically prove that the suboptimality of the estimated value in our proposed method scales with $\\tilde{\\mathcal O}\\Bigl(\\frac{|\\tilde {\\mathcal A}|}{\\sqrt{N}} + \\epsilon_\\mathrm{pruner} + \\epsilon_\\mathrm{critic}\\Bigr)$, where \\(N\\) is the number of simulations, $|\\tilde {\\mathcal A}|$ is the cardinality of the pruned action space by LLM, and $\\epsilon_\\mathrm{pruner}$ and $\\epsilon_\\mathrm{critic}$ quantify the errors incurred by adopting LLMs as action space pruner and value function proxy, respectively. Our experiments in chess and go demonstrate the capability of our method to address challenges beyond the scope of MCTS and improve the performance of the directly application of LLMs.",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/fa0d056dd585eeffb4333cb55807d357808f8440.pdf",
      "citation_key": "guo2024tlu",
      "metadata": {
        "title": "Can Large Language Models Play Games? A Case Study of A Self-Play Approach",
        "authors": [
          "Hongyi Guo",
          "Zhihan Liu",
          "Yufeng Zhang",
          "Zhaoran Wang"
        ],
        "published_date": "2024",
        "abstract": "Large Language Models (LLMs) harness extensive data from the Internet, storing a broad spectrum of prior knowledge. While LLMs have proven beneficial as decision-making aids, their reliability is hampered by limitations in reasoning, hallucination phenomenon, and so on. On the other hand, Monte-Carlo Tree Search (MCTS) is a heuristic search algorithm that provides reliable decision-making solutions, achieved through recursive rollouts and self-play. However, the effectiveness of MCTS relies heavily on heuristic pruning and external value functions, particularly in complex decision scenarios. This work introduces an innovative approach that bolsters LLMs with MCTS self-play to efficiently resolve deterministic turn-based zero-sum games (DTZG), such as chess and go, without the need for additional training. Specifically, we utilize LLMs as both action pruners and proxies for value functions without the need for additional training. We theoretically prove that the suboptimality of the estimated value in our proposed method scales with $\\tilde{\\mathcal O}\\Bigl(\\frac{|\\tilde {\\mathcal A}|}{\\sqrt{N}} + \\epsilon_\\mathrm{pruner} + \\epsilon_\\mathrm{critic}\\Bigr)$, where \\(N\\) is the number of simulations, $|\\tilde {\\mathcal A}|$ is the cardinality of the pruned action space by LLM, and $\\epsilon_\\mathrm{pruner}$ and $\\epsilon_\\mathrm{critic}$ quantify the errors incurred by adopting LLMs as action space pruner and value function proxy, respectively. Our experiments in chess and go demonstrate the capability of our method to address challenges beyond the scope of MCTS and improve the performance of the directly application of LLMs.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/fa0d056dd585eeffb4333cb55807d357808f8440.pdf",
        "venue": "arXiv.org",
        "citationCount": 10,
        "score": 10.0,
        "summary": "Large Language Models (LLMs) harness extensive data from the Internet, storing a broad spectrum of prior knowledge. While LLMs have proven beneficial as decision-making aids, their reliability is hampered by limitations in reasoning, hallucination phenomenon, and so on. On the other hand, Monte-Carlo Tree Search (MCTS) is a heuristic search algorithm that provides reliable decision-making solutions, achieved through recursive rollouts and self-play. However, the effectiveness of MCTS relies heavily on heuristic pruning and external value functions, particularly in complex decision scenarios. This work introduces an innovative approach that bolsters LLMs with MCTS self-play to efficiently resolve deterministic turn-based zero-sum games (DTZG), such as chess and go, without the need for additional training. Specifically, we utilize LLMs as both action pruners and proxies for value functions without the need for additional training. We theoretically prove that the suboptimality of the estimated value in our proposed method scales with $\\tilde{\\mathcal O}\\Bigl(\\frac{|\\tilde {\\mathcal A}|}{\\sqrt{N}} + \\epsilon_\\mathrm{pruner} + \\epsilon_\\mathrm{critic}\\Bigr)$, where \\(N\\) is the number of simulations, $|\\tilde {\\mathcal A}|$ is the cardinality of the pruned action space by LLM, and $\\epsilon_\\mathrm{pruner}$ and $\\epsilon_\\mathrm{critic}$ quantify the errors incurred by adopting LLMs as action space pruner and value function proxy, respectively. Our experiments in chess and go demonstrate the capability of our method to address challenges beyond the scope of MCTS and improve the performance of the directly application of LLMs.",
        "keywords": []
      },
      "file_name": "fa0d056dd585eeffb4333cb55807d357808f8440.pdf"
    },
    {
      "success": true,
      "doc_id": "9a3cd833131919a8dcbf8f03ca7ec4d9",
      "summary": "Large language models (LLMs) have generated significant attention since their inception, finding applications across various academic and industrial domains. However, these models often suffer from the\"hallucination problem\", where outputs, though grammatically and logically coherent, lack factual accuracy or are entirely fabricated. A particularly troubling issue discovered and widely discussed recently is the numerical comparison error where multiple LLMs incorrectly infer that\"9.11$>$9.9\". We discovered that the order in which LLMs generate answers and reasoning impacts their consistency. Specifically, results vary significantly when an LLM generates an answer first and then provides the reasoning versus generating the reasoning process first and then the conclusion. Inspired by this, we propose a new benchmark method for assessing LLM consistency: comparing responses generated through these two different approaches. This benchmark effectively identifies instances where LLMs fabricate answers and subsequently generate justifications. Furthermore, we introduce a novel and straightforward prompt strategy designed to mitigate this issue. Experimental results demonstrate that this strategy improves performance across various LLMs compared to direct questioning. This work not only sheds light on a critical flaw in LLMs but also offers a practical solution to enhance their reliability.",
      "intriguing_abstract": "Large language models (LLMs) have generated significant attention since their inception, finding applications across various academic and industrial domains. However, these models often suffer from the\"hallucination problem\", where outputs, though grammatically and logically coherent, lack factual accuracy or are entirely fabricated. A particularly troubling issue discovered and widely discussed recently is the numerical comparison error where multiple LLMs incorrectly infer that\"9.11$>$9.9\". We discovered that the order in which LLMs generate answers and reasoning impacts their consistency. Specifically, results vary significantly when an LLM generates an answer first and then provides the reasoning versus generating the reasoning process first and then the conclusion. Inspired by this, we propose a new benchmark method for assessing LLM consistency: comparing responses generated through these two different approaches. This benchmark effectively identifies instances where LLMs fabricate answers and subsequently generate justifications. Furthermore, we introduce a novel and straightforward prompt strategy designed to mitigate this issue. Experimental results demonstrate that this strategy improves performance across various LLMs compared to direct questioning. This work not only sheds light on a critical flaw in LLMs but also offers a practical solution to enhance their reliability.",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/0797f2d1366da1f3441ea7d33b2109d7f27d1ad7.pdf",
      "citation_key": "xie20247zk",
      "metadata": {
        "title": "Order Matters in Hallucination: Reasoning Order as Benchmark and Reflexive Prompting for Large-Language-Models",
        "authors": [
          "Zikai Xie"
        ],
        "published_date": "2024",
        "abstract": "Large language models (LLMs) have generated significant attention since their inception, finding applications across various academic and industrial domains. However, these models often suffer from the\"hallucination problem\", where outputs, though grammatically and logically coherent, lack factual accuracy or are entirely fabricated. A particularly troubling issue discovered and widely discussed recently is the numerical comparison error where multiple LLMs incorrectly infer that\"9.11$>$9.9\". We discovered that the order in which LLMs generate answers and reasoning impacts their consistency. Specifically, results vary significantly when an LLM generates an answer first and then provides the reasoning versus generating the reasoning process first and then the conclusion. Inspired by this, we propose a new benchmark method for assessing LLM consistency: comparing responses generated through these two different approaches. This benchmark effectively identifies instances where LLMs fabricate answers and subsequently generate justifications. Furthermore, we introduce a novel and straightforward prompt strategy designed to mitigate this issue. Experimental results demonstrate that this strategy improves performance across various LLMs compared to direct questioning. This work not only sheds light on a critical flaw in LLMs but also offers a practical solution to enhance their reliability.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/0797f2d1366da1f3441ea7d33b2109d7f27d1ad7.pdf",
        "venue": "arXiv.org",
        "citationCount": 9,
        "score": 9.0,
        "summary": "Large language models (LLMs) have generated significant attention since their inception, finding applications across various academic and industrial domains. However, these models often suffer from the\"hallucination problem\", where outputs, though grammatically and logically coherent, lack factual accuracy or are entirely fabricated. A particularly troubling issue discovered and widely discussed recently is the numerical comparison error where multiple LLMs incorrectly infer that\"9.11$>$9.9\". We discovered that the order in which LLMs generate answers and reasoning impacts their consistency. Specifically, results vary significantly when an LLM generates an answer first and then provides the reasoning versus generating the reasoning process first and then the conclusion. Inspired by this, we propose a new benchmark method for assessing LLM consistency: comparing responses generated through these two different approaches. This benchmark effectively identifies instances where LLMs fabricate answers and subsequently generate justifications. Furthermore, we introduce a novel and straightforward prompt strategy designed to mitigate this issue. Experimental results demonstrate that this strategy improves performance across various LLMs compared to direct questioning. This work not only sheds light on a critical flaw in LLMs but also offers a practical solution to enhance their reliability.",
        "keywords": []
      },
      "file_name": "0797f2d1366da1f3441ea7d33b2109d7f27d1ad7.pdf"
    },
    {
      "success": true,
      "doc_id": "65c850d7e6c23ba00d09bb867cdcae15",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/fbca0c2ec5425bbd8dc4898d684c909a58dab1de.pdf",
      "citation_key": "amirizaniani2024493",
      "metadata": {
        "title": "Developing a Framework for Auditing Large Language Models Using Human-in-the-Loop",
        "authors": [
          "Maryam Amirizaniani",
          "Jihan Yao",
          "Adrian Lavergne",
          "Elizabeth Snell Okada",
          "Aman Chadha",
          "Tanya Roosta",
          "Chirag Shah"
        ],
        "published_date": "2024",
        "abstract": "",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/fbca0c2ec5425bbd8dc4898d684c909a58dab1de.pdf",
        "venue": "arXiv.org",
        "citationCount": 9,
        "score": 9.0,
        "summary": "",
        "keywords": []
      },
      "file_name": "fbca0c2ec5425bbd8dc4898d684c909a58dab1de.pdf"
    },
    {
      "success": true,
      "doc_id": "296622da1b4d6302a205c18fec208f97",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/4dc1fbde861d1e97daa0677ac9cfae92b2832589.pdf",
      "citation_key": "yang2025n54",
      "metadata": {
        "title": "Understanding and Mitigating Hallucination in Large Vision-Language Models via Modular Attribution and Intervention",
        "authors": [
          "Tianyun Yang",
          "Ziniu Li",
          "Juan Cao",
          "Chang Xu"
        ],
        "published_date": "2025",
        "abstract": "",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/4dc1fbde861d1e97daa0677ac9cfae92b2832589.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 8,
        "score": 8.0,
        "summary": "",
        "keywords": []
      },
      "file_name": "4dc1fbde861d1e97daa0677ac9cfae92b2832589.pdf"
    },
    {
      "success": true,
      "doc_id": "372ecd1e1ef478609f243e8ebb05a746",
      "summary": "Large Language Models (LLMs) have shown promising potentials in program generation and no-code automation. However, LLMs are prone to generate hallucinations, i.e., they generate text which sounds plausible but is incorrect. Although there has been a recent surge in research on LLM hallucinations for text generation, similar hallucination phenomenon can happen in code generation. Sometimes the generated code can have syntactical or logical errors as well as more advanced issues like security vulnerabilities, memory leaks, etc. Given the wide adaptation of LLMs to enhance efficiency in code generation and development in general, it becomes imperative to investigate hallucinations in code generation. To the best of our knowledge, this is the first attempt at studying hallucinations in the code generated by LLMs. We start by introducing the code hallucination definition and a comprehensive taxonomy of code hallucination types. We propose the first benchmark CodeMirage dataset for code hallucinations. The benchmark contains 1,137 GPT-3.5 generated hallucinated code snippets for Python programming problems from two base datasets - HumanEval and MBPP. We then propose the methodology for code hallucination detection and experiment with open source LLMs such as CodeLLaMA as well as OpenAI's GPT-3.5 and GPT-4 models using one-shot prompt. We find that GPT-4 performs the best on HumanEval dataset and gives comparable results to the fine-tuned CodeBERT baseline on MBPP dataset. Towards the end, we discuss various mitigation strategies for code hallucinations and conclude our work.",
      "intriguing_abstract": "Large Language Models (LLMs) have shown promising potentials in program generation and no-code automation. However, LLMs are prone to generate hallucinations, i.e., they generate text which sounds plausible but is incorrect. Although there has been a recent surge in research on LLM hallucinations for text generation, similar hallucination phenomenon can happen in code generation. Sometimes the generated code can have syntactical or logical errors as well as more advanced issues like security vulnerabilities, memory leaks, etc. Given the wide adaptation of LLMs to enhance efficiency in code generation and development in general, it becomes imperative to investigate hallucinations in code generation. To the best of our knowledge, this is the first attempt at studying hallucinations in the code generated by LLMs. We start by introducing the code hallucination definition and a comprehensive taxonomy of code hallucination types. We propose the first benchmark CodeMirage dataset for code hallucinations. The benchmark contains 1,137 GPT-3.5 generated hallucinated code snippets for Python programming problems from two base datasets - HumanEval and MBPP. We then propose the methodology for code hallucination detection and experiment with open source LLMs such as CodeLLaMA as well as OpenAI's GPT-3.5 and GPT-4 models using one-shot prompt. We find that GPT-4 performs the best on HumanEval dataset and gives comparable results to the fine-tuned CodeBERT baseline on MBPP dataset. Towards the end, we discuss various mitigation strategies for code hallucinations and conclude our work.",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/f363c38f2a5d3ed8697a72d3dd014d228bfda91a.pdf",
      "citation_key": "agarwal202418c",
      "metadata": {
        "title": "CodeMirage: Hallucinations in Code Generated by Large Language Models",
        "authors": [
          "Vibhor Agarwal",
          "Yulong Pei",
          "Salwa Alamir",
          "Xiaomo Liu"
        ],
        "published_date": "2024",
        "abstract": "Large Language Models (LLMs) have shown promising potentials in program generation and no-code automation. However, LLMs are prone to generate hallucinations, i.e., they generate text which sounds plausible but is incorrect. Although there has been a recent surge in research on LLM hallucinations for text generation, similar hallucination phenomenon can happen in code generation. Sometimes the generated code can have syntactical or logical errors as well as more advanced issues like security vulnerabilities, memory leaks, etc. Given the wide adaptation of LLMs to enhance efficiency in code generation and development in general, it becomes imperative to investigate hallucinations in code generation. To the best of our knowledge, this is the first attempt at studying hallucinations in the code generated by LLMs. We start by introducing the code hallucination definition and a comprehensive taxonomy of code hallucination types. We propose the first benchmark CodeMirage dataset for code hallucinations. The benchmark contains 1,137 GPT-3.5 generated hallucinated code snippets for Python programming problems from two base datasets - HumanEval and MBPP. We then propose the methodology for code hallucination detection and experiment with open source LLMs such as CodeLLaMA as well as OpenAI's GPT-3.5 and GPT-4 models using one-shot prompt. We find that GPT-4 performs the best on HumanEval dataset and gives comparable results to the fine-tuned CodeBERT baseline on MBPP dataset. Towards the end, we discuss various mitigation strategies for code hallucinations and conclude our work.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/f363c38f2a5d3ed8697a72d3dd014d228bfda91a.pdf",
        "venue": "arXiv.org",
        "citationCount": 8,
        "score": 8.0,
        "summary": "Large Language Models (LLMs) have shown promising potentials in program generation and no-code automation. However, LLMs are prone to generate hallucinations, i.e., they generate text which sounds plausible but is incorrect. Although there has been a recent surge in research on LLM hallucinations for text generation, similar hallucination phenomenon can happen in code generation. Sometimes the generated code can have syntactical or logical errors as well as more advanced issues like security vulnerabilities, memory leaks, etc. Given the wide adaptation of LLMs to enhance efficiency in code generation and development in general, it becomes imperative to investigate hallucinations in code generation. To the best of our knowledge, this is the first attempt at studying hallucinations in the code generated by LLMs. We start by introducing the code hallucination definition and a comprehensive taxonomy of code hallucination types. We propose the first benchmark CodeMirage dataset for code hallucinations. The benchmark contains 1,137 GPT-3.5 generated hallucinated code snippets for Python programming problems from two base datasets - HumanEval and MBPP. We then propose the methodology for code hallucination detection and experiment with open source LLMs such as CodeLLaMA as well as OpenAI's GPT-3.5 and GPT-4 models using one-shot prompt. We find that GPT-4 performs the best on HumanEval dataset and gives comparable results to the fine-tuned CodeBERT baseline on MBPP dataset. Towards the end, we discuss various mitigation strategies for code hallucinations and conclude our work.",
        "keywords": []
      },
      "file_name": "f363c38f2a5d3ed8697a72d3dd014d228bfda91a.pdf"
    },
    {
      "success": true,
      "doc_id": "a1a445e555ec2e53ad166cc2cb7596dd",
      "summary": "This study explores the sycophantic tendencies of Large Language Models (LLMs), where these models tend to provide answers that match what users want to hear, even if they are not entirely correct. The motivation behind this exploration stems from the common behavior observed in individuals searching the internet for facts with partial or misleading knowledge. Similar to using web search engines, users may recall fragments of misleading keywords and submit them to an LLM, hoping for a comprehensive response. Our empirical analysis of several LLMs shows the potential danger of these models amplifying misinformation when presented with misleading keywords. Additionally, we thoroughly assess four existing hallucination mitigation strategies to reduce LLMs sycophantic behavior. Our experiments demonstrate the effectiveness of these strategies for generating factually correct statements. Furthermore, our analyses delve into knowledge-probing experiments on factual keywords and different categories of sycophancy mitigation.",
      "intriguing_abstract": "This study explores the sycophantic tendencies of Large Language Models (LLMs), where these models tend to provide answers that match what users want to hear, even if they are not entirely correct. The motivation behind this exploration stems from the common behavior observed in individuals searching the internet for facts with partial or misleading knowledge. Similar to using web search engines, users may recall fragments of misleading keywords and submit them to an LLM, hoping for a comprehensive response. Our empirical analysis of several LLMs shows the potential danger of these models amplifying misinformation when presented with misleading keywords. Additionally, we thoroughly assess four existing hallucination mitigation strategies to reduce LLMs sycophantic behavior. Our experiments demonstrate the effectiveness of these strategies for generating factually correct statements. Furthermore, our analyses delve into knowledge-probing experiments on factual keywords and different categories of sycophancy mitigation.",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/43210579b1ff7707afbd5d1ed045cc56ba52e938.pdf",
      "citation_key": "rrv2024gw0",
      "metadata": {
        "title": "Chaos with Keywords: Exposing Large Language Models Sycophantic Hallucination to Misleading Keywords and Evaluating Defense Strategies",
        "authors": [
          "Aswin Rrv",
          "Nemika Tyagi",
          "Md Nayem Uddin",
          "Neeraj Varshney",
          "Chitta Baral"
        ],
        "published_date": "2024",
        "abstract": "This study explores the sycophantic tendencies of Large Language Models (LLMs), where these models tend to provide answers that match what users want to hear, even if they are not entirely correct. The motivation behind this exploration stems from the common behavior observed in individuals searching the internet for facts with partial or misleading knowledge. Similar to using web search engines, users may recall fragments of misleading keywords and submit them to an LLM, hoping for a comprehensive response. Our empirical analysis of several LLMs shows the potential danger of these models amplifying misinformation when presented with misleading keywords. Additionally, we thoroughly assess four existing hallucination mitigation strategies to reduce LLMs sycophantic behavior. Our experiments demonstrate the effectiveness of these strategies for generating factually correct statements. Furthermore, our analyses delve into knowledge-probing experiments on factual keywords and different categories of sycophancy mitigation.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/43210579b1ff7707afbd5d1ed045cc56ba52e938.pdf",
        "venue": "",
        "citationCount": 7,
        "score": 7.0,
        "summary": "This study explores the sycophantic tendencies of Large Language Models (LLMs), where these models tend to provide answers that match what users want to hear, even if they are not entirely correct. The motivation behind this exploration stems from the common behavior observed in individuals searching the internet for facts with partial or misleading knowledge. Similar to using web search engines, users may recall fragments of misleading keywords and submit them to an LLM, hoping for a comprehensive response. Our empirical analysis of several LLMs shows the potential danger of these models amplifying misinformation when presented with misleading keywords. Additionally, we thoroughly assess four existing hallucination mitigation strategies to reduce LLMs sycophantic behavior. Our experiments demonstrate the effectiveness of these strategies for generating factually correct statements. Furthermore, our analyses delve into knowledge-probing experiments on factual keywords and different categories of sycophancy mitigation.",
        "keywords": []
      },
      "file_name": "43210579b1ff7707afbd5d1ed045cc56ba52e938.pdf"
    },
    {
      "success": true,
      "doc_id": "37d223f980b302f801f2e2c6ae04e363",
      "summary": "Large language models (LLM) have demonstrated remarkable capabilities in various biomedical natural language processing (NLP) tasks, leveraging the demonstration within the input context to adapt to new tasks. However, LLM is sensitive to the selection of demonstrations. To address the hallucination issue inherent in LLM, retrieval-augmented LLM (RAL) offers a solution by retrieving pertinent information from an established database. Nonetheless, existing research work lacks rigorous evaluation of the impact of retrieval-augmented large language models on different biomedical NLP tasks. This deficiency makes it challenging to ascertain the capabilities of RAL within the biomedical domain. Moreover, the outputs from RAL are affected by retrieving the unlabeled, counterfactual, or diverse knowledge that is not well studied in the biomedical domain. However, such knowledge is common in the real world. Finally, exploring the self-awareness ability is also crucial for the RAL system. So, in this paper, we systematically investigate the impact of RALs on 5 different biomedical tasks (triple extraction, link prediction, classification, question answering, and natural language inference). We analyze the performance of RALs in four fundamental abilities, including unlabeled robustness, counterfactual robustness, diverse robustness, and negative awareness. To this end, we proposed an evaluation framework to assess the RALs' performance on different biomedical NLP tasks and establish four different testbeds based on the aforementioned fundamental abilities. Then, we evaluate 3 representative LLMs with 3 different retrievers on 5 tasks over 9 datasets.",
      "intriguing_abstract": "Large language models (LLM) have demonstrated remarkable capabilities in various biomedical natural language processing (NLP) tasks, leveraging the demonstration within the input context to adapt to new tasks. However, LLM is sensitive to the selection of demonstrations. To address the hallucination issue inherent in LLM, retrieval-augmented LLM (RAL) offers a solution by retrieving pertinent information from an established database. Nonetheless, existing research work lacks rigorous evaluation of the impact of retrieval-augmented large language models on different biomedical NLP tasks. This deficiency makes it challenging to ascertain the capabilities of RAL within the biomedical domain. Moreover, the outputs from RAL are affected by retrieving the unlabeled, counterfactual, or diverse knowledge that is not well studied in the biomedical domain. However, such knowledge is common in the real world. Finally, exploring the self-awareness ability is also crucial for the RAL system. So, in this paper, we systematically investigate the impact of RALs on 5 different biomedical tasks (triple extraction, link prediction, classification, question answering, and natural language inference). We analyze the performance of RALs in four fundamental abilities, including unlabeled robustness, counterfactual robustness, diverse robustness, and negative awareness. To this end, we proposed an evaluation framework to assess the RALs' performance on different biomedical NLP tasks and establish four different testbeds based on the aforementioned fundamental abilities. Then, we evaluate 3 representative LLMs with 3 different retrievers on 5 tasks over 9 datasets.",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/1154343478e423fecb12501cf02208499cd57846.pdf",
      "citation_key": "li2024hl9",
      "metadata": {
        "title": "Benchmarking Retrieval-Augmented Large Language Models in Biomedical NLP: Application, Robustness, and Self-Awareness",
        "authors": [
          "Mingchen Li",
          "Zaifu Zhan",
          "Han Yang",
          "Yongkang Xiao",
          "Jiatan Huang",
          "Rui Zhang"
        ],
        "published_date": "2024",
        "abstract": "Large language models (LLM) have demonstrated remarkable capabilities in various biomedical natural language processing (NLP) tasks, leveraging the demonstration within the input context to adapt to new tasks. However, LLM is sensitive to the selection of demonstrations. To address the hallucination issue inherent in LLM, retrieval-augmented LLM (RAL) offers a solution by retrieving pertinent information from an established database. Nonetheless, existing research work lacks rigorous evaluation of the impact of retrieval-augmented large language models on different biomedical NLP tasks. This deficiency makes it challenging to ascertain the capabilities of RAL within the biomedical domain. Moreover, the outputs from RAL are affected by retrieving the unlabeled, counterfactual, or diverse knowledge that is not well studied in the biomedical domain. However, such knowledge is common in the real world. Finally, exploring the self-awareness ability is also crucial for the RAL system. So, in this paper, we systematically investigate the impact of RALs on 5 different biomedical tasks (triple extraction, link prediction, classification, question answering, and natural language inference). We analyze the performance of RALs in four fundamental abilities, including unlabeled robustness, counterfactual robustness, diverse robustness, and negative awareness. To this end, we proposed an evaluation framework to assess the RALs' performance on different biomedical NLP tasks and establish four different testbeds based on the aforementioned fundamental abilities. Then, we evaluate 3 representative LLMs with 3 different retrievers on 5 tasks over 9 datasets.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/1154343478e423fecb12501cf02208499cd57846.pdf",
        "venue": "arXiv.org",
        "citationCount": 7,
        "score": 7.0,
        "summary": "Large language models (LLM) have demonstrated remarkable capabilities in various biomedical natural language processing (NLP) tasks, leveraging the demonstration within the input context to adapt to new tasks. However, LLM is sensitive to the selection of demonstrations. To address the hallucination issue inherent in LLM, retrieval-augmented LLM (RAL) offers a solution by retrieving pertinent information from an established database. Nonetheless, existing research work lacks rigorous evaluation of the impact of retrieval-augmented large language models on different biomedical NLP tasks. This deficiency makes it challenging to ascertain the capabilities of RAL within the biomedical domain. Moreover, the outputs from RAL are affected by retrieving the unlabeled, counterfactual, or diverse knowledge that is not well studied in the biomedical domain. However, such knowledge is common in the real world. Finally, exploring the self-awareness ability is also crucial for the RAL system. So, in this paper, we systematically investigate the impact of RALs on 5 different biomedical tasks (triple extraction, link prediction, classification, question answering, and natural language inference). We analyze the performance of RALs in four fundamental abilities, including unlabeled robustness, counterfactual robustness, diverse robustness, and negative awareness. To this end, we proposed an evaluation framework to assess the RALs' performance on different biomedical NLP tasks and establish four different testbeds based on the aforementioned fundamental abilities. Then, we evaluate 3 representative LLMs with 3 different retrievers on 5 tasks over 9 datasets.",
        "keywords": []
      },
      "file_name": "1154343478e423fecb12501cf02208499cd57846.pdf"
    },
    {
      "success": true,
      "doc_id": "2c5872368b2c36c4486a4a3b91caae49",
      "summary": "The widespread adoption of location-based applications has created a growing demand for point-of-interest (POI) recommendation, which aims to predict a userâ€™s next POI based on their historical check-in data and current location. However, existing methods often struggle to capture the intricate relationships within check-in data. This is largely due to their limitations in representing temporal and spatial information and underutilizing rich semantic features. While large language models (LLMs) offer powerful semantic comprehension to solve them, they are limited by hallucination and the inability to incorporate global collaborative information. To address these issues, we propose a novel method SeCor, which treats POI recommendation as a multi-modal task and integrates semantic and collaborative representations to form an efficient hybrid encoding. SeCor first employs a basic collaborative filtering model to mine interaction features. These embeddings, as one modal information, are fed into LLM to align with semantic representation, leading to efficient hybrid embeddings. To mitigate the hallucination, SeCor recommends based on the hybrid embeddings rather than directly using the LLMâ€™s output text. Extensive experiments on three public real-world datasets show that SeCor outperforms all baselines, achieving improved recommendation performance by effectively integrating collaborative and semantic information through LLMs.",
      "intriguing_abstract": "The widespread adoption of location-based applications has created a growing demand for point-of-interest (POI) recommendation, which aims to predict a userâ€™s next POI based on their historical check-in data and current location. However, existing methods often struggle to capture the intricate relationships within check-in data. This is largely due to their limitations in representing temporal and spatial information and underutilizing rich semantic features. While large language models (LLMs) offer powerful semantic comprehension to solve them, they are limited by hallucination and the inability to incorporate global collaborative information. To address these issues, we propose a novel method SeCor, which treats POI recommendation as a multi-modal task and integrates semantic and collaborative representations to form an efficient hybrid encoding. SeCor first employs a basic collaborative filtering model to mine interaction features. These embeddings, as one modal information, are fed into LLM to align with semantic representation, leading to efficient hybrid embeddings. To mitigate the hallucination, SeCor recommends based on the hybrid embeddings rather than directly using the LLMâ€™s output text. Extensive experiments on three public real-world datasets show that SeCor outperforms all baselines, achieving improved recommendation performance by effectively integrating collaborative and semantic information through LLMs.",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/691f111348f3b19163e62a208de9803280205ed8.pdf",
      "citation_key": "wang2024t4o",
      "metadata": {
        "title": "SeCor: Aligning Semantic and Collaborative Representations by Large Language Models for Next-Point-of-Interest Recommendations",
        "authors": [
          "Shirui Wang",
          "Bohan Xie",
          "Ling Ding",
          "Xiaoying Gao",
          "Jianting Chen",
          "Yang Xiang"
        ],
        "published_date": "2024",
        "abstract": "The widespread adoption of location-based applications has created a growing demand for point-of-interest (POI) recommendation, which aims to predict a userâ€™s next POI based on their historical check-in data and current location. However, existing methods often struggle to capture the intricate relationships within check-in data. This is largely due to their limitations in representing temporal and spatial information and underutilizing rich semantic features. While large language models (LLMs) offer powerful semantic comprehension to solve them, they are limited by hallucination and the inability to incorporate global collaborative information. To address these issues, we propose a novel method SeCor, which treats POI recommendation as a multi-modal task and integrates semantic and collaborative representations to form an efficient hybrid encoding. SeCor first employs a basic collaborative filtering model to mine interaction features. These embeddings, as one modal information, are fed into LLM to align with semantic representation, leading to efficient hybrid embeddings. To mitigate the hallucination, SeCor recommends based on the hybrid embeddings rather than directly using the LLMâ€™s output text. Extensive experiments on three public real-world datasets show that SeCor outperforms all baselines, achieving improved recommendation performance by effectively integrating collaborative and semantic information through LLMs.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/691f111348f3b19163e62a208de9803280205ed8.pdf",
        "venue": "ACM Conference on Recommender Systems",
        "citationCount": 7,
        "score": 7.0,
        "summary": "The widespread adoption of location-based applications has created a growing demand for point-of-interest (POI) recommendation, which aims to predict a userâ€™s next POI based on their historical check-in data and current location. However, existing methods often struggle to capture the intricate relationships within check-in data. This is largely due to their limitations in representing temporal and spatial information and underutilizing rich semantic features. While large language models (LLMs) offer powerful semantic comprehension to solve them, they are limited by hallucination and the inability to incorporate global collaborative information. To address these issues, we propose a novel method SeCor, which treats POI recommendation as a multi-modal task and integrates semantic and collaborative representations to form an efficient hybrid encoding. SeCor first employs a basic collaborative filtering model to mine interaction features. These embeddings, as one modal information, are fed into LLM to align with semantic representation, leading to efficient hybrid embeddings. To mitigate the hallucination, SeCor recommends based on the hybrid embeddings rather than directly using the LLMâ€™s output text. Extensive experiments on three public real-world datasets show that SeCor outperforms all baselines, achieving improved recommendation performance by effectively integrating collaborative and semantic information through LLMs.",
        "keywords": []
      },
      "file_name": "691f111348f3b19163e62a208de9803280205ed8.pdf"
    },
    {
      "success": true,
      "doc_id": "24935a46aab72caf35354f6f5e437281",
      "summary": "Patients often face difficulties in understanding their hospitalizations, while healthcare workers have limited resources to provide explanations. In this work, we investigate the potential of large language models to generate patient summaries based on doctors' notes and study the effect of training data on the faithfulness and quality of the generated summaries. To this end, we release (i) a rigorous labeling protocol for errors in medical texts and (ii) a publicly available dataset of annotated hallucinations in 100 doctor-written and 100 generated summaries. We show that fine-tuning on hallucination-free data effectively reduces hallucinations from 2.60 to 1.55 per summary for Llama 2, while preserving relevant information. We observe a similar effect on GPT-4 (0.70 to 0.40), when the few-shot examples are hallucination-free. We also conduct a qualitative evaluation using hallucination-free and improved training data. We find that common quantitative metrics do not correlate well with faithfulness and quality. Finally, we test GPT-4 for automatic hallucination detection, which clearly outperforms common baselines.",
      "intriguing_abstract": "Patients often face difficulties in understanding their hospitalizations, while healthcare workers have limited resources to provide explanations. In this work, we investigate the potential of large language models to generate patient summaries based on doctors' notes and study the effect of training data on the faithfulness and quality of the generated summaries. To this end, we release (i) a rigorous labeling protocol for errors in medical texts and (ii) a publicly available dataset of annotated hallucinations in 100 doctor-written and 100 generated summaries. We show that fine-tuning on hallucination-free data effectively reduces hallucinations from 2.60 to 1.55 per summary for Llama 2, while preserving relevant information. We observe a similar effect on GPT-4 (0.70 to 0.40), when the few-shot examples are hallucination-free. We also conduct a qualitative evaluation using hallucination-free and improved training data. We find that common quantitative metrics do not correlate well with faithfulness and quality. Finally, we test GPT-4 for automatic hallucination detection, which clearly outperforms common baselines.",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/c0082580c4b9e5c6c96cf06f1be67c0cbbafb753.pdf",
      "citation_key": "hegselmann20249q4",
      "metadata": {
        "title": "A Data-Centric Approach To Generate Faithful and High Quality Patient Summaries with Large Language Models",
        "authors": [
          "S. Hegselmann",
          "Zejiang Shen",
          "Florian Gierse",
          "Monica Agrawal",
          "David Sontag",
          "Xiaoyi Jiang"
        ],
        "published_date": "2024",
        "abstract": "Patients often face difficulties in understanding their hospitalizations, while healthcare workers have limited resources to provide explanations. In this work, we investigate the potential of large language models to generate patient summaries based on doctors' notes and study the effect of training data on the faithfulness and quality of the generated summaries. To this end, we release (i) a rigorous labeling protocol for errors in medical texts and (ii) a publicly available dataset of annotated hallucinations in 100 doctor-written and 100 generated summaries. We show that fine-tuning on hallucination-free data effectively reduces hallucinations from 2.60 to 1.55 per summary for Llama 2, while preserving relevant information. We observe a similar effect on GPT-4 (0.70 to 0.40), when the few-shot examples are hallucination-free. We also conduct a qualitative evaluation using hallucination-free and improved training data. We find that common quantitative metrics do not correlate well with faithfulness and quality. Finally, we test GPT-4 for automatic hallucination detection, which clearly outperforms common baselines.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/c0082580c4b9e5c6c96cf06f1be67c0cbbafb753.pdf",
        "venue": "ACM Conference on Health, Inference, and Learning",
        "citationCount": 7,
        "score": 7.0,
        "summary": "Patients often face difficulties in understanding their hospitalizations, while healthcare workers have limited resources to provide explanations. In this work, we investigate the potential of large language models to generate patient summaries based on doctors' notes and study the effect of training data on the faithfulness and quality of the generated summaries. To this end, we release (i) a rigorous labeling protocol for errors in medical texts and (ii) a publicly available dataset of annotated hallucinations in 100 doctor-written and 100 generated summaries. We show that fine-tuning on hallucination-free data effectively reduces hallucinations from 2.60 to 1.55 per summary for Llama 2, while preserving relevant information. We observe a similar effect on GPT-4 (0.70 to 0.40), when the few-shot examples are hallucination-free. We also conduct a qualitative evaluation using hallucination-free and improved training data. We find that common quantitative metrics do not correlate well with faithfulness and quality. Finally, we test GPT-4 for automatic hallucination detection, which clearly outperforms common baselines.",
        "keywords": []
      },
      "file_name": "c0082580c4b9e5c6c96cf06f1be67c0cbbafb753.pdf"
    },
    {
      "success": true,
      "doc_id": "c07a2b9821c0c0c934bcc83cac85d54c",
      "summary": "In this study, we present EventRL, a reinforcement learning approach developed to enhance event extraction for large language models (LLMs). EventRL utilizes outcome supervision with specific reward functions to tackle prevalent challenges in LLMs, such as instruction following and hallucination, manifested as the mismatch of event structure and the generation of undefined event types. We evaluate EventRL against existing methods like Few-Shot Prompting (FSP) (based on GPT4) and Supervised Fine-Tuning (SFT) across various LLMs, including GPT-4, LLaMa, and CodeLLaMa models. Our findings show that EventRL significantly outperforms these conventional approaches by improving the performance in identifying and structuring events, particularly in handling novel event types. The study emphasizes the critical role of reward function selection and demonstrates the benefits of incorporating code data for better event extraction. While increasing model size leads to higher accuracy, maintaining the ability to generalize is essential to avoid overfitting.",
      "intriguing_abstract": "In this study, we present EventRL, a reinforcement learning approach developed to enhance event extraction for large language models (LLMs). EventRL utilizes outcome supervision with specific reward functions to tackle prevalent challenges in LLMs, such as instruction following and hallucination, manifested as the mismatch of event structure and the generation of undefined event types. We evaluate EventRL against existing methods like Few-Shot Prompting (FSP) (based on GPT4) and Supervised Fine-Tuning (SFT) across various LLMs, including GPT-4, LLaMa, and CodeLLaMa models. Our findings show that EventRL significantly outperforms these conventional approaches by improving the performance in identifying and structuring events, particularly in handling novel event types. The study emphasizes the critical role of reward function selection and demonstrates the benefits of incorporating code data for better event extraction. While increasing model size leads to higher accuracy, maintaining the ability to generalize is essential to avoid overfitting.",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/ded732209b0ba8a6704cc62ab8197a898b57f833.pdf",
      "citation_key": "gao2024ncr",
      "metadata": {
        "title": "EventRL: Enhancing Event Extraction with Outcome Supervision for Large Language Models",
        "authors": [
          "Jun Gao",
          "Huan Zhao",
          "Wei Wang",
          "Changlong Yu",
          "Ruifeng Xu"
        ],
        "published_date": "2024",
        "abstract": "In this study, we present EventRL, a reinforcement learning approach developed to enhance event extraction for large language models (LLMs). EventRL utilizes outcome supervision with specific reward functions to tackle prevalent challenges in LLMs, such as instruction following and hallucination, manifested as the mismatch of event structure and the generation of undefined event types. We evaluate EventRL against existing methods like Few-Shot Prompting (FSP) (based on GPT4) and Supervised Fine-Tuning (SFT) across various LLMs, including GPT-4, LLaMa, and CodeLLaMa models. Our findings show that EventRL significantly outperforms these conventional approaches by improving the performance in identifying and structuring events, particularly in handling novel event types. The study emphasizes the critical role of reward function selection and demonstrates the benefits of incorporating code data for better event extraction. While increasing model size leads to higher accuracy, maintaining the ability to generalize is essential to avoid overfitting.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/ded732209b0ba8a6704cc62ab8197a898b57f833.pdf",
        "venue": "arXiv.org",
        "citationCount": 7,
        "score": 7.0,
        "summary": "In this study, we present EventRL, a reinforcement learning approach developed to enhance event extraction for large language models (LLMs). EventRL utilizes outcome supervision with specific reward functions to tackle prevalent challenges in LLMs, such as instruction following and hallucination, manifested as the mismatch of event structure and the generation of undefined event types. We evaluate EventRL against existing methods like Few-Shot Prompting (FSP) (based on GPT4) and Supervised Fine-Tuning (SFT) across various LLMs, including GPT-4, LLaMa, and CodeLLaMa models. Our findings show that EventRL significantly outperforms these conventional approaches by improving the performance in identifying and structuring events, particularly in handling novel event types. The study emphasizes the critical role of reward function selection and demonstrates the benefits of incorporating code data for better event extraction. While increasing model size leads to higher accuracy, maintaining the ability to generalize is essential to avoid overfitting.",
        "keywords": []
      },
      "file_name": "ded732209b0ba8a6704cc62ab8197a898b57f833.pdf"
    },
    {
      "success": true,
      "doc_id": "07bb7fdd5e92154db5ae8de2a5f9e08b",
      "summary": "Step-by-step decision planning with large language models (LLMs) is gaining attention in AI agent development. This paper focuses on decision planning with uncertainty estimation to address the hallucination problem in language models. Existing approaches are either white-box or computationally demanding, limiting use of black-box proprietary LLMs within budgets. The paper's first contribution is a non-parametric uncertainty quantification method for LLMs, efficiently estimating point-wise dependencies between input-decision on the fly with a single inference, without access to token logits. This estimator informs the statistical interpretation of decision trustworthiness. The second contribution outlines a systematic design for a decision-making agent, generating actions like ``turn on the bathroom light'' based on user prompts such as ``take a bath''. Users will be asked to provide preferences when more than one action has high estimated point-wise dependencies. In conclusion, our uncertainty estimation and decision-making agent design offer a cost-efficient approach for AI agent development.",
      "intriguing_abstract": "Step-by-step decision planning with large language models (LLMs) is gaining attention in AI agent development. This paper focuses on decision planning with uncertainty estimation to address the hallucination problem in language models. Existing approaches are either white-box or computationally demanding, limiting use of black-box proprietary LLMs within budgets. The paper's first contribution is a non-parametric uncertainty quantification method for LLMs, efficiently estimating point-wise dependencies between input-decision on the fly with a single inference, without access to token logits. This estimator informs the statistical interpretation of decision trustworthiness. The second contribution outlines a systematic design for a decision-making agent, generating actions like ``turn on the bathroom light'' based on user prompts such as ``take a bath''. Users will be asked to provide preferences when more than one action has high estimated point-wise dependencies. In conclusion, our uncertainty estimation and decision-making agent design offer a cost-efficient approach for AI agent development.",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/6d3ae6d6b312b659b3a14ae3f3e86a36db63200d.pdf",
      "citation_key": "tsai2024klg",
      "metadata": {
        "title": "Efficient Non-Parametric Uncertainty Quantification for Black-Box Large Language Models and Decision Planning",
        "authors": [
          "Yao-Hung Tsai",
          "Walter Talbott",
          "Jian Zhang"
        ],
        "published_date": "2024",
        "abstract": "Step-by-step decision planning with large language models (LLMs) is gaining attention in AI agent development. This paper focuses on decision planning with uncertainty estimation to address the hallucination problem in language models. Existing approaches are either white-box or computationally demanding, limiting use of black-box proprietary LLMs within budgets. The paper's first contribution is a non-parametric uncertainty quantification method for LLMs, efficiently estimating point-wise dependencies between input-decision on the fly with a single inference, without access to token logits. This estimator informs the statistical interpretation of decision trustworthiness. The second contribution outlines a systematic design for a decision-making agent, generating actions like ``turn on the bathroom light'' based on user prompts such as ``take a bath''. Users will be asked to provide preferences when more than one action has high estimated point-wise dependencies. In conclusion, our uncertainty estimation and decision-making agent design offer a cost-efficient approach for AI agent development.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/6d3ae6d6b312b659b3a14ae3f3e86a36db63200d.pdf",
        "venue": "arXiv.org",
        "citationCount": 7,
        "score": 7.0,
        "summary": "Step-by-step decision planning with large language models (LLMs) is gaining attention in AI agent development. This paper focuses on decision planning with uncertainty estimation to address the hallucination problem in language models. Existing approaches are either white-box or computationally demanding, limiting use of black-box proprietary LLMs within budgets. The paper's first contribution is a non-parametric uncertainty quantification method for LLMs, efficiently estimating point-wise dependencies between input-decision on the fly with a single inference, without access to token logits. This estimator informs the statistical interpretation of decision trustworthiness. The second contribution outlines a systematic design for a decision-making agent, generating actions like ``turn on the bathroom light'' based on user prompts such as ``take a bath''. Users will be asked to provide preferences when more than one action has high estimated point-wise dependencies. In conclusion, our uncertainty estimation and decision-making agent design offer a cost-efficient approach for AI agent development.",
        "keywords": []
      },
      "file_name": "6d3ae6d6b312b659b3a14ae3f3e86a36db63200d.pdf"
    },
    {
      "success": true,
      "doc_id": "c3afdaff2309891772648d57e2ce40dc",
      "summary": "Hallucination is a key roadblock for applications of Large Language Models (LLMs), particularly for enterprise applications that are sensitive to information accuracy. To address this issue, two general approaches have been explored: Retrieval-Augmented Generation (RAG) to supply LLMs with updated information as context, and fine-tuning the LLMs with new information and desired output styles. In this paper, we propose Honest AI: a novel strategy to fine-tune\"small\"language models to say\"I don't know\"to reduce hallucination, along with several alternative RAG approaches. The solution ranked 1st in Task 2 for the false premise question. The alternative approaches include using RAG with search engine and knowledge graph results, fine-tuning base LLMs with new information and combinations of both approaches. Although all approaches improve the performance of the LLMs, RAG alone does not significantly improve the performance and fine-tuning is needed for better results. Finally, the hybrid approach achieved the highest score in the CRAG benchmark. In addition, our approach emphasizes the use of relatively small models with fewer than 10 billion parameters, promoting resource efficiency.",
      "intriguing_abstract": "Hallucination is a key roadblock for applications of Large Language Models (LLMs), particularly for enterprise applications that are sensitive to information accuracy. To address this issue, two general approaches have been explored: Retrieval-Augmented Generation (RAG) to supply LLMs with updated information as context, and fine-tuning the LLMs with new information and desired output styles. In this paper, we propose Honest AI: a novel strategy to fine-tune\"small\"language models to say\"I don't know\"to reduce hallucination, along with several alternative RAG approaches. The solution ranked 1st in Task 2 for the false premise question. The alternative approaches include using RAG with search engine and knowledge graph results, fine-tuning base LLMs with new information and combinations of both approaches. Although all approaches improve the performance of the LLMs, RAG alone does not significantly improve the performance and fine-tuning is needed for better results. Finally, the hybrid approach achieved the highest score in the CRAG benchmark. In addition, our approach emphasizes the use of relatively small models with fewer than 10 billion parameters, promoting resource efficiency.",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/e059a20ae41aa32837030fd6e4392b8217243a2c.pdf",
      "citation_key": "chen2024qs5",
      "metadata": {
        "title": "Honest AI: Fine-Tuning \"Small\" Language Models to Say \"I Don't Know\", and Reducing Hallucination in RAG",
        "authors": [
          "Xinxi Chen",
          "Li Wang",
          "Wei Wu",
          "Qizhi Tang",
          "Yiyao Liu"
        ],
        "published_date": "2024",
        "abstract": "Hallucination is a key roadblock for applications of Large Language Models (LLMs), particularly for enterprise applications that are sensitive to information accuracy. To address this issue, two general approaches have been explored: Retrieval-Augmented Generation (RAG) to supply LLMs with updated information as context, and fine-tuning the LLMs with new information and desired output styles. In this paper, we propose Honest AI: a novel strategy to fine-tune\"small\"language models to say\"I don't know\"to reduce hallucination, along with several alternative RAG approaches. The solution ranked 1st in Task 2 for the false premise question. The alternative approaches include using RAG with search engine and knowledge graph results, fine-tuning base LLMs with new information and combinations of both approaches. Although all approaches improve the performance of the LLMs, RAG alone does not significantly improve the performance and fine-tuning is needed for better results. Finally, the hybrid approach achieved the highest score in the CRAG benchmark. In addition, our approach emphasizes the use of relatively small models with fewer than 10 billion parameters, promoting resource efficiency.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/e059a20ae41aa32837030fd6e4392b8217243a2c.pdf",
        "venue": "arXiv.org",
        "citationCount": 7,
        "score": 7.0,
        "summary": "Hallucination is a key roadblock for applications of Large Language Models (LLMs), particularly for enterprise applications that are sensitive to information accuracy. To address this issue, two general approaches have been explored: Retrieval-Augmented Generation (RAG) to supply LLMs with updated information as context, and fine-tuning the LLMs with new information and desired output styles. In this paper, we propose Honest AI: a novel strategy to fine-tune\"small\"language models to say\"I don't know\"to reduce hallucination, along with several alternative RAG approaches. The solution ranked 1st in Task 2 for the false premise question. The alternative approaches include using RAG with search engine and knowledge graph results, fine-tuning base LLMs with new information and combinations of both approaches. Although all approaches improve the performance of the LLMs, RAG alone does not significantly improve the performance and fine-tuning is needed for better results. Finally, the hybrid approach achieved the highest score in the CRAG benchmark. In addition, our approach emphasizes the use of relatively small models with fewer than 10 billion parameters, promoting resource efficiency.",
        "keywords": []
      },
      "file_name": "e059a20ae41aa32837030fd6e4392b8217243a2c.pdf"
    },
    {
      "success": true,
      "doc_id": "260401f3e95dd89760c1e08732b6f858",
      "summary": "Large language models (LLMs) have demonstrated strong mathematical reasoning capabilities but remain susceptible to hallucinations producing plausible yet incorrect statements especially in theorem proving, symbolic manipulation, and numerical computation. While self-consistency (SC) has been explored as a means to improve factuality in LLMs, existing approaches primarily apply SC to final-answer selection, neglecting the logical consistency of intermediate reasoning steps. In this work, we introduce a structured self-consistency framework designed to enhance the reliability of mathematical reasoning. Our method enforces self-consistency across intermediate steps and final outputs, reducing logical inconsistencies and hallucinations. We evaluate our approach across three core mathematical tasks: theorem proving, symbolic transformation, and numerical computation. Experimental results demonstrate that SC significantly improves proof validity, symbolic reasoning accuracy, and numerical stability while maintaining computational efficiency. Further analysis reveals that structured self-consistency not only enhances problem-solving accuracy but also reduces the variance of model-generated outputs. These findings highlight self-consistency as a robust mechanism for improving mathematical reasoning in LLMs, paving the way for more reliable and interpretable AI-driven mathematics.",
      "intriguing_abstract": "Large language models (LLMs) have demonstrated strong mathematical reasoning capabilities but remain susceptible to hallucinations producing plausible yet incorrect statements especially in theorem proving, symbolic manipulation, and numerical computation. While self-consistency (SC) has been explored as a means to improve factuality in LLMs, existing approaches primarily apply SC to final-answer selection, neglecting the logical consistency of intermediate reasoning steps. In this work, we introduce a structured self-consistency framework designed to enhance the reliability of mathematical reasoning. Our method enforces self-consistency across intermediate steps and final outputs, reducing logical inconsistencies and hallucinations. We evaluate our approach across three core mathematical tasks: theorem proving, symbolic transformation, and numerical computation. Experimental results demonstrate that SC significantly improves proof validity, symbolic reasoning accuracy, and numerical stability while maintaining computational efficiency. Further analysis reveals that structured self-consistency not only enhances problem-solving accuracy but also reduces the variance of model-generated outputs. These findings highlight self-consistency as a robust mechanism for improving mathematical reasoning in LLMs, paving the way for more reliable and interpretable AI-driven mathematics.",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/55b193ab7967fb20a8a05f878ac85bf48c9fc615.pdf",
      "citation_key": "liu2025juo",
      "metadata": {
        "title": "Enhancing Mathematical Reasoning in Large Language Models with Self-Consistency-Based Hallucination Detection",
        "authors": [
          "MingShan Liu",
          "Shi Bo",
          "Jialing Fang"
        ],
        "published_date": "2025",
        "abstract": "Large language models (LLMs) have demonstrated strong mathematical reasoning capabilities but remain susceptible to hallucinations producing plausible yet incorrect statements especially in theorem proving, symbolic manipulation, and numerical computation. While self-consistency (SC) has been explored as a means to improve factuality in LLMs, existing approaches primarily apply SC to final-answer selection, neglecting the logical consistency of intermediate reasoning steps. In this work, we introduce a structured self-consistency framework designed to enhance the reliability of mathematical reasoning. Our method enforces self-consistency across intermediate steps and final outputs, reducing logical inconsistencies and hallucinations. We evaluate our approach across three core mathematical tasks: theorem proving, symbolic transformation, and numerical computation. Experimental results demonstrate that SC significantly improves proof validity, symbolic reasoning accuracy, and numerical stability while maintaining computational efficiency. Further analysis reveals that structured self-consistency not only enhances problem-solving accuracy but also reduces the variance of model-generated outputs. These findings highlight self-consistency as a robust mechanism for improving mathematical reasoning in LLMs, paving the way for more reliable and interpretable AI-driven mathematics.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/55b193ab7967fb20a8a05f878ac85bf48c9fc615.pdf",
        "venue": "arXiv.org",
        "citationCount": 6,
        "score": 6.0,
        "summary": "Large language models (LLMs) have demonstrated strong mathematical reasoning capabilities but remain susceptible to hallucinations producing plausible yet incorrect statements especially in theorem proving, symbolic manipulation, and numerical computation. While self-consistency (SC) has been explored as a means to improve factuality in LLMs, existing approaches primarily apply SC to final-answer selection, neglecting the logical consistency of intermediate reasoning steps. In this work, we introduce a structured self-consistency framework designed to enhance the reliability of mathematical reasoning. Our method enforces self-consistency across intermediate steps and final outputs, reducing logical inconsistencies and hallucinations. We evaluate our approach across three core mathematical tasks: theorem proving, symbolic transformation, and numerical computation. Experimental results demonstrate that SC significantly improves proof validity, symbolic reasoning accuracy, and numerical stability while maintaining computational efficiency. Further analysis reveals that structured self-consistency not only enhances problem-solving accuracy but also reduces the variance of model-generated outputs. These findings highlight self-consistency as a robust mechanism for improving mathematical reasoning in LLMs, paving the way for more reliable and interpretable AI-driven mathematics.",
        "keywords": []
      },
      "file_name": "55b193ab7967fb20a8a05f878ac85bf48c9fc615.pdf"
    },
    {
      "success": true,
      "doc_id": "0f6dc51a9639409fccb79b5b8822bc90",
      "summary": "Recently, while large language models (LLMs) have demonstrated impressive results, they still suffer from hallucination, i.e., the generation of false information. Model editing is the task of fixing factual mistakes in LLMs; yet, most previous works treat it as a one-time task, paying little attention to ever-emerging mistakes generated by LLMs. We address the task of sequential model editing (SME) that aims to rectify mistakes continuously. A Dynamic Auxiliary Fusion Network (DAFNet) is designed to enhance the semantic interaction among the factual knowledge within the entire sequence, preventing catastrophic forgetting during the editing process of multiple knowledge triples. Specifically, (1) for semantic fusion within a relation triple, we aggregate the intra-editing attention flow into auto-regressive self-attention with token-level granularity in LLMs. We further leverage multi-layer diagonal inter-editing attention flow to update the weighted representations of the entire sequence-level granularity. (2) Considering that auxiliary parameters are required to store the knowledge for sequential editing, we construct a new dataset named \\textbf{DAFSet}, fulfilling recent, popular, long-tail and robust properties to enhance the generality of sequential editing. Experiments show DAFNet significantly outperforms strong baselines in single-turn and sequential editing. The usage of DAFSet also consistently improves the performance of other auxiliary network-based methods in various scenarios",
      "intriguing_abstract": "Recently, while large language models (LLMs) have demonstrated impressive results, they still suffer from hallucination, i.e., the generation of false information. Model editing is the task of fixing factual mistakes in LLMs; yet, most previous works treat it as a one-time task, paying little attention to ever-emerging mistakes generated by LLMs. We address the task of sequential model editing (SME) that aims to rectify mistakes continuously. A Dynamic Auxiliary Fusion Network (DAFNet) is designed to enhance the semantic interaction among the factual knowledge within the entire sequence, preventing catastrophic forgetting during the editing process of multiple knowledge triples. Specifically, (1) for semantic fusion within a relation triple, we aggregate the intra-editing attention flow into auto-regressive self-attention with token-level granularity in LLMs. We further leverage multi-layer diagonal inter-editing attention flow to update the weighted representations of the entire sequence-level granularity. (2) Considering that auxiliary parameters are required to store the knowledge for sequential editing, we construct a new dataset named \\textbf{DAFSet}, fulfilling recent, popular, long-tail and robust properties to enhance the generality of sequential editing. Experiments show DAFNet significantly outperforms strong baselines in single-turn and sequential editing. The usage of DAFSet also consistently improves the performance of other auxiliary network-based methods in various scenarios",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/ea32e8511cde4a1b852d8c003e0ec64bdf64b0d8.pdf",
      "citation_key": "zhang2024htn",
      "metadata": {
        "title": "DAFNet: Dynamic Auxiliary Fusion for Sequential Model Editing in Large Language Models",
        "authors": [
          "Taolin Zhang",
          "Qizhou Chen",
          "Dongyang Li",
          "Chengyu Wang",
          "Xiaofeng He",
          "Longtao Huang",
          "Hui Xue",
          "Junyuan Huang"
        ],
        "published_date": "2024",
        "abstract": "Recently, while large language models (LLMs) have demonstrated impressive results, they still suffer from hallucination, i.e., the generation of false information. Model editing is the task of fixing factual mistakes in LLMs; yet, most previous works treat it as a one-time task, paying little attention to ever-emerging mistakes generated by LLMs. We address the task of sequential model editing (SME) that aims to rectify mistakes continuously. A Dynamic Auxiliary Fusion Network (DAFNet) is designed to enhance the semantic interaction among the factual knowledge within the entire sequence, preventing catastrophic forgetting during the editing process of multiple knowledge triples. Specifically, (1) for semantic fusion within a relation triple, we aggregate the intra-editing attention flow into auto-regressive self-attention with token-level granularity in LLMs. We further leverage multi-layer diagonal inter-editing attention flow to update the weighted representations of the entire sequence-level granularity. (2) Considering that auxiliary parameters are required to store the knowledge for sequential editing, we construct a new dataset named \\textbf{DAFSet}, fulfilling recent, popular, long-tail and robust properties to enhance the generality of sequential editing. Experiments show DAFNet significantly outperforms strong baselines in single-turn and sequential editing. The usage of DAFSet also consistently improves the performance of other auxiliary network-based methods in various scenarios",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/ea32e8511cde4a1b852d8c003e0ec64bdf64b0d8.pdf",
        "venue": "Annual Meeting of the Association for Computational Linguistics",
        "citationCount": 6,
        "score": 6.0,
        "summary": "Recently, while large language models (LLMs) have demonstrated impressive results, they still suffer from hallucination, i.e., the generation of false information. Model editing is the task of fixing factual mistakes in LLMs; yet, most previous works treat it as a one-time task, paying little attention to ever-emerging mistakes generated by LLMs. We address the task of sequential model editing (SME) that aims to rectify mistakes continuously. A Dynamic Auxiliary Fusion Network (DAFNet) is designed to enhance the semantic interaction among the factual knowledge within the entire sequence, preventing catastrophic forgetting during the editing process of multiple knowledge triples. Specifically, (1) for semantic fusion within a relation triple, we aggregate the intra-editing attention flow into auto-regressive self-attention with token-level granularity in LLMs. We further leverage multi-layer diagonal inter-editing attention flow to update the weighted representations of the entire sequence-level granularity. (2) Considering that auxiliary parameters are required to store the knowledge for sequential editing, we construct a new dataset named \\textbf{DAFSet}, fulfilling recent, popular, long-tail and robust properties to enhance the generality of sequential editing. Experiments show DAFNet significantly outperforms strong baselines in single-turn and sequential editing. The usage of DAFSet also consistently improves the performance of other auxiliary network-based methods in various scenarios",
        "keywords": []
      },
      "file_name": "ea32e8511cde4a1b852d8c003e0ec64bdf64b0d8.pdf"
    },
    {
      "success": true,
      "doc_id": "804a9fb40d85b8f9ba805178db8b3499",
      "summary": "Large language models (LLMs) have gained significant attention due to their prospective applications in medicine. Utilizing multimodal LLMs can potentially assist clinicians in medical image classification tasks. It is important to evaluate the performance of LLMs in medical image processing to potentially improve the medical system. We evaluated two multimodal LLMs (LLaVA and GPT-4-based ChatGPT) against the classic VGG in tumor classification across brain MRI, breast ultrasound, and kidney CT datasets. Despite LLMs facing significant hallucination issue in medical imaging, prompt engineering markedly enhanced their performance. In comparison to the baseline method, GPT-4-based ChatGPT with prompt engineering achieves 98%, 112%, and 69% of the baseline's performance in terms of accuracy (or 99%, 107%, and 62 % in terms of F1-score) in those three datasets, respectively. However, privacy, bias, accountability, and transparency concerns necessitate caution. Our study underscore LLMs' potential in medical imaging but emphasize the need for thorough performance and safety evaluations for their practical application.",
      "intriguing_abstract": "Large language models (LLMs) have gained significant attention due to their prospective applications in medicine. Utilizing multimodal LLMs can potentially assist clinicians in medical image classification tasks. It is important to evaluate the performance of LLMs in medical image processing to potentially improve the medical system. We evaluated two multimodal LLMs (LLaVA and GPT-4-based ChatGPT) against the classic VGG in tumor classification across brain MRI, breast ultrasound, and kidney CT datasets. Despite LLMs facing significant hallucination issue in medical imaging, prompt engineering markedly enhanced their performance. In comparison to the baseline method, GPT-4-based ChatGPT with prompt engineering achieves 98%, 112%, and 69% of the baseline's performance in terms of accuracy (or 99%, 107%, and 62 % in terms of F1-score) in those three datasets, respectively. However, privacy, bias, accountability, and transparency concerns necessitate caution. Our study underscore LLMs' potential in medical imaging but emphasize the need for thorough performance and safety evaluations for their practical application.",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/b7aa5af5bf96ee003543d5ff7dcfc3d9a46d43bb.pdf",
      "citation_key": "guo2024hgn",
      "metadata": {
        "title": "Performance Evaluation of Multimodal Large Language Models (LLaVA and GPT-4-based ChatGPT) in Medical Image Classification Tasks",
        "authors": [
          "Yuhang Guo",
          "Zhiyu Wan"
        ],
        "published_date": "2024",
        "abstract": "Large language models (LLMs) have gained significant attention due to their prospective applications in medicine. Utilizing multimodal LLMs can potentially assist clinicians in medical image classification tasks. It is important to evaluate the performance of LLMs in medical image processing to potentially improve the medical system. We evaluated two multimodal LLMs (LLaVA and GPT-4-based ChatGPT) against the classic VGG in tumor classification across brain MRI, breast ultrasound, and kidney CT datasets. Despite LLMs facing significant hallucination issue in medical imaging, prompt engineering markedly enhanced their performance. In comparison to the baseline method, GPT-4-based ChatGPT with prompt engineering achieves 98%, 112%, and 69% of the baseline's performance in terms of accuracy (or 99%, 107%, and 62 % in terms of F1-score) in those three datasets, respectively. However, privacy, bias, accountability, and transparency concerns necessitate caution. Our study underscore LLMs' potential in medical imaging but emphasize the need for thorough performance and safety evaluations for their practical application.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/b7aa5af5bf96ee003543d5ff7dcfc3d9a46d43bb.pdf",
        "venue": "IEEE International Conference on Healthcare Informatics",
        "citationCount": 6,
        "score": 6.0,
        "summary": "Large language models (LLMs) have gained significant attention due to their prospective applications in medicine. Utilizing multimodal LLMs can potentially assist clinicians in medical image classification tasks. It is important to evaluate the performance of LLMs in medical image processing to potentially improve the medical system. We evaluated two multimodal LLMs (LLaVA and GPT-4-based ChatGPT) against the classic VGG in tumor classification across brain MRI, breast ultrasound, and kidney CT datasets. Despite LLMs facing significant hallucination issue in medical imaging, prompt engineering markedly enhanced their performance. In comparison to the baseline method, GPT-4-based ChatGPT with prompt engineering achieves 98%, 112%, and 69% of the baseline's performance in terms of accuracy (or 99%, 107%, and 62 % in terms of F1-score) in those three datasets, respectively. However, privacy, bias, accountability, and transparency concerns necessitate caution. Our study underscore LLMs' potential in medical imaging but emphasize the need for thorough performance and safety evaluations for their practical application.",
        "keywords": []
      },
      "file_name": "b7aa5af5bf96ee003543d5ff7dcfc3d9a46d43bb.pdf"
    },
    {
      "success": true,
      "doc_id": "ba297f40eaeba7f1266ef8ad7d1df206",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/8767dcddfb856db4bfa1e150470fc99f51f43835.pdf",
      "citation_key": "luo2024uh8",
      "metadata": {
        "title": "KELLMRec: Knowledge-Enhanced Large Language Models for Recommendation",
        "authors": [
          "Weiqing Luo",
          "Chonggang Song",
          "Lingling Yi",
          "Gong Cheng"
        ],
        "published_date": "2024",
        "abstract": "",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/8767dcddfb856db4bfa1e150470fc99f51f43835.pdf",
        "venue": "arXiv.org",
        "citationCount": 6,
        "score": 6.0,
        "summary": "",
        "keywords": []
      },
      "file_name": "8767dcddfb856db4bfa1e150470fc99f51f43835.pdf"
    },
    {
      "success": true,
      "doc_id": "9c4b55218f0cc094a5f037372300761c",
      "summary": "Large language models (LLMs), like OpenAIâ€™s ChatGPT and Googleâ€™s Gemini, operate as probabilistic models, leveraging their ability to generalise and discern intricate patterns within data. By assigning probabilities to different tokens based on patterns learned during extensive training on large datasets, these models can generate a wide range of contextually appropriate responses, spanning from textual scripts to auditory and visual outputs (both static and moving images). However, the inherent probabilistic nature of LLMs introduces a notable challenge, leading to the phenomenon known in the field of artificial intelligence as â€˜AI-hallucination,â€™ where the model may produce responses that sound plausible but are factually incorrect or nonsensical. Despite being perceived as a drawback, we posit in this paper that AI-hallucinations can be reframed as a distinctive feature of LLMs rather than a mere limitation. Our argument stems from the understanding that attempts to mitigate the harms caused by AI-hallucinations might inadvertently lead to increased model rigidity. This delicate balance between minimising harm and preserving the modelâ€™s flexibility is a central theme in our discussion. Furthermore, we revisit the concept of â€˜context,â€™ contending that a complete definition goes beyond the mere description of circumstances, environment, or surrounding facts. We assert that context is enriched by a conscious embodiment, involving the choice or refusal of action (considering all associate ethical implications) among a set of available options.",
      "intriguing_abstract": "Large language models (LLMs), like OpenAIâ€™s ChatGPT and Googleâ€™s Gemini, operate as probabilistic models, leveraging their ability to generalise and discern intricate patterns within data. By assigning probabilities to different tokens based on patterns learned during extensive training on large datasets, these models can generate a wide range of contextually appropriate responses, spanning from textual scripts to auditory and visual outputs (both static and moving images). However, the inherent probabilistic nature of LLMs introduces a notable challenge, leading to the phenomenon known in the field of artificial intelligence as â€˜AI-hallucination,â€™ where the model may produce responses that sound plausible but are factually incorrect or nonsensical. Despite being perceived as a drawback, we posit in this paper that AI-hallucinations can be reframed as a distinctive feature of LLMs rather than a mere limitation. Our argument stems from the understanding that attempts to mitigate the harms caused by AI-hallucinations might inadvertently lead to increased model rigidity. This delicate balance between minimising harm and preserving the modelâ€™s flexibility is a central theme in our discussion. Furthermore, we revisit the concept of â€˜context,â€™ contending that a complete definition goes beyond the mere description of circumstances, environment, or surrounding facts. We assert that context is enriched by a conscious embodiment, involving the choice or refusal of action (considering all associate ethical implications) among a set of available options.",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/6bf4e95e63df023c81458ec60a8324788535a2f4.pdf",
      "citation_key": "hamid2024pwn",
      "metadata": {
        "title": "Beyond Probabilities: Unveiling the Delicate Dance of Large Language Models (LLMs) and AI-Hallucination",
        "authors": [
          "Oussama H. Hamid"
        ],
        "published_date": "2024",
        "abstract": "Large language models (LLMs), like OpenAIâ€™s ChatGPT and Googleâ€™s Gemini, operate as probabilistic models, leveraging their ability to generalise and discern intricate patterns within data. By assigning probabilities to different tokens based on patterns learned during extensive training on large datasets, these models can generate a wide range of contextually appropriate responses, spanning from textual scripts to auditory and visual outputs (both static and moving images). However, the inherent probabilistic nature of LLMs introduces a notable challenge, leading to the phenomenon known in the field of artificial intelligence as â€˜AI-hallucination,â€™ where the model may produce responses that sound plausible but are factually incorrect or nonsensical. Despite being perceived as a drawback, we posit in this paper that AI-hallucinations can be reframed as a distinctive feature of LLMs rather than a mere limitation. Our argument stems from the understanding that attempts to mitigate the harms caused by AI-hallucinations might inadvertently lead to increased model rigidity. This delicate balance between minimising harm and preserving the modelâ€™s flexibility is a central theme in our discussion. Furthermore, we revisit the concept of â€˜context,â€™ contending that a complete definition goes beyond the mere description of circumstances, environment, or surrounding facts. We assert that context is enriched by a conscious embodiment, involving the choice or refusal of action (considering all associate ethical implications) among a set of available options.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/6bf4e95e63df023c81458ec60a8324788535a2f4.pdf",
        "venue": "Conference on Cognitive and Computational Aspects of Situation Management",
        "citationCount": 5,
        "score": 5.0,
        "summary": "Large language models (LLMs), like OpenAIâ€™s ChatGPT and Googleâ€™s Gemini, operate as probabilistic models, leveraging their ability to generalise and discern intricate patterns within data. By assigning probabilities to different tokens based on patterns learned during extensive training on large datasets, these models can generate a wide range of contextually appropriate responses, spanning from textual scripts to auditory and visual outputs (both static and moving images). However, the inherent probabilistic nature of LLMs introduces a notable challenge, leading to the phenomenon known in the field of artificial intelligence as â€˜AI-hallucination,â€™ where the model may produce responses that sound plausible but are factually incorrect or nonsensical. Despite being perceived as a drawback, we posit in this paper that AI-hallucinations can be reframed as a distinctive feature of LLMs rather than a mere limitation. Our argument stems from the understanding that attempts to mitigate the harms caused by AI-hallucinations might inadvertently lead to increased model rigidity. This delicate balance between minimising harm and preserving the modelâ€™s flexibility is a central theme in our discussion. Furthermore, we revisit the concept of â€˜context,â€™ contending that a complete definition goes beyond the mere description of circumstances, environment, or surrounding facts. We assert that context is enriched by a conscious embodiment, involving the choice or refusal of action (considering all associate ethical implications) among a set of available options.",
        "keywords": []
      },
      "file_name": "6bf4e95e63df023c81458ec60a8324788535a2f4.pdf"
    },
    {
      "success": true,
      "doc_id": "dba274262d103abff79a9b99e1ac2082",
      "summary": "Large language models (LLMs) suffer from the hallucination problem and face significant challenges when applied to knowledge-intensive tasks. A promising approach is to leverage evidence documents as extra supporting knowledge, which can be obtained through retrieval or generation. However, existing methods directly leverage the entire contents of the evidence document, which may introduce noise information and impair the performance of large language models. To tackle this problem, we propose a novel Knowledge Selection of Large Language Models (KS-LLM) method, aiming to identify valuable information from evidence documents. The KS-LLM approach utilizes triples to effectively select knowledge snippets from evidence documents that are beneficial to answering questions. Specifically, we first generate triples based on the input question, then select the evidence sentences most similar to triples from the evidence document, and finally combine the evidence sentences and triples to assist large language models in generating answers. Experimental comparisons on several question answering datasets, such as TriviaQA, WebQ, and NQ, demonstrate that the proposed method surpasses the baselines and achieves the best results.",
      "intriguing_abstract": "Large language models (LLMs) suffer from the hallucination problem and face significant challenges when applied to knowledge-intensive tasks. A promising approach is to leverage evidence documents as extra supporting knowledge, which can be obtained through retrieval or generation. However, existing methods directly leverage the entire contents of the evidence document, which may introduce noise information and impair the performance of large language models. To tackle this problem, we propose a novel Knowledge Selection of Large Language Models (KS-LLM) method, aiming to identify valuable information from evidence documents. The KS-LLM approach utilizes triples to effectively select knowledge snippets from evidence documents that are beneficial to answering questions. Specifically, we first generate triples based on the input question, then select the evidence sentences most similar to triples from the evidence document, and finally combine the evidence sentences and triples to assist large language models in generating answers. Experimental comparisons on several question answering datasets, such as TriviaQA, WebQ, and NQ, demonstrate that the proposed method surpasses the baselines and achieves the best results.",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/8705389fd69be2a0cecd2242287a08e8280f2c52.pdf",
      "citation_key": "zheng20240qd",
      "metadata": {
        "title": "KS-LLM: Knowledge Selection of Large Language Models with Evidence Document for Question Answering",
        "authors": [
          "Xinxin Zheng",
          "Feihu Che",
          "Jinyang Wu",
          "Shuai Zhang",
          "Shuai Nie",
          "Kang Liu",
          "Jianhua Tao"
        ],
        "published_date": "2024",
        "abstract": "Large language models (LLMs) suffer from the hallucination problem and face significant challenges when applied to knowledge-intensive tasks. A promising approach is to leverage evidence documents as extra supporting knowledge, which can be obtained through retrieval or generation. However, existing methods directly leverage the entire contents of the evidence document, which may introduce noise information and impair the performance of large language models. To tackle this problem, we propose a novel Knowledge Selection of Large Language Models (KS-LLM) method, aiming to identify valuable information from evidence documents. The KS-LLM approach utilizes triples to effectively select knowledge snippets from evidence documents that are beneficial to answering questions. Specifically, we first generate triples based on the input question, then select the evidence sentences most similar to triples from the evidence document, and finally combine the evidence sentences and triples to assist large language models in generating answers. Experimental comparisons on several question answering datasets, such as TriviaQA, WebQ, and NQ, demonstrate that the proposed method surpasses the baselines and achieves the best results.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/8705389fd69be2a0cecd2242287a08e8280f2c52.pdf",
        "venue": "arXiv.org",
        "citationCount": 5,
        "score": 5.0,
        "summary": "Large language models (LLMs) suffer from the hallucination problem and face significant challenges when applied to knowledge-intensive tasks. A promising approach is to leverage evidence documents as extra supporting knowledge, which can be obtained through retrieval or generation. However, existing methods directly leverage the entire contents of the evidence document, which may introduce noise information and impair the performance of large language models. To tackle this problem, we propose a novel Knowledge Selection of Large Language Models (KS-LLM) method, aiming to identify valuable information from evidence documents. The KS-LLM approach utilizes triples to effectively select knowledge snippets from evidence documents that are beneficial to answering questions. Specifically, we first generate triples based on the input question, then select the evidence sentences most similar to triples from the evidence document, and finally combine the evidence sentences and triples to assist large language models in generating answers. Experimental comparisons on several question answering datasets, such as TriviaQA, WebQ, and NQ, demonstrate that the proposed method surpasses the baselines and achieves the best results.",
        "keywords": []
      },
      "file_name": "8705389fd69be2a0cecd2242287a08e8280f2c52.pdf"
    },
    {
      "success": true,
      "doc_id": "a6232d13d08966797cceb2a09dc3b961",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/21967f943a189a9171f9f880182894acff5b87a4.pdf",
      "citation_key": "rawte2024bu6",
      "metadata": {
        "title": "Tutorial Proposal: Hallucination in Large Language Models",
        "authors": [
          "Vipula Rawte",
          "Aman Chadha",
          "Amit P. Sheth",
          "Amitava Das"
        ],
        "published_date": "2024",
        "abstract": "",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/21967f943a189a9171f9f880182894acff5b87a4.pdf",
        "venue": "International Conference on Language Resources and Evaluation",
        "citationCount": 4,
        "score": 4.0,
        "summary": "",
        "keywords": []
      },
      "file_name": "21967f943a189a9171f9f880182894acff5b87a4.pdf"
    },
    {
      "success": true,
      "doc_id": "f2d7bf53b24575195552032beb6816c3",
      "summary": "Large language models have demonstrated impressive language processing capabilities in recent years, exhibiting unparalleled excellence in the field of natural language processing. However, the generated text sometimes contains hallucinations, which is the text that contradicts the knowledge in the real world, the context, and the user input. This problem is mainly due to the inherent limitations of the method itself in aspects such as data quality, the model training process, and the model generation process. The issue of hallucinations has always been closely monitored by the academic community. It is widely recognized that its potential consequences should not be underestimated. This paper systematically summarizes the research on the causes of hallucinations in large language models, and introduces mainstream classification methods as well as current measures to address the issue of hallucinations. To be more specific, the article divides the causes of hallucinations into two categories: 1. hallucinations come from the training process and 2. hallucinations come from the generation process. Also, 4 typical types of causes for the former and 5 typical types of causes for the latter are provided. Simultaneously, a detailed discussion of 16 methods to mitigate hallucinations that arise in the generation process is offered. Finally, this paper also discusses inherent flaws that may exist in large language models, aiming to help people gain a more comprehensive understanding and research into hallucinations and large language models. In general, the text details about the hallucinations that exist in the large language model. Meanwhile, according to the previous research, it is pointed out that it is difficult for the large language model based on autoregressive method for token prediction to avoid the hallucinations completely.",
      "intriguing_abstract": "Large language models have demonstrated impressive language processing capabilities in recent years, exhibiting unparalleled excellence in the field of natural language processing. However, the generated text sometimes contains hallucinations, which is the text that contradicts the knowledge in the real world, the context, and the user input. This problem is mainly due to the inherent limitations of the method itself in aspects such as data quality, the model training process, and the model generation process. The issue of hallucinations has always been closely monitored by the academic community. It is widely recognized that its potential consequences should not be underestimated. This paper systematically summarizes the research on the causes of hallucinations in large language models, and introduces mainstream classification methods as well as current measures to address the issue of hallucinations. To be more specific, the article divides the causes of hallucinations into two categories: 1. hallucinations come from the training process and 2. hallucinations come from the generation process. Also, 4 typical types of causes for the former and 5 typical types of causes for the latter are provided. Simultaneously, a detailed discussion of 16 methods to mitigate hallucinations that arise in the generation process is offered. Finally, this paper also discusses inherent flaws that may exist in large language models, aiming to help people gain a more comprehensive understanding and research into hallucinations and large language models. In general, the text details about the hallucinations that exist in the large language model. Meanwhile, according to the previous research, it is pointed out that it is difficult for the large language model based on autoregressive method for token prediction to avoid the hallucinations completely.",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/5e5336d0284e94cd835c40f931e0d379e21b464d.pdf",
      "citation_key": "yin2024iau",
      "metadata": {
        "title": "A review of methods for alleviating hallucination issues in large language models",
        "authors": [
          "Zhibo Yin"
        ],
        "published_date": "2024",
        "abstract": "Large language models have demonstrated impressive language processing capabilities in recent years, exhibiting unparalleled excellence in the field of natural language processing. However, the generated text sometimes contains hallucinations, which is the text that contradicts the knowledge in the real world, the context, and the user input. This problem is mainly due to the inherent limitations of the method itself in aspects such as data quality, the model training process, and the model generation process. The issue of hallucinations has always been closely monitored by the academic community. It is widely recognized that its potential consequences should not be underestimated. This paper systematically summarizes the research on the causes of hallucinations in large language models, and introduces mainstream classification methods as well as current measures to address the issue of hallucinations. To be more specific, the article divides the causes of hallucinations into two categories: 1. hallucinations come from the training process and 2. hallucinations come from the generation process. Also, 4 typical types of causes for the former and 5 typical types of causes for the latter are provided. Simultaneously, a detailed discussion of 16 methods to mitigate hallucinations that arise in the generation process is offered. Finally, this paper also discusses inherent flaws that may exist in large language models, aiming to help people gain a more comprehensive understanding and research into hallucinations and large language models. In general, the text details about the hallucinations that exist in the large language model. Meanwhile, according to the previous research, it is pointed out that it is difficult for the large language model based on autoregressive method for token prediction to avoid the hallucinations completely.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/5e5336d0284e94cd835c40f931e0d379e21b464d.pdf",
        "venue": "Applied and Computational Engineering",
        "citationCount": 4,
        "score": 4.0,
        "summary": "Large language models have demonstrated impressive language processing capabilities in recent years, exhibiting unparalleled excellence in the field of natural language processing. However, the generated text sometimes contains hallucinations, which is the text that contradicts the knowledge in the real world, the context, and the user input. This problem is mainly due to the inherent limitations of the method itself in aspects such as data quality, the model training process, and the model generation process. The issue of hallucinations has always been closely monitored by the academic community. It is widely recognized that its potential consequences should not be underestimated. This paper systematically summarizes the research on the causes of hallucinations in large language models, and introduces mainstream classification methods as well as current measures to address the issue of hallucinations. To be more specific, the article divides the causes of hallucinations into two categories: 1. hallucinations come from the training process and 2. hallucinations come from the generation process. Also, 4 typical types of causes for the former and 5 typical types of causes for the latter are provided. Simultaneously, a detailed discussion of 16 methods to mitigate hallucinations that arise in the generation process is offered. Finally, this paper also discusses inherent flaws that may exist in large language models, aiming to help people gain a more comprehensive understanding and research into hallucinations and large language models. In general, the text details about the hallucinations that exist in the large language model. Meanwhile, according to the previous research, it is pointed out that it is difficult for the large language model based on autoregressive method for token prediction to avoid the hallucinations completely.",
        "keywords": []
      },
      "file_name": "5e5336d0284e94cd835c40f931e0d379e21b464d.pdf"
    },
    {
      "success": true,
      "doc_id": "a952d480040c49598747098e82f57a12",
      "summary": "Machine Translation (MT) is undergoing a paradigm shift, with systems based on fine-tuned large language models (LLM) becoming increasingly competitive with traditional encoder-decoder models trained specifically for translation tasks. However, LLM-based systems are at a higher risk of generating hallucinations, which can severely undermine user's trust and safety. Most prior research on hallucination mitigation focuses on traditional MT models, with solutions that involve post-hoc mitigation - detecting hallucinated translations and re-translating them. While effective, this approach introduces additional complexity in deploying extra tools in production and also increases latency. To address these limitations, we propose a method that intrinsically learns to mitigate hallucinations during the model training phase. Specifically, we introduce a data creation framework to generate hallucination focused preference datasets. Fine-tuning LLMs on these preference datasets reduces the hallucination rate by an average of 96% across five language pairs, while preserving overall translation quality. In a zero-shot setting our approach reduces hallucinations by 89% on an average across three unseen target languages.",
      "intriguing_abstract": "Machine Translation (MT) is undergoing a paradigm shift, with systems based on fine-tuned large language models (LLM) becoming increasingly competitive with traditional encoder-decoder models trained specifically for translation tasks. However, LLM-based systems are at a higher risk of generating hallucinations, which can severely undermine user's trust and safety. Most prior research on hallucination mitigation focuses on traditional MT models, with solutions that involve post-hoc mitigation - detecting hallucinated translations and re-translating them. While effective, this approach introduces additional complexity in deploying extra tools in production and also increases latency. To address these limitations, we propose a method that intrinsically learns to mitigate hallucinations during the model training phase. Specifically, we introduce a data creation framework to generate hallucination focused preference datasets. Fine-tuning LLMs on these preference datasets reduces the hallucination rate by an average of 96% across five language pairs, while preserving overall translation quality. In a zero-shot setting our approach reduces hallucinations by 89% on an average across three unseen target languages.",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/1d1af14aa70f86b013e616cfd07fa8a164652d84.pdf",
      "citation_key": "tang2025mfi",
      "metadata": {
        "title": "Mitigating Hallucinated Translations in Large Language Models with Hallucination-focused Preference Optimization",
        "authors": [
          "Zilu Tang",
          "Rajen Chatterjee",
          "Sarthak Garg"
        ],
        "published_date": "2025",
        "abstract": "Machine Translation (MT) is undergoing a paradigm shift, with systems based on fine-tuned large language models (LLM) becoming increasingly competitive with traditional encoder-decoder models trained specifically for translation tasks. However, LLM-based systems are at a higher risk of generating hallucinations, which can severely undermine user's trust and safety. Most prior research on hallucination mitigation focuses on traditional MT models, with solutions that involve post-hoc mitigation - detecting hallucinated translations and re-translating them. While effective, this approach introduces additional complexity in deploying extra tools in production and also increases latency. To address these limitations, we propose a method that intrinsically learns to mitigate hallucinations during the model training phase. Specifically, we introduce a data creation framework to generate hallucination focused preference datasets. Fine-tuning LLMs on these preference datasets reduces the hallucination rate by an average of 96% across five language pairs, while preserving overall translation quality. In a zero-shot setting our approach reduces hallucinations by 89% on an average across three unseen target languages.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/1d1af14aa70f86b013e616cfd07fa8a164652d84.pdf",
        "venue": "North American Chapter of the Association for Computational Linguistics",
        "citationCount": 4,
        "score": 4.0,
        "summary": "Machine Translation (MT) is undergoing a paradigm shift, with systems based on fine-tuned large language models (LLM) becoming increasingly competitive with traditional encoder-decoder models trained specifically for translation tasks. However, LLM-based systems are at a higher risk of generating hallucinations, which can severely undermine user's trust and safety. Most prior research on hallucination mitigation focuses on traditional MT models, with solutions that involve post-hoc mitigation - detecting hallucinated translations and re-translating them. While effective, this approach introduces additional complexity in deploying extra tools in production and also increases latency. To address these limitations, we propose a method that intrinsically learns to mitigate hallucinations during the model training phase. Specifically, we introduce a data creation framework to generate hallucination focused preference datasets. Fine-tuning LLMs on these preference datasets reduces the hallucination rate by an average of 96% across five language pairs, while preserving overall translation quality. In a zero-shot setting our approach reduces hallucinations by 89% on an average across three unseen target languages.",
        "keywords": []
      },
      "file_name": "1d1af14aa70f86b013e616cfd07fa8a164652d84.pdf"
    },
    {
      "success": true,
      "doc_id": "11d41ac4cad5977c8d191f64e6f540ea",
      "summary": "Large language models (LLMs) exhibit impressive natural language capabilities but suffer from hallucination -- generating content ungrounded in the realities of training data. Recent work has focused on decoding techniques to improve factuality during inference by leveraging LLMs' hierarchical representation of factual knowledge, manipulating the predicted distributions at inference time. Current state-of-the-art approaches refine decoding by contrasting early-exit distributions from a lower layer with the final layer to exploit information related to factuality within the model forward procedure. However, such methods often assume the final layer is the most reliable and the lower layer selection process depends on it. In this work, we first propose extrapolation of critical token probabilities beyond the last layer for more accurate contrasting. We additionally employ layer-wise entropy-guided lower layer selection, decoupling the selection process from the final layer. Experiments demonstrate strong performance - surpassing state-of-the-art on multiple different datasets by large margins. Analyses show different kinds of prompts respond to different selection strategies.",
      "intriguing_abstract": "Large language models (LLMs) exhibit impressive natural language capabilities but suffer from hallucination -- generating content ungrounded in the realities of training data. Recent work has focused on decoding techniques to improve factuality during inference by leveraging LLMs' hierarchical representation of factual knowledge, manipulating the predicted distributions at inference time. Current state-of-the-art approaches refine decoding by contrasting early-exit distributions from a lower layer with the final layer to exploit information related to factuality within the model forward procedure. However, such methods often assume the final layer is the most reliable and the lower layer selection process depends on it. In this work, we first propose extrapolation of critical token probabilities beyond the last layer for more accurate contrasting. We additionally employ layer-wise entropy-guided lower layer selection, decoupling the selection process from the final layer. Experiments demonstrate strong performance - surpassing state-of-the-art on multiple different datasets by large margins. Analyses show different kinds of prompts respond to different selection strategies.",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/6073b9a4856d726c270f03ebee54ea7658f16ec1.pdf",
      "citation_key": "das2024jdt",
      "metadata": {
        "title": "Entropy Guided Extrapolative Decoding to Improve Factuality in Large Language Models",
        "authors": [
          "Souvik Das",
          "Lifeng Jin",
          "Linfeng Song",
          "Haitao Mi",
          "Baolin Peng",
          "Dong Yu"
        ],
        "published_date": "2024",
        "abstract": "Large language models (LLMs) exhibit impressive natural language capabilities but suffer from hallucination -- generating content ungrounded in the realities of training data. Recent work has focused on decoding techniques to improve factuality during inference by leveraging LLMs' hierarchical representation of factual knowledge, manipulating the predicted distributions at inference time. Current state-of-the-art approaches refine decoding by contrasting early-exit distributions from a lower layer with the final layer to exploit information related to factuality within the model forward procedure. However, such methods often assume the final layer is the most reliable and the lower layer selection process depends on it. In this work, we first propose extrapolation of critical token probabilities beyond the last layer for more accurate contrasting. We additionally employ layer-wise entropy-guided lower layer selection, decoupling the selection process from the final layer. Experiments demonstrate strong performance - surpassing state-of-the-art on multiple different datasets by large margins. Analyses show different kinds of prompts respond to different selection strategies.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/6073b9a4856d726c270f03ebee54ea7658f16ec1.pdf",
        "venue": "International Conference on Computational Linguistics",
        "citationCount": 4,
        "score": 4.0,
        "summary": "Large language models (LLMs) exhibit impressive natural language capabilities but suffer from hallucination -- generating content ungrounded in the realities of training data. Recent work has focused on decoding techniques to improve factuality during inference by leveraging LLMs' hierarchical representation of factual knowledge, manipulating the predicted distributions at inference time. Current state-of-the-art approaches refine decoding by contrasting early-exit distributions from a lower layer with the final layer to exploit information related to factuality within the model forward procedure. However, such methods often assume the final layer is the most reliable and the lower layer selection process depends on it. In this work, we first propose extrapolation of critical token probabilities beyond the last layer for more accurate contrasting. We additionally employ layer-wise entropy-guided lower layer selection, decoupling the selection process from the final layer. Experiments demonstrate strong performance - surpassing state-of-the-art on multiple different datasets by large margins. Analyses show different kinds of prompts respond to different selection strategies.",
        "keywords": []
      },
      "file_name": "6073b9a4856d726c270f03ebee54ea7658f16ec1.pdf"
    },
    {
      "success": true,
      "doc_id": "97e31304767be3354da59b7ce100bafe",
      "summary": "Tool-augmented large language models (LLMs) are rapidly being integrated into real-world applications. Due to the lack of benchmarks, the community has yet to fully understand the hallucination issues within these models. To address this challenge, we introduce a comprehensive diagnostic benchmark, ToolBH. Specifically, we assess the LLMâ€™s hallucinations through two perspectives: depth and breadth. In terms of depth, we propose a multi-level diagnostic process, including (1) solvability detection, (2) solution planning, and (3) missing-tool analysis. For breadth, we consider three scenarios based on the characteristics of the toolset: missing necessary tools, potential tools, and limited functionality tools. Furthermore, we developed seven tasks and collected 700 evaluation samples through multiple rounds of manual annotation. The results show the significant challenges presented by the ToolBH benchmark. The current advanced models Gemini-1.5-Pro and GPT-4o only achieve total scores of 45.3 and 37.0, respectively, on a scale of 100. In this benchmark, larger model parameters do not guarantee better performance; the training data and response strategies also play crucial roles in tool-enhanced LLM scenarios. Our diagnostic analysis indicates that the primary reason for model errors lies in assessing task solvability. Additionally, open-weight models suffer from performance drops with verbose replies, whereas proprietary models excel with longer reasoning.",
      "intriguing_abstract": "Tool-augmented large language models (LLMs) are rapidly being integrated into real-world applications. Due to the lack of benchmarks, the community has yet to fully understand the hallucination issues within these models. To address this challenge, we introduce a comprehensive diagnostic benchmark, ToolBH. Specifically, we assess the LLMâ€™s hallucinations through two perspectives: depth and breadth. In terms of depth, we propose a multi-level diagnostic process, including (1) solvability detection, (2) solution planning, and (3) missing-tool analysis. For breadth, we consider three scenarios based on the characteristics of the toolset: missing necessary tools, potential tools, and limited functionality tools. Furthermore, we developed seven tasks and collected 700 evaluation samples through multiple rounds of manual annotation. The results show the significant challenges presented by the ToolBH benchmark. The current advanced models Gemini-1.5-Pro and GPT-4o only achieve total scores of 45.3 and 37.0, respectively, on a scale of 100. In this benchmark, larger model parameters do not guarantee better performance; the training data and response strategies also play crucial roles in tool-enhanced LLM scenarios. Our diagnostic analysis indicates that the primary reason for model errors lies in assessing task solvability. Additionally, open-weight models suffer from performance drops with verbose replies, whereas proprietary models excel with longer reasoning.",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/4c4a3328153e85749c690e68acc13b42a7225e50.pdf",
      "citation_key": "zhang2024h4a",
      "metadata": {
        "title": "ToolBeHonest: A Multi-level Hallucination Diagnostic Benchmark for Tool-Augmented Large Language Models",
        "authors": [
          "Yuxiang Zhang",
          "Jing Chen",
          "Junjie Wang",
          "Yaxin Liu",
          "Cheng Yang",
          "Chufan Shi",
          "Xinyu Zhu",
          "Zihao Lin",
          "Hanwen Wan",
          "Yujiu Yang",
          "Tetsuya Sakai",
          "Tian Feng",
          "Hayato Yamana"
        ],
        "published_date": "2024",
        "abstract": "Tool-augmented large language models (LLMs) are rapidly being integrated into real-world applications. Due to the lack of benchmarks, the community has yet to fully understand the hallucination issues within these models. To address this challenge, we introduce a comprehensive diagnostic benchmark, ToolBH. Specifically, we assess the LLMâ€™s hallucinations through two perspectives: depth and breadth. In terms of depth, we propose a multi-level diagnostic process, including (1) solvability detection, (2) solution planning, and (3) missing-tool analysis. For breadth, we consider three scenarios based on the characteristics of the toolset: missing necessary tools, potential tools, and limited functionality tools. Furthermore, we developed seven tasks and collected 700 evaluation samples through multiple rounds of manual annotation. The results show the significant challenges presented by the ToolBH benchmark. The current advanced models Gemini-1.5-Pro and GPT-4o only achieve total scores of 45.3 and 37.0, respectively, on a scale of 100. In this benchmark, larger model parameters do not guarantee better performance; the training data and response strategies also play crucial roles in tool-enhanced LLM scenarios. Our diagnostic analysis indicates that the primary reason for model errors lies in assessing task solvability. Additionally, open-weight models suffer from performance drops with verbose replies, whereas proprietary models excel with longer reasoning.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/4c4a3328153e85749c690e68acc13b42a7225e50.pdf",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "citationCount": 4,
        "score": 4.0,
        "summary": "Tool-augmented large language models (LLMs) are rapidly being integrated into real-world applications. Due to the lack of benchmarks, the community has yet to fully understand the hallucination issues within these models. To address this challenge, we introduce a comprehensive diagnostic benchmark, ToolBH. Specifically, we assess the LLMâ€™s hallucinations through two perspectives: depth and breadth. In terms of depth, we propose a multi-level diagnostic process, including (1) solvability detection, (2) solution planning, and (3) missing-tool analysis. For breadth, we consider three scenarios based on the characteristics of the toolset: missing necessary tools, potential tools, and limited functionality tools. Furthermore, we developed seven tasks and collected 700 evaluation samples through multiple rounds of manual annotation. The results show the significant challenges presented by the ToolBH benchmark. The current advanced models Gemini-1.5-Pro and GPT-4o only achieve total scores of 45.3 and 37.0, respectively, on a scale of 100. In this benchmark, larger model parameters do not guarantee better performance; the training data and response strategies also play crucial roles in tool-enhanced LLM scenarios. Our diagnostic analysis indicates that the primary reason for model errors lies in assessing task solvability. Additionally, open-weight models suffer from performance drops with verbose replies, whereas proprietary models excel with longer reasoning.",
        "keywords": []
      },
      "file_name": "4c4a3328153e85749c690e68acc13b42a7225e50.pdf"
    },
    {
      "success": true,
      "doc_id": "07422ca0b62b445d544f01078e28d827",
      "summary": "The impressive capabilities of large language models (LLMs) have attracted extensive interests of applying LLMs to medical field. However, the complex nature of clinical environments presents significant hallucination challenges for LLMs, hindering their widespread adoption. In this paper, we address these hallucination issues in the context of Medical Information Extraction (MIE) tasks by introducing ALternate Contrastive Decoding (ALCD). We begin by redefining MIE tasks as an identify-and-classify process. We then separate the identification and classification functions of LLMs by selectively masking the optimization of tokens during fine-tuning. During the inference stage, we alternately contrast output distributions derived from sub-task models. This approach aims to selectively enhance the identification and classification capabilities while minimizing the influence of other inherent abilities in LLMs. Additionally, we propose an alternate adaptive constraint strategy to more effectively adjust the scale and scope of contrastive tokens. Through comprehensive experiments on two different backbones and six diverse medical information extraction tasks, ALCD demonstrates significant improvements in resolving hallucination issues compared to conventional decoding methods.",
      "intriguing_abstract": "The impressive capabilities of large language models (LLMs) have attracted extensive interests of applying LLMs to medical field. However, the complex nature of clinical environments presents significant hallucination challenges for LLMs, hindering their widespread adoption. In this paper, we address these hallucination issues in the context of Medical Information Extraction (MIE) tasks by introducing ALternate Contrastive Decoding (ALCD). We begin by redefining MIE tasks as an identify-and-classify process. We then separate the identification and classification functions of LLMs by selectively masking the optimization of tokens during fine-tuning. During the inference stage, we alternately contrast output distributions derived from sub-task models. This approach aims to selectively enhance the identification and classification capabilities while minimizing the influence of other inherent abilities in LLMs. Additionally, we propose an alternate adaptive constraint strategy to more effectively adjust the scale and scope of contrastive tokens. Through comprehensive experiments on two different backbones and six diverse medical information extraction tasks, ALCD demonstrates significant improvements in resolving hallucination issues compared to conventional decoding methods.",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/849b3727dbb41c37f92a338ac5860b764a5b94f4.pdf",
      "citation_key": "xu2024t34",
      "metadata": {
        "title": "Mitigating Hallucinations of Large Language Models in Medical Information Extraction via Contrastive Decoding",
        "authors": [
          "Derong Xu",
          "Ziheng Zhang",
          "Zhihong Zhu",
          "Zhenxi Lin",
          "Qidong Liu",
          "Xian Wu",
          "Tong Xu",
          "Xiangyu Zhao",
          "Yefeng Zheng",
          "Enhong Chen"
        ],
        "published_date": "2024",
        "abstract": "The impressive capabilities of large language models (LLMs) have attracted extensive interests of applying LLMs to medical field. However, the complex nature of clinical environments presents significant hallucination challenges for LLMs, hindering their widespread adoption. In this paper, we address these hallucination issues in the context of Medical Information Extraction (MIE) tasks by introducing ALternate Contrastive Decoding (ALCD). We begin by redefining MIE tasks as an identify-and-classify process. We then separate the identification and classification functions of LLMs by selectively masking the optimization of tokens during fine-tuning. During the inference stage, we alternately contrast output distributions derived from sub-task models. This approach aims to selectively enhance the identification and classification capabilities while minimizing the influence of other inherent abilities in LLMs. Additionally, we propose an alternate adaptive constraint strategy to more effectively adjust the scale and scope of contrastive tokens. Through comprehensive experiments on two different backbones and six diverse medical information extraction tasks, ALCD demonstrates significant improvements in resolving hallucination issues compared to conventional decoding methods.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/849b3727dbb41c37f92a338ac5860b764a5b94f4.pdf",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "citationCount": 4,
        "score": 4.0,
        "summary": "The impressive capabilities of large language models (LLMs) have attracted extensive interests of applying LLMs to medical field. However, the complex nature of clinical environments presents significant hallucination challenges for LLMs, hindering their widespread adoption. In this paper, we address these hallucination issues in the context of Medical Information Extraction (MIE) tasks by introducing ALternate Contrastive Decoding (ALCD). We begin by redefining MIE tasks as an identify-and-classify process. We then separate the identification and classification functions of LLMs by selectively masking the optimization of tokens during fine-tuning. During the inference stage, we alternately contrast output distributions derived from sub-task models. This approach aims to selectively enhance the identification and classification capabilities while minimizing the influence of other inherent abilities in LLMs. Additionally, we propose an alternate adaptive constraint strategy to more effectively adjust the scale and scope of contrastive tokens. Through comprehensive experiments on two different backbones and six diverse medical information extraction tasks, ALCD demonstrates significant improvements in resolving hallucination issues compared to conventional decoding methods.",
        "keywords": []
      },
      "file_name": "849b3727dbb41c37f92a338ac5860b764a5b94f4.pdf"
    },
    {
      "success": true,
      "doc_id": "ded57894ceb4382db79743f976fdcfbd",
      "summary": "Spatial reasoning in Large Language Models (LLMs) serves as a foundation for embodied intelligence. However, even in simple maze environments, LLMs often struggle to plan correct paths due to hallucination issues. To address this, we propose S2ERS, an LLM-based technique that integrates entity and relation extraction with the on-policy reinforcement learning algorithm Sarsa for optimal path planning. We introduce three key improvements: (1) To tackle the hallucination of spatial, we extract a graph structure of entities and relations from the text-based maze description, aiding LLMs in accurately comprehending spatial relationships. (2) To prevent LLMs from getting trapped in dead ends due to context inconsistency hallucination by long-term reasoning, we insert the state-action value function Q into the prompts, guiding the LLMâ€™s path planning. (3) To reduce the token consumption of LLMs, we utilize multi-step reasoning, dynamically inserting local Q-tables into the prompt to assist the LLM in outputting multiple steps of actions at once. Our comprehensive experimental evaluation, conducted using closed-source LLMs ChatGPT 3.5, ERNIE-Bot 4.0 and open-source LLM ChatGLM-6B, demonstrates that S2ERS significantly mitigates the spatial hallucination issues in LLMs, and improves the success rate and optimal rate by approximately 29% and 19%, respectively, in comparison to the SOTA CoT methods.",
      "intriguing_abstract": "Spatial reasoning in Large Language Models (LLMs) serves as a foundation for embodied intelligence. However, even in simple maze environments, LLMs often struggle to plan correct paths due to hallucination issues. To address this, we propose S2ERS, an LLM-based technique that integrates entity and relation extraction with the on-policy reinforcement learning algorithm Sarsa for optimal path planning. We introduce three key improvements: (1) To tackle the hallucination of spatial, we extract a graph structure of entities and relations from the text-based maze description, aiding LLMs in accurately comprehending spatial relationships. (2) To prevent LLMs from getting trapped in dead ends due to context inconsistency hallucination by long-term reasoning, we insert the state-action value function Q into the prompts, guiding the LLMâ€™s path planning. (3) To reduce the token consumption of LLMs, we utilize multi-step reasoning, dynamically inserting local Q-tables into the prompt to assist the LLM in outputting multiple steps of actions at once. Our comprehensive experimental evaluation, conducted using closed-source LLMs ChatGPT 3.5, ERNIE-Bot 4.0 and open-source LLM ChatGLM-6B, demonstrates that S2ERS significantly mitigates the spatial hallucination issues in LLMs, and improves the success rate and optimal rate by approximately 29% and 19%, respectively, in comparison to the SOTA CoT methods.",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/55adbe4a6511a9c036024c6e2a637782d53289f4.pdf",
      "citation_key": "zhang2025p1z",
      "metadata": {
        "title": "Mitigating spatial hallucination in large language models for path planning via prompt engineering",
        "authors": [
          "Hongjie Zhang",
          "Hourui Deng",
          "Jie Ou",
          "Chaosheng Feng"
        ],
        "published_date": "2025",
        "abstract": "Spatial reasoning in Large Language Models (LLMs) serves as a foundation for embodied intelligence. However, even in simple maze environments, LLMs often struggle to plan correct paths due to hallucination issues. To address this, we propose S2ERS, an LLM-based technique that integrates entity and relation extraction with the on-policy reinforcement learning algorithm Sarsa for optimal path planning. We introduce three key improvements: (1) To tackle the hallucination of spatial, we extract a graph structure of entities and relations from the text-based maze description, aiding LLMs in accurately comprehending spatial relationships. (2) To prevent LLMs from getting trapped in dead ends due to context inconsistency hallucination by long-term reasoning, we insert the state-action value function Q into the prompts, guiding the LLMâ€™s path planning. (3) To reduce the token consumption of LLMs, we utilize multi-step reasoning, dynamically inserting local Q-tables into the prompt to assist the LLM in outputting multiple steps of actions at once. Our comprehensive experimental evaluation, conducted using closed-source LLMs ChatGPT 3.5, ERNIE-Bot 4.0 and open-source LLM ChatGLM-6B, demonstrates that S2ERS significantly mitigates the spatial hallucination issues in LLMs, and improves the success rate and optimal rate by approximately 29% and 19%, respectively, in comparison to the SOTA CoT methods.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/55adbe4a6511a9c036024c6e2a637782d53289f4.pdf",
        "venue": "Scientific Reports",
        "citationCount": 3,
        "score": 3.0,
        "summary": "Spatial reasoning in Large Language Models (LLMs) serves as a foundation for embodied intelligence. However, even in simple maze environments, LLMs often struggle to plan correct paths due to hallucination issues. To address this, we propose S2ERS, an LLM-based technique that integrates entity and relation extraction with the on-policy reinforcement learning algorithm Sarsa for optimal path planning. We introduce three key improvements: (1) To tackle the hallucination of spatial, we extract a graph structure of entities and relations from the text-based maze description, aiding LLMs in accurately comprehending spatial relationships. (2) To prevent LLMs from getting trapped in dead ends due to context inconsistency hallucination by long-term reasoning, we insert the state-action value function Q into the prompts, guiding the LLMâ€™s path planning. (3) To reduce the token consumption of LLMs, we utilize multi-step reasoning, dynamically inserting local Q-tables into the prompt to assist the LLM in outputting multiple steps of actions at once. Our comprehensive experimental evaluation, conducted using closed-source LLMs ChatGPT 3.5, ERNIE-Bot 4.0 and open-source LLM ChatGLM-6B, demonstrates that S2ERS significantly mitigates the spatial hallucination issues in LLMs, and improves the success rate and optimal rate by approximately 29% and 19%, respectively, in comparison to the SOTA CoT methods.",
        "keywords": []
      },
      "file_name": "55adbe4a6511a9c036024c6e2a637782d53289f4.pdf"
    },
    {
      "success": true,
      "doc_id": "4661ab3988718e22a4e8d84dab0b6541",
      "summary": "Large Language Models (LLMs) have emerged as powerful tools in Artificial Intelligence, showcasing remarkable linguistic mastery. However, amidst their expansive capabilities, a nuanced challenge arises: the phenomenon of hallucination. Hallucination introduces unpredictability and creativity into LLM-generated content, raising concerns about its implications. This paper seeks to illuminate the complex ramifications of hallucination in LLMs by examining its subtleties. The goal is to evaluate current efforts to mitigate hallucinations and improve the clarity of language generation. We delve into the intriguing world of AI with a focused examination of hallucinations in LLMs, exploring various strategies and methods aimed at reducing their effects and enhancing the accuracy of language generation. The analysis highlights the potential consequences for various applications and underscores the significant impact of hallucinations on LLM-generated content. Current solutions to this issue are discussed, showcasing advancements in the reliability and clarity of language generation. In conclusion, the pursuit of accuracy in LLMs faces captivating challenges posed by hallucinations. By exploring the complexities of this phenomenon and investigating mitigation strategies, we aim to bring greater consistency and clarity to the vast world of LLMs.",
      "intriguing_abstract": "Large Language Models (LLMs) have emerged as powerful tools in Artificial Intelligence, showcasing remarkable linguistic mastery. However, amidst their expansive capabilities, a nuanced challenge arises: the phenomenon of hallucination. Hallucination introduces unpredictability and creativity into LLM-generated content, raising concerns about its implications. This paper seeks to illuminate the complex ramifications of hallucination in LLMs by examining its subtleties. The goal is to evaluate current efforts to mitigate hallucinations and improve the clarity of language generation. We delve into the intriguing world of AI with a focused examination of hallucinations in LLMs, exploring various strategies and methods aimed at reducing their effects and enhancing the accuracy of language generation. The analysis highlights the potential consequences for various applications and underscores the significant impact of hallucinations on LLM-generated content. Current solutions to this issue are discussed, showcasing advancements in the reliability and clarity of language generation. In conclusion, the pursuit of accuracy in LLMs faces captivating challenges posed by hallucinations. By exploring the complexities of this phenomenon and investigating mitigation strategies, we aim to bring greater consistency and clarity to the vast world of LLMs.",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/934635a82a338ddea6e0c0059663250eee259e8f.pdf",
      "citation_key": "ahmadi2024j88",
      "metadata": {
        "title": "Unravelling the Mysteries of Hallucination in Large Language Models: Strategies for Precision in Artificial Intelligence Language Generation",
        "authors": [
          "Ali Ahmadi"
        ],
        "published_date": "2024",
        "abstract": "Large Language Models (LLMs) have emerged as powerful tools in Artificial Intelligence, showcasing remarkable linguistic mastery. However, amidst their expansive capabilities, a nuanced challenge arises: the phenomenon of hallucination. Hallucination introduces unpredictability and creativity into LLM-generated content, raising concerns about its implications. This paper seeks to illuminate the complex ramifications of hallucination in LLMs by examining its subtleties. The goal is to evaluate current efforts to mitigate hallucinations and improve the clarity of language generation. We delve into the intriguing world of AI with a focused examination of hallucinations in LLMs, exploring various strategies and methods aimed at reducing their effects and enhancing the accuracy of language generation. The analysis highlights the potential consequences for various applications and underscores the significant impact of hallucinations on LLM-generated content. Current solutions to this issue are discussed, showcasing advancements in the reliability and clarity of language generation. In conclusion, the pursuit of accuracy in LLMs faces captivating challenges posed by hallucinations. By exploring the complexities of this phenomenon and investigating mitigation strategies, we aim to bring greater consistency and clarity to the vast world of LLMs.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/934635a82a338ddea6e0c0059663250eee259e8f.pdf",
        "venue": "Asian Journal of Computer Science and Technology",
        "citationCount": 3,
        "score": 3.0,
        "summary": "Large Language Models (LLMs) have emerged as powerful tools in Artificial Intelligence, showcasing remarkable linguistic mastery. However, amidst their expansive capabilities, a nuanced challenge arises: the phenomenon of hallucination. Hallucination introduces unpredictability and creativity into LLM-generated content, raising concerns about its implications. This paper seeks to illuminate the complex ramifications of hallucination in LLMs by examining its subtleties. The goal is to evaluate current efforts to mitigate hallucinations and improve the clarity of language generation. We delve into the intriguing world of AI with a focused examination of hallucinations in LLMs, exploring various strategies and methods aimed at reducing their effects and enhancing the accuracy of language generation. The analysis highlights the potential consequences for various applications and underscores the significant impact of hallucinations on LLM-generated content. Current solutions to this issue are discussed, showcasing advancements in the reliability and clarity of language generation. In conclusion, the pursuit of accuracy in LLMs faces captivating challenges posed by hallucinations. By exploring the complexities of this phenomenon and investigating mitigation strategies, we aim to bring greater consistency and clarity to the vast world of LLMs.",
        "keywords": []
      },
      "file_name": "934635a82a338ddea6e0c0059663250eee259e8f.pdf"
    },
    {
      "success": true,
      "doc_id": "ab88ae8fc37a863edfa1c9d3ec3341cc",
      "summary": ": Large language models (LLMs) have demonstrated impressive natural language understanding and generation capabilities, enabling advancements in diverse fields such as customer support, healthcare, and content creation. However, a significant challenge with LLMs is their tendency to produce factually inaccurate or nonsensical information, commonly known as hallucination. Hallucinations not only compromise the reliability of these models but can also lead to serious ethical and practical issues, particularly in high-stakes applications. This survey comprehensively reviews recent advancements in hallucination mitigation strategies for LLMs. We explore retrieval-augmented models, which enhance factual grounding by integrating external knowledge sources; human feedback mechanisms, such as reinforcement learning, which improve accuracy by aligning model responses with human evaluations; knowledge augmentation techniques that embed structured knowledge bases for enhanced consistency; and controlled generation, which restricts output to ensure alignment with factual constraints. Additionally, we examine the challenges of integrating these techniques and the limitations of current methods, including scalability, resource intensity, and dependency on quality data. Finally, we discuss future research directions to improve factual reliability in LLMs and explore hybrid solutions to create accurate and adaptable models for a wider range of real-world applications.",
      "intriguing_abstract": ": Large language models (LLMs) have demonstrated impressive natural language understanding and generation capabilities, enabling advancements in diverse fields such as customer support, healthcare, and content creation. However, a significant challenge with LLMs is their tendency to produce factually inaccurate or nonsensical information, commonly known as hallucination. Hallucinations not only compromise the reliability of these models but can also lead to serious ethical and practical issues, particularly in high-stakes applications. This survey comprehensively reviews recent advancements in hallucination mitigation strategies for LLMs. We explore retrieval-augmented models, which enhance factual grounding by integrating external knowledge sources; human feedback mechanisms, such as reinforcement learning, which improve accuracy by aligning model responses with human evaluations; knowledge augmentation techniques that embed structured knowledge bases for enhanced consistency; and controlled generation, which restricts output to ensure alignment with factual constraints. Additionally, we examine the challenges of integrating these techniques and the limitations of current methods, including scalability, resource intensity, and dependency on quality data. Finally, we discuss future research directions to improve factual reliability in LLMs and explore hybrid solutions to create accurate and adaptable models for a wider range of real-world applications.",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/2777cf13955e7683e0ec5446fdff7cdf7dd57d91.pdf",
      "citation_key": "abdelghafour2024efh",
      "metadata": {
        "title": "Hallucination Mitigation Techniques in Large Language Models",
        "authors": [
          "M. Abdelghafour",
          "Mohammed Mabrouk",
          "Zaki Taha"
        ],
        "published_date": "2024",
        "abstract": ": Large language models (LLMs) have demonstrated impressive natural language understanding and generation capabilities, enabling advancements in diverse fields such as customer support, healthcare, and content creation. However, a significant challenge with LLMs is their tendency to produce factually inaccurate or nonsensical information, commonly known as hallucination. Hallucinations not only compromise the reliability of these models but can also lead to serious ethical and practical issues, particularly in high-stakes applications. This survey comprehensively reviews recent advancements in hallucination mitigation strategies for LLMs. We explore retrieval-augmented models, which enhance factual grounding by integrating external knowledge sources; human feedback mechanisms, such as reinforcement learning, which improve accuracy by aligning model responses with human evaluations; knowledge augmentation techniques that embed structured knowledge bases for enhanced consistency; and controlled generation, which restricts output to ensure alignment with factual constraints. Additionally, we examine the challenges of integrating these techniques and the limitations of current methods, including scalability, resource intensity, and dependency on quality data. Finally, we discuss future research directions to improve factual reliability in LLMs and explore hybrid solutions to create accurate and adaptable models for a wider range of real-world applications.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/2777cf13955e7683e0ec5446fdff7cdf7dd57d91.pdf",
        "venue": "International Journal of Intelligent Computing and Information Sciences",
        "citationCount": 3,
        "score": 3.0,
        "summary": ": Large language models (LLMs) have demonstrated impressive natural language understanding and generation capabilities, enabling advancements in diverse fields such as customer support, healthcare, and content creation. However, a significant challenge with LLMs is their tendency to produce factually inaccurate or nonsensical information, commonly known as hallucination. Hallucinations not only compromise the reliability of these models but can also lead to serious ethical and practical issues, particularly in high-stakes applications. This survey comprehensively reviews recent advancements in hallucination mitigation strategies for LLMs. We explore retrieval-augmented models, which enhance factual grounding by integrating external knowledge sources; human feedback mechanisms, such as reinforcement learning, which improve accuracy by aligning model responses with human evaluations; knowledge augmentation techniques that embed structured knowledge bases for enhanced consistency; and controlled generation, which restricts output to ensure alignment with factual constraints. Additionally, we examine the challenges of integrating these techniques and the limitations of current methods, including scalability, resource intensity, and dependency on quality data. Finally, we discuss future research directions to improve factual reliability in LLMs and explore hybrid solutions to create accurate and adaptable models for a wider range of real-world applications.",
        "keywords": []
      },
      "file_name": "2777cf13955e7683e0ec5446fdff7cdf7dd57d91.pdf"
    },
    {
      "success": true,
      "doc_id": "58ee5a3207781ffc50194bc3346cafe2",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/f7a47a7de7289d6fd69e3bf226f7cdaffa670a3f.pdf",
      "citation_key": "zhou20253zv",
      "metadata": {
        "title": "HaDeMiF: Hallucination Detection and Mitigation in Large Language Models",
        "authors": [
          "Xiaoling Zhou",
          "Mingjie Zhang",
          "Zhemg Lee",
          "Wei Ye",
          "Shikun Zhang"
        ],
        "published_date": "2025",
        "abstract": "",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/f7a47a7de7289d6fd69e3bf226f7cdaffa670a3f.pdf",
        "venue": "International Conference on Learning Representations",
        "citationCount": 3,
        "score": 3.0,
        "summary": "",
        "keywords": []
      },
      "file_name": "f7a47a7de7289d6fd69e3bf226f7cdaffa670a3f.pdf"
    },
    {
      "success": true,
      "doc_id": "c506d097e816d9093f3752bed376739a",
      "summary": "Is automated hallucination detection possible? In this work, we introduce a theoretical framework to analyze the feasibility of automatically detecting hallucinations produced by large language models (LLMs). Inspired by the classical Gold-Angluin framework for language identification and its recent adaptation to language generation by Kleinberg and Mullainathan, we investigate whether an algorithm, trained on examples drawn from an unknown target language $K$ (selected from a countable collection) and given access to an LLM, can reliably determine whether the LLM's outputs are correct or constitute hallucinations. First, we establish an equivalence between hallucination detection and the classical task of language identification. We prove that any hallucination detection method can be converted into a language identification method, and conversely, algorithms solving language identification can be adapted for hallucination detection. Given the inherent difficulty of language identification, this implies that hallucination detection is fundamentally impossible for most language collections if the detector is trained using only correct examples from the target language. Second, we show that the use of expert-labeled feedback, i.e., training the detector with both positive examples (correct statements) and negative examples (explicitly labeled incorrect statements), dramatically changes this conclusion. Under this enriched training regime, automated hallucination detection becomes possible for all countable language collections. These results highlight the essential role of expert-labeled examples in training hallucination detectors and provide theoretical support for feedback-based methods, such as reinforcement learning with human feedback (RLHF), which have proven critical for reliable LLM deployment.",
      "intriguing_abstract": "Is automated hallucination detection possible? In this work, we introduce a theoretical framework to analyze the feasibility of automatically detecting hallucinations produced by large language models (LLMs). Inspired by the classical Gold-Angluin framework for language identification and its recent adaptation to language generation by Kleinberg and Mullainathan, we investigate whether an algorithm, trained on examples drawn from an unknown target language $K$ (selected from a countable collection) and given access to an LLM, can reliably determine whether the LLM's outputs are correct or constitute hallucinations. First, we establish an equivalence between hallucination detection and the classical task of language identification. We prove that any hallucination detection method can be converted into a language identification method, and conversely, algorithms solving language identification can be adapted for hallucination detection. Given the inherent difficulty of language identification, this implies that hallucination detection is fundamentally impossible for most language collections if the detector is trained using only correct examples from the target language. Second, we show that the use of expert-labeled feedback, i.e., training the detector with both positive examples (correct statements) and negative examples (explicitly labeled incorrect statements), dramatically changes this conclusion. Under this enriched training regime, automated hallucination detection becomes possible for all countable language collections. These results highlight the essential role of expert-labeled examples in training hallucination detectors and provide theoretical support for feedback-based methods, such as reinforcement learning with human feedback (RLHF), which have proven critical for reliable LLM deployment.",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/d6442ff9d10310071108f44734b00d182b6e2c28.pdf",
      "citation_key": "karbasi2025j7n",
      "metadata": {
        "title": "(Im)possibility of Automated Hallucination Detection in Large Language Models",
        "authors": [
          "Amin Karbasi",
          "Omar Montasser",
          "John Sous",
          "Grigoris Velegkas"
        ],
        "published_date": "2025",
        "abstract": "Is automated hallucination detection possible? In this work, we introduce a theoretical framework to analyze the feasibility of automatically detecting hallucinations produced by large language models (LLMs). Inspired by the classical Gold-Angluin framework for language identification and its recent adaptation to language generation by Kleinberg and Mullainathan, we investigate whether an algorithm, trained on examples drawn from an unknown target language $K$ (selected from a countable collection) and given access to an LLM, can reliably determine whether the LLM's outputs are correct or constitute hallucinations. First, we establish an equivalence between hallucination detection and the classical task of language identification. We prove that any hallucination detection method can be converted into a language identification method, and conversely, algorithms solving language identification can be adapted for hallucination detection. Given the inherent difficulty of language identification, this implies that hallucination detection is fundamentally impossible for most language collections if the detector is trained using only correct examples from the target language. Second, we show that the use of expert-labeled feedback, i.e., training the detector with both positive examples (correct statements) and negative examples (explicitly labeled incorrect statements), dramatically changes this conclusion. Under this enriched training regime, automated hallucination detection becomes possible for all countable language collections. These results highlight the essential role of expert-labeled examples in training hallucination detectors and provide theoretical support for feedback-based methods, such as reinforcement learning with human feedback (RLHF), which have proven critical for reliable LLM deployment.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/d6442ff9d10310071108f44734b00d182b6e2c28.pdf",
        "venue": "arXiv.org",
        "citationCount": 3,
        "score": 3.0,
        "summary": "Is automated hallucination detection possible? In this work, we introduce a theoretical framework to analyze the feasibility of automatically detecting hallucinations produced by large language models (LLMs). Inspired by the classical Gold-Angluin framework for language identification and its recent adaptation to language generation by Kleinberg and Mullainathan, we investigate whether an algorithm, trained on examples drawn from an unknown target language $K$ (selected from a countable collection) and given access to an LLM, can reliably determine whether the LLM's outputs are correct or constitute hallucinations. First, we establish an equivalence between hallucination detection and the classical task of language identification. We prove that any hallucination detection method can be converted into a language identification method, and conversely, algorithms solving language identification can be adapted for hallucination detection. Given the inherent difficulty of language identification, this implies that hallucination detection is fundamentally impossible for most language collections if the detector is trained using only correct examples from the target language. Second, we show that the use of expert-labeled feedback, i.e., training the detector with both positive examples (correct statements) and negative examples (explicitly labeled incorrect statements), dramatically changes this conclusion. Under this enriched training regime, automated hallucination detection becomes possible for all countable language collections. These results highlight the essential role of expert-labeled examples in training hallucination detectors and provide theoretical support for feedback-based methods, such as reinforcement learning with human feedback (RLHF), which have proven critical for reliable LLM deployment.",
        "keywords": []
      },
      "file_name": "d6442ff9d10310071108f44734b00d182b6e2c28.pdf"
    },
    {
      "success": true,
      "doc_id": "90230535e069c54cc7d360c2b445fae1",
      "summary": "",
      "intriguing_abstract": "",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/0e3d306997e4830e668f750bddc3ee28487ce59a.pdf",
      "citation_key": "mubarak2024lx6",
      "metadata": {
        "title": "Halwasa: Quantify and Analyze Hallucinations in Large Language Models: Arabic as a Case Study",
        "authors": [
          "Hamdy Mubarak",
          "Hend Suliman Al-Khalifa",
          "Khaloud Suliman Alkhalefah"
        ],
        "published_date": "2024",
        "abstract": "",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/0e3d306997e4830e668f750bddc3ee28487ce59a.pdf",
        "venue": "International Conference on Language Resources and Evaluation",
        "citationCount": 3,
        "score": 3.0,
        "summary": "",
        "keywords": []
      },
      "file_name": "0e3d306997e4830e668f750bddc3ee28487ce59a.pdf"
    },
    {
      "success": true,
      "doc_id": "b9d531f8090ac8bfe1a8a1a552563b0e",
      "summary": "Large language models (LLMs) have shown remarkable performance in various tasks but often fail to handle queries that exceed their knowledge and capabilities, leading to incorrect or fabricated responses. This paper addresses the need for LLMs to recognize and refuse infeasible tasks due to the requests surpassing their capabilities. We conceptualize four main categories of infeasible tasks for LLMs, which cover a broad spectrum of hallucination-related challenges identified in prior literature. We develop and benchmark a new dataset comprising diverse infeasible and feasible tasks to evaluate multiple LLMs'abilities to decline infeasible tasks. Furthermore, we explore the potential of increasing LLMs'refusal capabilities with fine-tuning. Our experiments validate the effectiveness of the trained models, suggesting promising directions for improving the performance of LLMs in real-world applications.",
      "intriguing_abstract": "Large language models (LLMs) have shown remarkable performance in various tasks but often fail to handle queries that exceed their knowledge and capabilities, leading to incorrect or fabricated responses. This paper addresses the need for LLMs to recognize and refuse infeasible tasks due to the requests surpassing their capabilities. We conceptualize four main categories of infeasible tasks for LLMs, which cover a broad spectrum of hallucination-related challenges identified in prior literature. We develop and benchmark a new dataset comprising diverse infeasible and feasible tasks to evaluate multiple LLMs'abilities to decline infeasible tasks. Furthermore, we explore the potential of increasing LLMs'refusal capabilities with fine-tuning. Our experiments validate the effectiveness of the trained models, suggesting promising directions for improving the performance of LLMs in real-world applications.",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/962cdae24cebb49c8870525fbf229554203aa5d2.pdf",
      "citation_key": "zhang2024sbu",
      "metadata": {
        "title": "Recognizing Limits: Investigating Infeasibility in Large Language Models",
        "authors": [
          "Wenbo Zhang",
          "Zihang Xu",
          "Hengrui Cai"
        ],
        "published_date": "2024",
        "abstract": "Large language models (LLMs) have shown remarkable performance in various tasks but often fail to handle queries that exceed their knowledge and capabilities, leading to incorrect or fabricated responses. This paper addresses the need for LLMs to recognize and refuse infeasible tasks due to the requests surpassing their capabilities. We conceptualize four main categories of infeasible tasks for LLMs, which cover a broad spectrum of hallucination-related challenges identified in prior literature. We develop and benchmark a new dataset comprising diverse infeasible and feasible tasks to evaluate multiple LLMs'abilities to decline infeasible tasks. Furthermore, we explore the potential of increasing LLMs'refusal capabilities with fine-tuning. Our experiments validate the effectiveness of the trained models, suggesting promising directions for improving the performance of LLMs in real-world applications.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/962cdae24cebb49c8870525fbf229554203aa5d2.pdf",
        "venue": "",
        "citationCount": 3,
        "score": 3.0,
        "summary": "Large language models (LLMs) have shown remarkable performance in various tasks but often fail to handle queries that exceed their knowledge and capabilities, leading to incorrect or fabricated responses. This paper addresses the need for LLMs to recognize and refuse infeasible tasks due to the requests surpassing their capabilities. We conceptualize four main categories of infeasible tasks for LLMs, which cover a broad spectrum of hallucination-related challenges identified in prior literature. We develop and benchmark a new dataset comprising diverse infeasible and feasible tasks to evaluate multiple LLMs'abilities to decline infeasible tasks. Furthermore, we explore the potential of increasing LLMs'refusal capabilities with fine-tuning. Our experiments validate the effectiveness of the trained models, suggesting promising directions for improving the performance of LLMs in real-world applications.",
        "keywords": []
      },
      "file_name": "962cdae24cebb49c8870525fbf229554203aa5d2.pdf"
    },
    {
      "success": true,
      "doc_id": "aa447ccdf7e334b393ecdf1fe65db022",
      "summary": "Background Large language models (LLMs) show promise in clinical contexts but can generate false facts (often referred to as â€œhallucinationsâ€). One subset of these errors arises from adversarial attacks, in which fabricated details embedded in prompts lead the model to produce or elaborate on the false information. We embedded fabricated content in clinical prompts to elicit adversarial hallucination attacks in multiple large language models. We quantified how often they elaborated on false details and tested whether a specialized mitigation prompt or altered temperature settings reduced errors. Methods We created 300 physician-validated simulated vignettes, each containing one fabricated detail (a laboratory test, a physical or radiological sign, or a medical condition). Each vignette was presented in short and long versionsâ€”differing only in word count but identical in medical content. We tested six LLMs under three conditions: default (standard settings), mitigating prompt (designed to reduce hallucinations), and temperature 0 (deterministic output with maximum response certainty), generating 5,400 outputs. If a model elaborated on the fabricated detail, the case was classified as a â€œhallucinationâ€. Results Hallucination rates range from 50 % to 82 % across models and prompting methods. Prompt-based mitigation lowers the overall hallucination rate (mean across all models) from 66 % to 44 % (pâ€‰<â€‰0.001). For the best-performing model, GPT-4o, rates decline from 53 % to 23 % (pâ€‰<â€‰0.001). Temperature adjustments offer no significant improvement. Short vignettes show slightly higher odds of hallucination. Conclusions LLMs are highly susceptible to adversarial hallucination attacks, frequently generating false clinical details that pose risks when used without safeguards. While prompt engineering reduces errors, it does not eliminate them.",
      "intriguing_abstract": "Background Large language models (LLMs) show promise in clinical contexts but can generate false facts (often referred to as â€œhallucinationsâ€). One subset of these errors arises from adversarial attacks, in which fabricated details embedded in prompts lead the model to produce or elaborate on the false information. We embedded fabricated content in clinical prompts to elicit adversarial hallucination attacks in multiple large language models. We quantified how often they elaborated on false details and tested whether a specialized mitigation prompt or altered temperature settings reduced errors. Methods We created 300 physician-validated simulated vignettes, each containing one fabricated detail (a laboratory test, a physical or radiological sign, or a medical condition). Each vignette was presented in short and long versionsâ€”differing only in word count but identical in medical content. We tested six LLMs under three conditions: default (standard settings), mitigating prompt (designed to reduce hallucinations), and temperature 0 (deterministic output with maximum response certainty), generating 5,400 outputs. If a model elaborated on the fabricated detail, the case was classified as a â€œhallucinationâ€. Results Hallucination rates range from 50 % to 82 % across models and prompting methods. Prompt-based mitigation lowers the overall hallucination rate (mean across all models) from 66 % to 44 % (pâ€‰<â€‰0.001). For the best-performing model, GPT-4o, rates decline from 53 % to 23 % (pâ€‰<â€‰0.001). Temperature adjustments offer no significant improvement. Short vignettes show slightly higher odds of hallucination. Conclusions LLMs are highly susceptible to adversarial hallucination attacks, frequently generating false clinical details that pose risks when used without safeguards. While prompt engineering reduces errors, it does not eliminate them.",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/1c3eee136c5fa85ad97ef62f353557f776059f3e.pdf",
      "citation_key": "omar2025cc3",
      "metadata": {
        "title": "Multi-model assurance analysis showing large language models are highly vulnerable to adversarial hallucination attacks during clinical decision support",
        "authors": [
          "Mahmud Omar",
          "Vera Sorin",
          "Jeremy D. Collins",
          "David Reich",
          "Robert Freeman",
          "Nicholas Gavin",
          "Alexander W. Charney",
          "L. Stump",
          "N. L. Bragazzi",
          "G. Nadkarni",
          "Eyal Klang"
        ],
        "published_date": "2025",
        "abstract": "Background Large language models (LLMs) show promise in clinical contexts but can generate false facts (often referred to as â€œhallucinationsâ€). One subset of these errors arises from adversarial attacks, in which fabricated details embedded in prompts lead the model to produce or elaborate on the false information. We embedded fabricated content in clinical prompts to elicit adversarial hallucination attacks in multiple large language models. We quantified how often they elaborated on false details and tested whether a specialized mitigation prompt or altered temperature settings reduced errors. Methods We created 300 physician-validated simulated vignettes, each containing one fabricated detail (a laboratory test, a physical or radiological sign, or a medical condition). Each vignette was presented in short and long versionsâ€”differing only in word count but identical in medical content. We tested six LLMs under three conditions: default (standard settings), mitigating prompt (designed to reduce hallucinations), and temperature 0 (deterministic output with maximum response certainty), generating 5,400 outputs. If a model elaborated on the fabricated detail, the case was classified as a â€œhallucinationâ€. Results Hallucination rates range from 50 % to 82 % across models and prompting methods. Prompt-based mitigation lowers the overall hallucination rate (mean across all models) from 66 % to 44 % (pâ€‰<â€‰0.001). For the best-performing model, GPT-4o, rates decline from 53 % to 23 % (pâ€‰<â€‰0.001). Temperature adjustments offer no significant improvement. Short vignettes show slightly higher odds of hallucination. Conclusions LLMs are highly susceptible to adversarial hallucination attacks, frequently generating false clinical details that pose risks when used without safeguards. While prompt engineering reduces errors, it does not eliminate them.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/1c3eee136c5fa85ad97ef62f353557f776059f3e.pdf",
        "venue": "Communications Medicine",
        "citationCount": 3,
        "score": 3.0,
        "summary": "Background Large language models (LLMs) show promise in clinical contexts but can generate false facts (often referred to as â€œhallucinationsâ€). One subset of these errors arises from adversarial attacks, in which fabricated details embedded in prompts lead the model to produce or elaborate on the false information. We embedded fabricated content in clinical prompts to elicit adversarial hallucination attacks in multiple large language models. We quantified how often they elaborated on false details and tested whether a specialized mitigation prompt or altered temperature settings reduced errors. Methods We created 300 physician-validated simulated vignettes, each containing one fabricated detail (a laboratory test, a physical or radiological sign, or a medical condition). Each vignette was presented in short and long versionsâ€”differing only in word count but identical in medical content. We tested six LLMs under three conditions: default (standard settings), mitigating prompt (designed to reduce hallucinations), and temperature 0 (deterministic output with maximum response certainty), generating 5,400 outputs. If a model elaborated on the fabricated detail, the case was classified as a â€œhallucinationâ€. Results Hallucination rates range from 50 % to 82 % across models and prompting methods. Prompt-based mitigation lowers the overall hallucination rate (mean across all models) from 66 % to 44 % (pâ€‰<â€‰0.001). For the best-performing model, GPT-4o, rates decline from 53 % to 23 % (pâ€‰<â€‰0.001). Temperature adjustments offer no significant improvement. Short vignettes show slightly higher odds of hallucination. Conclusions LLMs are highly susceptible to adversarial hallucination attacks, frequently generating false clinical details that pose risks when used without safeguards. While prompt engineering reduces errors, it does not eliminate them.",
        "keywords": []
      },
      "file_name": "1c3eee136c5fa85ad97ef62f353557f776059f3e.pdf"
    },
    {
      "success": true,
      "doc_id": "6c944c4e0e5e3f5032ac8cd52e8aed88",
      "summary": "Large Language Models (LLMs) have revolutionized Natural Language Processing (NLP) and have been widely adopted in various applications such as machine translation, chatbots, text summarization, and so on. However, the use of LLMs has raised concerns about their potential safety and security risks. In this survey, we explore the safety implications of LLMs, including ethical considerations, hallucination, and prompt injection. We also discuss current research efforts to mitigate these risks and identify areas for future research. Our survey provides a comprehensive overview of the safety concerns related to LLMs, which can help researchers and practitioners in the NLP community develop more safe and ethical applications of LLMs.",
      "intriguing_abstract": "Large Language Models (LLMs) have revolutionized Natural Language Processing (NLP) and have been widely adopted in various applications such as machine translation, chatbots, text summarization, and so on. However, the use of LLMs has raised concerns about their potential safety and security risks. In this survey, we explore the safety implications of LLMs, including ethical considerations, hallucination, and prompt injection. We also discuss current research efforts to mitigate these risks and identify areas for future research. Our survey provides a comprehensive overview of the safety concerns related to LLMs, which can help researchers and practitioners in the NLP community develop more safe and ethical applications of LLMs.",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/b29b7aaae0554e864409dfd1afadf9e1564a2616.pdf",
      "citation_key": "gao20242nu",
      "metadata": {
        "title": "A Brief Survey on Safety of Large Language Models",
        "authors": [
          "Zhengjie Gao",
          "Xuanzi Liu",
          "Yuanshuai Lan",
          "Zheng Yang"
        ],
        "published_date": "2024",
        "abstract": "Large Language Models (LLMs) have revolutionized Natural Language Processing (NLP) and have been widely adopted in various applications such as machine translation, chatbots, text summarization, and so on. However, the use of LLMs has raised concerns about their potential safety and security risks. In this survey, we explore the safety implications of LLMs, including ethical considerations, hallucination, and prompt injection. We also discuss current research efforts to mitigate these risks and identify areas for future research. Our survey provides a comprehensive overview of the safety concerns related to LLMs, which can help researchers and practitioners in the NLP community develop more safe and ethical applications of LLMs.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/b29b7aaae0554e864409dfd1afadf9e1564a2616.pdf",
        "venue": "Journal of computer & information technology",
        "citationCount": 3,
        "score": 3.0,
        "summary": "Large Language Models (LLMs) have revolutionized Natural Language Processing (NLP) and have been widely adopted in various applications such as machine translation, chatbots, text summarization, and so on. However, the use of LLMs has raised concerns about their potential safety and security risks. In this survey, we explore the safety implications of LLMs, including ethical considerations, hallucination, and prompt injection. We also discuss current research efforts to mitigate these risks and identify areas for future research. Our survey provides a comprehensive overview of the safety concerns related to LLMs, which can help researchers and practitioners in the NLP community develop more safe and ethical applications of LLMs.",
        "keywords": []
      },
      "file_name": "b29b7aaae0554e864409dfd1afadf9e1564a2616.pdf"
    },
    {
      "success": true,
      "doc_id": "61f7a77921fd6b4a2e3a1c46706dab37",
      "summary": "Large Language Models represent a significant breakthrough in Natural Language Processing research and opened a wide range of application domains. This paper demonstrates the successful integration of Large Language Models into immersive learning environments. The review highlights how this emerging technology aligns with pedagogical principles, enhancing the effectiveness of current educational systems. It also reflects recent advancements in integrating Large Language Models, including fine-tuning, hallucination reduction, fact-checking, and human evaluation of generated results.",
      "intriguing_abstract": "Large Language Models represent a significant breakthrough in Natural Language Processing research and opened a wide range of application domains. This paper demonstrates the successful integration of Large Language Models into immersive learning environments. The review highlights how this emerging technology aligns with pedagogical principles, enhancing the effectiveness of current educational systems. It also reflects recent advancements in integrating Large Language Models, including fine-tuning, hallucination reduction, fact-checking, and human evaluation of generated results.",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/6becd0d29f013a7aec01d453727cb1680c01979f.pdf",
      "citation_key": "pester20242zt",
      "metadata": {
        "title": "Conversational Agents, Virtual Worlds, and Beyond: A Review of Large Language Models Enabling Immersive Learning",
        "authors": [
          "Andreas Pester",
          "Ahmed Tammaa",
          "Christian GÃ¼tl",
          "Alexander Steinmaurer",
          "S. A. El-Seoud"
        ],
        "published_date": "2024",
        "abstract": "Large Language Models represent a significant breakthrough in Natural Language Processing research and opened a wide range of application domains. This paper demonstrates the successful integration of Large Language Models into immersive learning environments. The review highlights how this emerging technology aligns with pedagogical principles, enhancing the effectiveness of current educational systems. It also reflects recent advancements in integrating Large Language Models, including fine-tuning, hallucination reduction, fact-checking, and human evaluation of generated results.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/6becd0d29f013a7aec01d453727cb1680c01979f.pdf",
        "venue": "IEEE Global Engineering Education Conference",
        "citationCount": 3,
        "score": 3.0,
        "summary": "Large Language Models represent a significant breakthrough in Natural Language Processing research and opened a wide range of application domains. This paper demonstrates the successful integration of Large Language Models into immersive learning environments. The review highlights how this emerging technology aligns with pedagogical principles, enhancing the effectiveness of current educational systems. It also reflects recent advancements in integrating Large Language Models, including fine-tuning, hallucination reduction, fact-checking, and human evaluation of generated results.",
        "keywords": []
      },
      "file_name": "6becd0d29f013a7aec01d453727cb1680c01979f.pdf"
    },
    {
      "success": true,
      "doc_id": "2f8e62f9656cfbe62c9246a03cf452ba",
      "summary": "The black-box nature of large language models (LLMs) poses challenges in interpreting results, impacting issues such as data intellectual property protection and hallucination tracing. Training data attribution (TDA) methods are considered effective solutions to address these challenges.Most recent TDA methods rely on influence functions, assuming the model achieves minimized empirical risk. However, achieving this criterion is difficult, and sourcing accuracy can be compromised by fitting errors during model training. In this paper, we introduce a novel TDA method called Debias and Denoise Attribution (DDA), which enhances influence functions by addressing fitting errors. Specifically, the debias strategy seeks to improve the performance of influence functions by eliminating the knowledge bias present in the base model before fine-tuning, while the denoise strategy aims to reduce discrepancies in influence scores arising from varying degrees of fitting during the training process through smoothing techniques.Experimental results demonstrate that our method significantly outperforms existing approaches, achieving an averaged AUC of 91.64%. Moreover, DDA exhibits strong generality and scalability across various sources and different-scale models like LLaMA2, QWEN2, and Mistral.",
      "intriguing_abstract": "The black-box nature of large language models (LLMs) poses challenges in interpreting results, impacting issues such as data intellectual property protection and hallucination tracing. Training data attribution (TDA) methods are considered effective solutions to address these challenges.Most recent TDA methods rely on influence functions, assuming the model achieves minimized empirical risk. However, achieving this criterion is difficult, and sourcing accuracy can be compromised by fitting errors during model training. In this paper, we introduce a novel TDA method called Debias and Denoise Attribution (DDA), which enhances influence functions by addressing fitting errors. Specifically, the debias strategy seeks to improve the performance of influence functions by eliminating the knowledge bias present in the base model before fine-tuning, while the denoise strategy aims to reduce discrepancies in influence scores arising from varying degrees of fitting during the training process through smoothing techniques.Experimental results demonstrate that our method significantly outperforms existing approaches, achieving an averaged AUC of 91.64%. Moreover, DDA exhibits strong generality and scalability across various sources and different-scale models like LLaMA2, QWEN2, and Mistral.",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/dbcd51388bc622e7725782177c09cf8b5c1daf5d.pdf",
      "citation_key": "wu202415r",
      "metadata": {
        "title": "Enhancing Training Data Attribution for Large Language Models with Fitting Error Consideration",
        "authors": [
          "Kangxi Wu",
          "Liang Pang",
          "Huawei Shen",
          "Xueqi Cheng"
        ],
        "published_date": "2024",
        "abstract": "The black-box nature of large language models (LLMs) poses challenges in interpreting results, impacting issues such as data intellectual property protection and hallucination tracing. Training data attribution (TDA) methods are considered effective solutions to address these challenges.Most recent TDA methods rely on influence functions, assuming the model achieves minimized empirical risk. However, achieving this criterion is difficult, and sourcing accuracy can be compromised by fitting errors during model training. In this paper, we introduce a novel TDA method called Debias and Denoise Attribution (DDA), which enhances influence functions by addressing fitting errors. Specifically, the debias strategy seeks to improve the performance of influence functions by eliminating the knowledge bias present in the base model before fine-tuning, while the denoise strategy aims to reduce discrepancies in influence scores arising from varying degrees of fitting during the training process through smoothing techniques.Experimental results demonstrate that our method significantly outperforms existing approaches, achieving an averaged AUC of 91.64%. Moreover, DDA exhibits strong generality and scalability across various sources and different-scale models like LLaMA2, QWEN2, and Mistral.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/dbcd51388bc622e7725782177c09cf8b5c1daf5d.pdf",
        "venue": "Conference on Empirical Methods in Natural Language Processing",
        "citationCount": 3,
        "score": 3.0,
        "summary": "The black-box nature of large language models (LLMs) poses challenges in interpreting results, impacting issues such as data intellectual property protection and hallucination tracing. Training data attribution (TDA) methods are considered effective solutions to address these challenges.Most recent TDA methods rely on influence functions, assuming the model achieves minimized empirical risk. However, achieving this criterion is difficult, and sourcing accuracy can be compromised by fitting errors during model training. In this paper, we introduce a novel TDA method called Debias and Denoise Attribution (DDA), which enhances influence functions by addressing fitting errors. Specifically, the debias strategy seeks to improve the performance of influence functions by eliminating the knowledge bias present in the base model before fine-tuning, while the denoise strategy aims to reduce discrepancies in influence scores arising from varying degrees of fitting during the training process through smoothing techniques.Experimental results demonstrate that our method significantly outperforms existing approaches, achieving an averaged AUC of 91.64%. Moreover, DDA exhibits strong generality and scalability across various sources and different-scale models like LLaMA2, QWEN2, and Mistral.",
        "keywords": []
      },
      "file_name": "dbcd51388bc622e7725782177c09cf8b5c1daf5d.pdf"
    },
    {
      "success": true,
      "doc_id": "bdf5e55f4391dc0c1c6421d7ac74153b",
      "summary": "Hallucination poses a persistent challenge for multimodal large language models (MLLMs). However, existing benchmarks for evaluating hallucinations are generally static, which may overlook the potential risk of data contamination. To address this issue, we propose ODE, an openset, dynamic protocol designed to evaluate object hallucinations in MLLMs at both the existence and attribute levels. ODE employs a graph-based structure to represent real-world object concepts, their attributes, and the distributional associations between them. This structure facilitates the extraction of concept combinations based on diverse distributional criteria, generating varied samples for structured queries that evaluate hallucinations in both generative and discriminative tasks. Through the generation of new samples, dynamic concept combinations, and varied distribution frequencies, ODE mitigates the risk of data contamination and broadens the scope of evaluation. This protocol is applicable to both general and specialized scenarios, including those with limited data. Experimental results demonstrate the effectiveness of our protocol, revealing that MLLMs exhibit higher hallucination rates when evaluated with ODE-generated samples, which indicates potential data contamination. Furthermore, these generated samples aid in analyzing hallucination patterns and fine-tuning models, offering an effective approach to mitigating hallucinations in MLLMs. Our code are available at https://github.com/Iridescent-y/ODE.",
      "intriguing_abstract": "Hallucination poses a persistent challenge for multimodal large language models (MLLMs). However, existing benchmarks for evaluating hallucinations are generally static, which may overlook the potential risk of data contamination. To address this issue, we propose ODE, an openset, dynamic protocol designed to evaluate object hallucinations in MLLMs at both the existence and attribute levels. ODE employs a graph-based structure to represent real-world object concepts, their attributes, and the distributional associations between them. This structure facilitates the extraction of concept combinations based on diverse distributional criteria, generating varied samples for structured queries that evaluate hallucinations in both generative and discriminative tasks. Through the generation of new samples, dynamic concept combinations, and varied distribution frequencies, ODE mitigates the risk of data contamination and broadens the scope of evaluation. This protocol is applicable to both general and specialized scenarios, including those with limited data. Experimental results demonstrate the effectiveness of our protocol, revealing that MLLMs exhibit higher hallucination rates when evaluated with ODE-generated samples, which indicates potential data contamination. Furthermore, these generated samples aid in analyzing hallucination patterns and fine-tuning models, offering an effective approach to mitigating hallucinations in MLLMs. Our code are available at https://github.com/Iridescent-y/ODE.",
      "keywords": [],
      "file_path": "paper_data/Hallucination_in_Large_Language_Models/ac7cc880f897626ee2d2d5a5c40180d551f4e0f8.pdf",
      "citation_key": "tu2024v40",
      "metadata": {
        "title": "ODE: Open-Set Evaluation of Hallucinations in Multimodal Large Language Models",
        "authors": [
          "Yahan Tu",
          "Rui Hu",
          "Jitao Sang"
        ],
        "published_date": "2024",
        "abstract": "Hallucination poses a persistent challenge for multimodal large language models (MLLMs). However, existing benchmarks for evaluating hallucinations are generally static, which may overlook the potential risk of data contamination. To address this issue, we propose ODE, an openset, dynamic protocol designed to evaluate object hallucinations in MLLMs at both the existence and attribute levels. ODE employs a graph-based structure to represent real-world object concepts, their attributes, and the distributional associations between them. This structure facilitates the extraction of concept combinations based on diverse distributional criteria, generating varied samples for structured queries that evaluate hallucinations in both generative and discriminative tasks. Through the generation of new samples, dynamic concept combinations, and varied distribution frequencies, ODE mitigates the risk of data contamination and broadens the scope of evaluation. This protocol is applicable to both general and specialized scenarios, including those with limited data. Experimental results demonstrate the effectiveness of our protocol, revealing that MLLMs exhibit higher hallucination rates when evaluated with ODE-generated samples, which indicates potential data contamination. Furthermore, these generated samples aid in analyzing hallucination patterns and fine-tuning models, offering an effective approach to mitigating hallucinations in MLLMs. Our code are available at https://github.com/Iridescent-y/ODE.",
        "file_path": "paper_data/Hallucination_in_Large_Language_Models/info/ac7cc880f897626ee2d2d5a5c40180d551f4e0f8.pdf",
        "venue": "Computer Vision and Pattern Recognition",
        "citationCount": 3,
        "score": 3.0,
        "summary": "Hallucination poses a persistent challenge for multimodal large language models (MLLMs). However, existing benchmarks for evaluating hallucinations are generally static, which may overlook the potential risk of data contamination. To address this issue, we propose ODE, an openset, dynamic protocol designed to evaluate object hallucinations in MLLMs at both the existence and attribute levels. ODE employs a graph-based structure to represent real-world object concepts, their attributes, and the distributional associations between them. This structure facilitates the extraction of concept combinations based on diverse distributional criteria, generating varied samples for structured queries that evaluate hallucinations in both generative and discriminative tasks. Through the generation of new samples, dynamic concept combinations, and varied distribution frequencies, ODE mitigates the risk of data contamination and broadens the scope of evaluation. This protocol is applicable to both general and specialized scenarios, including those with limited data. Experimental results demonstrate the effectiveness of our protocol, revealing that MLLMs exhibit higher hallucination rates when evaluated with ODE-generated samples, which indicates potential data contamination. Furthermore, these generated samples aid in analyzing hallucination patterns and fine-tuning models, offering an effective approach to mitigating hallucinations in MLLMs. Our code are available at https://github.com/Iridescent-y/ODE.",
        "keywords": []
      },
      "file_name": "ac7cc880f897626ee2d2d5a5c40180d551f4e0f8.pdf"
    }
  ]
}