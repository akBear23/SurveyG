\subsection{The Rise of Large Language Models and the Hallucination Challenge}

The advent of Large Language Models (LLMs) has heralded a transformative era in artificial intelligence, demonstrating unprecedented capabilities in natural language understanding and generation \cite{ahmadi2024j88}. These models, characterized by their massive scale and emergent abilities, have revolutionized diverse domains, from automating complex cognitive tasks like code generation and scientific discovery to enhancing human-computer interaction through sophisticated dialogue systems. Their capacity to produce highly coherent, contextually relevant, and often creative text has positioned them as pivotal technologies poised to redefine numerous industries and research paradigms.

However, alongside these remarkable advancements, a pervasive and critical limitation, widely termed 'hallucination,' significantly impedes the reliability and trustworthiness of LLMs \cite{ahmadi2024j88, rawte2023ao8}. Hallucination refers to the generation of content that is factually incorrect, nonsensical, or ungrounded in the provided input or real-world knowledge. Early investigations into neural text generation, particularly in abstractive summarization, highlighted this issue, revealing that models frequently produced information not present in the source document, with a substantial portion being factually erroneous \cite{maynez2020h3q}. This foundational work introduced a critical distinction between 'intrinsic' hallucinations (misrepresenting source information) and 'extrinsic' hallucinations (adding ungrounded information), underscoring the challenge of ensuring faithfulness and factual accuracy. The problem's prevalence has only intensified with the increasing scale and generality of LLMs, making it a central concern for their safe and effective deployment \cite{rawte2023ao8}.

The implications of hallucination are profound and far-reaching, posing substantial risks to user trust, the credibility of AI systems, and the safety of their applications across various domains. In high-stakes environments such as healthcare, finance, or legal services, hallucinated information can lead to severe consequences, including misdiagnoses, flawed financial advice, or fabricated legal precedents \cite{li2025qzg, ahmadi2024j88}. Beyond factual errors, hallucinations can manifest as logical inconsistencies or ungrounded reasoning, eroding confidence in an LLM's ability to perform complex tasks reliably. This challenge extends beyond mere inconvenience, directly impacting the deployability of LLMs and necessitating robust mechanisms to ensure their outputs are verifiable and trustworthy.

The research trajectory has evolved from merely characterizing hallucination to seeking a deeper understanding of its origins, both empirical and theoretical. Initial observations revealed that even during pre-training, smaller language models could learn to reduce perplexity on grammatical sequences that *contained* hallucinations, suggesting that the propensity for generating plausible but incorrect text is embedded early in the learning process \cite{xia20224cl}. As models scale, some of these issues are mitigated, but the fundamental challenge persists. More recently, the understanding of hallucination has been elevated to a theoretical plane, with formal mathematical definitions being proposed \cite{li2025qzg}. Groundbreaking work has even posited that hallucination is not merely a transient engineering problem but an inherent and *inevitable* limitation of any computable LLM, stemming from fundamental principles of learning theory and computability \cite{xu2024n76}. This theoretical perspective fundamentally reshapes the problem, suggesting that while mitigation is crucial, complete eradication might be impossible, thus shifting the focus towards robust management and transparent communication of uncertainty rather than absolute elimination. This pervasive issue is also not confined to text, manifesting uniquely across multimodal contexts, including Large Vision-Language Models (LVLMs) and Large Audio-Language Models (LALMs), further complicating the landscape of trustworthy AI \cite{li2025qzg}.

In light of these challenges, the urgent need for robust research into the diverse causes, comprehensive evaluation, and effective mitigation strategies for hallucination is paramount. Addressing this phenomenon, which spans factual inaccuracies, nonsensical outputs, and ungrounded content across text and increasingly multimodal domains, is not merely an incremental improvement but a central hurdle to achieving truly trustworthy, verifiable, and safely deployable AI systems. This review aims to comprehensively explore the multifaceted problem of hallucination, from its foundational definitions and mechanistic causes to advanced evaluation methodologies and cutting-edge mitigation techniques, ultimately guiding the development of more reliable and accountable LLMs.