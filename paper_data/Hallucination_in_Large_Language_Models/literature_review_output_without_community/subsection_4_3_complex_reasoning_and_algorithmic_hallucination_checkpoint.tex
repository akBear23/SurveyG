\subsection{Complex Reasoning and Algorithmic Hallucination}

Evaluating Large Language Models (LLMs) on tasks that demand genuine algorithmic reasoning and complex problem-solving represents a critical frontier, moving beyond superficial knowledge retrieval or pattern matching to assess the integrity of the entire solution process \cite{zhang2023k1j}. While early work on hallucination often focused on factual inconsistencies in generative tasks \cite{maynez2020h3q}, the increasing capabilities of LLMs necessitate benchmarks that probe their ability to perform multi-step logical deductions, execute precise algorithms, and maintain coherence throughout complex reasoning paths. This shift reveals a deeper form of hallucination, rooted in a lack of true algorithmic understanding rather than mere factual error.

A significant area of research focuses on assessing LLMs' logical consistency in structured data and fact-checking scenarios. \cite{oh2024xa3} introduced ERBench, a benchmark that leverages relational databases and their integrity constraints to generate complex, automatically verifiable questions. Crucially, ERBench not only assesses the final answer but also rigorously verifies the LLM's generated rationales, exposing inconsistencies in the underlying thought process even when the final answer might appear correct. This highlights that LLMs often struggle with maintaining logical coherence across multiple deductive steps. Extending this, \cite{li2024osp} proposed Drowzee, a framework employing metamorphic testing to evaluate the logical consistency and robustness of LLM rationales in fact-checking. By generating semantically equivalent but syntactically varied inputs based on logic reasoning rules, Drowzee reveals how minor perturbations can expose brittle reasoning and lead to contradictory outputs, underscoring the fragility of LLM logical understanding in complex scenarios. Further, \cite{ghosh2024tj5} developed a framework to quantitatively assess LLMs' logical consistency in fact-checking, specifically for propositional logic queries derived from Knowledge Graphs. Their work defines measures for consistency across primitive logical operators, complex DNF/CNF facts, and logical rules, demonstrating that LLMs exhibit significant logical inconsistency on such complex queries, even when provided with authoritative context. These studies collectively emphasize the challenge of ensuring logical soundness and verifiable reasoning paths in LLMs.

Beyond structured logical deduction, a more demanding test for LLMs lies in their ability to perform pure algorithmic execution. \cite{tang2024a1j} developed GraphArena, a benchmark specifically tailored to evaluate LLMs on real-world graph computational problems, including NP-complete tasks, which require precise, step-by-step algorithmic execution rather than heuristic pattern matching. Their findings reveal alarmingly high hallucination rates, where LLMs frequently generate plausible but incorrect intermediate steps or entirely fabricated solutions, demonstrating a profound limitation in their capacity to understand and accurately execute algorithmic instructions. Similarly, in the domain of mathematical reasoning, \cite{liu2025juo} introduced a structured self-consistency framework to enhance reliability by enforcing consistency across intermediate steps and final outputs in tasks like theorem proving, symbolic manipulation, and numerical computation. Their results indicate that while self-consistency can improve accuracy, LLMs remain susceptible to hallucinations in these precise mathematical and algorithmic contexts, highlighting the difficulty in achieving robust, step-by-step correctness.

The integrity of the reasoning process itself is further probed by examining the impact of reasoning order. \cite{xie20247zk} introduced a benchmark demonstrating that the order in which LLMs generate answers and their corresponding reasoning significantly impacts consistency. They found that LLMs often fabricate answers and then retrospectively generate justifications, exposing a fundamental flaw where the reasoning path is constructed to fit a pre-determined (potentially incorrect) answer, rather than the answer being a product of sound reasoning. This "reasoning order hallucination" underscores the challenge of ensuring that LLMs genuinely understand and follow logical steps.

The problem of complex reasoning hallucination also manifests acutely in high-stakes, domain-specific applications. \cite{umapathi2023puv} introduced Med-HALT, a Medical Domain Hallucination Test, which includes "Reasoning Hallucination Tests" (RHTs) such as False Confidence Tests (FCT), None of the Above (NOTA) Tests, and Fake Questions Tests (FQT). These RHTs assess an LLM's ability to reason about complex medical problems, generate logically coherent and factually accurate output, and identify invalid queries without creating fake information. Their findings indicate that even state-of-the-art LLMs perform poorly on these reasoning-based medical tasks, struggling with complex logical inference and exhibiting a tendency to hallucinate with undue certainty, highlighting the severe implications of such failures in critical domains.

In conclusion, current LLMs exhibit significant limitations in tasks requiring genuine algorithmic reasoning and complex problem-solving. Benchmarks focusing on rationale verification, logical consistency in structured data, pure algorithmic execution, mathematical reasoning, and the integrity of the reasoning process consistently reveal high hallucination rates. These evaluations demonstrate that LLMs struggle with the precise execution, deep understanding, and robust logical coherence necessary for these tasks. The emphasis has decisively shifted from merely assessing final answers to rigorously evaluating the integrity and consistency of the entire solution process. Future research must focus on developing models that can perform verifiable, step-by-step algorithmic execution, moving beyond superficial pattern matching to achieve true algorithmic intelligence and logical soundness.