\subsection{Summary of Key Advancements}

The persistent challenge of hallucination in Large Language Models (LLMs) has driven a rapid and profound evolution in research, transforming the field from initial problem identification to a sophisticated, multi-faceted scientific endeavor. This trajectory highlights a collective effort towards building more reliable, transparent, and contextually grounded AI systems. Crucially, a profound intellectual shift has occurred, moving beyond empirical observation to a formal theoretical grounding, demonstrating the inherent inevitability of hallucination in computable LLMs \cite{xu2024n76, li2025qzg}. This fundamental insight re-frames the research goal from complete eradication to robust management and mitigation, acknowledging an innate limitation.

Early foundational work established the critical need to address unfaithful content. \cite{maynez2020h3q} provided a seminal large-scale human evaluation, categorizing hallucinations in abstractive summarization and demonstrating the utility of textual entailment for faithfulness evaluation. Recognizing the limitations of static knowledge, \cite{trivedi2022qsf} introduced Interleaving Retrieval with Chain-of-Thought (IRCoT), an early few-shot, training-free method that dynamically interleaved reasoning steps with knowledge retrieval to ground LLMs in external facts. This marked an early shift towards adaptive, real-time grounding during generation. Subsequent advancements in Retrieval-Augmented Generation (RAG) further refined this paradigm, with frameworks like Rowen \cite{ding20244yr} intelligently deciding *when* to retrieve based on cross-language/cross-model consistency, optimizing efficiency and mitigating both internal and external hallucinations. Complementing this, the strategic integration of structured Knowledge Graphs (KGs) through "Graph-guided retrieval" and "Graph-guided generation" has significantly enhanced trustworthiness and reduced hallucinations in open-ended QA by providing verifiable, structured knowledge \cite{sui20242u1}. However, it is critical to acknowledge that RAG systems, while powerful, are not infallible; their efficacy is inherently tied to the quality of retrieved information, making them vulnerable to noisy or biased sources and susceptible to 'confabulations' arising from limitations within RAG components themselves \cite{zhang20252at}.

As LLMs became more capable and complex, the focus expanded to developing sophisticated evaluation benchmarks that could rigorously assess trustworthiness beyond simple accuracy. \cite{gao2023ht7} introduced ALCE, the first reproducible benchmark for evaluating LLMs' ability to generate text with verifiable citations, complete with automatic metrics for fluency, correctness, and citation quality. Pushing evaluation further, \cite{oh2024xa3} developed ERBench, a novel benchmark leveraging relational databases to automatically verify not just final answers but also the *rationales* provided by LLMs, addressing a critical need for transparency in reasoning. Complementing this, \cite{ghosh2024tj5} developed new logical fact-checking datasets and quantitative measures to assess LLM consistency on complex propositional logic queries, expanding the definition of "hallucination" to include logical inconsistencies. For scenarios where external knowledge or human labels are scarce, methods like MetaQA \cite{yang20251dw} emerged as zero-resource, self-contained hallucination detection techniques leveraging metamorphic relations and prompt mutation. The theoretical underpinnings of such detection methods were further explored by \cite{karbasi2025j7n}, who established an equivalence between hallucination detection and language identification, proving that automated detection is fundamentally impossible for most language collections without expert-labeled feedback, thereby providing theoretical support for methods like RLHF. Furthermore, to foster more responsible AI behavior, \cite{tjandra2024umq} introduced the Accuracy-Engagement Distance (AED) metric to evaluate models capable of appropriately *abstaining* from answers when uncertain, utilizing semantic entropy for label-free uncertainty estimation.

Concurrently, mitigation strategies evolved from general retrieval to highly adaptive, proactive, and mechanistically targeted interventions. On a more mechanistic front, \cite{zhang2024qq9} identified "knowledge overshadowing" as a novel root cause of "amalgamated hallucinations" arising from data imbalance, proposing an inference-time, training-free self-contrastive decoding method for targeted mitigation. This represents a deeper understanding of internal model dynamics. Similarly, \cite{chen2024j0g} presented ICT, a training-free, forward-pass intervention method that targets specific attention heads to enhance focus on crucial information, mitigating the dominance of language priors. The development of self-correction mechanisms, as exemplified by \cite{tjandra2024umq}'s work on abstention, empowers LLMs to reflect on their uncertainty and proactively avoid generating ungrounded content, moving towards more self-aware and reliable systems.

A major conceptual and methodological shift has been the dedicated focus on multimodal hallucination, particularly in Large Vision-Language Models (LVLMs). These models introduce unique complexities due to the "modality gap" and the integration of diverse information sources \cite{lan20240yz}. \cite{kaul2024ta7} addressed the inadequacy of prior benchmarks by introducing THRONE, the first accurate object-based hallucination benchmark for *free-form* generations of LVLMs, utilizing LM-based semantic judgment. Building on the "snowballing" effect observed in LLMs, \cite{zhong2024mfi} investigated and mitigated "multimodal hallucination snowballing" in LVLMs, proposing Residual Visual Decoding (RVD) to emphasize direct visual evidence and prevent error propagation in conversational settings. To address the diverse nature of LVLM queries, \cite{chang2024u3t} introduced Dentist, a unified mitigation framework that classifies query types (perception vs. reasoning) and applies tailored, iterative validation strategies. Furthermore, intrinsic model interventions emerged, with \cite{wang2024vym} identifying "Visual Encoding Distortion" as a critical source of LVLM hallucinations and proposing Visual-Layer Fusion Contrastive Decoding (VaLiD) to correct it by fusing features from early visual layers. However, early multimodal evaluation benchmarks, such as object-based approaches, were found to be highly susceptible to prompt bias, leading to inaccurate assessments of real-world hallucination \cite{wang2023zop}, necessitating more robust, LLM-based evaluation frameworks like HaELM.

Collectively, these advancements underscore a profound intellectual trajectory: from merely identifying and correcting errors to a deep, multi-faceted understanding of hallucination's origins, its manifestation across modalities, and the development of sophisticated, adaptive, and intrinsically aware mechanisms for prevention and evaluation. This comprehensive progress marks a significant stride in the pursuit of building truly trustworthy, transparent, and contextually grounded AI systems, acknowledging both their immense potential and their inherent, theoretically proven limitations. The field is actively navigating the tension between external grounding, internal correction, and the acceptance of these fundamental limits, guiding research towards robust management rather than the elusive goal of complete eradication.