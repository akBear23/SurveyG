\subsection{Scope and Motivation of the Review}

The pervasive issue of hallucination in Large Language Models (LLMs), where models generate fluent yet factually incorrect or unfaithful information, poses a significant threat to their reliability and the development of trustworthy artificial intelligence. This literature review aims to comprehensively delineate the current research landscape surrounding LLM hallucination, from its foundational understanding and characterization to advanced detection, benchmarking, and mitigation techniques, including the emerging challenges in multimodal contexts. Our primary motivation is to synthesize the state-of-the-art, identify critical gaps in current methodologies, and highlight promising future directions to ensure LLMs are dependable, transparent, and safe for widespread deployment. The urgent need for robust solutions is underscored by concerns about misinformation, accountability, and the ethical implications of deploying unreliable AI systems in sensitive domains.

Initial research efforts laid the groundwork for understanding the nature of hallucination. \cite{maynez2020h3q} provided foundational definitions, distinguishing between intrinsic (misrepresenting source content) and extrinsic (adding uninferable information) hallucinations within abstractive summarization. Building upon this, comprehensive surveys such as \cite{zhang2023k1j} and \cite{ye2023yom} extended these taxonomies and offered mechanistic analyses specifically tailored for the unique challenges of LLMs, consolidating the fragmented understanding of the phenomenon. To move beyond qualitative descriptions, \cite{rawte2023ao8} introduced a fine-grained categorization of hallucination types and proposed the Hallucination Vulnerability Index (HVI) for quantitative assessment, addressing the need for standardized metrics. Further deepening this understanding, \cite{du2023qu7} developed an association analysis framework for unbiased quantification and attribution of hallucination to specific model capability deficiencies, moving beyond simple error rates. A pivotal theoretical contribution by \cite{xu2024n76} formally proved the inevitability of hallucination for all computable LLMs, fundamentally shifting the research paradigm from elimination to management. This theoretical grounding is complemented by investigations into the root causes of declining information quality, as explored by \cite{rejeleene2024okw}, reinforcing the necessity for a deeper, systematic understanding of LLM failures.

The evolution of foundational understanding has directly propelled the development of sophisticated benchmarking and detection methodologies. Early work by \cite{liu2021mo6} pioneered a token-level, reference-free hallucination detection benchmark (HADES), addressing the limitations of reference-dependent and coarse-grained evaluation methods. Recognizing the scalability challenges of manual annotation, \cite{cao2023ecl} introduced AutoHall for automated hallucination dataset generation, enabling more efficient and model-specific evaluation. The scope of evaluation expanded with benchmarks like HaluEval 2.0 by \cite{li2024qrj}, offering a comprehensive empirical study of factuality hallucination across diverse domains and LLM lifecycle stages. To enhance trustworthiness and verifiability, \cite{gao2023ht7} introduced ALCE, a reproducible benchmark for LLMs to generate text with verifiable citations, a critical step towards accountable AI. Pushing the boundaries of evaluation, \cite{oh2024xa3} developed ERBench, which uniquely leverages relational databases to construct automatically verifiable questions and, crucially, rationales, allowing for deeper scrutiny of LLM reasoning processes beyond just final answers. Addressing language-specific challenges, \cite{liang20236sh} presented UHGEval for benchmarking Chinese LLMs via unconstrained generation, providing a more realistic assessment of spontaneous hallucinations. The field also began tackling multimodal and dialogue-specific challenges: \cite{vu202337s} introduced FreshLLMs to evaluate LLMs on dynamic, real-world knowledge using search engine augmentation, while \cite{chen2024c4k} developed DiaHalu, the first dedicated benchmark for dialogue-level hallucination, capturing the complexities of multi-turn interactions. Most recently, \cite{yang20251dw} proposed MetaQA, a self-contained detection method leveraging metamorphic relations, offering a robust solution for black-box LLMs without reliance on external resources or internal model access. These advancements collectively provide increasingly granular, automated, and context-aware tools for identifying hallucinations.

Concurrently, a diverse array of mitigation and correction strategies has emerged to address the identified challenges. Early efforts focused on grounding dialogue systems in external knowledge, as demonstrated by \cite{dziri2021bw9}'s Neural Path Hunter, which used Knowledge Graphs (KGs) to correct hallucinated entities. Addressing data quality issues, \cite{adams202289x} proposed learning to revise noisy reference summaries, thereby improving training data for faithful summarization. The advent of Retrieval-Augmented Generation (RAG) and self-correction paradigms brought significant advancements: \cite{li2023v3v} introduced Chain-of-Knowledge, dynamically adapting over heterogeneous sources for improved factual grounding, while \cite{dhuliawala2023rqn} developed Chain-of-Verification, enabling LLMs to systematically self-critique and verify factual claims internally. Further integrating KGs, \cite{wen2023t6v}'s MindMap allowed LLMs to comprehend graphical inputs and reason over a "mind map," enhancing transparency and reducing hallucination. More sophisticated RAG techniques emerged to address efficiency and precision: \cite{lv2024k5x} proposed Coarse-to-Fine Highlighting to reduce knowledge hallucination in long contexts, and \cite{su2024gnz} introduced DRAD, a dynamic RAG framework that triggers retrieval based on real-time, entity-level hallucination detection. The landscape of self-correction was comprehensively surveyed by \cite{pan2023mwu} and \cite{pan2024y3a}, categorizing diverse strategies, while \cite{ji2023vhv} demonstrated an iterative self-reflection methodology for mitigating hallucinations in high-stakes medical generative QA. Finally, novel decoding strategies like \cite{zhang202396g}'s Induce-then-Contrast Decoding offered a unique approach by leveraging "induced hallucinations" to guide models towards factuality without retraining. These diverse techniques, further consolidated by comprehensive surveys on misinformation prevention and detection like \cite{liu2024gxh}, collectively aim to build more robust and reliable LLM systems.

In conclusion, the research trajectory on LLM hallucination has rapidly evolved from initial characterization to sophisticated, multimodal, and LLM-native approaches. This review highlights a clear progression towards deeper mechanistic understanding, including the theoretical inevitability of hallucination, and the development of increasingly automated, fine-grained, and verifiable evaluation benchmarks. Concurrently, mitigation strategies have advanced from post-hoc corrections to proactive prevention and dynamic knowledge integration, often leveraging LLM internal states and emphasizing safety-critical considerations. Despite significant progress, challenges remain in achieving absolute factual consistency across all modalities and complex reasoning tasks. This comprehensive analysis serves to guide future research towards developing more robust, transparent, and ultimately trustworthy LLM systems, which is paramount for their safe and beneficial integration into society.