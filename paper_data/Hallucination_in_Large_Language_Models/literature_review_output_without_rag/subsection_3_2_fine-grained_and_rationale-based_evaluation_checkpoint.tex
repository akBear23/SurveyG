\subsection{Fine-Grained and Rationale-Based Evaluation}

Traditional evaluation metrics for Large Language Models (LLMs) often fall short by only assessing the correctness of a final answer, failing to diagnose the underlying reasoning failures that lead to hallucinations. This limitation has spurred the development of advanced evaluation methodologies focused on providing a deeper, more granular understanding of LLM outputs, enabling token-level or sentence-level error analysis, and critically, verifying the LLM's reasoning process or "rationales."

Early work by \cite{maynez2020h3q} highlighted the inadequacy of surface-level metrics like ROUGE for assessing faithfulness and factuality in abstractive summarization, introducing the crucial distinction between intrinsic and extrinsic hallucinations and advocating for semantically-aware evaluation. Building on this, \cite{liu2021mo6} pioneered the first token-level, reference-free hallucination detection benchmark, HADES, which allows for precise identification of hallucinated content within free-form text, moving beyond coarse sentence or document-level assessments. Further refining this fine-grained analysis, \cite{rawte2023ao8} proposed a comprehensive taxonomy of hallucination, categorizing it by degree, orientation, and specific types, alongside the Hallucination Vulnerability Index (HVI) for quantifiable and nuanced assessment of LLM susceptibility. Similarly, \cite{li2024qrj} developed HaluEval 2.0, an LLM-based detection framework that extracts and judges factual statements at a fine-grained level, considering interrelations between statements. For Chinese LLMs, \cite{liang20236sh} introduced UHGEval, a benchmark for unconstrained generation, and proposed `kwPrec` (keyword precision) to measure factual relevance at a granular level. Extending the scope of fine-grained evaluation, \cite{rejeleene2024okw} offered a mathematical formulation for Information Quality (IQ) based on consistency, relevance, and accuracy, providing a structured framework for evaluating LLM outputs beyond simple correctness.

A significant advancement in evaluation methodologies is the focus on verifying the LLM's reasoning process, often by leveraging structured data or internal self-reflection mechanisms. \cite{oh2024xa3} introduced ERBench, a groundbreaking benchmark that utilizes relational databases and their integrity constraints (Functional Dependencies and Foreign Key Constraints) to automatically generate complex questions and, crucially, verify both the LLM's answer and its underlying rationale by checking for critical keywords. This approach provides an objective and scalable method for assessing logical steps. Complementing this, \cite{gao2023ht7} developed ALCE, a benchmark that enables LLMs to generate text with explicit citations, which serve as verifiable rationales. They introduced NLI-based metrics to automatically evaluate the recall and precision of these citations, ensuring that generated statements are adequately supported by evidence.

Beyond external structured data, methods have emerged that prompt LLMs to self-evaluate their own reasoning. \cite{dhuliawala2023rqn} proposed Chain-of-Verification (CoVe), a multi-step process where an LLM generates verification questions about its own draft response and then answers them independently to reduce factual hallucinations. This internal deliberation process provides a form of self-generated rationale verification. Similarly, \cite{wen2023t6v} introduced MindMap, which prompts LLMs to reason over knowledge graphs and explicitly generate a "mind map" (decision tree) of their reasoning pathways, thereby enhancing transparency and verifiability of the logical steps. The concept of self-reflection is further explored by \cite{ji2023vhv}, who developed an iterative self-reflection methodology for medical generative question-answering, allowing LLMs to generate, score, and refine their own background knowledge and answers until factual consistency is achieved. In a similar vein, \cite{li2023v3v}'s Chain-of-Knowledge framework emphasizes progressive rationale correction by dynamically adapting knowledge from heterogeneous sources, ensuring that errors are rectified sequentially to prevent propagation.

The drive for scalable and diagnostic evaluation has also led to automated benchmarking and advanced detection techniques. \cite{cao2023ecl} presented AutoHall, a pipeline for automatically generating model-specific hallucination datasets from existing fact-checking data, and a zero-resource detection method based on LLM self-contradiction, which implicitly checks internal consistency. \cite{yang20251dw} introduced MetaQA, a self-contained hallucination detection approach that uses metamorphic relations and prompt mutation to expose factual inconsistencies by generating and verifying diverse response mutations. For Retrieval-Augmented Generation (RAG) systems, \cite{chen2023h04} developed RGB, a diagnostic benchmark that evaluates LLMs across four critical RAG abilities (Noise Robustness, Negative Rejection, Information Integration, and Counterfactual Robustness), pinpointing specific reasoning failures in how models interact with external knowledge. Furthermore, \cite{chen2024c4k} addressed the complexities of conversational AI with DiaHalu, a dialogue-level hallucination evaluation benchmark that provides a comprehensive taxonomy for assessing both factuality and faithfulness across multi-turn interactions. Lastly, \cite{su2024gnz} proposed Dynamic Retrieval Augmentation based on hallucination Detection (DRAD), which utilizes real-time, entity-level uncertainty to detect potential hallucinations and trigger targeted retrieval, offering a fine-grained and adaptive approach to error mitigation.

These fine-grained and rationale-based evaluations are crucial for diagnosing complex reasoning failures and improving model transparency and verifiability, thereby fostering greater confidence in AI-generated content. However, challenges remain, including the inherent difficulty in precisely defining "correct" rationales for all tasks, the scalability of human annotation even with automated assistance, and the persistent "black-box" nature of many LLMs. The theoretical inevitability of hallucination, as posited by \cite{xu2024n76}, further underscores that the focus must shift from complete elimination to robust management through sophisticated detection and transparent reasoning. Future research will likely concentrate on developing more robust automatic rationale verification mechanisms, integrating diverse and dynamic knowledge sources for richer rationales, and fostering LLMs that can not only provide answers but also explain *why* they arrived at a particular conclusion.