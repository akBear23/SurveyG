\subsection{Safety-Critical Applications and Guardrails}

In high-stakes domains such as medical diagnosis, legal advice, and financial decision-making, the consequences of Large Language Model (LLM) hallucinations can be catastrophic, leading to "never events" where errors are unacceptable and ethical considerations are paramount. Moving beyond general factual accuracy, this subsection explores the development of highly dependable, accountable, and credible AI systems through application-specific mechanisms and robust guardrails designed for absolute error prevention and explicit uncertainty communication.

A foundational step towards building trustworthy AI systems in critical contexts involves enabling verifiable outputs. \cite{gao2023ht7} addresses this by introducing ALCE, a reproducible benchmark and automatic evaluation framework for LLMs to generate text with verifiable citations. This work highlights the necessity of grounding LLM outputs in retrievable evidence, providing a crucial mechanism for human oversight and accountability by allowing users to trace information back to its source, which is indispensable in auditing critical decisions. However, merely citing sources does not guarantee the fidelity of the reasoning process itself.

To advance beyond simple factual verification, the focus shifts to ensuring the integrity of the model's rationale. \cite{oh2024xa3} introduces ERBench, a benchmark designed for automatically verifying the *rationales* generated by LLMs using relational databases. This represents a significant step towards building accountable systems, as understanding *how* an LLM arrives at a conclusion is as critical as the conclusion itself in safety-critical environments, allowing for deeper scrutiny of the decision-making process and identification of flawed logic. Building on the need for proactive accuracy, \cite{ding20244yr} proposes Retrieve Only When It Needs (Rowen), an adaptive retrieval augmentation framework that dynamically decides when to retrieve external information based on cross-language and cross-model consistency. This proactive approach mitigates both internal and external hallucinations by ensuring that information is only retrieved and integrated when it genuinely enhances factual accuracy, thereby reducing the risk of propagating misinformation in sensitive applications.

Crucially, in scenarios where absolute certainty is unattainable, AI systems must explicitly communicate their uncertainty to prevent confident but erroneous outputs. \cite{tjandra2024umq} addresses this by proposing a method for fine-tuning LLMs to appropriately abstain from answering when uncertain, utilizing semantic entropy as a label-free mechanism. This capability is vital for safety-critical applications, allowing models to "say I don't know" rather than hallucinating, thereby preventing potentially harmful advice or diagnoses and fostering trust through transparency regarding the model's confidence levels. Complementing these proactive and uncertainty-aware strategies, robust, real-time detection mechanisms are essential. \cite{yang20251dw} introduces a zero-resource, self-contained method for hallucination detection in LLMs using metamorphic relations. This technique offers a powerful, model-agnostic way to identify inconsistencies and potential hallucinations without relying on external knowledge bases or internal model access, making it highly suitable for integration into real-time safety checks within diverse and resource-constrained critical systems.

Ultimately, these individual advancements culminate in the development of application-specific "semantic guardrails." \cite{hakim2024d4u} underscores the critical need for such guardrails in medical safety-critical settings, specifically in pharmacovigilance, to prevent "never event" errors. This work advocates for tailored mechanisms that go beyond general safety filters, embedding domain-specific knowledge and constraints directly into the AI's operational framework to ensure outputs are not only factually correct but also clinically and ethically sound. These semantic guardrails represent the synthesis of verifiable outputs, rationale integrity, proactive hallucination prevention, and explicit uncertainty communication into a holistic, domain-aware safety layer.

Despite these significant advancements, challenges remain in integrating these disparate mechanisms into a unified, formally verifiable, and auditable framework for safety-critical AI. Future research must focus on developing standardized protocols for designing, implementing, and evaluating semantic guardrails across diverse high-stakes domains, ensuring their robustness against adversarial attacks and their ability to adapt to evolving knowledge, while maintaining transparency and accountability in their decision-making processes.