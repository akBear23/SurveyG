\subsection{Historical Characterization and Evolving Taxonomies}

The phenomenon of hallucination, where large language models (LLMs) generate plausible but factually incorrect or unfaithful content, has emerged as a critical challenge to their reliability and trustworthiness. The field's understanding of this problem has evolved significantly, moving from initial empirical observations to sophisticated, multi-dimensional taxonomies and, more recently, a theoretical understanding of its fundamental limits. This intellectual lineage has been crucial in systematically analyzing the problem space and guiding initial detection efforts across diverse natural language generation tasks.

Early pioneering empirical studies first systematically identified and distinguished different types of errors. A seminal work in abstractive summarization by \cite{maynez2020h3q} conducted a large-scale human evaluation to categorize hallucinations into two primary types: \textit{intrinsic hallucinations}, which misrepresent information present in the source document, and \textit{extrinsic hallucinations}, which introduce information not inferable from the source. This study highlighted the inadequacy of traditional metrics like ROUGE and BERTScore for assessing faithfulness and advocated for semantically-aware evaluation, laying the groundwork for a more nuanced understanding of generative errors.

Building upon this foundational distinction, researchers began to develop more comprehensive taxonomies to classify hallucinations by their source, type, and severity, extending beyond summarization to general LLM outputs. \cite{du2023qu7} proposed a capability-based taxonomy, attributing hallucinations to deficiencies in commonsense memorization, relational reasoning, and instruction following, and introduced an association analysis framework to quantify and attribute these errors. This marked a shift towards understanding the underlying cognitive-like causes rather than just surface-level phenomena. Further refining the problem space, \cite{zhang2023k1j} provided an LLM-centric survey that formalized hallucination into \textit{input-conflicting}, \textit{context-conflicting}, and \textit{fact-conflicting} types, emphasizing the unique challenges posed by LLMs' scale and versatility. Complementing this, \cite{ye2023yom} offered a detailed taxonomy categorized by various text generation tasks and provided mechanistic analyses, linking hallucination origins to issues in data collection, knowledge gaps, and the optimization process.

The granularity of hallucination characterization continued to increase. \cite{rawte2023ao8} introduced a fine-grained framework profiling hallucination by its degree (mild, moderate, alarming), orientation (Factual Mirage, Silver Lining, each with intrinsic/extrinsic sub-categories), and six specific types (e.g., Acronym Ambiguity, Numeric Nuisance). They also proposed the Hallucination Vulnerability Index (HVI) for quantitative assessment, providing a standardized metric for comparing LLM susceptibility. Similarly, \cite{li2024qrj} presented a detailed taxonomy for \textit{factuality hallucination}, encompassing categories like Entity-error, Relation-error, Incompleteness, Outdatedness, Overclaim, and Unverifiability, and developed an LLM-based detection framework to operationalize these distinctions. The evolution of these taxonomies has been crucial for systematically analyzing and structuring the problem space, moving from general observations to precise, actionable classifications.

The expansion of LLM capabilities into multi-modal domains necessitated new characterizations. \cite{li2023249} conducted the first systematic empirical study of "object hallucination" in Large Vision-Language Models (LVLMs), where models describe objects inconsistent with or absent from images. They proposed the Polling-based Object Probing Evaluation (POPE) as a more stable evaluation method and crucially identified object frequency and co-occurrence in training data as key drivers of this multi-modal hallucination. This demonstrated the need for domain-specific taxonomies and evaluation methods. More recently, as LLMs engage in complex multi-turn interactions, \cite{chen2024c4k} introduced \textit{DiaHalu}, a benchmark for dialogue-level hallucination, proposing a taxonomy with five subtypes: Non-factual, Incoherence (input-conflicting, context-conflicting, self-conflicting), Irrelevance, Overreliance, and Reasoning Error. This highlights the ongoing need to adapt and expand hallucination taxonomies to new generation paradigms and interaction complexities.

These evolving characterizations and taxonomies have directly guided initial detection efforts. Early detection methods, such as those in \cite{maynez2020h3q}, relied on human evaluation informed by the intrinsic/extrinsic distinction. To enable more scalable and fine-grained detection, \cite{liu2021mo6} introduced HADES, the first token-level, reference-free hallucination detection benchmark for free-form text generation, which allowed for pinpointing errors at a granular level. The need for black-box detection, applicable to proprietary models, led to innovations like \textit{SelfCheckGPT} by \cite{manakul20236ex}, which leverages the consistency of stochastically sampled outputs to identify non-factual statements. Furthermore, the drive for verifiable outputs led \cite{gao2023ht7} to develop \textit{ALCE}, a benchmark for LLMs generating text with citations, incorporating Natural Language Inference (NLI)-based metrics for citation quality, directly addressing verifiability. For retrieval-augmented generation (RAG) systems, \cite{chen2023h04} developed the \textit{RGB} benchmark, which diagnoses specific RAG capabilities like noise robustness and negative rejection, reflecting a deeper understanding of how external information can lead to hallucination. More sophisticated detection mechanisms, such as \cite{oh2024xa3}'s \textit{ERBench}, leverage relational databases to automatically verify not just answers but also the LLM's rationale, demonstrating a move towards transparent and verifiable reasoning. Finally, \cite{yang20251dw} introduced \textit{MetaQA}, a self-contained detection approach using metamorphic relations, addressing the limitations of external resources and internal model access.

Ultimately, this extensive characterization has culminated in a theoretical understanding of hallucination's fundamental nature. \cite{xu2024n76} provided a groundbreaking theoretical proof, using a diagonalization argument, that hallucination is an inherent and inevitable limitation for all computable LLMs, regardless of their architecture or training. This profound insight shifts the paradigm from attempting complete elimination to focusing on robust management, detection, and mitigation. The continuous refinement of hallucination definitions, taxonomies, and the development of targeted evaluation methods remain critical for navigating this inherent limitation and fostering the development of more trustworthy and reliable LLM applications.