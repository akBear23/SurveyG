\subsection*{Summary of Key Developments}

The intellectual journey in addressing hallucination within Large Language Models (LLMs) and their multimodal extensions has undergone a significant transformation, evolving from initial characterization and empirical detection to sophisticated mitigation strategies and, crucially, a theoretical understanding of its inherent nature. This progression underscores a maturation of the field's approach to AI dependability, shifting from treating hallucination as a solvable bug to acknowledging its inevitability and focusing on robust management for trustworthy artificial intelligence across all modalities.

Early research laid the groundwork by defining and categorizing hallucinations in text-based LLMs. \cite{maynez2020h3q} pioneered a systematic human evaluation, distinguishing between intrinsic (misrepresenting source content) and extrinsic (adding uninferable information) hallucinations in abstractive summarization, highlighting the inadequacy of traditional metrics. Building on this foundational understanding, the field developed methods for detection. \cite{manakul20236ex} introduced \texttt{SelfCheckGPT}, a zero-resource, black-box method that leverages the consistency of stochastically sampled LLM outputs to identify non-factual statements. Concurrently, comprehensive surveys like \cite{zhang2023k1j} and \cite{ye2023yom} consolidated taxonomies and identified root causes, while \cite{rawte2023ao8} proposed a fine-grained categorization and a Hallucination Vulnerability Index (\texttt{HVI}) for quantification. \cite{du2023qu7} further refined quantification and attribution through association analysis, linking hallucination to specific model capability deficiencies. This empirical understanding was profoundly reshaped by \cite{xu2024n76}, which provided a theoretical proof, using a diagonalization argument, that hallucination is an inherent and inevitable limitation for all computable LLMs, regardless of their architecture or training. This seminal work shifted the paradigm from eradication to robust management, a perspective echoed by \cite{rejeleene2024okw} in their investigation of information quality.

The challenge of robust evaluation spurred the development of specialized benchmarks. \cite{liu2021mo6} introduced HADES, a token-level, reference-free hallucination detection benchmark for free-form text generation, addressing the need for fine-grained signals. To overcome the labor-intensive nature of manual annotation, \cite{cao2023ecl} proposed \texttt{AutoHall} for automated hallucination dataset generation. For Chinese LLMs, \cite{liang20236sh} developed \texttt{UHGEval}, benchmarking hallucination via unconstrained generation to better reflect real-world usage. More comprehensive empirical studies, such as \cite{li2024qrj}'s HaluEval 2.0, provided multi-domain datasets and LLM-based detection frameworks. \cite{oh2024xa3} introduced \texttt{ERBench}, leveraging entity-relationship databases for automatically verifiable rationales, while \cite{vu202337s} tackled dynamic knowledge with \texttt{FreshLLMs}, a benchmark for rapidly changing information. The increasing complexity of LLM interactions led to \cite{chen2024c4k}'s \texttt{DiaHalu}, the first dialogue-level hallucination evaluation benchmark, addressing multi-turn conversational challenges.

Following foundational understanding and robust evaluation, research focused on diverse mitigation strategies for text-based LLMs. A prominent direction involved grounding LLMs in external, verifiable knowledge. \cite{yao20229uz} introduced \texttt{ReAct}, a seminal paradigm that interleaves verbal reasoning with task-specific actions (e.g., API calls) to ground LLM responses in external environments. This was further refined by \cite{trivedi2022qsf}'s \texttt{IRCoT}, which dynamically uses intermediate Chain-of-Thought steps as queries for iterative knowledge retrieval, significantly reducing factual errors. To enhance trustworthiness, \cite{gao2023ht7} developed \texttt{ALCE}, a benchmark and evaluation framework for enabling LLMs to generate text with verifiable citations. However, \cite{chen2023h04}'s \texttt{RGB} benchmark critically evaluated Retrieval-Augmented Generation (RAG), revealing that despite its promise, RAG still faces significant challenges in noise robustness, negative rejection, and information integration.

Beyond RAG, other internal and external mitigation strategies emerged. \cite{dhuliawala2023rqn} proposed Chain-of-Verification (\texttt{CoVe}), enabling LLMs to self-deliberate and correct their own responses without external tools. Knowledge Graphs (KGs) were integrated for structured reasoning, as seen in \cite{wen2023t6v}'s \texttt{MindMap}, which prompts LLMs to comprehend graphical inputs and reason over a "mind map." \cite{adams202289x} addressed noisy training data by proposing \texttt{ReDRESS}, a method to revise references for faithful summarization. For dynamic, real-time correction, \cite{su2024gnz} introduced \texttt{DRAD}, a framework that synchronizes retrieval augmentation with real-time hallucination detection based on entity-level uncertainty. \cite{lv2024k5x}'s \texttt{COFT} (Coarse-to-Fine Highlighting) aimed to reduce hallucination by intelligently highlighting key texts within long retrieved contexts. A novel approach by \cite{zhang202396g} introduced Induce-then-Contrast Decoding (\texttt{ICD}), which leverages "induced hallucinations" as a penalty term during decoding to enhance factuality. The integration of KGs for trustworthiness in open-ended QA was further explored by \cite{sui20242u1}'s \texttt{OKGQA}, while \cite{dziri2021bw9}'s Neural Path Hunter (\texttt{NPH}) focused on reducing hallucination in dialogue systems via path grounding. \cite{li2023v3v}'s Chain-of-Knowledge (\texttt{CoK}) grounded LLMs via dynamic knowledge adapting over heterogeneous sources. These diverse mitigation techniques were comprehensively surveyed by \cite{tonmoy20244e4}, \cite{liu2024gxh}, \cite{pan2023mwu}, and \cite{pan2024y3a}, providing structured overviews of the rapidly evolving landscape.

The research frontier significantly expanded into multimodal AI, particularly Large Vision-Language Models (LVLMs) and Multimodal Large Language Models (MLLMs), where unique challenges arise from inconsistencies between generated text and visual input. \cite{li2023249} conducted the first systematic empirical study of "object hallucination" in LVLMs, proposing \texttt{POPE} as a stable evaluation method and identifying object frequency and co-occurrence in training data as key drivers. This led to a proliferation of specialized multimodal benchmarks. \cite{wu2024bxt} introduced \texttt{R-Bench} to evaluate relationship hallucinations, while \cite{sungbin2024r2g}'s \texttt{AVHBench} addressed cross-modal hallucinations in audio-visual LLMs. Domain-specific evaluations emerged, such as \cite{chen2024hfe}'s Med-HallMark for medical hallucinations. \cite{zheng20246fk}'s \texttt{Reefknot} provided a comprehensive benchmark for relation hallucination, and \cite{yan2024ux8} even meta-evaluated the quality of existing benchmarks with their \texttt{HQM} framework. Further advancements included \cite{guan2023z15}'s \texttt{Hallusionbench} for entangled language hallucination and visual illusion, \cite{zhang2025pex}'s \texttt{CCHall} for joint cross-lingual and cross-modal hallucinations, \cite{chen2024lc5}'s \texttt{UNIHD} for unified detection, \cite{jiang2024792}'s \texttt{Hal-Eval} for complex event hallucinations, \cite{wang2024rta}'s \texttt{VideoHallucer} for temporal hallucinations in video, \cite{chen2024vy7}'s \texttt{ROPE} for multi-object hallucination, and \cite{kaul2024ta7}'s \texttt{THRONE} for free-form generations. Diagnostic insights were also gained from adversarial attacks, such as \cite{wang2025jen}'s "attention sink" attack, which exposes vulnerabilities in MLLMs.

Multimodal mitigation strategies also diversified. \cite{liu2023882} introduced \texttt{LRV-Instruction}, a novel dataset with *negative instructions* to explicitly train LMMs to avoid hallucination, alongside \texttt{GAVIE} for GPT4-assisted evaluation. \cite{dai20229aa} proposed \texttt{ObjMLM} loss to mitigate object hallucination during Vision-Language Pre-training. \cite{wang2023ubf}'s \texttt{ReCaption} fine-tuned LVLMs with rewritten captions to reduce fine-grained hallucinations. Leveraging AI feedback, \cite{xiao2024hv1} developed \texttt{HSA-DPO} for severity-aware hallucination mitigation using fine-grained AI feedback. \cite{wu2024n00}'s \texttt{NoiseBoost} alleviated hallucination by injecting noise perturbation into visual tokens to balance attention. For more efficient self-improvement, \cite{deng202405j} proposed a "judge-free" approach for DPO. Inference-time interventions became prominent: \cite{chang2024u3t}'s "Dentist" framework adaptively applied visual verification or Chain-of-Thought based on query type. \cite{yin2025s2b}'s \texttt{ClearSight} (Visual Amplification Fusion) enhanced visual signals in MLLMs, while \cite{zou2024dp7}'s \texttt{MemVR} (Memory-space Visual Retracing) re-injected visual tokens into intermediate layers. \cite{zhou2024lvp}'s \texttt{CAUSAL MM} mitigated modality prior-induced hallucinations via deciphering attention causality. \cite{kim2024ozf}'s Counterfactual Inception prompted models to self-generate and avoid counterfactual keywords. Post-hoc correction was addressed by \cite{yin2023hx3}'s Woodpecker, which uses expert models for visual fact-checking. \cite{park20247cm}'s \texttt{ConVis} used Text-to-Image models for hallucination visualization during contrastive decoding, and \cite{qu2024pqc}'s \texttt{MVP} employed multi-view multi-path reasoning. The critical issue of "multimodal hallucination snowballing" in LVLMs was investigated and mitigated by \cite{zhong2024mfi} using Residual Visual Decoding. The comprehensive survey by \cite{liu2024sn3} further summarized the landscape of LVLM hallucination.

In conclusion, the research on AI hallucination has traversed a rich intellectual landscape. It began with defining and categorizing the problem in text-based LLMs, progressed through the development of increasingly sophisticated detection and mitigation techniques, and culminated in a theoretical understanding of hallucination's inherent nature. The expansion into multimodal AI has introduced new complexities, necessitating tailored evaluation and mitigation strategies that address the intricate interplay between modalities. The field's trajectory from viewing hallucination as a simple bug to acknowledging its fundamental inevitability marks a significant shift towards robust management and the pursuit of truly trustworthy artificial intelligence across all modalities. Future research must continue to bridge the gap between theoretical limits and practical dependability, focusing on adaptive, context-aware, and transparent hallucination management strategies.