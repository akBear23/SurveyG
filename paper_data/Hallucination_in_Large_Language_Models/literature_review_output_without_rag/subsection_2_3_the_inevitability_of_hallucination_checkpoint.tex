\subsection{The Inevitability of Hallucination}

The pervasive issue of hallucination in Large Language Models (LLMs), where models generate plausible but factually incorrect or nonsensical information, has long been viewed as a critical engineering challenge to be overcome. Initial research focused on empirically characterizing and detecting these errors, with the implicit goal of eventual eradication. For instance, \cite{maynez2020h3q} provided a foundational human evaluation, systematically categorizing hallucinations in abstractive summarization into intrinsic (misrepresenting source content) and extrinsic (adding uninferable information), highlighting the inadequacy of traditional metrics for assessing factual consistency. Building on the need for robust detection, \cite{manakul20236ex} introduced \textit{SelfCheckGPT}, a zero-resource, black-box method that leverages the consistency of stochastically sampled LLM outputs to identify non-factual statements, demonstrating that internal model uncertainty could be a signal for hallucination.

However, a pivotal shift in understanding has emerged, moving beyond an engineering problem towards recognizing hallucination as a fundamental, inherent limitation of LLMs. This paradigm shift is most profoundly articulated by \cite{xu2024n76}, which presents a theoretical proof demonstrating that hallucination is an innate and unavoidable characteristic for any computable LLM. By formally defining LLMs as total computable functions and hallucination as inconsistencies with a computable ground truth, \cite{xu2024n76} employs a diagonalization argument, akin to Cantor's theorem, to show that no computable LLM can perfectly learn all computable functions. Consequently, any such model will inevitably hallucinate on some, and indeed infinitely many, inputs, regardless of architectural advancements or training data improvements. This groundbreaking insight suggests that the complete elimination of hallucination is mathematically impossible, setting a theoretical ceiling on LLM dependability.

This theoretical inevitability fundamentally re-frames the research agenda. Instead of striving for eradication, the focus has pivoted towards robust detection, effective mitigation, and responsible deployment strategies. Advanced detection methods continue to be crucial, evolving beyond earlier approaches. For example, \cite{yang20251dw} proposed \textit{MetaQA}, a self-contained, zero-resource hallucination detection method that leverages metamorphic relations and prompt mutation to expose factual inconsistencies, outperforming \textit{SelfCheckGPT} by generating diverse response mutations that more effectively reveal underlying inaccuracies.

Concurrently, mitigation efforts are now understood as strategies to \textit{reduce} the frequency and impact of hallucinations, rather than eliminate them entirely. Approaches that ground LLMs in external, verifiable knowledge or enhance their internal reasoning capabilities have become paramount. \cite{yao20229uz} introduced \textit{ReAct}, a seminal paradigm that interleaves verbal reasoning ("thoughts") with task-specific actions (e.g., API calls) to ground LLM responses in external environments, thereby reducing hallucination. \cite{trivedi2022qsf} further refined this by proposing \textit{IRCoT}, which dynamically uses intermediate Chain-of-Thought steps as queries for iterative knowledge retrieval, significantly reducing factual errors in multi-step question answering. To enhance verifiability, \cite{gao2023ht7} developed \textit{ALCE}, a benchmark and evaluation framework for enabling LLMs to generate text with verifiable citations, directly addressing trustworthiness. Furthermore, internal self-correction mechanisms, such as \textit{Chain-of-Verification (CoVe)} by \cite{dhuliawala2023rqn}, allow LLMs to systematically plan and execute verification questions to fact-check their own claims, significantly reducing hallucinations through independent self-deliberation.

However, even these sophisticated mitigation strategies face inherent limitations, reinforcing the notion of inevitability. \cite{chen2023h04} critically evaluated Retrieval-Augmented Generation (RAG) through the \textit{RGB} benchmark, revealing that current LLMs still struggle significantly with noise robustness, negative rejection (failing to abstain when no relevant information is available), and counterfactual robustness (prioritizing incorrect retrieved information over internal knowledge). This empirical evidence underscores that while mitigation is vital, it does not lead to complete eradication. The challenge extends to multi-modal models as well, where \cite{li2023249} systematically studied "object hallucination" in Large Vision-Language Models (LVLMs) and proposed \textit{POPE} for more stable evaluation, while \cite{liu2023882} demonstrated mitigation through robust instruction tuning with negative examples. These works show that the problem of hallucination is pervasive across modalities, further cementing its fundamental nature.

In conclusion, the theoretical proof of hallucination's inevitability marks a profound shift in the AI research landscape. It necessitates a paradigm where LLMs are viewed not as infallible oracles, but as powerful tools with inherent limitations. The research agenda is now firmly directed towards developing robust detection mechanisms, continuously refining mitigation strategies to minimize errors, and implementing responsible deployment frameworks that explicitly account for and communicate the unavoidable presence of hallucination. Future work must focus on building trustworthy systems that can effectively manage, rather than eliminate, this innate characteristic of computable LLMs, fostering greater transparency and user awareness regarding AI dependability and credibility.