\subsection{Training-Based Approaches and Preference Optimization}

Training-based approaches aim to embed hallucination resistance directly into Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) during their learning phases, fostering an intrinsic capability to avoid factual inaccuracies. These proactive strategies reduce reliance on post-hoc corrections by building robustness from the ground up, typically through robust instruction tuning or sophisticated preference optimization techniques.

Initial efforts focused on modifying pre-training objectives to enhance visual grounding. \cite{dai20229aa} provided a systematic investigation into object hallucination in Vision-Language Pre-training (VLP) models, revealing that optimizing for standard metrics could paradoxically increase hallucination. To counter this, they proposed ObjMLM (Object-Masked Language Modeling), a novel pre-training objective that improves token-level image-text alignment and controlled generation, thereby reducing object hallucination by up to 17.4\%. Building on the idea of fine-tuning, \cite{wang2023ubf} addressed fine-grained object hallucination (incorrect attributes or behaviors) by introducing \textit{ReCaption}. This framework leverages a two-stage prompting strategy with ChatGPT to generate diverse, high-quality rewritten captions for training images, which are then used to fine-tune LVLMs, effectively mitigating these granular errors.

Beyond specific fine-tuning data, generalizable training techniques have also been explored. \cite{wu2024n00} introduced NoiseBoost, a simple yet effective method that injects Gaussian noise into projected visual tokens during Supervised Fine-tuning (SFT), Reinforcement Learning (RL), or Semi-Supervised Learning (SSL). This perturbation forces the MLLM to distribute its attention more evenly between visual and linguistic tokens, reducing over-reliance on language priors and improving hallucination accuracy by 8.1\% in human evaluations of dense captions.

More advanced training-based strategies involve preference optimization, which leverages AI-generated or human-annotated feedback to align model behavior with desired factual accuracy. \cite{xiao2024hv1} proposed a comprehensive framework for detecting and mitigating hallucination in Large Vision Language Models (LVLMs) via fine-grained AI feedback. Their approach includes a "detect-then-rewrite" pipeline to construct large-scale, fine-grained preference datasets and introduces Hallucination Severity-Aware Direct Preference Optimization (HSA-DPO). This novel DPO variant incorporates hallucination severity into the objective, prioritizing the mitigation of critical errors and reducing hallucination rates by 36.1\% on the AMBER benchmark. While effective, such methods can be computationally expensive due to their reliance on large models for feedback and verification.

To address the efficiency and potential bias concerns of MLLM-as-judge approaches, \cite{deng202405j} developed an efficient "model-level judge-free" self-improvement framework for MLLMs. This framework generates controllable negative samples by blending conditional and unconditional decoding paths, and then uses a lightweight, objective verifier (a CLIP model) to invert preference labels based on average sentence-level CLIP scores. This refined preference data is subsequently used for Direct Preference Optimization (DPO), offering a more scalable and robust alternative to expensive MLLM-based reward models.

In conclusion, training-based approaches have evolved from modifying foundational pre-training objectives to sophisticated instruction tuning with augmented data and advanced preference optimization techniques. While significant progress has been made in embedding hallucination resistance intrinsically, challenges remain in scaling high-quality, unbiased preference data generation, particularly for complex multimodal scenarios. Future research will likely focus on developing more efficient and robust "judge-free" mechanisms and exploring novel architectural designs that inherently balance modality priors to further instill hallucination avoidance as a core capability.