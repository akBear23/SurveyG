\subsection{Decoding-Time Strategies and Logit Manipulation}

Hallucinations in large language models (LLMs) and multimodal large language models (MLLMs) pose significant challenges to their trustworthiness and reliability. While retraining models can be effective, it is often computationally expensive and time-consuming. Consequently, a growing body of research focuses on efficient, flexible, and training-free strategies that intervene during the token generation (decoding) process to prevent hallucinations by directly influencing the probability distribution of generated tokens or steering the model's internal states. These decoding-time interventions leverage internal uncertainty or external signals to guide LLMs towards more factual and coherent outputs on-the-fly.

A prominent category of decoding-time strategies is \textit{contrastive decoding}, which penalizes tokens inconsistent with a reference or a subtly altered input. While effective in reducing reliance on language priors, traditional contrastive decoding methods often suffer from increased inference latency and potential degradation of content quality \cite{yin2025s2b}. Addressing these limitations, \cite{park20247cm} introduced \textit{ConVis}, a novel contrastive decoding method that leverages a Text-to-Image (T2I) model to visualize potential hallucinations from an initial caption. By comparing logit distributions from the original image and the T2I-reconstructed image, ConVis amplifies and penalizes tokens corresponding to visualized hallucinations, offering a training-free approach to enhance visual grounding.

Building upon the insights and limitations of contrastive decoding, several works have explored more direct \textit{logit-based steering} or manipulation of internal model states. \cite{yin2025s2b} proposed \textit{Visual Amplification Fusion (VAF)} as part of their ClearSight framework, a plug-and-play technique that directly enhances attention to visual signals within the MLLM's middle layers during inference. Unlike contrastive decoding, VAF avoids additional forward passes, thereby preserving content quality and inference speed while mitigating object hallucinations by addressing insufficient attention to visual information. Similarly, \cite{zou2024dp7} introduced \textit{Memory-space Visual Retracing (MemVR)}, which dynamically re-injects visual tokens as supplementary evidence into the MLLM's Feed Forward Networks (FFNs) at intermediate layers, triggered by high token uncertainty. This method directly enhances hidden states, offering superior efficiency and performance compared to prior attention intervention and contrastive decoding methods.

Further refining internal state manipulation, \cite{zhou2024lvp} presented \textit{CAUSAL MM}, a causal inference framework that mitigates modality prior-induced hallucinations by deciphering attention causality. It applies back-door adjustment and counterfactual reasoning to visual and language attention mechanisms during decoding, ensuring outputs are more aligned with multimodal inputs by causally balancing modality priors. Extending this idea of targeted intervention, \cite{chen2024j0g} developed \textit{ICT (Image-Object Cross-Level Trusted Intervention)}, a training-free method that calculates an "intervention direction" to shift the model's focus towards different levels of visual information (image-level and object-level) by manipulating specific attention heads during the forward pass. This approach enhances visual grounding without indiscriminately eliminating beneficial language priors, a common drawback of some contrastive methods, and introduces no additional inference latency.

Beyond direct logit or attention manipulation, other inference-time strategies leverage internal model uncertainty or external signals for adaptive steering. \cite{qu2024pqc} introduced \textit{MVP (Multi-View Multi-Path Reasoning)}, a training-free and tool-free framework that maximizes the innate capabilities of existing LVLMs. MVP enhances image comprehension through multi-view information seeking (generating captions from different perspectives) and employs a multi-path, certainty-driven reasoning mechanism that selects the answer with the highest aggregated certainty score across multiple decoding paths and information views. This approach effectively mitigates hallucinations by leveraging the correlation between low token certainty and hallucination occurrence.

Another class of decoding-time strategies involves external tool-augmented or iterative refinement frameworks. \cite{yin2023hx3} proposed \textit{Woodpecker}, a training-free, five-stage post-remedy pipeline that corrects hallucinations by extracting key concepts, formulating questions, validating visual knowledge with expert models (e.g., Grounding DINO, BLIP-2), and then using an LLM to generate a corrected response with explicit bounding box evidence. While effective, Woodpecker's reliance on multiple external models can introduce dependencies and computational overhead. To address the diversity of hallucination types and the "one-size-fits-all" limitation of earlier methods, \cite{chang2024u3t} developed "Dentist," a unified framework that first classifies the query type (perception or reasoning) and then applies a tailored mitigation strategy within an iterative validation loop. For perception queries, it uses visual verification with sub-questions, while for reasoning queries, it employs Chain-of-Thought (CoT) prompting, iteratively refining answers until semantic convergence. Finally, \cite{kim2024ozf} introduced \textit{Counterfactual Inception}, a training-free method that prompts LMMs to self-generate "counterfactual keywords" (filtered by a CLIP-based Plausibility Verification Process) that intentionally deviate from visual content. The model is then instructed to avoid these keywords, thereby engaging in counterfactual thinking to mitigate hallucinations. This approach steers the model's output by shaping its internal reasoning context through prompt engineering, without direct logit manipulation.

In conclusion, decoding-time strategies offer a flexible and efficient paradigm for mitigating hallucinations in LLMs and MLLMs without costly retraining. The field has evolved from initial contrastive decoding methods to more nuanced logit and internal state manipulation techniques that directly enhance visual grounding or balance modality priors. Furthermore, uncertainty-driven reasoning and adaptive, tool-augmented frameworks provide robust post-hoc correction and iterative refinement. A key challenge remains in developing methods that are truly "tool-free" and solely leverage the model's internal capabilities, while also ensuring that these interventions do not inadvertently compromise other desirable generative qualities like fluency or creativity. Future research will likely focus on combining these diverse strategies, exploring more dynamic and context-aware interventions, and further understanding the causal mechanisms of hallucination during generation.