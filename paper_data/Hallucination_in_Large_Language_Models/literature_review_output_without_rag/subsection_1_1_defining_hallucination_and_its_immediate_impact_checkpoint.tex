\subsection{Defining Hallucination and Its Immediate Impact}

Hallucination in Large Language Models (LLMs) represents a critical challenge to their dependability and credibility, manifesting as the generation of factually inaccurate, nonsensical, or unfaithful information. This phenomenon fundamentally undermines user trust and poses significant risks across various applications, necessitating a clear conceptual foundation and common terminology for robust solutions and accountable AI deployment.

Early research into generative models began to characterize these inconsistencies. For instance, \cite{maynez2020h3q} provided a foundational definition in abstractive summarization, categorizing hallucinations into two primary forms: \textit{intrinsic hallucinations}, which misrepresent information present in the source document, and \textit{extrinsic hallucinations}, which introduce information not directly inferable from the input. This work highlighted that the majority of extrinsic hallucinations were erroneous, emphasizing the need for semantic-aware evaluation beyond traditional metrics. Building on this, \cite{dziri2021bw9} further explored hallucination in dialogue systems, identifying the injection of erroneous entities (a form of extrinsic hallucination) as a prevalent issue, underscoring the challenge of maintaining factual consistency with external knowledge graphs. The difficulty of detecting such errors in free-form generation, where no explicit reference exists, was addressed by \cite{liu2021mo6}, which introduced the HADES benchmark for token-level, reference-free hallucination detection, enabling finer-grained analysis of unfaithful content.

As LLMs scaled and became more versatile, the understanding and categorization of hallucination evolved to capture their unique complexities. \cite{zhang2023k1j} offered a comprehensive LLM-centric taxonomy, expanding on earlier definitions by classifying hallucinations into \textit{input-conflicting}, \textit{context-conflicting}, and \textit{fact-conflicting} types. This framework specifically highlighted fact-conflicting errors as a major concern due to the absence of an authoritative knowledge source within the model itself. Complementing this, \cite{ye2023yom} provided a detailed review, attributing hallucination to multifaceted issues spanning data collection, knowledge gaps, and optimization processes, and offering a broad taxonomy across various generation tasks. Further refining this granularity, \cite{rawte2023ao8} introduced a fine-grained profiling framework encompassing "Factual Mirage" and "Silver Lining" orientations, along with specific types like "Numeric Nuisance" and "Generated Golem," and proposed the Hallucination Vulnerability Index (HVI) for quantitative assessment. This extensive categorization underscores the diverse manifestations of hallucination. \cite{du2023qu7} then refined the definition for LLMs as "generations of the model that violate human instructions," and proposed an association analysis to attribute hallucinations to specific model capability deficiencies, such as commonsense memorization or relational reasoning, moving beyond mere descriptive categorization to mechanistic understanding.

The immediate impact of hallucination on LLM dependability and credibility is profound and far-reaching. The theoretical work by \cite{xu2024n76} delivered a sobering insight, proving that hallucination is an \textit{inherent and inevitable limitation} for all computable LLMs, regardless of their architecture or training. This fundamental theoretical ceiling shifts the research paradigm from complete eradication to robust detection, effective mitigation, and responsible deployment strategies. The practical implications are evident across various applications. For instance, in information retrieval, the generation of fact-conflicting errors directly impacts trustworthiness, leading to efforts like those by \cite{gao2023ht7} to enable LLMs to generate text with verifiable citations, directly addressing the need for accountability. However, even advanced mitigation techniques like Retrieval-Augmented Generation (RAG) face significant challenges, as demonstrated by \cite{chen2023h04}, whose RGB benchmark revealed that LLMs still struggle with noise robustness, negative rejection (refusing to answer when no relevant information is available), and counterfactual scenarios, often prioritizing incorrect retrieved information over their own correct internal knowledge. This highlights the persistent difficulty in grounding LLM responses reliably.

Moreover, the problem extends beyond text-only models. \cite{li2023249} identified "object hallucination" in Large Vision-Language Models (LVLMs), where models describe nonexistent objects in images, demonstrating that inconsistencies can arise across modalities and are driven by statistical biases in training data. This broadens the scope of the hallucination challenge significantly. A comprehensive empirical study by \cite{li2024qrj} on factuality hallucination across the entire LLM lifecycle (pre-training, supervised fine-tuning, RLHF, and inference) further confirmed the pervasive nature of these errors and identified their diverse sources, emphasizing the need for holistic solutions. Ultimately, the decline in information quality and trustworthiness due to hallucination is a critical concern, leading \cite{rejeleene2024okw} to propose a mathematical formulation for Information Quality (IQ) based on consistency, relevance, and accuracy, underscoring the urgent need for explicit quality metrics to ensure secure and accountable AI deployment.

In conclusion, hallucination in LLMs is a multifaceted and inherent limitation, evolving from simple factual errors to complex inconsistencies across various modalities and reasoning steps. The immediate impact is a severe erosion of trust and reliability, making LLMs unsuitable for critical applications without robust safeguards. The ongoing efforts to define, categorize, and understand its underlying mechanisms are crucial, as the inevitability of hallucination necessitates a shift towards sophisticated detection, effective mitigation, and comprehensive evaluation strategies to build more trustworthy and accountable AI systems.