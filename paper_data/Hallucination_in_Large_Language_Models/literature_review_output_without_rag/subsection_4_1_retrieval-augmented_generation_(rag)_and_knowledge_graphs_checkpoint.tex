\subsection{Retrieval-Augmented Generation (RAG) and Knowledge Graphs}

Large Language Models (LLMs) often suffer from "hallucinations," generating plausible but factually incorrect or outdated information due to reliance on their static internal knowledge \cite{xu2024n76, maynez2020h3q}. To address this, Retrieval-Augmented Generation (RAG) and Knowledge Graphs (KGs) have emerged as critical strategies, providing LLMs with access to external, up-to-date, and verifiable information, thereby enhancing factual consistency and credibility \cite{tonmoy20244e4, liu2024gxh, pan2024y3a}.

The evolution of RAG began with the recognition that LLMs need dynamic interaction with external knowledge sources. Early approaches, such as \cite{yao20229uz}'s ReAct, introduced the paradigm of interleaving reasoning "thoughts" with task-specific "actions" (e.g., API calls for retrieval), enabling LLMs to dynamically interact with external environments and ground their responses. Building on this, \cite{trivedi2022qsf} further refined dynamic retrieval by proposing Interleaving Retrieval with Chain-of-Thought (IRCoT), which uses intermediate reasoning steps as queries for iterative knowledge retrieval, significantly reducing factual errors in knowledge-intensive multi-step question answering.

As RAG systems matured, the need for robust evaluation and improved context handling became evident. To enhance verifiability, \cite{gao2023ht7} introduced ALCE, a benchmark and evaluation framework that enables LLMs to generate text with explicit citations to retrieved passages, directly addressing trustworthiness. However, RAG still faces challenges, as highlighted by \cite{chen2023h04}'s RGB benchmark, which systematically evaluates RAG capabilities like noise robustness, negative rejection (refusing to answer when no information is found), and information integration. Their findings revealed that LLMs often struggle with conflicting or noisy retrieved information and may even prioritize incorrect external facts over their own internal knowledge. To combat the issue of LLMs "getting lost in long contexts" with irrelevant information, \cite{lv2024k5x} proposed Coarse-to-Fine Highlighting (COFT), an intelligent method that dynamically identifies and emphasizes key information at varying granularities within lengthy retrieved documents, improving factual accuracy by guiding the LLM's focus. Furthermore, to ensure LLMs are refreshed with current events, \cite{vu202337s} developed FreshLLMs and FRESH PROMPT, a training-free method that augments LLMs with real-time search engine results, demonstrating substantial improvements in factuality for rapidly changing world knowledge. Moving towards more intelligent RAG, \cite{su2024gnz} introduced Dynamic Retrieval Augmentation based on hallucination Detection (DRAD), a framework that conditionally triggers retrieval *only when* a potential hallucination is detected (based on entity-level uncertainty), thereby making the RAG process more efficient and targeted.

Beyond unstructured text retrieval, Knowledge Graphs (KGs) offer a structured, explicit, and verifiable form of external knowledge. Early work demonstrated the potential of KGs in reducing hallucination, such as \cite{dziri2021bw9}'s Neural Path Hunter, which used KG path grounding to identify and correct erroneous entity mentions in dialogue systems. Advancing KG integration, \cite{wen2023t6v}'s MindMap enabled LLMs to *comprehend and reason over structured graphical inputs*, synergistically combining explicit KG facts with implicit LLM knowledge and generating an explainable "mind map" of the reasoning process. This approach helps overcome the black-box nature of LLMs and provides transparent, grounded reasoning. Further extending this, \cite{li2023v3v}'s Chain-of-Knowledge introduced a framework for dynamic knowledge adapting over *heterogeneous* sources, including both structured KGs and unstructured text, featuring an Adaptive Query Generator for various query languages and a progressive rationale correction mechanism to prevent error propagation across reasoning steps. The effectiveness of KGs in enhancing trustworthiness was empirically validated by \cite{sui20242u1}, who introduced the OKGQA benchmark for open-ended question answering. Their study demonstrated that KG integration significantly reduces hallucinations even when the KG itself is partially contaminated, particularly for complex reasoning tasks. Complementing this, \cite{oh2024xa3}'s ERBench leveraged relational databases (structurally similar to KGs) to create automatically verifiable benchmarks, allowing for fine-grained evaluation of LLM factual consistency and the correctness of their generated rationales.

Despite these advancements, challenges remain. The quality and coverage of retrieved documents or KGs are paramount; irrelevant or incorrect external information can still mislead LLMs, as shown by \cite{chen2023h04}. The computational overhead of dynamic retrieval and complex KG reasoning can be substantial, and effectively integrating heterogeneous knowledge sources while maintaining coherence and preventing new forms of hallucination is an ongoing research area. Future work needs to focus on more robust mechanisms for filtering noisy retrieved content, developing more sophisticated KG-LLM fusion architectures that truly understand and leverage graph structures, and creating adaptive retrieval strategies that can dynamically choose the most appropriate external knowledge source (text, KG, or specialized tool) based on the query and context.