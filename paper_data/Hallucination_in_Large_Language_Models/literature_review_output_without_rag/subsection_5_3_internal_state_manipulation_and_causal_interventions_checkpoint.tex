\subsection*{Internal State Manipulation and Causal Interventions}

Advanced mitigation techniques for large language models (LLMs) are increasingly moving beyond superficial output filtering to directly manipulate the internal workings of models, such as their hidden states or attention mechanisms. This paradigm shift offers fine-grained control and mechanistic insights into model failures, enabling more precise and targeted corrections that enhance factual accuracy and reduce spurious correlations.

One prominent approach involves actively re-injecting or enhancing visual information within the model's intermediate layers to combat "visual amnesia" and strengthen visual grounding. \cite{zou2024dp7} introduced Memory-space Visual Retracing (MemVR), a decoding paradigm that re-injects visual tokens as supplementary evidence into the model's Feed Forward Network (FFN) at a "middle trigger layer." This "look-twice" mechanism is dynamically activated based on the model's uncertainty, effectively refreshing visual memory and mitigating hallucinations by balancing modality attention. Complementing this, \cite{yin2025s2b} proposed Visual Amplification Fusion (VAF), a plug-and-play technique that enhances attention to visual signals specifically within the Multimodal Large Language Model's (MLLM) middle layers (8th-15th), where modality fusion predominantly occurs. Unlike contrastive decoding methods that can compromise content quality, VAF directly amplifies visual features without additional forward passes, preserving inference speed and output coherence. Further refining this concept, \cite{chen2024j0g} presented Image-Object Cross-Level Trusted Intervention (ICT), a training-free method that calculates an "intervention direction" to shift the model's focus towards different levels of visual information during the forward pass. ICT identifies and intervenes on specific attention heads responsible for encoding overall image information or fine-grained object details, leveraging trusted and untrusted data pairs to derive activation shift vectors. This targeted head-level intervention mitigates excessive reliance on language priors while preserving beneficial ones, addressing a key limitation of indiscriminate suppression techniques.

Beyond direct feature manipulation, researchers are applying causal inference to attention mechanisms to understand and balance modality priors, offering a principled approach to mitigate modality-induced hallucinations. \cite{zhou2024lvp} developed CAUSAL MM, a causal inference framework that applies structural causal modeling (SCM) to MLLMs. By treating modality priors (visual and language) as confounding factors, CAUSAL MM employs back-door adjustment and counterfactual reasoning at both visual encoder and LLM backbone attention levels. This allows for deciphering the causal impact of effective attention on MLLM output by isolating the effects of modality priors, leading to more balanced multimodal outputs and a significant reduction in hallucination.

Mechanistic insights into model failures, sometimes derived from adversarial attacks, also inform targeted internal interventions. \cite{wang2025jen} revealed that MLLM hallucinations can be induced by exploiting the "attention sink" phenomenon, where columnar patterns of high attention scores emerge at the turning point of image-text relevance, aggregating misleading global information. This analysis of attention sink vulnerabilities provides a critical target for internal state manipulation, suggesting that interventions could be designed to counteract such misleading information aggregation. Even training-based methods, while not inference-time interventions, offer insights into internal state manipulation. For instance, \cite{wu2024n00}'s NoiseBoost injects Gaussian noise into projected visual tokens during supervised fine-tuning or reinforcement learning. This perturbation increases the "hardship" in visual understanding, compelling the MLLM to distribute its attention weights more evenly between visual and linguistic tokens, thereby reducing over-reliance on language priors. While applied during training, this method directly manipulates internal visual representations to influence the dynamics of attention, providing a foundational understanding for similar inference-time adjustments.

Collectively, these methods represent a significant step towards developing more robust and trustworthy MLLMs by moving beyond external fixes to address the root causes of hallucinations within the model's internal processing. The ability to precisely control attention, re-inject salient visual features, and apply causal reasoning to balance modality influences offers unprecedented control. However, challenges remain in dynamically identifying optimal intervention points and magnitudes across diverse tasks and model architectures, and in ensuring these interventions do not inadvertently suppress creativity or beneficial abstract reasoning. Future work will likely explore adaptive, real-time internal interventions that can learn and adjust based on contextual demands, potentially integrating uncertainty quantification with causal models for more intelligent and dynamic hallucination mitigation.