\subsection{Adversarial Hallucination and Robustness}

The integrity of Large Language Models (LLMs) and multimodal models is critically challenged by adversarial attacks specifically designed to induce hallucinations. This emerging field moves beyond traditional security concerns to target the factual consistency and trustworthiness of AI-generated content, necessitating a shift from reactive mitigation to proactive robustness engineering. Understanding these adversarial techniques is paramount for developing resilient AI systems capable of withstanding sophisticated, targeted manipulations.

A seminal work in this area, \cite{wang2025jen}, introduces a novel hallucination attack that exploits the "attention sink" phenomenon in multimodal LLMs (MLLMs). This attack reveals a fundamental vulnerability where specific patterns of high attention scores, emerging at the turning point of image-text relevance, can aggregate misleading global information, thereby triggering hallucinated content with minimal visual grounding. By manipulating attention scores and hidden embeddings, this method dynamically induces object, attribute, and relationship hallucinations without predefined patterns or degrading overall response quality, underscoring a critical weakness stemming from instruction-tuning data patterns.

The identification of such vulnerabilities is often facilitated by specialized diagnostic benchmarks. For instance, \cite{sungbin2024r2g} introduces AVHBench to evaluate cross-modal driven hallucinations, where models misinterpret information due to subtle inter-modal relationships or over-reliance on a single modality. Their use of synthetic videos with swapped audio effectively creates adversarial scenarios that expose these inherent weaknesses. Similarly, \cite{guan2023z15}'s Hallusionbench employs human-edited images and a novel VQ structure with control groups to systematically diagnose entangled language hallucination and visual illusion, highlighting specific failure modes that could be targeted by attackers. In the video domain, \cite{wang2024rta} proposes VideoHallucer, which utilizes an adversarial binary VideoQA method to rigorously evaluate intrinsic and extrinsic hallucinations in Large Video-Language Models (LVLMs), directly testing their robustness against crafted queries. Even in audio-language models, \cite{kuan20249pm} employs "adversarial sampling strategies" to generate non-existent objects, revealing LALMs' tendency to hallucinate in discriminative tasks.

Beyond direct attacks, understanding how hallucinations propagate reveals critical failure modes. \cite{zhong2024mfi} investigates "Multimodal Hallucination Snowballing," demonstrating how an LVLM's self-generated hallucinations can mislead subsequent responses, leading to accumulated errors. This phenomenon represents a significant vulnerability that could be triggered by an initial, subtle adversarial hallucination. Furthermore, the creation of complex, challenging benchmarks like CCHall \cite{zhang2025pex}, which generates joint cross-lingual and cross-modal hallucinations, highlights the compounded vulnerabilities in models operating in diverse, real-world conditions. Even the meta-evaluation of benchmarks by \cite{yan2024ux8} reveals issues like response bias, which, if unaddressed, could be exploited by an adversary to elicit predictable hallucinatory behavior.

To counter these sophisticated attacks and inherent vulnerabilities, researchers are developing proactive robustness engineering strategies. Training-based approaches aim to instill hallucination resistance from the ground up. \cite{wu2024n00} introduces NoiseBoost, a method that injects Gaussian noise into projected visual tokens during supervised fine-tuning (SFT) or reinforcement learning (RL). This perturbation forces MLLMs to distribute attention more evenly between visual and linguistic tokens, thereby reducing over-reliance on language priorsâ€”a common source of hallucination and a potential target for adversarial manipulation. Earlier, \cite{dai20229aa} proposed ObjMLM loss during pre-training to enhance token-level image-text alignment, directly mitigating object hallucination. Building on this, \cite{wang2023ubf} uses ChatGPT to rewrite captions for fine-tuning, aiming to mitigate fine-grained hallucinations by improving visual-textual alignment. A more advanced approach by \cite{deng202405j} presents a "judge-free" self-improvement framework that generates "controllable negative samples" with a specific hallucination ratio for Direct Preference Optimization (DPO), effectively training models to be robust against self-generated errors.

Inference-time and post-hoc methods offer flexible, training-free defenses. \cite{zhou2024lvp} proposes CAUSAL MM, a causal inference framework that uses back-door adjustment and counterfactual reasoning to mitigate modality prior-induced hallucinations, directly addressing the causal roots of model vulnerability. \cite{kim2024ozf}'s Counterfactual Inception prompts MLLMs to self-generate and avoid "counterfactual keywords," acting as a self-correction mechanism against generating false information. For targeted visual grounding, \cite{chen2024j0g} introduces ICT, which intervenes on specific attention heads during the forward pass to enhance focus on both overall image information and fine-grained object details, thereby strengthening visual fidelity against subtle manipulations. Similarly, \cite{yin2025s2b}'s VAF enhances visual signals in MLLM middle layers to reduce over-reliance on language priors, a common pathway for hallucination. \cite{zou2024dp7} proposes MemVR, a decoding paradigm that re-injects visual tokens to prevent "amnesia" about visual information, making models more robust to context loss. Other inference-time strategies include \cite{park20247cm}'s ConVis, which leverages a Text-to-Image model for hallucination visualization during contrastive decoding, providing a visual feedback loop for correction. \cite{qu2024pqc}'s MVP framework employs multi-view information seeking and certainty-driven reasoning to improve factual consistency, making models less susceptible to subtle manipulations. Post-hoc correction frameworks like "Dentist" \cite{chang2024u3t} adaptively apply visual verification or Chain-of-Thought based on query type, with an iterative validation loop for refinement, while \cite{yin2023hx3}'s Woodpecker uses expert models for visual fact-checking.

In conclusion, the investigation into adversarial hallucination underscores the critical need for robust AI systems. The shift from merely detecting hallucinations to understanding and exploiting their underlying vulnerabilities, such as the "attention sink" phenomenon, is crucial for developing proactive defenses. While significant progress has been made in both identifying attack vectors and engineering robustness through diverse training and inference-time strategies, the dynamic nature of adversarial attacks means this remains an ongoing challenge. Future research must continue to explore more sophisticated attack methodologies to uncover novel failure modes, concurrently driving the development of integrated, multi-layered defense mechanisms that ensure the credibility and reliability of AI models in real-world deployments.