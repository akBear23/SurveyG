\subsection{Multimodal Mitigation Strategies}

Hallucinations in Large Vision-Language Models (LVLMs) and other multimodal architectures represent a critical challenge, where generated content is factually inconsistent with visual inputs or established knowledge \cite{li2023249, xu2024n76}. Addressing this issue is paramount for the dependability and credibility of multimodal AI systems. Mitigation strategies have evolved from data-centric training approaches to sophisticated inference-time interventions and unified frameworks that adapt to the nature of the query.

Early efforts focused on enhancing model robustness through improved training data and objectives. \cite{liu2023882} introduced `LRV-Instruction`, a novel dataset incorporating diverse *negative instructions* (e.g., describing nonexistent objects) to explicitly train LMMs to avoid hallucination, alongside `GAVIE`, a GPT4-assisted evaluation framework. This data-centric approach demonstrated the effectiveness of training with explicit negative examples. Building on this, \cite{dai20229aa} proposed `ObjMLM` (Object-Masked Language Modeling) as a pre-training objective to mitigate object hallucination by enhancing token-level image-text alignment, showing that even state-of-the-art models frequently hallucinate and standard metrics can be misleading. Further refining data-driven methods, \cite{wang2023ubf} developed `ReCaption`, a framework that leverages ChatGPT to rewrite captions for fine-grained visual-text alignment during fine-tuning, specifically targeting object attributes and behaviors.

Beyond data augmentation, other training-based strategies aim to embed hallucination resistance directly into the model's learning process. \cite{wu2024n00} introduced `NoiseBoost`, a generalizable method that injects Gaussian noise into projected visual tokens during supervised fine-tuning (SFT), reinforcement learning (RL), or semi-supervised learning (SSL). This perturbation forces the MLLM to balance attention between visual and linguistic tokens, reducing over-reliance on language priors without significant additional training costs. Addressing the efficiency and bias of using MLLMs as judges for self-improvement, \cite{deng202405j} proposed a "model-level judge-free" framework. This method generates controllable negative samples using a "hallucination ratio" and employs a lightweight CLIP model for preference data inversion, enabling efficient Direct Preference Optimization (DPO) without the computational overhead or biases of MLLM-as-judge approaches. Similarly, \cite{xiao2024hv1} presented a framework for detecting and mitigating hallucinations via fine-grained AI feedback, introducing `HSA-DPO` (Hallucination Severity-Aware Direct Preference Optimization) which incorporates hallucination severity scores into the DPO objective to prioritize the mitigation of critical errors.

While training-based methods instill foundational robustness, inference-time interventions offer flexible, training-free solutions for deployed models. An early post-hoc correction method is `Woodpecker` \cite{yin2023hx3}, a five-stage pipeline that uses expert models (e.g., Grounding DINO for object detection, BLIP-2-FlanT5 XXL for VQA) to validate visual facts extracted from an MLLM's initial response, then employs an LLM to correct the hallucinated output with explicit bounding box evidence. This approach provides high interpretability by clearly showing the visual grounding for corrections.

More advanced inference-time interventions focus on enhancing visual grounding and balancing modality priors. `ClearSight` \cite{yin2025s2b}, with its Visual Amplification Fusion (VAF) technique, mitigates object hallucinations by amplifying visual signals in the MLLM's middle layers during the forward pass. Unlike contrastive decoding, VAF preserves content quality and inference speed by directly targeting attention mechanisms rather than suppressing language priors. Complementing this, `MemVR` (Memory-space Visual Retracing) \cite{zou2024dp7} re-injects visual tokens as supplementary evidence into intermediate layers when the model exhibits high uncertainty, effectively addressing the "amnesia" of visual information without additional inference latency. Taking a principled approach, \cite{zhou2024lvp} introduced `CAUSAL MM`, a causal inference framework that applies structural causal modeling to MLLMs. By using back-door adjustment and counterfactual reasoning on attention mechanisms, `CAUSAL MM` mitigates modality prior-induced hallucinations by deciphering their causal impact, providing a plug-and-play solution that balances visual and language priors. Furthermore, \cite{chen2024j0g} proposed `ICT` (Image-Object Cross-Level Trusted Intervention), a training-free method that calculates an "intervention direction" to shift the model's focus towards different levels of visual information (image-level and object-level) by targeting specific attention heads during the forward pass, thereby reducing over-reliance on language priors while preserving beneficial ones.

Beyond direct visual grounding, strategies involving enhanced reasoning and multi-view processing have emerged. Inspired by the dynamic reasoning and acting principles in text-based models like `ReAct` \cite{yao20229uz} and `IRCoT` \cite{trivedi2022qsf}, multimodal approaches leverage iterative self-correction. `MVP` (Multi-View Multi-Path Reasoning) \cite{qu2024pqc} is a training-free framework that enhances LVLM inference by seeking comprehensive image information from "multi-views" (top-down, regular, bottom-up) and employing "multi-path certainty-driven reasoning" during decoding. This approach leverages the LVLM's own capabilities to explore multiple reasoning paths and select the answer with the highest certainty, significantly reducing hallucinations without external tools. Another cognitive-inspired approach is `Counterfactual Inception` \cite{kim2024ozf}, a training-free method that prompts LMMs to generate "counterfactual keywords" (deliberately deviated from visual content) and then instructs the model to avoid them. This process, coupled with a Plausibility Verification Process (PVP) using CLIP scores, guides the LMM towards more factually consistent outputs.

Finally, unified frameworks aim to adapt mitigation strategies based on the nature of the query. `Dentist` \cite{chang2024u3t} is a unified hallucination mitigation framework that first classifies the query type (perception vs. reasoning) using an external LLM (ChatGPT). It then applies tailored mitigation: visual verification with sub-questions for perception queries and Chain-of-Thought (CoT) prompting for reasoning queries. This process is embedded in a validation loop that iteratively refines answers until semantic convergence, addressing the "one-size-fits-all" limitation of prior methods and ensuring more complete hallucination removal. While `Reefknot` \cite{zheng20246fk} primarily serves as a benchmark for relation hallucinations, it also proposes a "Detect-then-Calibrate" mitigation strategy based on token-level confidence and entropy analysis, which identifies and reduces hallucinations by applying calibration.

Despite these advancements, challenges remain. The scalability of fine-grained, human-annotated data for training-based methods is a persistent issue, though LLM-assisted generation helps \cite{liu2023882}. Balancing the efficiency of inference-time interventions with their robustness and generalizability across diverse hallucination types and model architectures is also an ongoing area of research. Furthermore, as theoretical work suggests hallucination is an inherent limitation for all computable LLMs \cite{xu2024n76}, future directions must focus not on eradication, but on developing increasingly sophisticated, adaptive, and trustworthy mitigation and detection mechanisms that account for the complex interplay between modalities and reasoning processes.