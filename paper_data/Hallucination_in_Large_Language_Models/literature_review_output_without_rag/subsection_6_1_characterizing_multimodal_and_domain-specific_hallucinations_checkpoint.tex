\subsection{Characterizing Multimodal and Domain-Specific Hallucinations}

Hallucinations in multimodal Large Language Models (LLMs), such as Large Vision-Language Models (LVLMs) and Large Audio-Language Models (LALMs), present distinct and often more complex challenges than those observed in their unimodal counterparts. These models frequently generate content that is plausible but factually inconsistent with the visual or audio input, a phenomenon largely attributed to the inherent "modality gap" and the struggle to robustly align heterogeneous data streams with linguistic generation \cite{liu2024sn3}. This necessitates a specialized understanding and evaluation framework beyond those developed for text-only LLMs.

A foundational type of multimodal hallucination is \textit{object hallucination}, where models describe non-existent objects in an image. \cite{li2023249} conducted the first systematic empirical study of object hallucination in LVLMs, proposing \texttt{POPE} as a more stable evaluation method and identifying object frequency and co-occurrence in training data as key drivers. Earlier, \cite{dai20229aa} probed object hallucination in Vision-Language Pre-training (VLP), revealing that optimizing for standard metrics could paradoxically increase unfaithfulness and introducing \texttt{ObjMLM} to improve token-level image-text alignment. Extending this, \cite{chen2024vy7} introduced \texttt{ROPE} to specifically evaluate \textit{multi-object hallucination}, addressing referential ambiguity with visual prompts and analyzing how object class distributions at test time influence hallucinatory behaviors. Further refining evaluation for open-ended generation, \cite{kaul2024ta7} developed \texttt{THRONE} for "Type I" object hallucinations in free-form LVLM responses, demonstrating that these are distinct from fixed-format "Type II" hallucinations and highlighting the limitations of prior metrics like \texttt{CHAIR}.

Beyond individual objects, LVLMs also struggle with \textit{relation hallucination}, misinterpreting the logical connections between entities. \cite{wu2024bxt} introduced \texttt{R-Bench} to evaluate these relationship hallucinations, emphasizing the need for data-leakage-free benchmarks and identifying co-occurrence patterns as contributors. Building on this, \cite{zheng20246fk} presented \texttt{Reefknot}, a comprehensive benchmark that categorizes relation hallucinations into "perceptive" (concrete) and "cognitive" (abstract) types, and empirically showed that current MLLMs are surprisingly more susceptible to perceptive errors. The complexity escalates further with \cite{jiang2024792}'s introduction of \textit{event hallucination}, a novel category involving the invention of fictional entities and narratives around them, which significantly increases with output length and necessitates fine-grained evaluation like their \texttt{Hal-Eval} framework.

The modality gap becomes particularly pronounced in dynamic and multi-sensory contexts, leading to \textit{temporal hallucination} in video-language models and \textit{cross-modal driven hallucinations} in audio-visual models. \cite{wang2024rta} developed \texttt{VideoHallucer}, the first comprehensive benchmark for LVLMs, introducing a new taxonomy that distinguishes between "intrinsic" (contradicting video content) and "extrinsic" (unverifiable from video) hallucinations, including temporal aspects. Focusing specifically on dynamic errors, \cite{li2024wyb} introduced \texttt{VidHalluc} to evaluate temporal hallucinations in MLLMs for video understanding, covering action, temporal sequence, and scene transition inconsistencies. For audio-visual models, \cite{sungbin2024r2g} presented \texttt{AVHBench}, a benchmark for "cross-modal driven hallucinations" that assesses misinterpretations arising from the interaction between audio and visual signals, such as perceiving imaginary sounds from visual cues or vice versa. Similarly, \cite{kuan20249pm} systematically explored object hallucination in Large Audio-Language Models (LALMs), revealing that these models often struggle with discriminative queries despite proficient audio captioning, indicating a fundamental challenge in query understanding within the audio modality. These works collectively underscore that the "modality gap" and inconsistencies between visual/audio inputs and generated text lead to unique challenges, requiring a specialized understanding beyond unimodal LLM hallucinations. \cite{guan2023z15}'s \texttt{Hallusionbench} further dissects these issues by diagnosing "entangled language hallucination and visual illusion," showing how language priors often conflict with visual context in LVLMs. Moreover, \cite{wang2025jen} provided diagnostic insights into MLLM vulnerabilities by demonstrating how "attention sinks" can be exploited to induce hallucinations, revealing internal mechanisms of failure.

The critical implications of hallucinations necessitate context-aware analysis, especially in sensitive domains. \textit{Domain-specific hallucinations}, such as those in medical imaging, demand tailored solutions. \cite{chen2024hfe} addressed this by introducing \texttt{Med-HallMark}, the first benchmark for medical hallucinations in LVLMs, which features a novel hierarchical categorization based on clinical severity (e.g., Catastrophic, Critical, Attribute, Prompt-induced, Minor) and a corresponding \texttt{MediHall Score} for fine-grained evaluation. This work highlights the need for domain-specific metrics that capture the multi-layered complexities and potential clinical impact of errors, moving beyond general object-level hallucination assessments.

The pervasive nature and diverse forms of multimodal hallucinations have also spurred efforts toward unified detection and robust evaluation. \cite{liu2024sn3} provided a comprehensive survey, categorizing hallucination symptoms, root causes (including data bias and modality misalignment), and evaluation methods, reinforcing the unique challenges. \cite{chen2024lc5} proposed \texttt{UNIHD}, a task-agnostic, tool-enhanced framework for unified hallucination detection across various tasks (image-to-text, text-to-image) and categories (object, attribute, scene-text, factual inconsistencies). However, the reliability of these evaluation tools themselves remains a concern, as highlighted by \cite{yan2024ux8}, who meta-evaluated existing hallucination benchmarks and revealed issues like response bias, inconsistency, and misalignment with human judgment, emphasizing the ongoing challenge of creating truly robust and unbiased assessment tools for multimodal hallucination.

In conclusion, characterizing multimodal and domain-specific hallucinations reveals a complex landscape where errors manifest across various modalities, granularities, and temporal dimensions. The "modality gap" is a central challenge, leading to unique hallucination types like multi-object, relation, temporal, and cross-modal driven inconsistencies. Domain-specific applications, such as medicine, further necessitate specialized, context-aware evaluation. While significant progress has been made in identifying and categorizing these phenomena, the development of comprehensive, reliable, and unbiased evaluation benchmarks remains an active and critical area of research.