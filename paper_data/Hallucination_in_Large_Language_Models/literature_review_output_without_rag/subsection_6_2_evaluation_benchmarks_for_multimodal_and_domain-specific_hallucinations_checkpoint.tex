\subsection{Evaluation Benchmarks for Multimodal and Domain-Specific Hallucinations}

The rigorous assessment of hallucinations in Multimodal Large Language Models (MLLMs) and specialized application domains necessitates the development of tailored benchmarks and metrics. These evaluation frameworks are crucial for systematically identifying, categorizing, and quantifying the diverse forms of errors, providing a foundational understanding for targeted mitigation efforts.

Early efforts to quantify hallucinations primarily focused on object-level inconsistencies in vision-language models (VLMs). \cite{dai20229aa} conducted a systematic study on object hallucination in Vision-Language Pre-training (VLP) models, utilizing metrics like CHAIR to reveal that optimizing for standard captioning metrics could paradoxically increase hallucination rates. Building on this, \cite{kaul2024ta7} introduced THRONE, an object-based benchmark specifically designed for "Type I" hallucinations in free-form LVLM generations, addressing the limitations of prior benchmarks like POPE (which focused on "Type II" fixed-format questions) and CHAIR (which struggled with semantic understanding in complex text). THRONE leverages open-source LMs for Abstractive Question Answering (AQA) to semantically judge object existence, significantly reducing misjudgment errors. Further extending object-level evaluation, \cite{chen2024vy7} proposed Recognition-based Object Probing Evaluation (ROPE) to assess *multi-object* hallucination. ROPE employs visual referring prompts (e.g., bounding boxes) to eliminate referential ambiguity and systematically investigates hallucination across various object class distributions within an image, revealing how LVLMs exploit shortcuts and spurious correlations.

As models advanced, the scope of hallucination evaluation expanded beyond simple object presence to more complex relational and attribute errors. \cite{wu2024bxt} introduced R-Bench, a novel benchmark specifically for *relationship hallucinations* in Large Vision-Language Models (LVLMs), addressing the critical issue of data leakage prevalent in earlier benchmarks that used pre-training datasets. R-Bench includes both image-level and instance-level questions to assess inter-object relationships. \cite{zheng20246fk} further advanced this by proposing Reefknot, a comprehensive benchmark for relation hallucinations, categorizing them into *perceptive* and *cognitive* types and offering multi-faceted evaluation tasks (Yes/No, MCQ, VQA). Their analysis revealed that MLLMs are surprisingly more susceptible to perceptive hallucinations.

Beyond general visual domains, specialized benchmarks have emerged for critical application areas. For instance, \cite{chen2024hfe} developed Med-HallMark, the first benchmark dedicated to detecting and evaluating *medical hallucinations* in LVLMs. This benchmark features a novel hierarchical categorization of hallucinations based on clinical severity and supports multi-task evaluation (e.g., Med-VQA, imaging report generation), providing a more nuanced assessment than traditional metrics.

The rise of MLLMs incorporating diverse modalities spurred the creation of cross-modal and temporal hallucination benchmarks. \cite{kuan20249pm} explored object hallucination in Large Audio-Language Models (LALMs), introducing ECHO and Cover metrics for generative audio captioning and revealing LALMs' struggles with discriminative audio queries. Addressing the complex interplay between modalities, \cite{sungbin2024r2g} presented AVHBench, the first benchmark for *cross-modal driven hallucinations* in Audio-Visual Large Language Models (AV-LLMs). AVHBench assesses both audio-driven video and video-driven audio hallucinations through judgment and description tasks, utilizing synthetic videos with swapped audio to challenge models. For video-language models, \cite{wang2024rta} introduced VideoHallucer, which evaluates *intrinsic* (contradicting video content) and *extrinsic* (not verifiable from video) hallucinations, covering object-relation, temporal, and semantic details in dynamic contexts. Building on this, \cite{li2024wyb} proposed VIDHALLUC, a large-scale benchmark specifically targeting *temporal hallucinations* (action, temporal sequence, scene transition) in video understanding, using adversarial video pairs to expose MLLMs' vulnerabilities to dynamic inconsistencies. In an even more complex scenario, \cite{zhang2025pex} developed CCHall, a novel benchmark for detecting *joint cross-lingual and cross-modal hallucinations*, highlighting the significant performance degradation of MLLMs when faced with inconsistencies across both visual content and multiple languages.

To provide a more unified and fine-grained understanding of hallucinations, researchers have also focused on comprehensive evaluation frameworks and meta-evaluation of benchmarks themselves. \cite{jiang2024792} introduced Hal-Eval, a universal and fine-grained framework that expanded the hallucination taxonomy to include "event hallucination" (narratives around non-existent entities) and integrated both discriminative and generative evaluation methods. \cite{chen2024lc5} presented UNIHD (Unified Multimodal Hallucination Detection), a task-agnostic, tool-enhanced framework that extracts claims, autonomously selects and executes specialized tools (e.g., object detectors, OCR, search engines), and verifies hallucinations with rationales across image-to-text and text-to-image tasks. \cite{guan2023z15} offered Hallusionbench, an advanced diagnostic suite for *entangled language hallucination and visual illusion* in LVLMs, utilizing human-edited images and control groups to quantitatively dissect specific failure modes. Furthermore, \cite{zhong2024mfi} investigated the phenomenon of "multimodal hallucination snowballing," where initial hallucinations propagate in conversational contexts, proposing the MMHalSnowball framework to evaluate this dynamic error. Crucially, \cite{yan2024ux8} took a meta-evaluation approach, proposing the Hallucination benchmark Quality Measurement (HQM) framework to systematically assess the reliability and validity of existing benchmarks. Their work revealed significant issues like response bias and misalignment with human judgment in many benchmarks, leading to the development of HQH (High-Quality Hallucination Benchmark) with a simplified binary detection metric for improved consistency.

In conclusion, the evolution of hallucination evaluation benchmarks for MLLMs reflects a progression from simple object presence to complex, dynamic, cross-modal, and domain-specific inconsistencies. While significant strides have been made in identifying and categorizing diverse errors, challenges remain in developing truly comprehensive, unbiased, and scalable evaluation frameworks that can keep pace with the rapid advancements and increasing complexity of MLLMs. The ongoing need for human-aligned, fine-grained, and cost-effective evaluation methodologies, particularly for emerging modalities and complex reasoning tasks, continues to drive research in this critical area.