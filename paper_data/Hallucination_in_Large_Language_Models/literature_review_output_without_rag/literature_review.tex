\documentclass[12pt,a4paper]{article}
    \usepackage[utf8]{inputenc}
    \usepackage[T1]{fontenc}
    \usepackage{amsmath,amsfonts,amssymb}
    \usepackage{graphicx}
    \usepackage[margin=2.5cm]{geometry}
    \usepackage{setspace}
    \usepackage{natbib}
    \usepackage{url}
    \usepackage{hyperref}
    \usepackage{booktabs}
    \usepackage{longtable}
    \usepackage{array}
    \usepackage{multirow}
    \usepackage{wrapfig}
    \usepackage{float}
    \usepackage{colortbl}
    \usepackage{pdflscape}
    \usepackage{tabu}
    \usepackage{threeparttable}
    \usepackage{threeparttablex}
    \usepackage[normalem]{ulem}
    \usepackage{makecell}
    \usepackage{xcolor}

    % Set line spacing
    \doublespacing

    % Configure hyperref
    \hypersetup{
        colorlinks=true,
        linkcolor=blue,
        filecolor=magenta,      
        urlcolor=cyan,
        citecolor=red,
    }

    % Title and author information
    \title{A Comprehensive Literature Review with Self-Reflection}
    \author{Literature Review}
    \date{\today}

    \begin{document}

    \maketitle

    % Abstract (optional)
    \begin{abstract}
    This literature review provides a comprehensive analysis of recent research in the field. The review synthesizes findings from 277 research papers, identifying key themes, methodological approaches, and future research directions.
    \end{abstract}

    \newpage
    \tableofcontents
    \newpage

    \label{sec:introduction}

\section{Introduction}
\label{sec:introduction}

\subsection{Defining Hallucination and Its Immediate Impact}
\label{sec:1\_1\_defining\_hallucination\_\_and\_\_its\_immediate\_impact}

Hallucination in Large Language Models (LLMs) represents a critical challenge to their dependability and credibility, manifesting as the generation of factually inaccurate, nonsensical, or unfaithful information. This phenomenon fundamentally undermines user trust and poses significant risks across various applications, necessitating a clear conceptual foundation and common terminology for robust solutions and accountable AI deployment.

Early research into generative models began to characterize these inconsistencies. For instance, \cite{maynez2020h3q} provided a foundational definition in abstractive summarization, categorizing hallucinations into two primary forms: \textit{intrinsic hallucinations}, which misrepresent information present in the source document, and \textit{extrinsic hallucinations}, which introduce information not directly inferable from the input. This work highlighted that the majority of extrinsic hallucinations were erroneous, emphasizing the need for semantic-aware evaluation beyond traditional metrics. Building on this, \cite{dziri2021bw9} further explored hallucination in dialogue systems, identifying the injection of erroneous entities (a form of extrinsic hallucination) as a prevalent issue, underscoring the challenge of maintaining factual consistency with external knowledge graphs. The difficulty of detecting such errors in free-form generation, where no explicit reference exists, was addressed by \cite{liu2021mo6}, which introduced the HADES benchmark for token-level, reference-free hallucination detection, enabling finer-grained analysis of unfaithful content.

As LLMs scaled and became more versatile, the understanding and categorization of hallucination evolved to capture their unique complexities. \cite{zhang2023k1j} offered a comprehensive LLM-centric taxonomy, expanding on earlier definitions by classifying hallucinations into \textit{input-conflicting}, \textit{context-conflicting}, and \textit{fact-conflicting} types. This framework specifically highlighted fact-conflicting errors as a major concern due to the absence of an authoritative knowledge source within the model itself. Complementing this, \cite{ye2023yom} provided a detailed review, attributing hallucination to multifaceted issues spanning data collection, knowledge gaps, and optimization processes, and offering a broad taxonomy across various generation tasks. Further refining this granularity, \cite{rawte2023ao8} introduced a fine-grained profiling framework encompassing "Factual Mirage" and "Silver Lining" orientations, along with specific types like "Numeric Nuisance" and "Generated Golem," and proposed the Hallucination Vulnerability Index (HVI) for quantitative assessment. This extensive categorization underscores the diverse manifestations of hallucination. \cite{du2023qu7} then refined the definition for LLMs as "generations of the model that violate human instructions," and proposed an association analysis to attribute hallucinations to specific model capability deficiencies, such as commonsense memorization or relational reasoning, moving beyond mere descriptive categorization to mechanistic understanding.

The immediate impact of hallucination on LLM dependability and credibility is profound and far-reaching. The theoretical work by \cite{xu2024n76} delivered a sobering insight, proving that hallucination is an \textit{inherent and inevitable limitation} for all computable LLMs, regardless of their architecture or training. This fundamental theoretical ceiling shifts the research paradigm from complete eradication to robust detection, effective mitigation, and responsible deployment strategies. The practical implications are evident across various applications. For instance, in information retrieval, the generation of fact-conflicting errors directly impacts trustworthiness, leading to efforts like those by \cite{gao2023ht7} to enable LLMs to generate text with verifiable citations, directly addressing the need for accountability. However, even advanced mitigation techniques like Retrieval-Augmented Generation (RAG) face significant challenges, as demonstrated by \cite{chen2023h04}, whose RGB benchmark revealed that LLMs still struggle with noise robustness, negative rejection (refusing to answer when no relevant information is available), and counterfactual scenarios, often prioritizing incorrect retrieved information over their own correct internal knowledge. This highlights the persistent difficulty in grounding LLM responses reliably.

Moreover, the problem extends beyond text-only models. \cite{li2023249} identified "object hallucination" in Large Vision-Language Models (LVLMs), where models describe nonexistent objects in images, demonstrating that inconsistencies can arise across modalities and are driven by statistical biases in training data. This broadens the scope of the hallucination challenge significantly. A comprehensive empirical study by \cite{li2024qrj} on factuality hallucination across the entire LLM lifecycle (pre-training, supervised fine-tuning, RLHF, and inference) further confirmed the pervasive nature of these errors and identified their diverse sources, emphasizing the need for holistic solutions. Ultimately, the decline in information quality and trustworthiness due to hallucination is a critical concern, leading \cite{rejeleene2024okw} to propose a mathematical formulation for Information Quality (IQ) based on consistency, relevance, and accuracy, underscoring the urgent need for explicit quality metrics to ensure secure and accountable AI deployment.

In conclusion, hallucination in LLMs is a multifaceted and inherent limitation, evolving from simple factual errors to complex inconsistencies across various modalities and reasoning steps. The immediate impact is a severe erosion of trust and reliability, making LLMs unsuitable for critical applications without robust safeguards. The ongoing efforts to define, categorize, and understand its underlying mechanisms are crucial, as the inevitability of hallucination necessitates a shift towards sophisticated detection, effective mitigation, and comprehensive evaluation strategies to build more trustworthy and accountable AI systems.
\subsection{Scope and Motivation of the Review}
\label{sec:1\_2\_scope\_\_and\_\_motivation\_of\_the\_review}

The pervasive issue of hallucination in Large Language Models (LLMs), where models generate fluent yet factually incorrect or unfaithful information, poses a significant threat to their reliability and the development of trustworthy artificial intelligence. This literature review aims to comprehensively delineate the current research landscape surrounding LLM hallucination, from its foundational understanding and characterization to advanced detection, benchmarking, and mitigation techniques, including the emerging challenges in multimodal contexts. Our primary motivation is to synthesize the state-of-the-art, identify critical gaps in current methodologies, and highlight promising future directions to ensure LLMs are dependable, transparent, and safe for widespread deployment. The urgent need for robust solutions is underscored by concerns about misinformation, accountability, and the ethical implications of deploying unreliable AI systems in sensitive domains.

Initial research efforts laid the groundwork for understanding the nature of hallucination. \cite{maynez2020h3q} provided foundational definitions, distinguishing between intrinsic (misrepresenting source content) and extrinsic (adding uninferable information) hallucinations within abstractive summarization. Building upon this, comprehensive surveys such as \cite{zhang2023k1j} and \cite{ye2023yom} extended these taxonomies and offered mechanistic analyses specifically tailored for the unique challenges of LLMs, consolidating the fragmented understanding of the phenomenon. To move beyond qualitative descriptions, \cite{rawte2023ao8} introduced a fine-grained categorization of hallucination types and proposed the Hallucination Vulnerability Index (HVI) for quantitative assessment, addressing the need for standardized metrics. Further deepening this understanding, \cite{du2023qu7} developed an association analysis framework for unbiased quantification and attribution of hallucination to specific model capability deficiencies, moving beyond simple error rates. A pivotal theoretical contribution by \cite{xu2024n76} formally proved the inevitability of hallucination for all computable LLMs, fundamentally shifting the research paradigm from elimination to management. This theoretical grounding is complemented by investigations into the root causes of declining information quality, as explored by \cite{rejeleene2024okw}, reinforcing the necessity for a deeper, systematic understanding of LLM failures.

The evolution of foundational understanding has directly propelled the development of sophisticated benchmarking and detection methodologies. Early work by \cite{liu2021mo6} pioneered a token-level, reference-free hallucination detection benchmark (HADES), addressing the limitations of reference-dependent and coarse-grained evaluation methods. Recognizing the scalability challenges of manual annotation, \cite{cao2023ecl} introduced AutoHall for automated hallucination dataset generation, enabling more efficient and model-specific evaluation. The scope of evaluation expanded with benchmarks like HaluEval 2.0 by \cite{li2024qrj}, offering a comprehensive empirical study of factuality hallucination across diverse domains and LLM lifecycle stages. To enhance trustworthiness and verifiability, \cite{gao2023ht7} introduced ALCE, a reproducible benchmark for LLMs to generate text with verifiable citations, a critical step towards accountable AI. Pushing the boundaries of evaluation, \cite{oh2024xa3} developed ERBench, which uniquely leverages relational databases to construct automatically verifiable questions and, crucially, rationales, allowing for deeper scrutiny of LLM reasoning processes beyond just final answers. Addressing language-specific challenges, \cite{liang20236sh} presented UHGEval for benchmarking Chinese LLMs via unconstrained generation, providing a more realistic assessment of spontaneous hallucinations. The field also began tackling multimodal and dialogue-specific challenges: \cite{vu202337s} introduced FreshLLMs to evaluate LLMs on dynamic, real-world knowledge using search engine augmentation, while \cite{chen2024c4k} developed DiaHalu, the first dedicated benchmark for dialogue-level hallucination, capturing the complexities of multi-turn interactions. Most recently, \cite{yang20251dw} proposed MetaQA, a self-contained detection method leveraging metamorphic relations, offering a robust solution for black-box LLMs without reliance on external resources or internal model access. These advancements collectively provide increasingly granular, automated, and context-aware tools for identifying hallucinations.

Concurrently, a diverse array of mitigation and correction strategies has emerged to address the identified challenges. Early efforts focused on grounding dialogue systems in external knowledge, as demonstrated by \cite{dziri2021bw9}'s Neural Path Hunter, which used Knowledge Graphs (KGs) to correct hallucinated entities. Addressing data quality issues, \cite{adams202289x} proposed learning to revise noisy reference summaries, thereby improving training data for faithful summarization. The advent of Retrieval-Augmented Generation (RAG) and self-correction paradigms brought significant advancements: \cite{li2023v3v} introduced Chain-of-Knowledge, dynamically adapting over heterogeneous sources for improved factual grounding, while \cite{dhuliawala2023rqn} developed Chain-of-Verification, enabling LLMs to systematically self-critique and verify factual claims internally. Further integrating KGs, \cite{wen2023t6v}'s MindMap allowed LLMs to comprehend graphical inputs and reason over a "mind map," enhancing transparency and reducing hallucination. More sophisticated RAG techniques emerged to address efficiency and precision: \cite{lv2024k5x} proposed Coarse-to-Fine Highlighting to reduce knowledge hallucination in long contexts, and \cite{su2024gnz} introduced DRAD, a dynamic RAG framework that triggers retrieval based on real-time, entity-level hallucination detection. The landscape of self-correction was comprehensively surveyed by \cite{pan2023mwu} and \cite{pan2024y3a}, categorizing diverse strategies, while \cite{ji2023vhv} demonstrated an iterative self-reflection methodology for mitigating hallucinations in high-stakes medical generative QA. Finally, novel decoding strategies like \cite{zhang202396g}'s Induce-then-Contrast Decoding offered a unique approach by leveraging "induced hallucinations" to guide models towards factuality without retraining. These diverse techniques, further consolidated by comprehensive surveys on misinformation prevention and detection like \cite{liu2024gxh}, collectively aim to build more robust and reliable LLM systems.

In conclusion, the research trajectory on LLM hallucination has rapidly evolved from initial characterization to sophisticated, multimodal, and LLM-native approaches. This review highlights a clear progression towards deeper mechanistic understanding, including the theoretical inevitability of hallucination, and the development of increasingly automated, fine-grained, and verifiable evaluation benchmarks. Concurrently, mitigation strategies have advanced from post-hoc corrections to proactive prevention and dynamic knowledge integration, often leveraging LLM internal states and emphasizing safety-critical considerations. Despite significant progress, challenges remain in achieving absolute factual consistency across all modalities and complex reasoning tasks. This comprehensive analysis serves to guide future research towards developing more robust, transparent, and ultimately trustworthy LLM systems, which is paramount for their safe and beneficial integration into society.


\label{sec:foundational_understanding_and_theoretical_limits}

\section{Foundational Understanding and Theoretical Limits}
\label{sec:foundational\_underst\_and\_ing\_\_and\_\_theoretical\_limits}

\subsection{Historical Characterization and Evolving Taxonomies}
\label{sec:2\_1\_historical\_characterization\_\_and\_\_evolving\_taxonomies}

The phenomenon of hallucination, where large language models (LLMs) generate plausible but factually incorrect or unfaithful content, has emerged as a critical challenge to their reliability and trustworthiness. The field's understanding of this problem has evolved significantly, moving from initial empirical observations to sophisticated, multi-dimensional taxonomies and, more recently, a theoretical understanding of its fundamental limits. This intellectual lineage has been crucial in systematically analyzing the problem space and guiding initial detection efforts across diverse natural language generation tasks.

Early pioneering empirical studies first systematically identified and distinguished different types of errors. A seminal work in abstractive summarization by \cite{maynez2020h3q} conducted a large-scale human evaluation to categorize hallucinations into two primary types: \textit{intrinsic hallucinations}, which misrepresent information present in the source document, and \textit{extrinsic hallucinations}, which introduce information not inferable from the source. This study highlighted the inadequacy of traditional metrics like ROUGE and BERTScore for assessing faithfulness and advocated for semantically-aware evaluation, laying the groundwork for a more nuanced understanding of generative errors.

Building upon this foundational distinction, researchers began to develop more comprehensive taxonomies to classify hallucinations by their source, type, and severity, extending beyond summarization to general LLM outputs. \cite{du2023qu7} proposed a capability-based taxonomy, attributing hallucinations to deficiencies in commonsense memorization, relational reasoning, and instruction following, and introduced an association analysis framework to quantify and attribute these errors. This marked a shift towards understanding the underlying cognitive-like causes rather than just surface-level phenomena. Further refining the problem space, \cite{zhang2023k1j} provided an LLM-centric survey that formalized hallucination into \textit{input-conflicting}, \textit{context-conflicting}, and \textit{fact-conflicting} types, emphasizing the unique challenges posed by LLMs' scale and versatility. Complementing this, \cite{ye2023yom} offered a detailed taxonomy categorized by various text generation tasks and provided mechanistic analyses, linking hallucination origins to issues in data collection, knowledge gaps, and the optimization process.

The granularity of hallucination characterization continued to increase. \cite{rawte2023ao8} introduced a fine-grained framework profiling hallucination by its degree (mild, moderate, alarming), orientation (Factual Mirage, Silver Lining, each with intrinsic/extrinsic sub-categories), and six specific types (e.g., Acronym Ambiguity, Numeric Nuisance). They also proposed the Hallucination Vulnerability Index (HVI) for quantitative assessment, providing a standardized metric for comparing LLM susceptibility. Similarly, \cite{li2024qrj} presented a detailed taxonomy for \textit{factuality hallucination}, encompassing categories like Entity-error, Relation-error, Incompleteness, Outdatedness, Overclaim, and Unverifiability, and developed an LLM-based detection framework to operationalize these distinctions. The evolution of these taxonomies has been crucial for systematically analyzing and structuring the problem space, moving from general observations to precise, actionable classifications.

The expansion of LLM capabilities into multi-modal domains necessitated new characterizations. \cite{li2023249} conducted the first systematic empirical study of "object hallucination" in Large Vision-Language Models (LVLMs), where models describe objects inconsistent with or absent from images. They proposed the Polling-based Object Probing Evaluation (POPE) as a more stable evaluation method and crucially identified object frequency and co-occurrence in training data as key drivers of this multi-modal hallucination. This demonstrated the need for domain-specific taxonomies and evaluation methods. More recently, as LLMs engage in complex multi-turn interactions, \cite{chen2024c4k} introduced \textit{DiaHalu}, a benchmark for dialogue-level hallucination, proposing a taxonomy with five subtypes: Non-factual, Incoherence (input-conflicting, context-conflicting, self-conflicting), Irrelevance, Overreliance, and Reasoning Error. This highlights the ongoing need to adapt and expand hallucination taxonomies to new generation paradigms and interaction complexities.

These evolving characterizations and taxonomies have directly guided initial detection efforts. Early detection methods, such as those in \cite{maynez2020h3q}, relied on human evaluation informed by the intrinsic/extrinsic distinction. To enable more scalable and fine-grained detection, \cite{liu2021mo6} introduced HADES, the first token-level, reference-free hallucination detection benchmark for free-form text generation, which allowed for pinpointing errors at a granular level. The need for black-box detection, applicable to proprietary models, led to innovations like \textit{SelfCheckGPT} by \cite{manakul20236ex}, which leverages the consistency of stochastically sampled outputs to identify non-factual statements. Furthermore, the drive for verifiable outputs led \cite{gao2023ht7} to develop \textit{ALCE}, a benchmark for LLMs generating text with citations, incorporating Natural Language Inference (NLI)-based metrics for citation quality, directly addressing verifiability. For retrieval-augmented generation (RAG) systems, \cite{chen2023h04} developed the \textit{RGB} benchmark, which diagnoses specific RAG capabilities like noise robustness and negative rejection, reflecting a deeper understanding of how external information can lead to hallucination. More sophisticated detection mechanisms, such as \cite{oh2024xa3}'s \textit{ERBench}, leverage relational databases to automatically verify not just answers but also the LLM's rationale, demonstrating a move towards transparent and verifiable reasoning. Finally, \cite{yang20251dw} introduced \textit{MetaQA}, a self-contained detection approach using metamorphic relations, addressing the limitations of external resources and internal model access.

Ultimately, this extensive characterization has culminated in a theoretical understanding of hallucination's fundamental nature. \cite{xu2024n76} provided a groundbreaking theoretical proof, using a diagonalization argument, that hallucination is an inherent and inevitable limitation for all computable LLMs, regardless of their architecture or training. This profound insight shifts the paradigm from attempting complete elimination to focusing on robust management, detection, and mitigation. The continuous refinement of hallucination definitions, taxonomies, and the development of targeted evaluation methods remain critical for navigating this inherent limitation and fostering the development of more trustworthy and reliable LLM applications.
\subsection{Root Causes and Mechanistic Insights}
\label{sec:2\_2\_root\_causes\_\_and\_\_mechanistic\_insights}

The pervasive phenomenon of hallucination in Large Language Models (LLMs) stems from a complex interplay of underlying factors, moving beyond mere superficial errors to deep mechanistic origins within their architecture, training data, and inference processes. Understanding these granular insights is paramount for developing targeted and effective mitigation strategies. Initially, hallucinations were broadly categorized as intrinsic (misrepresenting source information) or extrinsic (adding ungrounded information) \cite{maynez2020h3q}. More recent taxonomies for LLMs further delineate these into input-conflicting, context-conflicting, and fact-conflicting types, with the latter posing significant challenges due to the absence of an authoritative knowledge source \cite{zhang2023k1j}.

Comprehensive surveys attribute the mechanistic origins of hallucinations to three primary factors: issues during data collection, inherent knowledge gaps, and deficiencies in the optimization process \cite{ye2023yom}. Similarly, a systematic analysis across the LLM lifecycle points to vulnerabilities during pre-training, supervised fine-tuning (SFT), reinforcement learning from human feedback (RLHF), and inference \cite{zhang2023k1j}.

A significant root cause lies in the \textbf{quality and characteristics of the vast training data}. LLMs are trained on web-scale corpora that inevitably contain fabricated, outdated, or biased information \cite{zhang2023k1j}. This noisy data directly contributes to unfaithful generations; for instance, abstractive summarization models trained on noisy reference summaries are prone to hallucination, a problem exacerbated in resource-limited domains where filtering is not feasible \cite{adams202289x}. Beyond explicit errors, the very process of tokenization and the lack of data diversity can introduce biases and impact information quality, leading to unreliable, inconsistent, or inaccurate outputs \cite{rejeleene2024okw}. Furthermore, LLMs inherently struggle with rapidly changing world knowledge due to their static training data, generating outdated or false information on dynamic topics \cite{vu202337s}. This issue highlights a fundamental knowledge gap that external knowledge integration, such as Knowledge Graphs, aims to address \cite{wen2023t6v, li2023v3v}. A specific phenomenon, termed "knowledge overshadowing," occurs when dominant conditions or over-represented facts in training data lead to over-generalization and the creation of "amalgamated hallucinations." Empirical studies show that lower frequency of pre-training knowledge correlates with higher hallucination rates, suggesting that sparse or less emphasized facts are more susceptible to being misrepresented or fabricated \cite{li2024qrj}.

Beyond data, \textbf{limitations in model architecture and knowledge representation} play a crucial role. A theoretical proof suggests that hallucination is an innate and inevitable limitation for all computable LLMs, implying that complete elimination is fundamentally impossible regardless of architectural improvements or training data quality \cite{xu2024n76}. Mechanistically, LLMs can exhibit deficiencies in fundamental capabilities such as commonsense memorization and relational reasoning, which directly contribute to the generation of incorrect information \cite{du2023qu7}. The internal representation of knowledge can also be problematic; LLMs may assign probabilities to non-factual information during maximum-likelihood-based pre-training, or over-rely on superficial patterns, making them susceptible to generating plausible but untruthful content \cite{zhang202396g}.

\textbf{Inference-time errors and reasoning failures} represent another critical category of mechanistic origins. The choice of decoding strategy significantly impacts hallucination; diversity-oriented decoding can increase hallucinations in professional domains, while greedy search exacerbates them in open-ended contexts \cite{li2024qrj}. The phenomenon of "exposure bias," where LLMs, when conditioned on their own prior (potentially incorrect) generations, tend to repeat or reinforce hallucinations, is a known issue in the optimization process \cite{ye2023yom, dhuliawala2023rqn}. This highlights a lack of robust self-correction mechanisms during generation.

In Retrieval-Augmented Generation (RAG) systems, specific mechanistic failures emerge from the interaction with external knowledge. LLMs demonstrate poor \textbf{noise robustness}, often confusing similar information or being distracted by irrelevant text when processing long retrieved contexts, a problem addressed by intelligent highlighting techniques \cite{chen2023h04, lv2024k5x}. They also exhibit \textbf{negative rejection failures}, frequently generating incorrect answers even when no relevant information is available in the retrieved documents, and critically, suffer from \textbf{counterfactual robustness issues}, where they prioritize factually incorrect retrieved information over their own correct internal knowledge \cite{chen2023h04}. Furthermore, LLMs often struggle with \textbf{information integration}, failing to synthesize facts from multiple documents to answer complex questions \cite{chen2023h04}. These issues indicate a mechanistic inability to reliably discern, prioritize, and integrate external evidence.

Finally, challenges persist in \textbf{attributing hallucinations to specific internal model behaviors or layers}. While a definitive mapping remains elusive, research is moving towards finer-grained attribution. For instance, real-time hallucination detection methods can identify potential errors by analyzing the uncertainty of output entities (low predictive probability and high entropy) \cite{su2024gnz}. Similarly, token-level hallucination detection benchmarks provide high-resolution signals, revealing that adverbs, adjectives, and proper nouns (especially acronyms) are more prone to hallucination \cite{liu2021mo6}. Dialogue-level hallucinations further expose mechanistic failures like incoherence, irrelevance, overreliance, and reasoning errors that propagate across turns \cite{chen2024c4k}. The need for self-reflection and progressive rationale correction in complex tasks, especially in high-stakes domains like medical generative QA, underscores that LLMs frequently exhibit "fact inconsistency" and "query inconsistency" due to reasoning flaws \cite{ji2023vhv, li2023v3v}.

In conclusion, the root causes of hallucination are multifaceted, spanning from biases and incompleteness in training data to inherent architectural limitations, and complex failures during inference, particularly in handling external knowledge and performing multi-step reasoning. Phenomena like knowledge overshadowing and exposure bias reveal specific mechanistic vulnerabilities. While attributing hallucinations to precise internal model layers remains a challenge, advancements in fine-grained detection and self-correction mechanisms are beginning to shed light on these intricate origins, paving the way for more robust and trustworthy LLM development.
\subsection{The Inevitability of Hallucination}
\label{sec:2\_3\_the\_inevitability\_of\_hallucination}

The pervasive issue of hallucination in Large Language Models (LLMs), where models generate plausible but factually incorrect or nonsensical information, has long been viewed as a critical engineering challenge to be overcome. Initial research focused on empirically characterizing and detecting these errors, with the implicit goal of eventual eradication. For instance, \cite{maynez2020h3q} provided a foundational human evaluation, systematically categorizing hallucinations in abstractive summarization into intrinsic (misrepresenting source content) and extrinsic (adding uninferable information), highlighting the inadequacy of traditional metrics for assessing factual consistency. Building on the need for robust detection, \cite{manakul20236ex} introduced \textit{SelfCheckGPT}, a zero-resource, black-box method that leverages the consistency of stochastically sampled LLM outputs to identify non-factual statements, demonstrating that internal model uncertainty could be a signal for hallucination.

However, a pivotal shift in understanding has emerged, moving beyond an engineering problem towards recognizing hallucination as a fundamental, inherent limitation of LLMs. This paradigm shift is most profoundly articulated by \cite{xu2024n76}, which presents a theoretical proof demonstrating that hallucination is an innate and unavoidable characteristic for any computable LLM. By formally defining LLMs as total computable functions and hallucination as inconsistencies with a computable ground truth, \cite{xu2024n76} employs a diagonalization argument, akin to Cantor's theorem, to show that no computable LLM can perfectly learn all computable functions. Consequently, any such model will inevitably hallucinate on some, and indeed infinitely many, inputs, regardless of architectural advancements or training data improvements. This groundbreaking insight suggests that the complete elimination of hallucination is mathematically impossible, setting a theoretical ceiling on LLM dependability.

This theoretical inevitability fundamentally re-frames the research agenda. Instead of striving for eradication, the focus has pivoted towards robust detection, effective mitigation, and responsible deployment strategies. Advanced detection methods continue to be crucial, evolving beyond earlier approaches. For example, \cite{yang20251dw} proposed \textit{MetaQA}, a self-contained, zero-resource hallucination detection method that leverages metamorphic relations and prompt mutation to expose factual inconsistencies, outperforming \textit{SelfCheckGPT} by generating diverse response mutations that more effectively reveal underlying inaccuracies.

Concurrently, mitigation efforts are now understood as strategies to \textit{reduce} the frequency and impact of hallucinations, rather than eliminate them entirely. Approaches that ground LLMs in external, verifiable knowledge or enhance their internal reasoning capabilities have become paramount. \cite{yao20229uz} introduced \textit{ReAct}, a seminal paradigm that interleaves verbal reasoning ("thoughts") with task-specific actions (e.g., API calls) to ground LLM responses in external environments, thereby reducing hallucination. \cite{trivedi2022qsf} further refined this by proposing \textit{IRCoT}, which dynamically uses intermediate Chain-of-Thought steps as queries for iterative knowledge retrieval, significantly reducing factual errors in multi-step question answering. To enhance verifiability, \cite{gao2023ht7} developed \textit{ALCE}, a benchmark and evaluation framework for enabling LLMs to generate text with verifiable citations, directly addressing trustworthiness. Furthermore, internal self-correction mechanisms, such as \textit{Chain-of-Verification (CoVe)} by \cite{dhuliawala2023rqn}, allow LLMs to systematically plan and execute verification questions to fact-check their own claims, significantly reducing hallucinations through independent self-deliberation.

However, even these sophisticated mitigation strategies face inherent limitations, reinforcing the notion of inevitability. \cite{chen2023h04} critically evaluated Retrieval-Augmented Generation (RAG) through the \textit{RGB} benchmark, revealing that current LLMs still struggle significantly with noise robustness, negative rejection (failing to abstain when no relevant information is available), and counterfactual robustness (prioritizing incorrect retrieved information over internal knowledge). This empirical evidence underscores that while mitigation is vital, it does not lead to complete eradication. The challenge extends to multi-modal models as well, where \cite{li2023249} systematically studied "object hallucination" in Large Vision-Language Models (LVLMs) and proposed \textit{POPE} for more stable evaluation, while \cite{liu2023882} demonstrated mitigation through robust instruction tuning with negative examples. These works show that the problem of hallucination is pervasive across modalities, further cementing its fundamental nature.

In conclusion, the theoretical proof of hallucination's inevitability marks a profound shift in the AI research landscape. It necessitates a paradigm where LLMs are viewed not as infallible oracles, but as powerful tools with inherent limitations. The research agenda is now firmly directed towards developing robust detection mechanisms, continuously refining mitigation strategies to minimize errors, and implementing responsible deployment frameworks that explicitly account for and communicate the unavoidable presence of hallucination. Future work must focus on building trustworthy systems that can effectively manage, rather than eliminate, this innate characteristic of computable LLMs, fostering greater transparency and user awareness regarding AI dependability and credibility.


\label{sec:benchmarking_and_detection_methodologies}

\section{Benchmarking and Detection Methodologies}
\label{sec:benchmarking\_\_and\_\_detection\_methodologies}

\subsection{Reference-Free and Consistency-Based Detection}
\label{sec:3\_1\_reference-free\_\_and\_\_consistency-based\_detection}

Detecting hallucinations in Large Language Models (LLMs) often necessitates external ground truth or human-annotated references, which can be impractical for proprietary models, real-time applications, or at scale. This subsection explores advanced methods that circumvent this dependency by leveraging internal model consistency, self-contained detection mechanisms, and automated dataset generation to identify non-factual or incoherent statements. These techniques are crucial for efficient and flexible assessment of factual correctness and internal coherence.

A foundational approach in reference-free detection is \textit{consistency-based sampling}, exemplified by \textit{SelfCheckGPT} \cite{manakul20236ex}. This method posits that if an LLM genuinely "knows" a fact, stochastically sampled responses to the same prompt will be consistent; conversely, hallucinations will lead to divergent and contradictory outputs. \cite{manakul20236ex} proposes five variants for measuring this informational consistency, including BERTScore, Question Answering (QA), N-gram models, Natural Language Inference (NLI), and direct LLM prompting, demonstrating its applicability to black-box models without internal probability access. While effective, \textit{SelfCheckGPT} can sometimes struggle if the LLM consistently repeats its own hallucinated information across samples.

To address this limitation, \textit{MetaQA} \cite{yang20251dw} introduces a novel self-contained detection approach leveraging \textit{metamorphic relations} (MRs) and prompt mutation. Instead of simple re-sampling, \cite{yang20251dw} generates diverse synonymous and antonymous mutations of an LLM's response, then uses the LLM itself to verify the factual consistency of these mutations, thereby exposing inconsistencies more effectively. This method, compatible with both open and closed-source LLMs, consistently outperforms \textit{SelfCheckGPT} by generating more challenging inconsistencies and reducing the LLM's tendency to reinforce its own errors.

Beyond direct detection, automating the generation of datasets for reference-free evaluation is vital for scalability. \textit{HADES} \cite{liu2021mo6} pioneered a token-level, reference-free hallucination detection benchmark for free-form text generation, created by perturbing Wikipedia text to simulate machine-generated inconsistencies. Building on this, \textit{AutoHall} \cite{cao2023ecl} proposes an automated pipeline to construct model-specific hallucination datasets from existing fact-checking data by prompting an LLM to generate references and then classify their support for a claim. If the LLM's classification contradicts the ground truth, the generated reference is flagged as hallucinatory, enabling scalable dataset creation and a self-contradiction-based detection. Further advancing self-contained evaluation, \textit{ERBench} \cite{oh2024xa3} utilizes relational databases and their integrity constraints (e.g., Functional Dependencies, Foreign Key Constraints) to automatically generate complex, verifiable questions and rationales. This allows for fine-grained evaluation of both LLM answers and their underlying reasoning process without human annotation. Similarly, \cite{li2024qrj} presents a robust LLM-based detection framework within \textit{HaluEval 2.0}, where GPT-4 is leveraged to extract factual statements from LLM responses and then judge their truthfulness, even considering interrelations between statements, effectively using a powerful LLM as its own internal arbiter of factuality.

Another significant direction involves integrating consistency-based detection directly into the LLM's generation and reasoning process, enabling self-correction. \textit{Chain-of-Verification (CoVe)} \cite{dhuliawala2023rqn} empowers LLMs to self-critique by generating a baseline response, then planning and executing specific verification questions. Crucially, its "factored" variant answers each verification question independently, preventing the LLM from repeating its own initial hallucinations by avoiding conditioning on the potentially incorrect baseline. This systematic self-deliberation significantly reduces factual errors. Extending this, an iterative \textit{Self-Reflection} methodology \cite{ji2023vhv} enables LLMs to dynamically acquire and refine background knowledge, and subsequently generate and refine answers until factual consistency and entailment are achieved. This continuous feedback loop within the LLM's own reasoning process is particularly effective in high-stakes domains like medical generative question-answering.

Finally, real-time, reference-free detection can also be achieved by analyzing internal model uncertainty. The \textit{Real-time Hallucination Detection (RHD)} component within the \textit{Dynamic Retrieval Augmentation based on hallucination Detection (DRAD)} framework \cite{su2024gnz} identifies potential hallucinations by analyzing the uncertainty (low predictive probability and high entropy) of output entities. This method operates without external models or multiple responses, providing an immediate, model-agnostic signal for triggering targeted retrieval and self-correction.

In summary, reference-free and consistency-based detection methods offer a powerful paradigm for evaluating LLM factuality and coherence, especially for black-box models and large-scale deployment. Techniques ranging from stochastic sampling and metamorphic relations to automated dataset generation and internal self-verification mechanisms provide flexible and efficient alternatives to traditional reference-dependent evaluations. However, challenges remain in ensuring the robustness of LLM-as-judge approaches, managing the computational cost of multiple samples or iterative refinement, and generalizing these detection mechanisms across the full spectrum of hallucination types and domains. Future work will likely focus on refining the reliability of internal consistency checks and developing more computationally efficient self-contained verification processes.
``\texttt{
Detecting hallucinations in Large Language Models (LLMs) often necessitates external ground truth or human-annotated references, which can be impractical for proprietary models, real-time applications, or at scale. This subsection explores advanced methods that circumvent this dependency by leveraging internal model consistency, self-contained detection mechanisms, and automated dataset generation to identify non-factual or incoherent statements. These techniques are crucial for efficient and flexible assessment of factual correctness and internal coherence.

A foundational approach in reference-free detection is \textit{consistency-based sampling}, exemplified by \textit{SelfCheckGPT} \cite{manakul20236ex}. This method posits that if an LLM genuinely "knows" a fact, stochastically sampled responses to the same prompt will be consistent; conversely, hallucinations will lead to divergent and contradictory outputs. \cite{manakul20236ex} proposes five variants for measuring this informational consistency, including BERTScore, Question Answering (QA), N-gram models, Natural Language Inference (NLI), and direct LLM prompting, demonstrating its applicability to black-box models without internal probability access. While effective, \textit{SelfCheckGPT} can sometimes struggle if the LLM consistently repeats its own hallucinated information across samples.

To address this limitation, \textit{MetaQA} \cite{yang20251dw} introduces a novel self-contained detection approach leveraging \textit{metamorphic relations} (MRs) and prompt mutation. Instead of simple re-sampling, \cite{yang20251dw} generates diverse synonymous and antonymous mutations of an LLM's response, then uses the LLM itself to verify the factual consistency of these mutations, thereby exposing inconsistencies more effectively. This method, compatible with both open and closed-source LLMs, consistently outperforms \textit{SelfCheckGPT} by generating more challenging inconsistencies and reducing the LLM's tendency to reinforce its own errors.

Beyond direct detection, automating the generation of datasets for reference-free evaluation is vital for scalability. \textit{HADES} \cite{liu2021mo6} pioneered a token-level, reference-free hallucination detection benchmark for free-form text generation, created by perturbing Wikipedia text to simulate machine-generated inconsistencies. Building on this, \textit{AutoHall} \cite{cao2023ecl} proposes an automated pipeline to construct model-specific hallucination datasets from existing fact-checking data by prompting an LLM to generate references and then classify their support for a claim. If the LLM's classification contradicts the ground truth, the generated reference is flagged as hallucinatory, enabling scalable dataset creation and a self-contradiction-based detection. Further advancing self-contained evaluation, \textit{ERBench} \cite{oh2024xa3} utilizes relational databases and their integrity constraints (e.g., Functional Dependencies, Foreign Key Constraints) to automatically generate complex, verifiable questions and rationales. This allows for fine-grained evaluation of both LLM answers and their underlying reasoning process without human annotation. Similarly, \cite{li2024qrj} presents a robust LLM-based detection framework within \textit{HaluEval 2.0}, where GPT-4 is leveraged to extract factual statements from LLM responses and then judge their truthfulness, even considering interrelations between statements, effectively using a powerful LLM as its own internal arbiter of factuality.

Another significant direction involves integrating consistency-based detection directly into the LLM's generation and reasoning process, enabling self-correction. \textit{Chain-of-Verification (CoVe)} \cite{dhuliawala2023rqn} empowers LLMs to self-critique by generating a baseline response, then planning and executing specific verification questions. Crucially, its "factored" variant answers each verification question independently, preventing the LLM from repeating its own initial hallucinations by avoiding conditioning on the potentially incorrect baseline. This systematic self-deliberation significantly reduces factual errors. Extending this, an iterative \textit{Self-Reflection} methodology \cite{ji2023vhv} enables LLMs to dynamically acquire and refine background knowledge, and subsequently generate and refine answers until factual consistency and entailment are achieved. This continuous feedback loop within the LLM's own reasoning process is particularly effective in high-stakes domains like medical generative question-answering.

Finally, real-time, reference-free detection can also be achieved by analyzing internal model uncertainty. The \textit{Real-time Hallucination Detection (RHD)} component within the \textit{Dynamic Retrieval Augmentation based on hallucination Detection (DRAD)} framework \cite{su2024gnz} identifies potential hallucinations by analyzing the uncertainty (low predictive probability and high entropy) of output entities. This method operates without external models or multiple responses, providing an immediate, model-agnostic signal for triggering targeted retrieval and self-correction.

In summary, reference-free and consistency-based detection methods offer a powerful paradigm for evaluating LLM factuality and coherence, especially for black-box models and large-scale deployment. Techniques ranging from stochastic sampling and metamorphic relations to automated dataset generation and internal self-verification mechanisms provide flexible and efficient alternatives to traditional reference-dependent evaluations. However, challenges remain in ensuring the robustness of LLM-as-judge approaches, managing the computational cost of multiple samples or iterative refinement, and generalizing these detection mechanisms across the full spectrum of hallucination types and domains. Future work will likely focus on refining the reliability of internal consistency checks and developing more computationally efficient self-contained verification processes.
\subsection{Fine-Grained and Rationale-Based Evaluation}
\label{sec:3\_2\_fine-grained\_\_and\_\_rationale-based\_evaluation}

Traditional evaluation metrics for Large Language Models (LLMs) often fall short by only assessing the correctness of a final answer, failing to diagnose the underlying reasoning failures that lead to hallucinations. This limitation has spurred the development of advanced evaluation methodologies focused on providing a deeper, more granular understanding of LLM outputs, enabling token-level or sentence-level error analysis, and critically, verifying the LLM's reasoning process or "rationales."

Early work by \cite{maynez2020h3q} highlighted the inadequacy of surface-level metrics like ROUGE for assessing faithfulness and factuality in abstractive summarization, introducing the crucial distinction between intrinsic and extrinsic hallucinations and advocating for semantically-aware evaluation. Building on this, \cite{liu2021mo6} pioneered the first token-level, reference-free hallucination detection benchmark, HADES, which allows for precise identification of hallucinated content within free-form text, moving beyond coarse sentence or document-level assessments. Further refining this fine-grained analysis, \cite{rawte2023ao8} proposed a comprehensive taxonomy of hallucination, categorizing it by degree, orientation, and specific types, alongside the Hallucination Vulnerability Index (HVI) for quantifiable and nuanced assessment of LLM susceptibility. Similarly, \cite{li2024qrj} developed HaluEval 2.0, an LLM-based detection framework that extracts and judges factual statements at a fine-grained level, considering interrelations between statements. For Chinese LLMs, \cite{liang20236sh} introduced UHGEval, a benchmark for unconstrained generation, and proposed }kwPrec` (keyword precision) to measure factual relevance at a granular level. Extending the scope of fine-grained evaluation, \cite{rejeleene2024okw} offered a mathematical formulation for Information Quality (IQ) based on consistency, relevance, and accuracy, providing a structured framework for evaluating LLM outputs beyond simple correctness.

A significant advancement in evaluation methodologies is the focus on verifying the LLM's reasoning process, often by leveraging structured data or internal self-reflection mechanisms. \cite{oh2024xa3} introduced ERBench, a groundbreaking benchmark that utilizes relational databases and their integrity constraints (Functional Dependencies and Foreign Key Constraints) to automatically generate complex questions and, crucially, verify both the LLM's answer and its underlying rationale by checking for critical keywords. This approach provides an objective and scalable method for assessing logical steps. Complementing this, \cite{gao2023ht7} developed ALCE, a benchmark that enables LLMs to generate text with explicit citations, which serve as verifiable rationales. They introduced NLI-based metrics to automatically evaluate the recall and precision of these citations, ensuring that generated statements are adequately supported by evidence.

Beyond external structured data, methods have emerged that prompt LLMs to self-evaluate their own reasoning. \cite{dhuliawala2023rqn} proposed Chain-of-Verification (CoVe), a multi-step process where an LLM generates verification questions about its own draft response and then answers them independently to reduce factual hallucinations. This internal deliberation process provides a form of self-generated rationale verification. Similarly, \cite{wen2023t6v} introduced MindMap, which prompts LLMs to reason over knowledge graphs and explicitly generate a "mind map" (decision tree) of their reasoning pathways, thereby enhancing transparency and verifiability of the logical steps. The concept of self-reflection is further explored by \cite{ji2023vhv}, who developed an iterative self-reflection methodology for medical generative question-answering, allowing LLMs to generate, score, and refine their own background knowledge and answers until factual consistency is achieved. In a similar vein, \cite{li2023v3v}'s Chain-of-Knowledge framework emphasizes progressive rationale correction by dynamically adapting knowledge from heterogeneous sources, ensuring that errors are rectified sequentially to prevent propagation.

The drive for scalable and diagnostic evaluation has also led to automated benchmarking and advanced detection techniques. \cite{cao2023ecl} presented AutoHall, a pipeline for automatically generating model-specific hallucination datasets from existing fact-checking data, and a zero-resource detection method based on LLM self-contradiction, which implicitly checks internal consistency. \cite{yang20251dw} introduced MetaQA, a self-contained hallucination detection approach that uses metamorphic relations and prompt mutation to expose factual inconsistencies by generating and verifying diverse response mutations. For Retrieval-Augmented Generation (RAG) systems, \cite{chen2023h04} developed RGB, a diagnostic benchmark that evaluates LLMs across four critical RAG abilities (Noise Robustness, Negative Rejection, Information Integration, and Counterfactual Robustness), pinpointing specific reasoning failures in how models interact with external knowledge. Furthermore, \cite{chen2024c4k} addressed the complexities of conversational AI with DiaHalu, a dialogue-level hallucination evaluation benchmark that provides a comprehensive taxonomy for assessing both factuality and faithfulness across multi-turn interactions. Lastly, \cite{su2024gnz} proposed Dynamic Retrieval Augmentation based on hallucination Detection (DRAD), which utilizes real-time, entity-level uncertainty to detect potential hallucinations and trigger targeted retrieval, offering a fine-grained and adaptive approach to error mitigation.

These fine-grained and rationale-based evaluations are crucial for diagnosing complex reasoning failures and improving model transparency and verifiability, thereby fostering greater confidence in AI-generated content. However, challenges remain, including the inherent difficulty in precisely defining "correct" rationales for all tasks, the scalability of human annotation even with automated assistance, and the persistent "black-box" nature of many LLMs. The theoretical inevitability of hallucination, as posited by \cite{xu2024n76}, further underscores that the focus must shift from complete elimination to robust management through sophisticated detection and transparent reasoning. Future research will likely concentrate on developing more robust automatic rationale verification mechanisms, integrating diverse and dynamic knowledge sources for richer rationales, and fostering LLMs that can not only provide answers but also explain \textit{why} they arrived at a particular conclusion.
\subsection{Benchmarking Retrieval-Augmented Generation (RAG)}
\label{sec:3\_3\_benchmarking\_retrieval-augmented\_generation\_(rag)}

The pervasive issue of hallucination in Large Language Models (LLMs), characterized by the generation of factually incorrect or unfaithful content \cite{maynez2020h3q}, necessitates robust mitigation strategies. Retrieval-Augmented Generation (RAG) has emerged as a promising paradigm to ground LLMs in external, verifiable knowledge, thereby reducing factual errors and enhancing the credibility of generated content. However, the effectiveness of RAG systems is not uniform, and their performance in mitigating hallucination requires specialized diagnostic benchmarks that rigorously evaluate how LLMs interact with and utilize retrieved information. This section focuses on such benchmarks, designed to identify specific failure modes of RAG-augmented LLMs and guide improvements in retrieval mechanisms.

A foundational diagnostic framework in this area is the Retrieval-Augmented Generation Benchmark (RGB) introduced by \cite{chen2023h04}. RGB is a multi-lingual corpus specifically designed to assess four critical RAG capabilities: Noise Robustness, Negative Rejection, Information Integration, and Counterfactual Robustness. Their evaluation of state-of-the-art LLMs revealed that while RAG can improve accuracy, models often struggle with these fundamental challenges, such as confusing similar information when noise is present, failing to reject answering when no relevant information is available, and exhibiting a significant lack of ability to synthesize facts from multiple documents. Crucially, \cite{chen2023h04} found that LLMs tend to trust and prioritize factually incorrect retrieved information even when possessing correct internal knowledge and being explicitly warned, highlighting a critical counterfactual robustness issue.

Beyond general RAG capabilities, benchmarks also focus on specific aspects of verifiable generation. \cite{gao2023ht7} introduced ALCE (Automatic LLMs' Citation Evaluation), the first reproducible benchmark for evaluating end-to-end systems that retrieve evidence, generate answers, and provide explicit citations. This benchmark addresses the trustworthiness of RAG outputs by providing automatic metrics for correctness and, uniquely, for citation quality (citation recall and precision). Their experiments demonstrated that even advanced models like GPT-4 show significant room for improvement, with a substantial portion of generations lacking complete citation support, indicating challenges in grounding and attributing generated content.

The dynamic nature of real-world knowledge and the need for up-to-date information present another critical area for RAG benchmarking. \cite{vu202337s} developed FRESH QA, a dynamic question-answering benchmark designed to test LLMs augmented with search engines on fast-changing and false-premise knowledge. Their two-mode evaluation (RELAXED for primary answer correctness and STRICT for overall factual accuracy) revealed that baseline LLMs struggle significantly with current and evolving information, and even with search augmentation, models still face challenges with false premises. FRESH QA highlights that simply scaling LLMs or providing search results is insufficient to guarantee factual accuracy, necessitating more robust RAG strategies.

For RAG systems that leverage structured knowledge, specialized benchmarks are emerging. \cite{sui20242u1} introduced OKGQA, a benchmark for evaluating LLMs augmented with Knowledge Graphs (KGs) in open-ended question answering, specifically designed to measure hallucination reduction and nuanced reasoning. They also proposed OKGQA-P to assess model robustness under deliberately perturbed KGs, showing that while KGs generally reduce factual errors, LLMs still face challenges in reasoning over structured data, especially when the KG itself contains noise. Complementing this, \cite{oh2024xa3} developed ERBench, a novel approach to construct automatically verifiable hallucination benchmarks from relational databases (RDBs). ERBench uniquely leverages database integrity constraints, such as Functional Dependencies (FDs) and Foreign Key Constraints (FKCs), to generate complex multi-hop questions and, critically, to automatically verify the correctness of both the LLM's answer and its underlying rationale. This provides a fine-grained diagnostic tool for evaluating the factual grounding of RAG systems, moving beyond mere answer accuracy to assess the integrity of the reasoning process.

Finally, as RAG systems are increasingly deployed in interactive settings, dialogue-level hallucination becomes a critical concern. \cite{chen2024c4k} introduced DiaHalu, the first dedicated benchmark for evaluating hallucination in multi-turn dialogues. While not exclusively for RAG, the challenges identifiedincluding non-factual, incoherent, irrelevant, over-reliant, and reasoning error hallucinationsare highly pertinent to RAG systems operating in conversational contexts. DiaHalu's comprehensive taxonomy and challenging nature indicate that current models and detection methods struggle with the complexities of dialogue-level consistency and coherence, underscoring the need for RAG systems to be evaluated and improved in these interactive scenarios.

In conclusion, these specialized benchmarks collectively reveal that while RAG offers a powerful approach to mitigate hallucination, current LLMs still exhibit significant limitations in effectively utilizing external knowledge. Challenges persist in handling noisy or conflicting information, rejecting irrelevant contexts, integrating facts from multiple sources, generating verifiable citations, adapting to dynamic knowledge, reasoning over structured data, and maintaining consistency in dialogue. Future research must leverage these diagnostic frameworks to develop more robust RAG mechanisms that can intelligently process, synthesize, and attribute external knowledge, ultimately leading to more dependable and trustworthy AI applications.


\label{sec:mitigation_strategies:_external_grounding_and_reasoning}

\section{Mitigation Strategies: External Grounding and Reasoning}
\label{sec:mitigation\_strategies:\_external\_grounding\_\_and\_\_reasoning}

\subsection{Retrieval-Augmented Generation (RAG) and Knowledge Graphs}
\label{sec:4\_1\_retrieval-augmented\_generation\_(rag)\_\_and\_\_knowledge\_graphs}

Large Language Models (LLMs) often suffer from "hallucinations," generating plausible but factually incorrect or outdated information due to reliance on their static internal knowledge \cite{xu2024n76, maynez2020h3q}. To address this, Retrieval-Augmented Generation (RAG) and Knowledge Graphs (KGs) have emerged as critical strategies, providing LLMs with access to external, up-to-date, and verifiable information, thereby enhancing factual consistency and credibility \cite{tonmoy20244e4, liu2024gxh, pan2024y3a}.

The evolution of RAG began with the recognition that LLMs need dynamic interaction with external knowledge sources. Early approaches, such as \cite{yao20229uz}'s ReAct, introduced the paradigm of interleaving reasoning "thoughts" with task-specific "actions" (e.g., API calls for retrieval), enabling LLMs to dynamically interact with external environments and ground their responses. Building on this, \cite{trivedi2022qsf} further refined dynamic retrieval by proposing Interleaving Retrieval with Chain-of-Thought (IRCoT), which uses intermediate reasoning steps as queries for iterative knowledge retrieval, significantly reducing factual errors in knowledge-intensive multi-step question answering.

As RAG systems matured, the need for robust evaluation and improved context handling became evident. To enhance verifiability, \cite{gao2023ht7} introduced ALCE, a benchmark and evaluation framework that enables LLMs to generate text with explicit citations to retrieved passages, directly addressing trustworthiness. However, RAG still faces challenges, as highlighted by \cite{chen2023h04}'s RGB benchmark, which systematically evaluates RAG capabilities like noise robustness, negative rejection (refusing to answer when no information is found), and information integration. Their findings revealed that LLMs often struggle with conflicting or noisy retrieved information and may even prioritize incorrect external facts over their own internal knowledge. To combat the issue of LLMs "getting lost in long contexts" with irrelevant information, \cite{lv2024k5x} proposed Coarse-to-Fine Highlighting (COFT), an intelligent method that dynamically identifies and emphasizes key information at varying granularities within lengthy retrieved documents, improving factual accuracy by guiding the LLM's focus. Furthermore, to ensure LLMs are refreshed with current events, \cite{vu202337s} developed FreshLLMs and FRESH PROMPT, a training-free method that augments LLMs with real-time search engine results, demonstrating substantial improvements in factuality for rapidly changing world knowledge. Moving towards more intelligent RAG, \cite{su2024gnz} introduced Dynamic Retrieval Augmentation based on hallucination Detection (DRAD), a framework that conditionally triggers retrieval \textit{only when} a potential hallucination is detected (based on entity-level uncertainty), thereby making the RAG process more efficient and targeted.

Beyond unstructured text retrieval, Knowledge Graphs (KGs) offer a structured, explicit, and verifiable form of external knowledge. Early work demonstrated the potential of KGs in reducing hallucination, such as \cite{dziri2021bw9}'s Neural Path Hunter, which used KG path grounding to identify and correct erroneous entity mentions in dialogue systems. Advancing KG integration, \cite{wen2023t6v}'s MindMap enabled LLMs to \textit{comprehend and reason over structured graphical inputs}, synergistically combining explicit KG facts with implicit LLM knowledge and generating an explainable "mind map" of the reasoning process. This approach helps overcome the black-box nature of LLMs and provides transparent, grounded reasoning. Further extending this, \cite{li2023v3v}'s Chain-of-Knowledge introduced a framework for dynamic knowledge adapting over \textit{heterogeneous} sources, including both structured KGs and unstructured text, featuring an Adaptive Query Generator for various query languages and a progressive rationale correction mechanism to prevent error propagation across reasoning steps. The effectiveness of KGs in enhancing trustworthiness was empirically validated by \cite{sui20242u1}, who introduced the OKGQA benchmark for open-ended question answering. Their study demonstrated that KG integration significantly reduces hallucinations even when the KG itself is partially contaminated, particularly for complex reasoning tasks. Complementing this, \cite{oh2024xa3}'s ERBench leveraged relational databases (structurally similar to KGs) to create automatically verifiable benchmarks, allowing for fine-grained evaluation of LLM factual consistency and the correctness of their generated rationales.

Despite these advancements, challenges remain. The quality and coverage of retrieved documents or KGs are paramount; irrelevant or incorrect external information can still mislead LLMs, as shown by \cite{chen2023h04}. The computational overhead of dynamic retrieval and complex KG reasoning can be substantial, and effectively integrating heterogeneous knowledge sources while maintaining coherence and preventing new forms of hallucination is an ongoing research area. Future work needs to focus on more robust mechanisms for filtering noisy retrieved content, developing more sophisticated KG-LLM fusion architectures that truly understand and leverage graph structures, and creating adaptive retrieval strategies that can dynamically choose the most appropriate external knowledge source (text, KG, or specialized tool) based on the query and context.
\subsection{Synergizing Reasoning and Acting (ReAct)}
\label{sec:4\_2\_synergizing\_reasoning\_\_and\_\_acting\_(react)}

Large Language Models (LLMs) frequently suffer from ungrounded hallucinations, generating plausible but factually incorrect information due to their reliance on internal, potentially outdated or fallacious, knowledge \cite{maynez2020h3q}. To address this, a critical paradigm shift involves intertwining an LLM's internal reasoning ("thoughts") with external actions (e.g., API calls, tool use) to dynamically gather information and verify facts, thereby enhancing factual grounding and robustness.

A seminal contribution in this area is the \textit{ReAct} framework, introduced by \cite{yao20229uz}, which synergizes reasoning and acting in language models. ReAct prompts LLMs to generate interleaved verbal reasoning traces ("thoughts") and task-specific actions, allowing the model to dynamically formulate plans ("reason to act") and interact with external environments (e.g., Wikipedia API) to gather information and refine its understanding ("act to reason"). This approach directly mitigates hallucination by grounding LLM responses in verifiable external information, demonstrating significant few-shot performance gains in knowledge-intensive tasks like HotpotQA and FEVER, as well as interactive decision-making benchmarks \cite{yao20229uz}. While ReAct showcased the power of this synergy, its prompting setup had limitations for highly complex behaviors and faced challenges with data scarcity for large-scale annotation.

Building upon the principle of dynamic reasoning and external information retrieval, \cite{trivedi2022qsf} proposed \textit{Interleaving Retrieval with Chain-of-Thought Reasoning (IRCoT)} for knowledge-intensive multi-step questions. IRCoT refines the "act to reason" component by iteratively using intermediate Chain-of-Thought sentences as dynamic queries for knowledge retrieval, thereby overcoming the limitations of static, one-shot retrieval. This adaptive information-seeking mechanism significantly improved retrieval recall and reduced factual errors in generated reasoning steps by up to 50\\%, even outperforming much larger models with simpler retrieval methods \cite{trivedi2022qsf}. IRCoT thus demonstrated a more granular and efficient way to integrate external knowledge during complex reasoning, making the grounding process more precise.

Beyond merely incorporating external information, ensuring the verifiability and trustworthiness of LLM outputs is paramount. \cite{gao2023ht7} addressed this by enabling LLMs to generate text with explicit citations through their \textit{ALCE (Automatic LLMs Citation Evaluation)} benchmark. ALCE provides a reproducible framework and novel NLI-based metrics to evaluate not only the fluency and correctness of generated text but also the quality of citations, including citation recall and precision \cite{gao2023ht7}. This work directly tackles the problem of ungrounded hallucination by providing a mechanism for LLMs to explicitly link their statements to supporting evidence, thereby enhancing the trustworthiness and interpretability of their responses, a crucial step in operationalizing the external grounding achieved by ReAct-like paradigms.

Despite the promise of these retrieval-augmented and reasoning-enhanced approaches, a critical evaluation of their robustness and limitations is essential. \cite{chen2023h04} provided a comprehensive assessment of Retrieval-Augmented Generation (RAG) systems, which encompass ReAct-like frameworks, through their \textit{RGB} benchmark. RGB systematically diagnoses LLMs' capabilities across four crucial dimensions: Noise Robustness, Negative Rejection, Information Integration, and Counterfactual Robustness \cite{chen2023h04}. Their findings revealed that even state-of-the-art LLMs struggle significantly with these challenges, often failing to reject answering when no relevant information is available, confusing similar noisy information, or even prioritizing factually incorrect retrieved information over their own internal knowledge, despite explicit warnings \cite{chen2023h04}. This highlights that simply providing external tools or dynamic retrieval does not automatically guarantee robust, hallucination-free behavior.

In conclusion, synergizing reasoning and acting, exemplified by the ReAct framework, represents a significant advancement in mitigating LLM hallucination by providing mechanisms for external validation and dynamic information gathering. Approaches like IRCoT have refined this synergy for complex multi-step reasoning, while benchmarks such as ALCE have pushed for greater verifiability through explicit citations. However, as critically evaluated by RGB, current systems still face substantial challenges in effectively handling noisy, irrelevant, or conflicting external information. These limitations underscore that while external grounding is vital, the complete elimination of hallucination remains an elusive goal, a theoretical inevitability for all computable LLMs \cite{xu2024n76}. Future research must therefore focus on enhancing LLMs' ability to critically evaluate retrieved information, integrate diverse knowledge sources robustly, and develop more sophisticated error-handling mechanisms in interactive environments, moving towards more dependable and trustworthy AI agents.
\subsection{Self-Correction and Verification Mechanisms}
\label{sec:4\_3\_self-correction\_\_and\_\_verification\_mechanisms}

To enhance the intrinsic dependability and logical coherence of Large Language Model (LLM) outputs, a critical area of research focuses on self-correction and verification mechanisms. These techniques empower LLMs to critically evaluate their own generated content, identify potential errors, and perform revisions, often leveraging automated feedback to mimic human learning strategies \cite{pan2023mwu, pan2024y3a}. Such internal scrutiny is vital for fostering greater confidence and accountability in their autonomous operation across various applications.

One prominent approach is \textit{Chain-of-Verification} (CoVe) \cite{dhuliawala2023rqn}, a multi-step process designed to enable an LLM to deliberate on and correct its own responses. CoVe first generates a baseline response, then plans specific verification questions to fact-check claims within that draft. Crucially, CoVe executes these verification questions independently, particularly through its "factored" variant, to prevent the model from repeating its initial hallucinations by conditioning on potentially incorrect prior generations \cite{dhuliawala2023rqn}. This systematic self-deliberation significantly reduces factual errors and improves precision across various tasks, including list generation and longform biographies, without relying on external tools \cite{dhuliawala2023rqn}.

Building on the idea of iterative refinement, \textit{self-reflection} mechanisms empower LLMs to dynamically evaluate and refine their responses. \cite{ji2023vhv} introduces an iterative self-reflection methodology, particularly effective in high-stakes domains like medical generative question-answering. This method involves a dynamic feedback loop where the LLM first acquires and fact-checks background knowledge, then generates an answer, and subsequently scores its consistency and entailment with the original question \cite{ji2023vhv}. If discrepancies are found, the model iteratively refines both the knowledge and the answer, demonstrating superior performance in hallucination reduction and consistency across diverse medical datasets \cite{ji2023vhv}.

Beyond explicit multi-step reasoning, mechanisms for LLMs to recognize their own uncertainty and appropriately abstain from answering, or trigger targeted corrections, are vital. \cite{su2024gnz} proposes Dynamic Retrieval Augmentation based on hallucination Detection (DRAD), which integrates a Real-time Hallucination Detection (RHD) module. RHD identifies potential hallucinations by analyzing the uncertainty of output entitiesspecifically, those with low predictive probability and high semantic entropywithout requiring external models or multiple generations \cite{su2024gnz}. This allows for conditional and efficient retrieval augmentation, where external knowledge is sought and used for self-correction only when a hallucination is detected, thereby mitigating entity-level hallucinations more effectively than indiscriminate retrieval \cite{su2024gnz}. This approach enhances efficiency by focusing corrective actions precisely where and when they are most needed.

While these self-correction and verification mechanisms significantly enhance the dependability and logical coherence of LLM outputs, challenges remain. The computational overhead of multi-step reasoning and iterative refinement, especially for complex tasks, needs optimization to ensure real-time applicability. Furthermore, effectively integrating these internal self-critique capabilities with external knowledge sources, ensuring robust error identification across all hallucination types (e.g., subtle logical inconsistencies), and developing more sophisticated metrics for evaluating the \textit{quality} of self-correction itself are crucial directions for future research.


\label{sec:mitigation_strategies:_internal_model_interventions_and_training}

\section{Mitigation Strategies: Internal Model Interventions and Training}
\label{sec:mitigation\_strategies:\_internal\_model\_interventions\_\_and\_\_training}

\subsection{Decoding-Time Strategies and Logit Manipulation}
\label{sec:5\_1\_decoding-time\_strategies\_\_and\_\_logit\_manipulation}

Hallucinations in large language models (LLMs) and multimodal large language models (MLLMs) pose significant challenges to their trustworthiness and reliability. While retraining models can be effective, it is often computationally expensive and time-consuming. Consequently, a growing body of research focuses on efficient, flexible, and training-free strategies that intervene during the token generation (decoding) process to prevent hallucinations by directly influencing the probability distribution of generated tokens or steering the model's internal states. These decoding-time interventions leverage internal uncertainty or external signals to guide LLMs towards more factual and coherent outputs on-the-fly.

A prominent category of decoding-time strategies is \textit{contrastive decoding}, which penalizes tokens inconsistent with a reference or a subtly altered input. While effective in reducing reliance on language priors, traditional contrastive decoding methods often suffer from increased inference latency and potential degradation of content quality \cite{yin2025s2b}. Addressing these limitations, \cite{park20247cm} introduced \textit{ConVis}, a novel contrastive decoding method that leverages a Text-to-Image (T2I) model to visualize potential hallucinations from an initial caption. By comparing logit distributions from the original image and the T2I-reconstructed image, ConVis amplifies and penalizes tokens corresponding to visualized hallucinations, offering a training-free approach to enhance visual grounding.

Building upon the insights and limitations of contrastive decoding, several works have explored more direct \textit{logit-based steering} or manipulation of internal model states. \cite{yin2025s2b} proposed \textit{Visual Amplification Fusion (VAF)} as part of their ClearSight framework, a plug-and-play technique that directly enhances attention to visual signals within the MLLM's middle layers during inference. Unlike contrastive decoding, VAF avoids additional forward passes, thereby preserving content quality and inference speed while mitigating object hallucinations by addressing insufficient attention to visual information. Similarly, \cite{zou2024dp7} introduced \textit{Memory-space Visual Retracing (MemVR)}, which dynamically re-injects visual tokens as supplementary evidence into the MLLM's Feed Forward Networks (FFNs) at intermediate layers, triggered by high token uncertainty. This method directly enhances hidden states, offering superior efficiency and performance compared to prior attention intervention and contrastive decoding methods.

Further refining internal state manipulation, \cite{zhou2024lvp} presented \textit{CAUSAL MM}, a causal inference framework that mitigates modality prior-induced hallucinations by deciphering attention causality. It applies back-door adjustment and counterfactual reasoning to visual and language attention mechanisms during decoding, ensuring outputs are more aligned with multimodal inputs by causally balancing modality priors. Extending this idea of targeted intervention, \cite{chen2024j0g} developed \textit{ICT (Image-Object Cross-Level Trusted Intervention)}, a training-free method that calculates an "intervention direction" to shift the model's focus towards different levels of visual information (image-level and object-level) by manipulating specific attention heads during the forward pass. This approach enhances visual grounding without indiscriminately eliminating beneficial language priors, a common drawback of some contrastive methods, and introduces no additional inference latency.

Beyond direct logit or attention manipulation, other inference-time strategies leverage internal model uncertainty or external signals for adaptive steering. \cite{qu2024pqc} introduced \textit{MVP (Multi-View Multi-Path Reasoning)}, a training-free and tool-free framework that maximizes the innate capabilities of existing LVLMs. MVP enhances image comprehension through multi-view information seeking (generating captions from different perspectives) and employs a multi-path, certainty-driven reasoning mechanism that selects the answer with the highest aggregated certainty score across multiple decoding paths and information views. This approach effectively mitigates hallucinations by leveraging the correlation between low token certainty and hallucination occurrence.

Another class of decoding-time strategies involves external tool-augmented or iterative refinement frameworks. \cite{yin2023hx3} proposed \textit{Woodpecker}, a training-free, five-stage post-remedy pipeline that corrects hallucinations by extracting key concepts, formulating questions, validating visual knowledge with expert models (e.g., Grounding DINO, BLIP-2), and then using an LLM to generate a corrected response with explicit bounding box evidence. While effective, Woodpecker's reliance on multiple external models can introduce dependencies and computational overhead. To address the diversity of hallucination types and the "one-size-fits-all" limitation of earlier methods, \cite{chang2024u3t} developed "Dentist," a unified framework that first classifies the query type (perception or reasoning) and then applies a tailored mitigation strategy within an iterative validation loop. For perception queries, it uses visual verification with sub-questions, while for reasoning queries, it employs Chain-of-Thought (CoT) prompting, iteratively refining answers until semantic convergence. Finally, \cite{kim2024ozf} introduced \textit{Counterfactual Inception}, a training-free method that prompts LMMs to self-generate "counterfactual keywords" (filtered by a CLIP-based Plausibility Verification Process) that intentionally deviate from visual content. The model is then instructed to avoid these keywords, thereby engaging in counterfactual thinking to mitigate hallucinations. This approach steers the model's output by shaping its internal reasoning context through prompt engineering, without direct logit manipulation.

In conclusion, decoding-time strategies offer a flexible and efficient paradigm for mitigating hallucinations in LLMs and MLLMs without costly retraining. The field has evolved from initial contrastive decoding methods to more nuanced logit and internal state manipulation techniques that directly enhance visual grounding or balance modality priors. Furthermore, uncertainty-driven reasoning and adaptive, tool-augmented frameworks provide robust post-hoc correction and iterative refinement. A key challenge remains in developing methods that are truly "tool-free" and solely leverage the model's internal capabilities, while also ensuring that these interventions do not inadvertently compromise other desirable generative qualities like fluency or creativity. Future research will likely focus on combining these diverse strategies, exploring more dynamic and context-aware interventions, and further understanding the causal mechanisms of hallucination during generation.
\subsection{Training-Based Approaches and Preference Optimization}
\label{sec:5\_2\_training-based\_approaches\_\_and\_\_preference\_optimization}

Training-based approaches aim to embed hallucination resistance directly into Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) during their learning phases, fostering an intrinsic capability to avoid factual inaccuracies. These proactive strategies reduce reliance on post-hoc corrections by building robustness from the ground up, typically through robust instruction tuning or sophisticated preference optimization techniques.

Initial efforts focused on modifying pre-training objectives to enhance visual grounding. \cite{dai20229aa} provided a systematic investigation into object hallucination in Vision-Language Pre-training (VLP) models, revealing that optimizing for standard metrics could paradoxically increase hallucination. To counter this, they proposed ObjMLM (Object-Masked Language Modeling), a novel pre-training objective that improves token-level image-text alignment and controlled generation, thereby reducing object hallucination by up to 17.4\\%. Building on the idea of fine-tuning, \cite{wang2023ubf} addressed fine-grained object hallucination (incorrect attributes or behaviors) by introducing \textit{ReCaption}. This framework leverages a two-stage prompting strategy with ChatGPT to generate diverse, high-quality rewritten captions for training images, which are then used to fine-tune LVLMs, effectively mitigating these granular errors.

Beyond specific fine-tuning data, generalizable training techniques have also been explored. \cite{wu2024n00} introduced NoiseBoost, a simple yet effective method that injects Gaussian noise into projected visual tokens during Supervised Fine-tuning (SFT), Reinforcement Learning (RL), or Semi-Supervised Learning (SSL). This perturbation forces the MLLM to distribute its attention more evenly between visual and linguistic tokens, reducing over-reliance on language priors and improving hallucination accuracy by 8.1\\% in human evaluations of dense captions.

More advanced training-based strategies involve preference optimization, which leverages AI-generated or human-annotated feedback to align model behavior with desired factual accuracy. \cite{xiao2024hv1} proposed a comprehensive framework for detecting and mitigating hallucination in Large Vision Language Models (LVLMs) via fine-grained AI feedback. Their approach includes a "detect-then-rewrite" pipeline to construct large-scale, fine-grained preference datasets and introduces Hallucination Severity-Aware Direct Preference Optimization (HSA-DPO). This novel DPO variant incorporates hallucination severity into the objective, prioritizing the mitigation of critical errors and reducing hallucination rates by 36.1\\% on the AMBER benchmark. While effective, such methods can be computationally expensive due to their reliance on large models for feedback and verification.

To address the efficiency and potential bias concerns of MLLM-as-judge approaches, \cite{deng202405j} developed an efficient "model-level judge-free" self-improvement framework for MLLMs. This framework generates controllable negative samples by blending conditional and unconditional decoding paths, and then uses a lightweight, objective verifier (a CLIP model) to invert preference labels based on average sentence-level CLIP scores. This refined preference data is subsequently used for Direct Preference Optimization (DPO), offering a more scalable and robust alternative to expensive MLLM-based reward models.

In conclusion, training-based approaches have evolved from modifying foundational pre-training objectives to sophisticated instruction tuning with augmented data and advanced preference optimization techniques. While significant progress has been made in embedding hallucination resistance intrinsically, challenges remain in scaling high-quality, unbiased preference data generation, particularly for complex multimodal scenarios. Future research will likely focus on developing more efficient and robust "judge-free" mechanisms and exploring novel architectural designs that inherently balance modality priors to further instill hallucination avoidance as a core capability.
\subsection{Internal State Manipulation and Causal Interventions}
\label{sec:5\_3\_internal\_state\_manipulation\_\_and\_\_causal\_interventions}

Advanced mitigation techniques for large language models (LLMs) are increasingly moving beyond superficial output filtering to directly manipulate the internal workings of models, such as their hidden states or attention mechanisms. This paradigm shift offers fine-grained control and mechanistic insights into model failures, enabling more precise and targeted corrections that enhance factual accuracy and reduce spurious correlations.

One prominent approach involves actively re-injecting or enhancing visual information within the model's intermediate layers to combat "visual amnesia" and strengthen visual grounding. \cite{zou2024dp7} introduced Memory-space Visual Retracing (MemVR), a decoding paradigm that re-injects visual tokens as supplementary evidence into the model's Feed Forward Network (FFN) at a "middle trigger layer." This "look-twice" mechanism is dynamically activated based on the model's uncertainty, effectively refreshing visual memory and mitigating hallucinations by balancing modality attention. Complementing this, \cite{yin2025s2b} proposed Visual Amplification Fusion (VAF), a plug-and-play technique that enhances attention to visual signals specifically within the Multimodal Large Language Model's (MLLM) middle layers (8th-15th), where modality fusion predominantly occurs. Unlike contrastive decoding methods that can compromise content quality, VAF directly amplifies visual features without additional forward passes, preserving inference speed and output coherence. Further refining this concept, \cite{chen2024j0g} presented Image-Object Cross-Level Trusted Intervention (ICT), a training-free method that calculates an "intervention direction" to shift the model's focus towards different levels of visual information during the forward pass. ICT identifies and intervenes on specific attention heads responsible for encoding overall image information or fine-grained object details, leveraging trusted and untrusted data pairs to derive activation shift vectors. This targeted head-level intervention mitigates excessive reliance on language priors while preserving beneficial ones, addressing a key limitation of indiscriminate suppression techniques.

Beyond direct feature manipulation, researchers are applying causal inference to attention mechanisms to understand and balance modality priors, offering a principled approach to mitigate modality-induced hallucinations. \cite{zhou2024lvp} developed CAUSAL MM, a causal inference framework that applies structural causal modeling (SCM) to MLLMs. By treating modality priors (visual and language) as confounding factors, CAUSAL MM employs back-door adjustment and counterfactual reasoning at both visual encoder and LLM backbone attention levels. This allows for deciphering the causal impact of effective attention on MLLM output by isolating the effects of modality priors, leading to more balanced multimodal outputs and a significant reduction in hallucination.

Mechanistic insights into model failures, sometimes derived from adversarial attacks, also inform targeted internal interventions. \cite{wang2025jen} revealed that MLLM hallucinations can be induced by exploiting the "attention sink" phenomenon, where columnar patterns of high attention scores emerge at the turning point of image-text relevance, aggregating misleading global information. This analysis of attention sink vulnerabilities provides a critical target for internal state manipulation, suggesting that interventions could be designed to counteract such misleading information aggregation. Even training-based methods, while not inference-time interventions, offer insights into internal state manipulation. For instance, \cite{wu2024n00}'s NoiseBoost injects Gaussian noise into projected visual tokens during supervised fine-tuning or reinforcement learning. This perturbation increases the "hardship" in visual understanding, compelling the MLLM to distribute its attention weights more evenly between visual and linguistic tokens, thereby reducing over-reliance on language priors. While applied during training, this method directly manipulates internal visual representations to influence the dynamics of attention, providing a foundational understanding for similar inference-time adjustments.

Collectively, these methods represent a significant step towards developing more robust and trustworthy MLLMs by moving beyond external fixes to address the root causes of hallucinations within the model's internal processing. The ability to precisely control attention, re-inject salient visual features, and apply causal reasoning to balance modality influences offers unprecedented control. However, challenges remain in dynamically identifying optimal intervention points and magnitudes across diverse tasks and model architectures, and in ensuring these interventions do not inadvertently suppress creativity or beneficial abstract reasoning. Future work will likely explore adaptive, real-time internal interventions that can learn and adjust based on contextual demands, potentially integrating uncertainty quantification with causal models for more intelligent and dynamic hallucination mitigation.


\label{sec:applications,_specialized_domains,_and_multimodal_hallucination}

\section{Applications, Specialized Domains, and Multimodal Hallucination}
\label{sec:applications,\_specialized\_domains,\_\_and\_\_multimodal\_hallucination}

\subsection{Characterizing Multimodal and Domain-Specific Hallucinations}
\label{sec:6\_1\_characterizing\_multimodal\_\_and\_\_domain-specific\_hallucinations}

Hallucinations in multimodal Large Language Models (LLMs), such as Large Vision-Language Models (LVLMs) and Large Audio-Language Models (LALMs), present distinct and often more complex challenges than those observed in their unimodal counterparts. These models frequently generate content that is plausible but factually inconsistent with the visual or audio input, a phenomenon largely attributed to the inherent "modality gap" and the struggle to robustly align heterogeneous data streams with linguistic generation \cite{liu2024sn3}. This necessitates a specialized understanding and evaluation framework beyond those developed for text-only LLMs.

A foundational type of multimodal hallucination is \textit{object hallucination}, where models describe non-existent objects in an image. \cite{li2023249} conducted the first systematic empirical study of object hallucination in LVLMs, proposing \texttt{POPE} as a more stable evaluation method and identifying object frequency and co-occurrence in training data as key drivers. Earlier, \cite{dai20229aa} probed object hallucination in Vision-Language Pre-training (VLP), revealing that optimizing for standard metrics could paradoxically increase unfaithfulness and introducing \texttt{ObjMLM} to improve token-level image-text alignment. Extending this, \cite{chen2024vy7} introduced \texttt{ROPE} to specifically evaluate \textit{multi-object hallucination}, addressing referential ambiguity with visual prompts and analyzing how object class distributions at test time influence hallucinatory behaviors. Further refining evaluation for open-ended generation, \cite{kaul2024ta7} developed \texttt{THRONE} for "Type I" object hallucinations in free-form LVLM responses, demonstrating that these are distinct from fixed-format "Type II" hallucinations and highlighting the limitations of prior metrics like \texttt{CHAIR}.

Beyond individual objects, LVLMs also struggle with \textit{relation hallucination}, misinterpreting the logical connections between entities. \cite{wu2024bxt} introduced \texttt{R-Bench} to evaluate these relationship hallucinations, emphasizing the need for data-leakage-free benchmarks and identifying co-occurrence patterns as contributors. Building on this, \cite{zheng20246fk} presented \texttt{Reefknot}, a comprehensive benchmark that categorizes relation hallucinations into "perceptive" (concrete) and "cognitive" (abstract) types, and empirically showed that current MLLMs are surprisingly more susceptible to perceptive errors. The complexity escalates further with \cite{jiang2024792}'s introduction of \textit{event hallucination}, a novel category involving the invention of fictional entities and narratives around them, which significantly increases with output length and necessitates fine-grained evaluation like their \texttt{Hal-Eval} framework.

The modality gap becomes particularly pronounced in dynamic and multi-sensory contexts, leading to \textit{temporal hallucination} in video-language models and \textit{cross-modal driven hallucinations} in audio-visual models. \cite{wang2024rta} developed \texttt{VideoHallucer}, the first comprehensive benchmark for LVLMs, introducing a new taxonomy that distinguishes between "intrinsic" (contradicting video content) and "extrinsic" (unverifiable from video) hallucinations, including temporal aspects. Focusing specifically on dynamic errors, \cite{li2024wyb} introduced \texttt{VidHalluc} to evaluate temporal hallucinations in MLLMs for video understanding, covering action, temporal sequence, and scene transition inconsistencies. For audio-visual models, \cite{sungbin2024r2g} presented \texttt{AVHBench}, a benchmark for "cross-modal driven hallucinations" that assesses misinterpretations arising from the interaction between audio and visual signals, such as perceiving imaginary sounds from visual cues or vice versa. Similarly, \cite{kuan20249pm} systematically explored object hallucination in Large Audio-Language Models (LALMs), revealing that these models often struggle with discriminative queries despite proficient audio captioning, indicating a fundamental challenge in query understanding within the audio modality. These works collectively underscore that the "modality gap" and inconsistencies between visual/audio inputs and generated text lead to unique challenges, requiring a specialized understanding beyond unimodal LLM hallucinations. \cite{guan2023z15}'s \texttt{Hallusionbench} further dissects these issues by diagnosing "entangled language hallucination and visual illusion," showing how language priors often conflict with visual context in LVLMs. Moreover, \cite{wang2025jen} provided diagnostic insights into MLLM vulnerabilities by demonstrating how "attention sinks" can be exploited to induce hallucinations, revealing internal mechanisms of failure.

The critical implications of hallucinations necessitate context-aware analysis, especially in sensitive domains. \textit{Domain-specific hallucinations}, such as those in medical imaging, demand tailored solutions. \cite{chen2024hfe} addressed this by introducing \texttt{Med-HallMark}, the first benchmark for medical hallucinations in LVLMs, which features a novel hierarchical categorization based on clinical severity (e.g., Catastrophic, Critical, Attribute, Prompt-induced, Minor) and a corresponding \texttt{MediHall Score} for fine-grained evaluation. This work highlights the need for domain-specific metrics that capture the multi-layered complexities and potential clinical impact of errors, moving beyond general object-level hallucination assessments.

The pervasive nature and diverse forms of multimodal hallucinations have also spurred efforts toward unified detection and robust evaluation. \cite{liu2024sn3} provided a comprehensive survey, categorizing hallucination symptoms, root causes (including data bias and modality misalignment), and evaluation methods, reinforcing the unique challenges. \cite{chen2024lc5} proposed \texttt{UNIHD}, a task-agnostic, tool-enhanced framework for unified hallucination detection across various tasks (image-to-text, text-to-image) and categories (object, attribute, scene-text, factual inconsistencies). However, the reliability of these evaluation tools themselves remains a concern, as highlighted by \cite{yan2024ux8}, who meta-evaluated existing hallucination benchmarks and revealed issues like response bias, inconsistency, and misalignment with human judgment, emphasizing the ongoing challenge of creating truly robust and unbiased assessment tools for multimodal hallucination.

In conclusion, characterizing multimodal and domain-specific hallucinations reveals a complex landscape where errors manifest across various modalities, granularities, and temporal dimensions. The "modality gap" is a central challenge, leading to unique hallucination types like multi-object, relation, temporal, and cross-modal driven inconsistencies. Domain-specific applications, such as medicine, further necessitate specialized, context-aware evaluation. While significant progress has been made in identifying and categorizing these phenomena, the development of comprehensive, reliable, and unbiased evaluation benchmarks remains an active and critical area of research.
\subsection{Evaluation Benchmarks for Multimodal and Domain-Specific Hallucinations}
\label{sec:6\_2\_evaluation\_benchmarks\_for\_multimodal\_\_and\_\_domain-specific\_hallucinations}

The rigorous assessment of hallucinations in Multimodal Large Language Models (MLLMs) and specialized application domains necessitates the development of tailored benchmarks and metrics. These evaluation frameworks are crucial for systematically identifying, categorizing, and quantifying the diverse forms of errors, providing a foundational understanding for targeted mitigation efforts.

Early efforts to quantify hallucinations primarily focused on object-level inconsistencies in vision-language models (VLMs). \cite{dai20229aa} conducted a systematic study on object hallucination in Vision-Language Pre-training (VLP) models, utilizing metrics like CHAIR to reveal that optimizing for standard captioning metrics could paradoxically increase hallucination rates. Building on this, \cite{kaul2024ta7} introduced THRONE, an object-based benchmark specifically designed for "Type I" hallucinations in free-form LVLM generations, addressing the limitations of prior benchmarks like POPE (which focused on "Type II" fixed-format questions) and CHAIR (which struggled with semantic understanding in complex text). THRONE leverages open-source LMs for Abstractive Question Answering (AQA) to semantically judge object existence, significantly reducing misjudgment errors. Further extending object-level evaluation, \cite{chen2024vy7} proposed Recognition-based Object Probing Evaluation (ROPE) to assess \textit{multi-object} hallucination. ROPE employs visual referring prompts (e.g., bounding boxes) to eliminate referential ambiguity and systematically investigates hallucination across various object class distributions within an image, revealing how LVLMs exploit shortcuts and spurious correlations.

As models advanced, the scope of hallucination evaluation expanded beyond simple object presence to more complex relational and attribute errors. \cite{wu2024bxt} introduced R-Bench, a novel benchmark specifically for \textit{relationship hallucinations} in Large Vision-Language Models (LVLMs), addressing the critical issue of data leakage prevalent in earlier benchmarks that used pre-training datasets. R-Bench includes both image-level and instance-level questions to assess inter-object relationships. \cite{zheng20246fk} further advanced this by proposing Reefknot, a comprehensive benchmark for relation hallucinations, categorizing them into \textit{perceptive} and \textit{cognitive} types and offering multi-faceted evaluation tasks (Yes/No, MCQ, VQA). Their analysis revealed that MLLMs are surprisingly more susceptible to perceptive hallucinations.

Beyond general visual domains, specialized benchmarks have emerged for critical application areas. For instance, \cite{chen2024hfe} developed Med-HallMark, the first benchmark dedicated to detecting and evaluating \textit{medical hallucinations} in LVLMs. This benchmark features a novel hierarchical categorization of hallucinations based on clinical severity and supports multi-task evaluation (e.g., Med-VQA, imaging report generation), providing a more nuanced assessment than traditional metrics.

The rise of MLLMs incorporating diverse modalities spurred the creation of cross-modal and temporal hallucination benchmarks. \cite{kuan20249pm} explored object hallucination in Large Audio-Language Models (LALMs), introducing ECHO and Cover metrics for generative audio captioning and revealing LALMs' struggles with discriminative audio queries. Addressing the complex interplay between modalities, \cite{sungbin2024r2g} presented AVHBench, the first benchmark for \textit{cross-modal driven hallucinations} in Audio-Visual Large Language Models (AV-LLMs). AVHBench assesses both audio-driven video and video-driven audio hallucinations through judgment and description tasks, utilizing synthetic videos with swapped audio to challenge models. For video-language models, \cite{wang2024rta} introduced VideoHallucer, which evaluates \textit{intrinsic} (contradicting video content) and \textit{extrinsic} (not verifiable from video) hallucinations, covering object-relation, temporal, and semantic details in dynamic contexts. Building on this, \cite{li2024wyb} proposed VIDHALLUC, a large-scale benchmark specifically targeting \textit{temporal hallucinations} (action, temporal sequence, scene transition) in video understanding, using adversarial video pairs to expose MLLMs' vulnerabilities to dynamic inconsistencies. In an even more complex scenario, \cite{zhang2025pex} developed CCHall, a novel benchmark for detecting \textit{joint cross-lingual and cross-modal hallucinations}, highlighting the significant performance degradation of MLLMs when faced with inconsistencies across both visual content and multiple languages.

To provide a more unified and fine-grained understanding of hallucinations, researchers have also focused on comprehensive evaluation frameworks and meta-evaluation of benchmarks themselves. \cite{jiang2024792} introduced Hal-Eval, a universal and fine-grained framework that expanded the hallucination taxonomy to include "event hallucination" (narratives around non-existent entities) and integrated both discriminative and generative evaluation methods. \cite{chen2024lc5} presented UNIHD (Unified Multimodal Hallucination Detection), a task-agnostic, tool-enhanced framework that extracts claims, autonomously selects and executes specialized tools (e.g., object detectors, OCR, search engines), and verifies hallucinations with rationales across image-to-text and text-to-image tasks. \cite{guan2023z15} offered Hallusionbench, an advanced diagnostic suite for \textit{entangled language hallucination and visual illusion} in LVLMs, utilizing human-edited images and control groups to quantitatively dissect specific failure modes. Furthermore, \cite{zhong2024mfi} investigated the phenomenon of "multimodal hallucination snowballing," where initial hallucinations propagate in conversational contexts, proposing the MMHalSnowball framework to evaluate this dynamic error. Crucially, \cite{yan2024ux8} took a meta-evaluation approach, proposing the Hallucination benchmark Quality Measurement (HQM) framework to systematically assess the reliability and validity of existing benchmarks. Their work revealed significant issues like response bias and misalignment with human judgment in many benchmarks, leading to the development of HQH (High-Quality Hallucination Benchmark) with a simplified binary detection metric for improved consistency.

In conclusion, the evolution of hallucination evaluation benchmarks for MLLMs reflects a progression from simple object presence to complex, dynamic, cross-modal, and domain-specific inconsistencies. While significant strides have been made in identifying and categorizing diverse errors, challenges remain in developing truly comprehensive, unbiased, and scalable evaluation frameworks that can keep pace with the rapid advancements and increasing complexity of MLLMs. The ongoing need for human-aligned, fine-grained, and cost-effective evaluation methodologies, particularly for emerging modalities and complex reasoning tasks, continues to drive research in this critical area.
\subsection{Multimodal Mitigation Strategies}
\label{sec:6\_3\_multimodal\_mitigation\_strategies}

Hallucinations in Large Vision-Language Models (LVLMs) and other multimodal architectures represent a critical challenge, where generated content is factually inconsistent with visual inputs or established knowledge \cite{li2023249, xu2024n76}. Addressing this issue is paramount for the dependability and credibility of multimodal AI systems. Mitigation strategies have evolved from data-centric training approaches to sophisticated inference-time interventions and unified frameworks that adapt to the nature of the query.

Early efforts focused on enhancing model robustness through improved training data and objectives. \cite{liu2023882} introduced \texttt{LRV-Instruction}, a novel dataset incorporating diverse \textit{negative instructions} (e.g., describing nonexistent objects) to explicitly train LMMs to avoid hallucination, alongside \texttt{GAVIE}, a GPT4-assisted evaluation framework. This data-centric approach demonstrated the effectiveness of training with explicit negative examples. Building on this, \cite{dai20229aa} proposed \texttt{ObjMLM} (Object-Masked Language Modeling) as a pre-training objective to mitigate object hallucination by enhancing token-level image-text alignment, showing that even state-of-the-art models frequently hallucinate and standard metrics can be misleading. Further refining data-driven methods, \cite{wang2023ubf} developed \texttt{ReCaption}, a framework that leverages ChatGPT to rewrite captions for fine-grained visual-text alignment during fine-tuning, specifically targeting object attributes and behaviors.

Beyond data augmentation, other training-based strategies aim to embed hallucination resistance directly into the model's learning process. \cite{wu2024n00} introduced \texttt{NoiseBoost}, a generalizable method that injects Gaussian noise into projected visual tokens during supervised fine-tuning (SFT), reinforcement learning (RL), or semi-supervised learning (SSL). This perturbation forces the MLLM to balance attention between visual and linguistic tokens, reducing over-reliance on language priors without significant additional training costs. Addressing the efficiency and bias of using MLLMs as judges for self-improvement, \cite{deng202405j} proposed a "model-level judge-free" framework. This method generates controllable negative samples using a "hallucination ratio" and employs a lightweight CLIP model for preference data inversion, enabling efficient Direct Preference Optimization (DPO) without the computational overhead or biases of MLLM-as-judge approaches. Similarly, \cite{xiao2024hv1} presented a framework for detecting and mitigating hallucinations via fine-grained AI feedback, introducing \texttt{HSA-DPO} (Hallucination Severity-Aware Direct Preference Optimization) which incorporates hallucination severity scores into the DPO objective to prioritize the mitigation of critical errors.

While training-based methods instill foundational robustness, inference-time interventions offer flexible, training-free solutions for deployed models. An early post-hoc correction method is \texttt{Woodpecker} \cite{yin2023hx3}, a five-stage pipeline that uses expert models (e.g., Grounding DINO for object detection, BLIP-2-FlanT5 XXL for VQA) to validate visual facts extracted from an MLLM's initial response, then employs an LLM to correct the hallucinated output with explicit bounding box evidence. This approach provides high interpretability by clearly showing the visual grounding for corrections.

More advanced inference-time interventions focus on enhancing visual grounding and balancing modality priors. \texttt{ClearSight} \cite{yin2025s2b}, with its Visual Amplification Fusion (VAF) technique, mitigates object hallucinations by amplifying visual signals in the MLLM's middle layers during the forward pass. Unlike contrastive decoding, VAF preserves content quality and inference speed by directly targeting attention mechanisms rather than suppressing language priors. Complementing this, \texttt{MemVR} (Memory-space Visual Retracing) \cite{zou2024dp7} re-injects visual tokens as supplementary evidence into intermediate layers when the model exhibits high uncertainty, effectively addressing the "amnesia" of visual information without additional inference latency. Taking a principled approach, \cite{zhou2024lvp} introduced \texttt{CAUSAL MM}, a causal inference framework that applies structural causal modeling to MLLMs. By using back-door adjustment and counterfactual reasoning on attention mechanisms, \texttt{CAUSAL MM} mitigates modality prior-induced hallucinations by deciphering their causal impact, providing a plug-and-play solution that balances visual and language priors. Furthermore, \cite{chen2024j0g} proposed \texttt{ICT} (Image-Object Cross-Level Trusted Intervention), a training-free method that calculates an "intervention direction" to shift the model's focus towards different levels of visual information (image-level and object-level) by targeting specific attention heads during the forward pass, thereby reducing over-reliance on language priors while preserving beneficial ones.

Beyond direct visual grounding, strategies involving enhanced reasoning and multi-view processing have emerged. Inspired by the dynamic reasoning and acting principles in text-based models like \texttt{ReAct} \cite{yao20229uz} and \texttt{IRCoT} \cite{trivedi2022qsf}, multimodal approaches leverage iterative self-correction. \texttt{MVP} (Multi-View Multi-Path Reasoning) \cite{qu2024pqc} is a training-free framework that enhances LVLM inference by seeking comprehensive image information from "multi-views" (top-down, regular, bottom-up) and employing "multi-path certainty-driven reasoning" during decoding. This approach leverages the LVLM's own capabilities to explore multiple reasoning paths and select the answer with the highest certainty, significantly reducing hallucinations without external tools. Another cognitive-inspired approach is \texttt{Counterfactual Inception} \cite{kim2024ozf}, a training-free method that prompts LMMs to generate "counterfactual keywords" (deliberately deviated from visual content) and then instructs the model to avoid them. This process, coupled with a Plausibility Verification Process (PVP) using CLIP scores, guides the LMM towards more factually consistent outputs.

Finally, unified frameworks aim to adapt mitigation strategies based on the nature of the query. \texttt{Dentist} \cite{chang2024u3t} is a unified hallucination mitigation framework that first classifies the query type (perception vs. reasoning) using an external LLM (ChatGPT). It then applies tailored mitigation: visual verification with sub-questions for perception queries and Chain-of-Thought (CoT) prompting for reasoning queries. This process is embedded in a validation loop that iteratively refines answers until semantic convergence, addressing the "one-size-fits-all" limitation of prior methods and ensuring more complete hallucination removal. While \texttt{Reefknot} \cite{zheng20246fk} primarily serves as a benchmark for relation hallucinations, it also proposes a "Detect-then-Calibrate" mitigation strategy based on token-level confidence and entropy analysis, which identifies and reduces hallucinations by applying calibration.

Despite these advancements, challenges remain. The scalability of fine-grained, human-annotated data for training-based methods is a persistent issue, though LLM-assisted generation helps \cite{liu2023882}. Balancing the efficiency of inference-time interventions with their robustness and generalizability across diverse hallucination types and model architectures is also an ongoing area of research. Furthermore, as theoretical work suggests hallucination is an inherent limitation for all computable LLMs \cite{xu2024n76}, future directions must focus not on eradication, but on developing increasingly sophisticated, adaptive, and trustworthy mitigation and detection mechanisms that account for the complex interplay between modalities and reasoning processes.


\label{sec:advanced_topics_and_future_directions}

\section{Advanced Topics and Future Directions}
\label{sec:advanced\_topics\_\_and\_\_future\_directions}

\subsection{Adversarial Hallucination and Robustness}
\label{sec:7\_1\_adversarial\_hallucination\_\_and\_\_robustness}

The integrity of Large Language Models (LLMs) and multimodal models is critically challenged by adversarial attacks specifically designed to induce hallucinations. This emerging field moves beyond traditional security concerns to target the factual consistency and trustworthiness of AI-generated content, necessitating a shift from reactive mitigation to proactive robustness engineering. Understanding these adversarial techniques is paramount for developing resilient AI systems capable of withstanding sophisticated, targeted manipulations.

A seminal work in this area, \cite{wang2025jen}, introduces a novel hallucination attack that exploits the "attention sink" phenomenon in multimodal LLMs (MLLMs). This attack reveals a fundamental vulnerability where specific patterns of high attention scores, emerging at the turning point of image-text relevance, can aggregate misleading global information, thereby triggering hallucinated content with minimal visual grounding. By manipulating attention scores and hidden embeddings, this method dynamically induces object, attribute, and relationship hallucinations without predefined patterns or degrading overall response quality, underscoring a critical weakness stemming from instruction-tuning data patterns.

The identification of such vulnerabilities is often facilitated by specialized diagnostic benchmarks. For instance, \cite{sungbin2024r2g} introduces AVHBench to evaluate cross-modal driven hallucinations, where models misinterpret information due to subtle inter-modal relationships or over-reliance on a single modality. Their use of synthetic videos with swapped audio effectively creates adversarial scenarios that expose these inherent weaknesses. Similarly, \cite{guan2023z15}'s Hallusionbench employs human-edited images and a novel VQ structure with control groups to systematically diagnose entangled language hallucination and visual illusion, highlighting specific failure modes that could be targeted by attackers. In the video domain, \cite{wang2024rta} proposes VideoHallucer, which utilizes an adversarial binary VideoQA method to rigorously evaluate intrinsic and extrinsic hallucinations in Large Video-Language Models (LVLMs), directly testing their robustness against crafted queries. Even in audio-language models, \cite{kuan20249pm} employs "adversarial sampling strategies" to generate non-existent objects, revealing LALMs' tendency to hallucinate in discriminative tasks.

Beyond direct attacks, understanding how hallucinations propagate reveals critical failure modes. \cite{zhong2024mfi} investigates "Multimodal Hallucination Snowballing," demonstrating how an LVLM's self-generated hallucinations can mislead subsequent responses, leading to accumulated errors. This phenomenon represents a significant vulnerability that could be triggered by an initial, subtle adversarial hallucination. Furthermore, the creation of complex, challenging benchmarks like CCHall \cite{zhang2025pex}, which generates joint cross-lingual and cross-modal hallucinations, highlights the compounded vulnerabilities in models operating in diverse, real-world conditions. Even the meta-evaluation of benchmarks by \cite{yan2024ux8} reveals issues like response bias, which, if unaddressed, could be exploited by an adversary to elicit predictable hallucinatory behavior.

To counter these sophisticated attacks and inherent vulnerabilities, researchers are developing proactive robustness engineering strategies. Training-based approaches aim to instill hallucination resistance from the ground up. \cite{wu2024n00} introduces NoiseBoost, a method that injects Gaussian noise into projected visual tokens during supervised fine-tuning (SFT) or reinforcement learning (RL). This perturbation forces MLLMs to distribute attention more evenly between visual and linguistic tokens, thereby reducing over-reliance on language priorsa common source of hallucination and a potential target for adversarial manipulation. Earlier, \cite{dai20229aa} proposed ObjMLM loss during pre-training to enhance token-level image-text alignment, directly mitigating object hallucination. Building on this, \cite{wang2023ubf} uses ChatGPT to rewrite captions for fine-tuning, aiming to mitigate fine-grained hallucinations by improving visual-textual alignment. A more advanced approach by \cite{deng202405j} presents a "judge-free" self-improvement framework that generates "controllable negative samples" with a specific hallucination ratio for Direct Preference Optimization (DPO), effectively training models to be robust against self-generated errors.

Inference-time and post-hoc methods offer flexible, training-free defenses. \cite{zhou2024lvp} proposes CAUSAL MM, a causal inference framework that uses back-door adjustment and counterfactual reasoning to mitigate modality prior-induced hallucinations, directly addressing the causal roots of model vulnerability. \cite{kim2024ozf}'s Counterfactual Inception prompts MLLMs to self-generate and avoid "counterfactual keywords," acting as a self-correction mechanism against generating false information. For targeted visual grounding, \cite{chen2024j0g} introduces ICT, which intervenes on specific attention heads during the forward pass to enhance focus on both overall image information and fine-grained object details, thereby strengthening visual fidelity against subtle manipulations. Similarly, \cite{yin2025s2b}'s VAF enhances visual signals in MLLM middle layers to reduce over-reliance on language priors, a common pathway for hallucination. \cite{zou2024dp7} proposes MemVR, a decoding paradigm that re-injects visual tokens to prevent "amnesia" about visual information, making models more robust to context loss. Other inference-time strategies include \cite{park20247cm}'s ConVis, which leverages a Text-to-Image model for hallucination visualization during contrastive decoding, providing a visual feedback loop for correction. \cite{qu2024pqc}'s MVP framework employs multi-view information seeking and certainty-driven reasoning to improve factual consistency, making models less susceptible to subtle manipulations. Post-hoc correction frameworks like "Dentist" \cite{chang2024u3t} adaptively apply visual verification or Chain-of-Thought based on query type, with an iterative validation loop for refinement, while \cite{yin2023hx3}'s Woodpecker uses expert models for visual fact-checking.

In conclusion, the investigation into adversarial hallucination underscores the critical need for robust AI systems. The shift from merely detecting hallucinations to understanding and exploiting their underlying vulnerabilities, such as the "attention sink" phenomenon, is crucial for developing proactive defenses. While significant progress has been made in both identifying attack vectors and engineering robustness through diverse training and inference-time strategies, the dynamic nature of adversarial attacks means this remains an ongoing challenge. Future research must continue to explore more sophisticated attack methodologies to uncover novel failure modes, concurrently driving the development of integrated, multi-layered defense mechanisms that ensure the credibility and reliability of AI models in real-world deployments.
\subsection{Safety-Critical Applications and Guardrails}
\label{sec:7\_2\_safety-critical\_applications\_\_and\_\_guardrails}

In high-stakes domains such as medical diagnosis, legal advice, and financial decision-making, the consequences of Large Language Model (LLM) hallucinations can be catastrophic, leading to "never events" where errors are unacceptable and ethical considerations are paramount. Moving beyond general factual accuracy, this subsection explores the development of highly dependable, accountable, and credible AI systems through application-specific mechanisms and robust guardrails designed for absolute error prevention and explicit uncertainty communication.

A foundational step towards building trustworthy AI systems in critical contexts involves enabling verifiable outputs. \cite{gao2023ht7} addresses this by introducing ALCE, a reproducible benchmark and automatic evaluation framework for LLMs to generate text with verifiable citations. This work highlights the necessity of grounding LLM outputs in retrievable evidence, providing a crucial mechanism for human oversight and accountability by allowing users to trace information back to its source, which is indispensable in auditing critical decisions. However, merely citing sources does not guarantee the fidelity of the reasoning process itself.

To advance beyond simple factual verification, the focus shifts to ensuring the integrity of the model's rationale. \cite{oh2024xa3} introduces ERBench, a benchmark designed for automatically verifying the \textit{rationales} generated by LLMs using relational databases. This represents a significant step towards building accountable systems, as understanding \textit{how} an LLM arrives at a conclusion is as critical as the conclusion itself in safety-critical environments, allowing for deeper scrutiny of the decision-making process and identification of flawed logic. Building on the need for proactive accuracy, \cite{ding20244yr} proposes Retrieve Only When It Needs (Rowen), an adaptive retrieval augmentation framework that dynamically decides when to retrieve external information based on cross-language and cross-model consistency. This proactive approach mitigates both internal and external hallucinations by ensuring that information is only retrieved and integrated when it genuinely enhances factual accuracy, thereby reducing the risk of propagating misinformation in sensitive applications.

Crucially, in scenarios where absolute certainty is unattainable, AI systems must explicitly communicate their uncertainty to prevent confident but erroneous outputs. \cite{tjandra2024umq} addresses this by proposing a method for fine-tuning LLMs to appropriately abstain from answering when uncertain, utilizing semantic entropy as a label-free mechanism. This capability is vital for safety-critical applications, allowing models to "say I don't know" rather than hallucinating, thereby preventing potentially harmful advice or diagnoses and fostering trust through transparency regarding the model's confidence levels. Complementing these proactive and uncertainty-aware strategies, robust, real-time detection mechanisms are essential. \cite{yang20251dw} introduces a zero-resource, self-contained method for hallucination detection in LLMs using metamorphic relations. This technique offers a powerful, model-agnostic way to identify inconsistencies and potential hallucinations without relying on external knowledge bases or internal model access, making it highly suitable for integration into real-time safety checks within diverse and resource-constrained critical systems.

Ultimately, these individual advancements culminate in the development of application-specific "semantic guardrails." \cite{hakim2024d4u} underscores the critical need for such guardrails in medical safety-critical settings, specifically in pharmacovigilance, to prevent "never event" errors. This work advocates for tailored mechanisms that go beyond general safety filters, embedding domain-specific knowledge and constraints directly into the AI's operational framework to ensure outputs are not only factually correct but also clinically and ethically sound. These semantic guardrails represent the synthesis of verifiable outputs, rationale integrity, proactive hallucination prevention, and explicit uncertainty communication into a holistic, domain-aware safety layer.

Despite these significant advancements, challenges remain in integrating these disparate mechanisms into a unified, formally verifiable, and auditable framework for safety-critical AI. Future research must focus on developing standardized protocols for designing, implementing, and evaluating semantic guardrails across diverse high-stakes domains, ensuring their robustness against adversarial attacks and their ability to adapt to evolving knowledge, while maintaining transparency and accountability in their decision-making processes.
\subsection{Meta-Evaluation and Unified Theoretical Frameworks}
\label{sec:7\_3\_meta-evaluation\_\_and\_\_unified\_theoretical\_frameworks}

As the field of AI hallucination research matures, a critical need has emerged for rigorous self-assessment through meta-evaluation and the development of unified theoretical frameworks. This forward-looking perspective is essential to ensure the scientific rigor of research, guide the creation of fundamentally dependable AI systems, and establish a robust foundation for future advancements in AI reliability.

Initial efforts in evaluating hallucination primarily focused on assessing the outputs of Large Language Models (LLMs) themselves. For instance, \cite{gao2023ht7} introduced ALCE, a pioneering reproducible benchmark for evaluating LLMs' ability to generate text with verifiable citations, establishing crucial metrics for correctness and citation quality. While foundational, such benchmarks highlighted the need for deeper scrutiny into the evaluation methodologies and the reasoning processes of AI systems. This led to the emergence of meta-evaluation, which critically assesses the quality, reliability, and validity of the benchmarks and evaluation methods used to detect hallucination. \cite{oh2024xa3} addressed this by proposing ERBench, a benchmark that leverages relational databases to create automatically verifiable questions and rationales, thereby moving beyond mere answer correctness to scrutinize the underlying reasoning process. This approach inherently meta-evaluates the assessment process by ensuring the \textit{validity} of the rationale itself, providing a more robust measure of an LLM's understanding. Similarly, \cite{tang2024a1j}'s GraphArena benchmark for complex algorithmic reasoning on graphs implicitly contributes to meta-evaluation by demonstrating the limitations of simpler benchmarks in capturing sophisticated reasoning failures, thus pushing the boundaries for what constitutes a comprehensive hallucination evaluation. In the multimodal domain, \cite{kaul2024ta7}'s THRONE benchmark for free-form Large Vision-Language Model (LVLM) generations, which employs LM-based semantic judgment, further refines evaluation methodologies to accurately capture diverse hallucination types, ensuring the evaluation tools themselves are fit for purpose across modalities. The development of zero-resource detection methods, such as that by \cite{yang20251dw} using metamorphic relations, also offers a powerful meta-evaluative tool by providing an independent, self-contained means to validate the robustness of hallucination detection mechanisms without relying on external knowledge or internal model access.

Complementing the drive for robust meta-evaluation is the ongoing pursuit of unified theoretical frameworks that can provide a holistic understanding of hallucination's origins and limits. Early mechanistic understandings, as provided by surveys like \cite{zhang2023k1j} and \cite{ye2023yom}, offered empirical taxonomies and categorizations of hallucination. However, a deeper scientific understanding necessitates moving beyond descriptive observations to fundamental principles. \cite{li2025qzg} makes a significant leap in this direction with "Loki's Dance of Illusions," which proposes a formal mathematical definition of hallucination and explores its mathematical origins, such as undecidability principles, alongside empirical causes related to data, architecture, and cognitive processing. This work is pivotal in integrating empirical observations with fundamental mathematical insights, aiming to unify the diverse manifestations of hallucination under a coherent theoretical umbrella. Further contributing to this theoretical grounding, \cite{zhang2024qq9} identifies "knowledge overshadowing" as a novel root cause of hallucination, where dominant conditions in training data lead to over-generalization and "amalgamated hallucinations." This mechanistic insight is supported by a derived generalization bound, bridging empirical observation with a more formal, quantifiable understanding of hallucination's genesis. The extensive surveys on multimodal hallucination, including those by \cite{liu2024sn3}, \cite{lan20240yz}, and the comprehensive overview by \cite{tonmoy20244e4}, also lay groundwork for a unified theory by systematizing observations across different modalities (vision, audio, language), which is a prerequisite for a theory that can explain hallucination across diverse AI architectures.

Ultimately, meta-evaluation and the development of unified theoretical frameworks are synergistic endeavors. A robust theoretical understanding, as championed by \cite{li2025qzg}, provides the fundamental principles necessary to design more valid, reliable, and comprehensive benchmarks, thereby informing meta-evaluation efforts. Conversely, the limitations and insights revealed through rigorous meta-evaluation, such as the challenges in evaluating complex reasoning or multimodal outputs, highlight critical areas where theoretical frameworks need to be expanded and refined. This dual focus ensures that the field not only develops effective solutions but also understands \textit{why} these solutions work, moving towards a deeper scientific understanding of AI reliability. The integration of these perspectives is crucial for establishing a robust foundation for future advancements, ensuring that AI systems are not only powerful but also fundamentally dependable and trustworthy.


\label{sec:conclusion}

\section{Conclusion}
\label{sec:conclusion}

\subsection{Summary of Key Developments}
\label{sec:8\_1\_summary\_of\_key\_developments}

The intellectual journey in addressing hallucination within Large Language Models (LLMs) and their multimodal extensions has undergone a significant transformation, evolving from initial characterization and empirical detection to sophisticated mitigation strategies and, crucially, a theoretical understanding of its inherent nature. This progression underscores a maturation of the field's approach to AI dependability, shifting from treating hallucination as a solvable bug to acknowledging its inevitability and focusing on robust management for trustworthy artificial intelligence across all modalities.

Early research laid the groundwork by defining and categorizing hallucinations in text-based LLMs. \cite{maynez2020h3q} pioneered a systematic human evaluation, distinguishing between intrinsic (misrepresenting source content) and extrinsic (adding uninferable information) hallucinations in abstractive summarization, highlighting the inadequacy of traditional metrics. Building on this foundational understanding, the field developed methods for detection. \cite{manakul20236ex} introduced \texttt{SelfCheckGPT}, a zero-resource, black-box method that leverages the consistency of stochastically sampled LLM outputs to identify non-factual statements. Concurrently, comprehensive surveys like \cite{zhang2023k1j} and \cite{ye2023yom} consolidated taxonomies and identified root causes, while \cite{rawte2023ao8} proposed a fine-grained categorization and a Hallucination Vulnerability Index (\texttt{HVI}) for quantification. \cite{du2023qu7} further refined quantification and attribution through association analysis, linking hallucination to specific model capability deficiencies. This empirical understanding was profoundly reshaped by \cite{xu2024n76}, which provided a theoretical proof, using a diagonalization argument, that hallucination is an inherent and inevitable limitation for all computable LLMs, regardless of their architecture or training. This seminal work shifted the paradigm from eradication to robust management, a perspective echoed by \cite{rejeleene2024okw} in their investigation of information quality.

The challenge of robust evaluation spurred the development of specialized benchmarks. \cite{liu2021mo6} introduced HADES, a token-level, reference-free hallucination detection benchmark for free-form text generation, addressing the need for fine-grained signals. To overcome the labor-intensive nature of manual annotation, \cite{cao2023ecl} proposed \texttt{AutoHall} for automated hallucination dataset generation. For Chinese LLMs, \cite{liang20236sh} developed \texttt{UHGEval}, benchmarking hallucination via unconstrained generation to better reflect real-world usage. More comprehensive empirical studies, such as \cite{li2024qrj}'s HaluEval 2.0, provided multi-domain datasets and LLM-based detection frameworks. \cite{oh2024xa3} introduced \texttt{ERBench}, leveraging entity-relationship databases for automatically verifiable rationales, while \cite{vu202337s} tackled dynamic knowledge with \texttt{FreshLLMs}, a benchmark for rapidly changing information. The increasing complexity of LLM interactions led to \cite{chen2024c4k}'s \texttt{DiaHalu}, the first dialogue-level hallucination evaluation benchmark, addressing multi-turn conversational challenges.

Following foundational understanding and robust evaluation, research focused on diverse mitigation strategies for text-based LLMs. A prominent direction involved grounding LLMs in external, verifiable knowledge. \cite{yao20229uz} introduced \texttt{ReAct}, a seminal paradigm that interleaves verbal reasoning with task-specific actions (e.g., API calls) to ground LLM responses in external environments. This was further refined by \cite{trivedi2022qsf}'s \texttt{IRCoT}, which dynamically uses intermediate Chain-of-Thought steps as queries for iterative knowledge retrieval, significantly reducing factual errors. To enhance trustworthiness, \cite{gao2023ht7} developed \texttt{ALCE}, a benchmark and evaluation framework for enabling LLMs to generate text with verifiable citations. However, \cite{chen2023h04}'s \texttt{RGB} benchmark critically evaluated Retrieval-Augmented Generation (RAG), revealing that despite its promise, RAG still faces significant challenges in noise robustness, negative rejection, and information integration.

Beyond RAG, other internal and external mitigation strategies emerged. \cite{dhuliawala2023rqn} proposed Chain-of-Verification (\texttt{CoVe}), enabling LLMs to self-deliberate and correct their own responses without external tools. Knowledge Graphs (KGs) were integrated for structured reasoning, as seen in \cite{wen2023t6v}'s \texttt{MindMap}, which prompts LLMs to comprehend graphical inputs and reason over a "mind map." \cite{adams202289x} addressed noisy training data by proposing \texttt{ReDRESS}, a method to revise references for faithful summarization. For dynamic, real-time correction, \cite{su2024gnz} introduced \texttt{DRAD}, a framework that synchronizes retrieval augmentation with real-time hallucination detection based on entity-level uncertainty. \cite{lv2024k5x}'s \texttt{COFT} (Coarse-to-Fine Highlighting) aimed to reduce hallucination by intelligently highlighting key texts within long retrieved contexts. A novel approach by \cite{zhang202396g} introduced Induce-then-Contrast Decoding (\texttt{ICD}), which leverages "induced hallucinations" as a penalty term during decoding to enhance factuality. The integration of KGs for trustworthiness in open-ended QA was further explored by \cite{sui20242u1}'s \texttt{OKGQA}, while \cite{dziri2021bw9}'s Neural Path Hunter (\texttt{NPH}) focused on reducing hallucination in dialogue systems via path grounding. \cite{li2023v3v}'s Chain-of-Knowledge (\texttt{CoK}) grounded LLMs via dynamic knowledge adapting over heterogeneous sources. These diverse mitigation techniques were comprehensively surveyed by \cite{tonmoy20244e4}, \cite{liu2024gxh}, \cite{pan2023mwu}, and \cite{pan2024y3a}, providing structured overviews of the rapidly evolving landscape.

The research frontier significantly expanded into multimodal AI, particularly Large Vision-Language Models (LVLMs) and Multimodal Large Language Models (MLLMs), where unique challenges arise from inconsistencies between generated text and visual input. \cite{li2023249} conducted the first systematic empirical study of "object hallucination" in LVLMs, proposing \texttt{POPE} as a stable evaluation method and identifying object frequency and co-occurrence in training data as key drivers. This led to a proliferation of specialized multimodal benchmarks. \cite{wu2024bxt} introduced \texttt{R-Bench} to evaluate relationship hallucinations, while \cite{sungbin2024r2g}'s \texttt{AVHBench} addressed cross-modal hallucinations in audio-visual LLMs. Domain-specific evaluations emerged, such as \cite{chen2024hfe}'s Med-HallMark for medical hallucinations. \cite{zheng20246fk}'s \texttt{Reefknot} provided a comprehensive benchmark for relation hallucination, and \cite{yan2024ux8} even meta-evaluated the quality of existing benchmarks with their \texttt{HQM} framework. Further advancements included \cite{guan2023z15}'s \texttt{Hallusionbench} for entangled language hallucination and visual illusion, \cite{zhang2025pex}'s \texttt{CCHall} for joint cross-lingual and cross-modal hallucinations, \cite{chen2024lc5}'s \texttt{UNIHD} for unified detection, \cite{jiang2024792}'s \texttt{Hal-Eval} for complex event hallucinations, \cite{wang2024rta}'s \texttt{VideoHallucer} for temporal hallucinations in video, \cite{chen2024vy7}'s \texttt{ROPE} for multi-object hallucination, and \cite{kaul2024ta7}'s \texttt{THRONE} for free-form generations. Diagnostic insights were also gained from adversarial attacks, such as \cite{wang2025jen}'s "attention sink" attack, which exposes vulnerabilities in MLLMs.

Multimodal mitigation strategies also diversified. \cite{liu2023882} introduced \texttt{LRV-Instruction}, a novel dataset with \textit{negative instructions} to explicitly train LMMs to avoid hallucination, alongside \texttt{GAVIE} for GPT4-assisted evaluation. \cite{dai20229aa} proposed \texttt{ObjMLM} loss to mitigate object hallucination during Vision-Language Pre-training. \cite{wang2023ubf}'s \texttt{ReCaption} fine-tuned LVLMs with rewritten captions to reduce fine-grained hallucinations. Leveraging AI feedback, \cite{xiao2024hv1} developed \texttt{HSA-DPO} for severity-aware hallucination mitigation using fine-grained AI feedback. \cite{wu2024n00}'s \texttt{NoiseBoost} alleviated hallucination by injecting noise perturbation into visual tokens to balance attention. For more efficient self-improvement, \cite{deng202405j} proposed a "judge-free" approach for DPO. Inference-time interventions became prominent: \cite{chang2024u3t}'s "Dentist" framework adaptively applied visual verification or Chain-of-Thought based on query type. \cite{yin2025s2b}'s \texttt{ClearSight} (Visual Amplification Fusion) enhanced visual signals in MLLMs, while \cite{zou2024dp7}'s \texttt{MemVR} (Memory-space Visual Retracing) re-injected visual tokens into intermediate layers. \cite{zhou2024lvp}'s \texttt{CAUSAL MM} mitigated modality prior-induced hallucinations via deciphering attention causality. \cite{kim2024ozf}'s Counterfactual Inception prompted models to self-generate and avoid counterfactual keywords. Post-hoc correction was addressed by \cite{yin2023hx3}'s Woodpecker, which uses expert models for visual fact-checking. \cite{park20247cm}'s \texttt{ConVis} used Text-to-Image models for hallucination visualization during contrastive decoding, and \cite{qu2024pqc}'s \texttt{MVP} employed multi-view multi-path reasoning. The critical issue of "multimodal hallucination snowballing" in LVLMs was investigated and mitigated by \cite{zhong2024mfi} using Residual Visual Decoding. The comprehensive survey by \cite{liu2024sn3} further summarized the landscape of LVLM hallucination.

In conclusion, the research on AI hallucination has traversed a rich intellectual landscape. It began with defining and categorizing the problem in text-based LLMs, progressed through the development of increasingly sophisticated detection and mitigation techniques, and culminated in a theoretical understanding of hallucination's inherent nature. The expansion into multimodal AI has introduced new complexities, necessitating tailored evaluation and mitigation strategies that address the intricate interplay between modalities. The field's trajectory from viewing hallucination as a simple bug to acknowledging its fundamental inevitability marks a significant shift towards robust management and the pursuit of truly trustworthy artificial intelligence across all modalities. Future research must continue to bridge the gap between theoretical limits and practical dependability, focusing on adaptive, context-aware, and transparent hallucination management strategies.
\subsection{Remaining Challenges and Open Questions}
\label{sec:8\_2\_remaining\_challenges\_\_and\_\_open\_questions}

Despite significant advancements in understanding, detecting, and mitigating hallucinations in Large Language Models (LLMs), several persistent challenges and open questions continue to shape the research agenda, emphasizing the ongoing pursuit of trustworthy AI and responsible innovation. The theoretical proof that hallucination is an inherent and inevitable limitation for all computable LLMs \cite{xu2024n76} fundamentally shifts the research paradigm from eradication to robust management and mitigation, underscoring the enduring nature of these challenges.

One primary challenge lies in the \textbf{scalability and fine-grained nature of hallucination evaluation}. Early efforts relied on costly human evaluation to characterize hallucinations, as demonstrated by \cite{maynez2020h3q} in abstractive summarization, which highlighted the inadequacy of traditional metrics. While methods like \textit{SelfCheckGPT} \cite{manakul20236ex} introduced zero-resource, black-box detection by leveraging output consistency, they can be computationally expensive and sometimes fail if the LLM consistently hallucinates. The need for fine-grained, token-level detection is critical for real-time intervention, as explored by the HADES benchmark \cite{liu2021mo6}, but creating such granular datasets is arduous. Automated dataset generation, like \textit{AutoHall} \cite{cao2023ecl}, attempts to alleviate this but often relies on powerful LLMs as judges, raising concerns about circularity and inherent biases. Recent work by \cite{li2024qrj} with HaluEval 2.0 further leverages GPT-4 for fine-grained detection, yet the question of true independence and scalability for diverse, unconstrained outputs remains. The meta-evaluation of benchmarks by \cite{yan2024ux8} with HQM/HQH reveals persistent issues like response bias and misalignment with human judgment, underscoring the difficulty in creating truly reliable and comprehensive evaluation tools. Furthermore, evaluating hallucinations in complex, multi-turn dialogues \cite{chen2024c4k} or free-form generations \cite{kaul2024ta7} presents unique difficulties that current metrics struggle to capture fully.

Another significant area of concern is the \textbf{generalizability of mitigation techniques across diverse tasks and domains}, coupled with the need for a deeper understanding of complex causal mechanisms. Retrieval-Augmented Generation (RAG) and reasoning-enhanced methods, such as \textit{ReAct} \cite{yao20229uz} and \textit{IRCoT} \cite{trivedi2022qsf}, have shown promise in grounding LLM responses in external knowledge. However, comprehensive benchmarking by \cite{chen2023h04} with RGB reveals that RAG systems still struggle with noise robustness, negative rejection (refusing to answer when no relevant information is found), and multi-document information integration, limiting their generalizability. Similarly, while generating verifiable citations \cite{gao2023ht7} improves trustworthiness, ensuring faithfulness across all generated statements remains challenging. Self-correction and reflection mechanisms, exemplified by \textit{Chain-of-Verification} (CoVe) \cite{dhuliawala2023rqn} and iterative self-reflection for medical QA \cite{ji2023vhv}, offer promising internal accountability, but their scalability and effectiveness across vastly different tasks are still under active investigation.

The emergence of multimodal models introduces new layers of complexity. Object hallucination in Large Vision-Language Models (LVLMs) \cite{li2023249}, relationship hallucinations \cite{wu2024bxt, zheng20246fk}, and temporal hallucinations in video-language models \cite{wang2024rta, li2024wyb} necessitate tailored evaluation and mitigation strategies. While negative instruction tuning \cite{liu2023882} and noise perturbation \cite{wu2024n00} show initial success, the generalizability of these techniques across diverse multi-modal tasks and hallucination types (e.g., cross-modal hallucinations in audio-visual models \cite{sungbin2024r2g, zhang2025pex}) remains an open question. Furthermore, the phenomenon of "multimodal hallucination snowballing," where initial errors propagate and accumulate in subsequent generations \cite{zhong2024mfi}, highlights the need for robust, real-time error correction. A deeper understanding of the causal mechanisms driving these errors, beyond mere correlation, is crucial. Efforts like \cite{du2023qu7}'s association analysis attempt to attribute hallucination to specific model capability deficiencies, while \cite{zhou2024lvp}'s \textit{CAUSAL MM} framework delves into deciphering attention causality to mitigate modality prior-induced hallucinations, pointing towards a more fundamental understanding.

Finally, the \textbf{ethical implications of deploying systems with inherent hallucinatory tendencies} are paramount. The theoretical inevitability of hallucination \cite{xu2024n76} means that LLMs will always, to some extent, generate misinformation. This raises profound questions about responsible deployment, particularly in high-stakes domains like healthcare, where medical hallucinations in LVLMs can have disastrous consequences \cite{chen2024hfe}. The ease with which LLMs can generate misinformation \cite{liu2024gxh} and the potential for "hallucination attacks" that exploit internal vulnerabilities like attention sinks \cite{wang2025jen} underscore the urgent need for robust safeguards. While efforts to generate text with citations \cite{gao2023ht7} and verify rationales \cite{oh2024xa3} are crucial steps towards transparency and accountability, truly transparent LLMs that can explain \textit{why} they generated certain content and \textit{why} it is trustworthy remain elusive. The ongoing pursuit of "trustable" models \cite{rejeleene2024okw, sui20242u1} is a central theme, emphasizing the need for systems that can not only produce accurate information but also articulate their confidence and sources.

Looking ahead, promising future directions include the development of more \textbf{adaptive and context-aware AI}. This involves moving beyond static knowledge to dynamic, continuously updated systems, as seen in \textit{FreshLLMs} \cite{vu202337s} which uses search engine augmentation for up-to-date knowledge. Intelligent context management, such as \textit{COFT}'s coarse-to-fine highlighting in RAG \cite{lv2024k5x}, and adaptive mitigation strategies like \textit{Dentist}'s query-type classification for tailored interventions \cite{chang2024u3t}, represent steps towards more nuanced and effective hallucination control. Real-time hallucination detection that dynamically triggers retrieval \cite{su2024gnz} further enhances adaptivity. For multimodal models, advancements like \textit{MemVR}'s memory-space visual retracing \cite{zou2024dp7} and \textit{ClearSight}'s visual signal enhancement \cite{yin2025s2b} aim for more robust and context-aware visual grounding.

The development of \textbf{truly transparent and accountable LLMs} is another critical future direction. Beyond merely providing citations, understanding the LLM's reasoning process is key. \textit{MindMap} \cite{wen2023t6v} aims to spark "Graph of Thoughts" with knowledge graphs for more explainable reasoning pathways. Iterative self-reflection and verification loops \cite{dhuliawala2023rqn, ji2023vhv, deng202405j} are crucial for fostering internal accountability, allowing models to critique and refine their own outputs. Post-hoc correction frameworks like \textit{Woodpecker} \cite{yin2023hx3}, which provide visual grounding with bounding boxes, enhance interpretability. The application of causal inference \cite{zhou2024lvp} is a step towards explaining \textit{why} models behave in certain ways. Ultimately, the field is moving towards a future where LLMs are not just powerful generators but also reliable, interpretable, and ethically deployed agents that can articulate their knowledge, acknowledge their limitations, and actively work to prevent and correct their own errors.


\newpage
\section*{References}
\addcontentsline{toc}{section}{References}

\begin{thebibliography}{277}

\bibitem{vu202337s}
Tu Vu, Mohit Iyyer, Xuezhi Wang, et al. (2023). \textit{FreshLLMs: Refreshing Large Language Models with Search Engine Augmentation}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{chang2024u3t}
Yue Chang, Liqiang Jing, Xiaopeng Zhang, et al. (2024). \textit{A Unified Hallucination Mitigation Framework for Large Vision-Language Models}. Trans. Mach. Learn. Res..

\bibitem{wang2024vym}
Jiaqi Wang, Yifei Gao, and Jitao Sang (2024). \textit{VaLiD: Mitigating the Hallucination of Large Vision Language Models by Visual Layer Fusion Contrastive Decoding}. arXiv.org.

\bibitem{niu2024v97}
Mengjia Niu, Hao Li, Jie Shi, et al. (2024). \textit{Mitigating Hallucinations in Large Language Models via Self-Refinement-Enhanced Knowledge Retrieval}. arXiv.org.

\bibitem{liu2024gxh}
Aiwei Liu, Qiang Sheng, and Xuming Hu (2024). \textit{Preventing and Detecting Misinformation Generated by Large Language Models}. Annual International ACM SIGIR Conference on Research and Development in Information Retrieval.

\bibitem{li2023v3v}
Xingxuan Li, Ruochen Zhao, Yew Ken Chia, et al. (2023). \textit{Chain-of-Knowledge: Grounding Large Language Models via Dynamic Knowledge Adapting over Heterogeneous Sources}. International Conference on Learning Representations.

\bibitem{liang2024hoo}
Mengfei Liang, Archish Arun, Zekun Wu, et al. (2024). \textit{THaMES: An End-to-End Tool for Hallucination Mitigation and Evaluation in Large Language Models}. arXiv.org.

\bibitem{zhang2023k1j}
Yue Zhang, Yafu Li, Leyang Cui, et al. (2023). \textit{Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models}. Computational Linguistics.

\bibitem{zhou2024lvp}
Guanyu Zhou, Yibo Yan, Xin Zou, et al. (2024). \textit{Mitigating Modality Prior-Induced Hallucinations in Multimodal Large Language Models via Deciphering Attention Causality}. International Conference on Learning Representations.

\bibitem{bai2024tkm}
Zechen Bai, Pichao Wang, Tianjun Xiao, et al. (2024). \textit{Hallucination of Multimodal Large Language Models: A Survey}. arXiv.org.

\bibitem{yin2023hx3}
Shukang Yin, Chaoyou Fu, Sirui Zhao, et al. (2023). \textit{Woodpecker: Hallucination Correction for Multimodal Large Language Models}. Science China Information Sciences.

\bibitem{cao2023ecl}
Zouying Cao, Yifei Yang, and Hai Zhao (2023). \textit{AutoHall: Automated Hallucination Dataset Generation for Large Language Models}. arXiv.org.

\bibitem{wu2024bxt}
Ming-Kuan Wu, Jiayi Ji, Oucheng Huang, et al. (2024). \textit{Evaluating and Analyzing Relationship Hallucinations in Large Vision-Language Models}. International Conference on Machine Learning.

\bibitem{ghosh2024tj5}
Bishwamittra Ghosh, Sarah Hasan, Naheed Anjum Arafat, et al. (2024). \textit{Logical Consistency of Large Language Models in Fact-checking}. International Conference on Learning Representations.

\bibitem{gao2023ht7}
Tianyu Gao, Howard Yen, Jiatong Yu, et al. (2023). \textit{Enabling Large Language Models to Generate Text with Citations}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{yang20251dw}
Borui Yang, Md Afif Al Mamun, Jie M. Zhang, et al. (2025). \textit{Hallucination Detection in Large Language Models with Metamorphic Relations}. Proc. ACM Softw. Eng..

\bibitem{zhang2025pex}
Yongheng Zhang, Xu Liu, Ruoxi Zhou, et al. (2025). \textit{CCHall: A Novel Benchmark for Joint Cross-Lingual and Cross-Modal Hallucinations Detection in Large Language Models}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{xing2024itg}
Shangyu Xing, Fei Zhao, Zhen Wu, et al. (2024). \textit{EFUF: Efficient Fine-Grained Unlearning Framework for Mitigating Hallucinations in Multimodal Large Language Models}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{leng2023ohr}
Sicong Leng, Hang Zhang, Guanzheng Chen, et al. (2023). \textit{Mitigating Object Hallucinations in Large Vision-Language Models through Visual Contrastive Decoding}. Computer Vision and Pattern Recognition.

\bibitem{kim2024ozf}
Junho Kim, Yeonju Kim, and Yonghyun Ro (2024). \textit{What if...?: Thinking Counterfactual Keywords Helps to Mitigate Hallucination in Large Multi-modal Models}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{li2024wyb}
Chaoyu Li, Eun Woo Im, and Pooyan Fazli (2024). \textit{VidHalluc: Evaluating Temporal Hallucinations in Multimodal Large Language Models for Video Understanding}. Computer Vision and Pattern Recognition.

\bibitem{ji20243j6}
Ziwei Ji, Yuzhe Gu, Wenwei Zhang, et al. (2024). \textit{ANAH: Analytical Annotation of Hallucinations in Large Language Models}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{gao20232zb}
Yunfan Gao, Yun Xiong, Xinyu Gao, et al. (2023). \textit{Retrieval-Augmented Generation for Large Language Models: A Survey}. arXiv.org.

\bibitem{ji20227ii}
Ziwei Ji, Zihan Liu, Nayeon Lee, et al. (2022). \textit{RHO ($$): Reducing Hallucination in Open-domain Dialogues with Knowledge Grounding}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{adams202289x}
Griffin Adams, Han-Chin Shing, Q. Sun, et al. (2022). \textit{Learning to Revise References for Faithful Summarization}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{su2024gnz}
Weihang Su, Yichen Tang, Qingyao Ai, et al. (2024). \textit{Mitigating Entity-Level Hallucination in Large Language Models}. SIGIR-AP.

\bibitem{du2023qu7}
LI DU, Yequan Wang, Xingrun Xing, et al. (2023). \textit{Quantifying and Attributing the Hallucination of Large Language Models via Association Analysis}. arXiv.org.

\bibitem{ji2023vhv}
Ziwei Ji, Tiezheng Yu, Yan Xu, et al. (2023). \textit{Towards Mitigating Hallucination in Large Language Models via Self-Reflection}. arXiv.org.

\bibitem{pan2023mwu}
Liangming Pan, Michael Stephen Saxon, Wenda Xu, et al. (2023). \textit{Automatically Correcting Large Language Models: Surveying the landscape of diverse self-correction strategies}. arXiv.org.

\bibitem{kang20238j0}
Haoqiang Kang, and Xiao-Yang Liu (2023). \textit{Deficiency of Large Language Models in Finance: An Empirical Examination of Hallucination}. arXiv.org.

\bibitem{kang202378c}
Haoqiang Kang, Juntong Ni, and Huaxiu Yao (2023). \textit{Ever: Mitigating Hallucination in Large Language Models through Real-Time Verification and Rectification}. arXiv.org.

\bibitem{dong20223yz}
Yue Dong, J. Wieting, and Pat Verga (2022). \textit{Faithful to the Document or to the World? Mitigating Hallucinations via Entity-linked Knowledge in Abstractive Summarization}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{qu20240f7}
Xiaoye Qu, Mingyang Song, Wei Wei, et al. (2024). \textit{Mitigating Multilingual Hallucination in Large Vision-Language Models}. arXiv.org.

\bibitem{mckenna2023pzc}
Nick McKenna, Tianyi Li, Liang Cheng, et al. (2023). \textit{Sources of Hallucination by Large Language Models on Inference Tasks}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{rejeleene2024okw}
Rick Rejeleene, Xiaowei Xu, and John R. Talburt (2024). \textit{Towards Trustable Language Models: Investigating Information Quality of Large Language Models}. arXiv.org.

\bibitem{liu2024p39}
Xinxin Liu (2024). \textit{A Survey of Hallucination Problems Based on Large Language Models}. Applied and Computational Engineering.

\bibitem{liu2024sn3}
Hanchao Liu, Wenyuan Xue, Yifei Chen, et al. (2024). \textit{A Survey on Hallucination in Large Vision-Language Models}. arXiv.org.

\bibitem{chen2024hfe}
Jiawei Chen, Dingkang Yang, Tong Wu, et al. (2024). \textit{Detecting and Evaluating Medical Hallucinations in Large Vision Language Models}. arXiv.org.

\bibitem{li2024hdc}
Qing Li, Chenyang Lyu, Jiahui Geng, et al. (2024). \textit{Reference-free Hallucination Detection for Large Vision-Language Models}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{yan2024ux8}
Bei Yan, Jie Zhang, Zheng Yuan, et al. (2024). \textit{Evaluating the Quality of Hallucination Benchmarks for Large Vision-Language Models}. arXiv.org.

\bibitem{wang2024rta}
Yuxuan Wang, Yueqian Wang, Dongyan Zhao, et al. (2024). \textit{VideoHallucer: Evaluating Intrinsic and Extrinsic Hallucinations in Large Video-Language Models}. arXiv.org.

\bibitem{xie2024l8a}
Yuxi Xie, Guanzhen Li, Xiao Xu, et al. (2024). \textit{V-DPO: Mitigating Hallucination in Large Vision Language Models via Vision-Guided Direct Preference Optimization}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{guan2023z15}
Tianrui Guan, Fuxiao Liu, Xiyang Wu, et al. (2023). \textit{Hallusionbench: An Advanced Diagnostic Suite for Entangled Language Hallucination and Visual Illusion in Large Vision-Language Models}. Computer Vision and Pattern Recognition.

\bibitem{liang20236sh}
Xun Liang, Shichao Song, Simin Niu, et al. (2023). \textit{UHGEval: Benchmarking the Hallucination of Chinese Large Language Models via Unconstrained Generation}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{huang2023akj}
Lei Huang, Weijiang Yu, Weitao Ma, et al. (2023). \textit{A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions}. ACM Trans. Inf. Syst..

\bibitem{lv2024k5x}
Qitan Lv, Jie Wang, Hanzhu Chen, et al. (2024). \textit{Coarse-to-Fine Highlighting: Reducing Knowledge Hallucination in Large Language Models}. International Conference on Machine Learning.

\bibitem{gu202414e}
Yuzhe Gu, Ziwei Ji, Wenwei Zhang, et al. (2024). \textit{ANAH-v2: Scaling Analytical Hallucination Annotation of Large Language Models}. Neural Information Processing Systems.

\bibitem{huang20247wn}
Wen Huang, Hongbin Liu, Minxin Guo, et al. (2024). \textit{Visual Hallucinations of Multi-modal Large Language Models}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{liu2023882}
Fuxiao Liu, Kevin Lin, Linjie Li, et al. (2023). \textit{Mitigating Hallucination in Large Multi-Modal Models via Robust Instruction Tuning}. International Conference on Learning Representations.

\bibitem{ding2024o88}
Peng Ding, Jingyu Wu, Jun Kuang, et al. (2024). \textit{Hallu-PI: Evaluating Hallucination in Multi-modal Large Language Models within Perturbed Inputs}. ACM Multimedia.

\bibitem{rawte2023ao8}
Vipula Rawte, Swagata Chakraborty, Agnibh Pathak, et al. (2023). \textit{The Troubling Emergence of Hallucination in Large Language Models - An Extensive Definition, Quantification, and Prescriptive Remediations}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{pan2024y3a}
Liangming Pan, Michael Stephen Saxon, Wenda Xu, et al. (2024). \textit{Automatically Correcting Large Language Models: Surveying the Landscape of Diverse Automated Correction Strategies}. Transactions of the Association for Computational Linguistics.

\bibitem{li2023rvf}
Junyi Li, Xiaoxue Cheng, Wayne Xin Zhao, et al. (2023). \textit{HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{zhou2023zu6}
Yiyang Zhou, Chenhang Cui, Jaehong Yoon, et al. (2023). \textit{Analyzing and Mitigating Object Hallucination in Large Vision-Language Models}. International Conference on Learning Representations.

\bibitem{han202439z}
Zongbo Han, Zechen Bai, Haiyang Mei, et al. (2024). \textit{Skip \n: A Simple Method to Reduce Hallucination in Large Vision-Language Models}. arXiv.org.

\bibitem{wang2025jen}
Yining Wang, Mi Zhang, Junjie Sun, et al. (2025). \textit{Mirage in the Eyes: Hallucination Attack on Multi-modal Large Language Models with Only Attention Sink}. arXiv.org.

\bibitem{qu2024pqc}
Xiaoye Qu, Jiashuo Sun, Wei Wei, et al. (2024). \textit{Look, Compare, Decide: Alleviating Hallucination in Large Vision-Language Models via Multi-View Multi-Path Reasoning}. International Conference on Computational Linguistics.

\bibitem{dai20229aa}
Wenliang Dai, Zihan Liu, Ziwei Ji, et al. (2022). \textit{Plausible May Not Be Faithful: Probing Object Hallucination in Vision-Language Pre-training}. Conference of the European Chapter of the Association for Computational Linguistics.

\bibitem{dziri2021bw9}
Nouha Dziri, Andrea Madotto, Osmar Zaiane, et al. (2021). \textit{Neural Path Hunter: Reducing Hallucination in Dialogue Systems via Path Grounding}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{sungbin2024r2g}
Kim Sung-Bin, Oh Hyun-Bin, JungMok Lee, et al. (2024). \textit{AVHBench: A Cross-Modal Hallucination Benchmark for Audio-Visual Large Language Models}. International Conference on Learning Representations.

\bibitem{hakim2024d4u}
Joe B Hakim, Jeffery L. Painter, D. Ramcharran, et al. (2024). \textit{The Need for Guardrails with Large Language Models in Medical Safety-Critical Settings: An Artificial Intelligence Application in the Pharmacovigilance Ecosystem}. arXiv.org.

\bibitem{li2025qzg}
Chaozhuo Li, Pengbo Wang, Chenxu Wang, et al. (2025). \textit{Loki's Dance of Illusions: A Comprehensive Survey of Hallucination in Large Language Models}. arXiv.org.

\bibitem{wang2023ubf}
Lei Wang, Jiabang He, Shenshen Li, et al. (2023). \textit{Mitigating Fine-Grained Hallucination by Fine-Tuning Large Vision-Language Models with Caption Rewrites}. Conference on Multimedia Modeling.

\bibitem{chen2024c4k}
Kedi Chen, Qin Chen, Jie Zhou, et al. (2024). \textit{DiaHalu: A Dialogue-level Hallucination Evaluation Benchmark for Large Language Models}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{ding20244yr}
Hanxing Ding, Liang Pang, Zihao Wei, et al. (2024). \textit{Retrieve Only When It Needs: Adaptive Retrieval Augmentation for Hallucination Mitigation in Large Language Models}. arXiv.org.

\bibitem{deng202405j}
Shijian Deng, Wentian Zhao, Yu-Jhe Li, et al. (2024). \textit{Efficient Self-Improvement in Multimodal Large Language Models: A Model-Level Judge-Free Approach}. arXiv.org.

\bibitem{chen2024j0g}
Junzhe Chen, Tianshu Zhang, Shiyu Huang, et al. (2024). \textit{ICT: Image-Object Cross-Level Trusted Intervention for Mitigating Object Hallucination in Large Vision-Language Models}. Computer Vision and Pattern Recognition.

\bibitem{chen20247jb}
Beitao Chen, Xinyu Lyu, Lianli Gao, et al. (2024). \textit{Alleviating Hallucinations in Large Vision-Language Models through Hallucination-Induced Optimization}. Neural Information Processing Systems.

\bibitem{maynez2020h3q}
Joshua Maynez, Shashi Narayan, Bernd Bohnet, et al. (2020). \textit{On Faithfulness and Factuality in Abstractive Summarization}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{qu20246yn}
Xiaoye Qu, Qiyuan Chen, Wei Wei, et al. (2024). \textit{Alleviating Hallucination in Large Vision-Language Models with Active Retrieval Augmentation}. ACM Transactions on Multimedia Computing, Communications, and Applications (TOMCCAP).

\bibitem{chen2023h04}
Jiawei Chen, Hongyu Lin, Xianpei Han, et al. (2023). \textit{Benchmarking Large Language Models in Retrieval-Augmented Generation}. AAAI Conference on Artificial Intelligence.

\bibitem{zhou2024wbi}
Yiyang Zhou, Chenhang Cui, Rafael Rafailov, et al. (2024). \textit{Aligning Modalities in Vision Large Language Models via Preference Fine-tuning}. arXiv.org.

\bibitem{jiang2024792}
Chaoya Jiang, Wei Ye, Mengfan Dong, et al. (2024). \textit{Hal-Eval: A Universal and Fine-grained Hallucination Evaluation Framework for Large Vision Language Models}. ACM Multimedia.

\bibitem{tjandra2024umq}
Benedict Aaron Tjandra, Muhammed Razzak, Jannik Kossen, et al. (2024). \textit{Fine-Tuning Large Language Models to Appropriately Abstain with Semantic Entropy}. arXiv.org.

\bibitem{umapathi2023puv}
Logesh Kumar Umapathi, Ankit Pal, and Malaikannan Sankarasubbu (2023). \textit{Med-HALT: Medical Domain Hallucination Test for Large Language Models}. Conference on Computational Natural Language Learning.

\bibitem{zou2024dp7}
Xin Zou, Yizhou Wang, Yibo Yan, et al. (2024). \textit{Look Twice Before You Answer: Memory-Space Visual Retracing for Hallucination Mitigation in Multimodal Large Language Models}. arXiv.org.

\bibitem{li2024osp}
Ningke Li, Yuekang Li, Yi Liu, et al. (2024). \textit{Drowzee: Metamorphic Testing for Fact-Conflicting Hallucination Detection in Large Language Models}. Proc. ACM Program. Lang..

\bibitem{chuang20248ey}
Yung-Sung Chuang, Linlu Qiu, Cheng-Yu Hsieh, et al. (2024). \textit{Lookback Lens: Detecting and Mitigating Contextual Hallucinations in Large Language Models Using Only Attention Maps}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{li2024qrj}
Junyi Li, Jie Chen, Ruiyang Ren, et al. (2024). \textit{The Dawn After the Dark: An Empirical Study on Factuality Hallucination in Large Language Models}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{wang2023zop}
Junyan Wang, Yi Zhou, Guohai Xu, et al. (2023). \textit{Evaluation and Analysis of Hallucination in Large Vision-Language Models}. arXiv.org.

\bibitem{xu2024n76}
Ziwei Xu, Sanjay Jain, and Mohan Kankanhalli (2024). \textit{Hallucination is Inevitable: An Innate Limitation of Large Language Models}. arXiv.org.

\bibitem{liu2021mo6}
Tianyu Liu, Yizhe Zhang, C. Brockett, et al. (2021). \textit{A Token-level Reference-free Hallucination Detection Benchmark for Free-form Text Generation}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{manakul20236ex}
Potsawee Manakul, Adian Liusie, and M. Gales (2023). \textit{SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{zhong2024mfi}
Weihong Zhong, Xiaocheng Feng, Liang Zhao, et al. (2024). \textit{Investigating and Mitigating the Multimodal Hallucination Snowballing in Large Vision-Language Models}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{zhang2024qq9}
Yuji Zhang, Sha Li, Jiateng Liu, et al. (2024). \textit{Knowledge Overshadowing Causes Amalgamated Hallucination in Large Language Models}. arXiv.org.

\bibitem{wu20241us}
J. Wu, Tsz Ting Chung, Kai Chen, et al. (2024). \textit{Unified Triplet-Level Hallucination Evaluation for Large Vision-Language Models}. Trans. Mach. Learn. Res..

\bibitem{tonmoy20244e4}
S. Tonmoy, S. M. M. Zaman, Vinija Jain, et al. (2024). \textit{A Comprehensive Survey of Hallucination Mitigation Techniques in Large Language Models}. arXiv.org.

\bibitem{wu2024n00}
Kai Wu, Boyuan Jiang, Zhengkai Jiang, et al. (2024). \textit{NoiseBoost: Alleviating Hallucination with Noise Perturbation for Multimodal Large Language Models}. arXiv.org.

\bibitem{wen2023t6v}
Yilin Wen, Zifeng Wang, and Jimeng Sun (2023). \textit{MindMap: Knowledge Graph Prompting Sparks Graph of Thoughts in Large Language Models}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{li2023249}
Yifan Li, Yifan Du, Kun Zhou, et al. (2023). \textit{Evaluating Object Hallucination in Large Vision-Language Models}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{lan20240yz}
Wei Lan, Wenyi Chen, Qingfeng Chen, et al. (2024). \textit{A Survey of Hallucination in Large Visual Language Models}. arXiv.org.

\bibitem{dhuliawala2023rqn}
S. Dhuliawala, M. Komeili, Jing Xu, et al. (2023). \textit{Chain-of-Verification Reduces Hallucination in Large Language Models}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{sui20242u1}
Yuan Sui, and Bryan Hooi (2024). \textit{Can Knowledge Graphs Make Large Language Models More Trustworthy? An Empirical Study over Open-ended Question Answering}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{xiao2024hv1}
Wenyi Xiao, Ziwei Huang, Leilei Gan, et al. (2024). \textit{Detecting and Mitigating Hallucination in Large Vision Language Models via Fine-Grained AI Feedback}. arXiv.org.

\bibitem{trivedi2022qsf}
H. Trivedi, Niranjan Balasubramanian, Tushar Khot, et al. (2022). \textit{Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{park20247cm}
Yeji Park, Deokyeong Lee, Junsuk Choe, et al. (2024). \textit{ConVis: Contrastive Decoding with Hallucination Visualization for Mitigating Hallucinations in Multimodal Large Language Models}. AAAI Conference on Artificial Intelligence.

\bibitem{sridhar2022l1c}
A. Sridhar, and Erik M. Visser (2022). \textit{Improved Beam Search for Hallucination Mitigation in Abstractive Summarization}. arXiv.org.

\bibitem{su2024lem}
Weihang Su, Changyue Wang, Qingyao Ai, et al. (2024). \textit{Unsupervised Real-Time Hallucination Detection based on the Internal States of Large Language Models}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{luo2023xyc}
Junyu Luo, Cao Xiao, and Fenglong Ma (2023). \textit{Zero-Resource Hallucination Prevention for Large Language Models}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{wu2024h81}
Jun Wu, Q. Liu, Ding Wang, et al. (2024). \textit{Logical Closed Loop: Uncovering Object Hallucinations in Large Vision-Language Models}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{chen2024vy7}
Xuweiyi Chen, Ziqiao Ma, Xuejun Zhang, et al. (2024). \textit{Multi-Object Hallucination in Vision-Language Models}. Neural Information Processing Systems.

\bibitem{zheng20246fk}
Kening Zheng, Junkai Chen, Yibo Yan, et al. (2024). \textit{Reefknot: A Comprehensive Benchmark for Relation Hallucination Evaluation, Analysis and Mitigation in Multimodal Large Language Models}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{chen2024lc5}
Xiang Chen, Chenxi Wang, Yida Xue, et al. (2024). \textit{Unified Hallucination Detection for Multimodal Large Language Models}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{sahoo2024hcb}
Pranab Sahoo, Prabhash Meharia, Akash Ghosh, et al. (2024). \textit{A Comprehensive Survey of Hallucination in Large Language, Image, Video and Audio Foundation Models}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{zhang2024mmj}
Ruiyang Zhang, Hu Zhang, and Zhedong Zheng (2024). \textit{VL-Uncertainty: Detecting Hallucination in Large Vision-Language Model via Uncertainty Estimation}. arXiv.org.

\bibitem{qiu2024zyc}
Han Qiu, Jiaxing Huang, Peng Gao, et al. (2024). \textit{LongHalQA: Long-Context Hallucination Evaluation for MultiModal Large Language Models}. arXiv.org.

\bibitem{oh2024xa3}
Jio Oh, Soyeon Kim, Junseok Seo, et al. (2024). \textit{ERBench: An Entity-Relationship based Automatically Verifiable Hallucination Benchmark for Large Language Models}. Neural Information Processing Systems.

\bibitem{zhang202396g}
Yue Zhang, Leyang Cui, Wei Bi, et al. (2023). \textit{Alleviating Hallucinations of Large Language Models through Induced Hallucinations}. North American Chapter of the Association for Computational Linguistics.

\bibitem{yin2025s2b}
Hao Yin, Guangzong Si, and Zilei Wang (2025). \textit{ClearSight: Visual Signal Enhancement for Object Hallucination Mitigation in Multimodal Large Language Models}. Computer Vision and Pattern Recognition.

\bibitem{goyal2021onb}
Tanya Goyal, Jiacheng Xu, J. Li, et al. (2021). \textit{Training Dynamics for Text Summarization Models}. Findings.

\bibitem{kuan20249pm}
Chun-Yi Kuan, Wei-Ping Huang, and Hung-yi Lee (2024). \textit{Understanding Sounds, Missing the Questions: The Challenge of Object Hallucination in Large Audio-Language Models}. Interspeech.

\bibitem{kaul2024ta7}
Prannay Kaul, Zhizhong Li, Hao Yang, et al. (2024). \textit{THRONE: An Object-Based Hallucination Benchmark for the Free-Form Generations of Large Vision-Language Models}. Computer Vision and Pattern Recognition.

\bibitem{yebin2024txh}
Moon Ye-Bin, Nam Hyeon-Woo, Wonseok Choi, et al. (2024). \textit{BEAF: Observing BEfore-AFter Changes to Evaluate Hallucination in Vision-language Models}. European Conference on Computer Vision.

\bibitem{zhao2024ge8}
Linxi Zhao, Yihe Deng, Weitong Zhang, et al. (2024). \textit{Mitigating Object Hallucination in Large Vision-Language Models via Image-Grounded Guidance}. Unpublished manuscript.

\bibitem{ye2023yom}
Hongbin Ye, Tong Liu, Aijia Zhang, et al. (2023). \textit{Cognitive Mirage: A Review of Hallucinations in Large Language Models}. LKM@IJCAI.

\bibitem{zhang2023k5a}
Hanning Zhang, Shizhe Diao, Yong Lin, et al. (2023). \textit{R-Tuning: Instructing Large Language Models to Say I Dont Know}. North American Chapter of the Association for Computational Linguistics.

\bibitem{li2022ypy}
Yanyang Li, Jianqiao Zhao, M. Lyu, et al. (2022). \textit{Eliciting Knowledge from Large Pre-Trained Models for Unsupervised Knowledge-Grounded Conversation}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{huang2023du3}
Qidong Huang, Xiao-wen Dong, Pan Zhang, et al. (2023). \textit{OPERA: Alleviating Hallucination in Multi-Modal Large Language Models via Over-Trust Penalty and Retrospection-Allocation}. Computer Vision and Pattern Recognition.

\bibitem{tang2024a1j}
Jianheng Tang, Qifan Zhang, Yuhan Li, et al. (2024). \textit{GraphArena: Evaluating and Exploring Large Language Models on Graph Computation}. International Conference on Learning Representations.

\bibitem{fu2024yqj}
Yuhan Fu, Ruobing Xie, Xingwu Sun, et al. (2024). \textit{Mitigating Hallucination in Multimodal Large Language Model via Hallucination-targeted Direct Preference Optimization}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{yao20229uz}
Shunyu Yao, Jeffrey Zhao, Dian Yu, et al. (2022). \textit{ReAct: Synergizing Reasoning and Acting in Language Models}. International Conference on Learning Representations.

\bibitem{kanthara2022kuj}
Shankar Kanthara, Rixie Tiffany Ko Leong, Xiang Lin, et al. (2022). \textit{Chart-to-Text: A Large-Scale Benchmark for Chart Summarization}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{kim2021obx}
Boseop Kim, Hyoungseok Kim, Sang-Woo Lee, et al. (2021). \textit{What Changes Can Large-scale Language Models Bring? Intensive Study on HyperCLOVA: Billions-scale Korean Generative Pretrained Transformers}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{xia20224cl}
Mengzhou Xia, Mikel Artetxe, Chunting Zhou, et al. (2022). \textit{Training Trajectories of Language Models Across Scales}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{aharoni2022ioz}
Roee Aharoni, Shashi Narayan, Joshua Maynez, et al. (2022). \textit{mFACE: Multilingual Summarization with Factual Consistency Evaluation}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{zhang2022p55}
Haopeng Zhang, Semih Yavuz, Wojciech Kryscinski, et al. (2022). \textit{Improving the Faithfulness of Abstractive Summarization via Entity Coverage Control}. NAACL-HLT.

\bibitem{jiang2022reg}
Wenhui Jiang, Minwei Zhu, Yuming Fang, et al. (2022). \textit{Visual Cluster Grounding for Image Captioning}. IEEE Transactions on Image Processing.

\bibitem{wang2020vz6}
Hongmin Wang (2020). \textit{Revisiting Challenges in Data-to-Text Generation with Fact Grounding}. International Conference on Natural Language Generation.

\bibitem{chen2022gkm}
Sihao Chen, S. Buthpitiya, Alex Fabrikant, et al. (2022). \textit{PropSegmEnt: A Large-Scale Corpus for Proposition-Level Segmentation and Entailment Recognition}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{korbak202191w}
Tomasz Korbak, Hady ElSahar, Germn Kruszewski, et al. (2021). \textit{Controlling Conditional Language Models without Catastrophic Forgetting}. International Conference on Machine Learning.

\bibitem{raman20229ce}
K. Raman, Iftekhar Naim, Jiecao Chen, et al. (2022). \textit{Transforming Sequence Tagging Into A Seq2Seq Task}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{liu2021h6c}
Ling Liu, and Mans Hulden (2021). \textit{Can a Transformer Pass the Wug Test? Tuning Copying Bias in Neural Morphological Inflection Models}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{norlund2021462}
Tobias Norlund, Lovisa Hagstrm, and Richard Johansson (2021). \textit{Transferring Knowledge from Vision to Language: How to Achieve it and how to Measure it?}. BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP.

\bibitem{wu20206gt}
Tao Wu, E. Chio, Heng-Tze Cheng, et al. (2020). \textit{Zero-Shot Heterogeneous Transfer Learning from Recommender Systems to Cold-Start Search Retrieval}. International Conference on Information and Knowledge Management.

\bibitem{liu2022hw7}
Yongtai Liu, Joshua Maynez, Gonalo Simes, et al. (2022). \textit{Data Augmentation for Low-Resource Dialogue Summarization}. NAACL-HLT.

\bibitem{jelinek2016205}
L. Jelinek, M. Hauschildt, C. Wittekind, et al. (2016). \textit{Efficacy of Metacognitive Training for Depression: A Randomized Controlled Trial}. Psychotherapy and Psychosomatics.

\bibitem{raunak2022r58}
Vikas Raunak, and Arul Menezes (2022). \textit{Finding Memo: Extractive Memorization in Constrained Sequence Generation Tasks}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{saha20229lo}
Swarnadeep Saha, Xinyan Velocity Yu, Mohit Bansal, et al. (2022). \textit{MURMUR: Modular Multi-Step Reasoning for Semi-Structured Data-to-Text Generation}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{ham20213fx}
Soomin Ham, Kibaek Park, Yeongjun Jang, et al. (2021). \textit{KSL-Guide: A Large-scale Korean Sign Language Dataset Including Interrogative Sentences for Guiding the Deaf and Hard-of-Hearing}. IEEE International Conference on Automatic Face & Gesture Recognition.

\bibitem{jeong20180d2}
Eunji Jeong, Sungwoo Cho, Gyeong-In Yu, et al. (2018). \textit{JANUS: Fast and Flexible Deep Learning via Symbolic Graph Execution of Imperative Programs}. Symposium on Networked Systems Design and Implementation.

\bibitem{kedia2022c03}
Akhil Kedia, Mohd Abbas Zaidi, and Haejun Lee (2022). \textit{FiE: Building a Global Probability Space by Leveraging Early Fusion in Encoder for Open-Domain Question Answering}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{gallace201080s}
A. Gallace, and A. Gallace (2010). \textit{Touch and the body: The role of the somatosensory cortex in tactile awareness}. Unpublished manuscript.

\bibitem{li20203k7}
Xintong Li, Aleksandre Maskharashvili, S. Stevens-Guille, et al. (2020). \textit{Leveraging Large Pretrained Models for WebNLG 2020}. WEBNLG.

\bibitem{zhou2011j8m}
Tom Chao Zhou, Chin-Yew Lin, Irwin King, et al. (2011). \textit{Learning to Suggest Questions in Online Forums}. AAAI Conference on Artificial Intelligence.

\bibitem{injae2016yq6}
Shin In-Jae, Byungkwen Song, and D. Eom (2016). \textit{Auto-Mapping and Configuration Method of IEC 61850 Information Model Based on OPC UA}. Unpublished manuscript.

\bibitem{jeong2019z3k}
Eunji Jeong, Sungwoo Cho, Gyeong-In Yu, et al. (2019). \textit{Speculative Symbolic Graph Execution of Imperative Deep Learning Programs}. ACM SIGOPS Operating Systems Review.

\bibitem{rajani20171n9}
Nazneen Rajani, Mihaela A. Bornea, and Ken Barker (2017). \textit{Stacking With Auxiliary Features for Entity Linking in the Medical Domain}. Workshop on Biomedical Natural Language Processing.

\bibitem{wang202379k}
Shuhe Wang, Xiaofei Sun, Xiaoya Li, et al. (2023). \textit{GPT-NER: Named Entity Recognition via Large Language Models}. North American Chapter of the Association for Computational Linguistics.

\bibitem{alshahwan2024v64}
N. Alshahwan, Jubin Chheda, Anastasia Finogenova, et al. (2024). \textit{Automated Unit Test Improvement using Large Language Models at Meta}. SIGSOFT FSE Companion.

\bibitem{zou2024ucl}
Wei Zou, Runpeng Geng, Binghui Wang, et al. (2024). \textit{PoisonedRAG: Knowledge Poisoning Attacks to Retrieval-Augmented Generation of Large Language Models}. arXiv.org.

\bibitem{fadeeva2024lt8}
Ekaterina Fadeeva, Aleksandr Rubashevskii, Artem Shelmanov, et al. (2024). \textit{Fact-Checking the Output of Large Language Models via Token-Level Uncertainty Quantification}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{nguyen2023obn}
Thuat Nguyen, C. Nguyen, Viet Dac Lai, et al. (2023). \textit{CulturaX: A Cleaned, Enormous, and Multilingual Dataset for Large Language Models in 167 Languages}. International Conference on Language Resources and Evaluation.

\bibitem{zou2024c26}
Wei Zou, Runpeng Geng, Binghui Wang, et al. (2024). \textit{PoisonedRAG: Knowledge Corruption Attacks to Retrieval-Augmented Generation of Large Language Models}. Unpublished manuscript.

\bibitem{li2023f7d}
Huao Li, Yu Quan Chong, Simon Stepputtis, et al. (2023). \textit{Theory of Mind for Multi-Agent Collaboration via Large Language Models}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{wang2023hgw}
Yiming Wang, Zhuosheng Zhang, and Rui Wang (2023). \textit{Element-aware Summarization with Large Language Models: Expert-aligned Evaluation and Chain-of-Thought Method}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{chen2023gii}
Yuyan Chen, Qiang Fu, Yichen Yuan, et al. (2023). \textit{Hallucination Detection: Robustly Discerning Reliable Answers in Large Language Models}. International Conference on Information and Knowledge Management.

\bibitem{gilbert2024uu2}
S. Gilbert, J. Kather, and Aidan Hogan (2024). \textit{Augmented non-hallucinating large language models as medical information curators}. npj Digit. Medicine.

\bibitem{kim2024vgn}
Sunkyu Kim, Choong-kun Lee, and Seung-seob Kim (2024). \textit{Large Language Models: A Guide for Radiologists}. Korean Journal of Radiology.

\bibitem{wang2024sae}
Jianing Wang, Junda Wu, Yupeng Hou, et al. (2024). \textit{InstructGraph: Boosting Large Language Models via Graph-centric Instruction Tuning and Preference Alignment}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{huang20233v0}
Yuheng Huang, Jiayang Song, Zhijie Wang, et al. (2023). \textit{Look Before You Leap: An Exploratory Study of Uncertainty Measurement for Large Language Models}. arXiv.org.

\bibitem{zhao2024s3a}
Linxi Zhao, Yihe Deng, Weitong Zhang, et al. (2024). \textit{Mitigating Object Hallucination in Large Vision-Language Models via Classifier-Free Guidance}. arXiv.org.

\bibitem{leiser2024kfo}
Florian Leiser, S. Eckhardt, Valentin Leuthe, et al. (2024). \textit{HILL: A Hallucination Identifier for Large Language Models}. International Conference on Human Factors in Computing Systems.

\bibitem{malmqvist2024k7x}
Lars Malmqvist (2024). \textit{Sycophancy in Large Language Models: Causes and Mitigations}. arXiv.org.

\bibitem{lin2024gru}
Sheng-Chieh Lin, Luyu Gao, Barlas Ouz, et al. (2024). \textit{FLAME: Factuality-Aware Alignment for Large Language Models}. Neural Information Processing Systems.

\bibitem{li2023dw0}
Xingxuan Li, Ruochen Zhao, Yew Ken Chia, et al. (2023). \textit{Chain of Knowledge: A Framework for Grounding Large Language Models with Structured Knowledge Bases}. arXiv.org.

\bibitem{ma2023mka}
Fan Ma, Xiaojie Jin, Heng Wang, et al. (2023). \textit{Vista-llama: Reducing Hallucination in Video Language Models via Equal Distance to Visual Tokens}. Computer Vision and Pattern Recognition.

\bibitem{song2024t8k}
Peiyang Song, Kaiyu Yang, and Anima Anandkumar (2024). \textit{Towards Large Language Models as Copilots for Theorem Proving in Lean}. arXiv.org.

\bibitem{jiang20242kz}
Che Jiang, Biqing Qi, Xiangyu Hong, et al. (2024). \textit{On Large Language Models Hallucination with Regard to Known Facts}. North American Chapter of the Association for Computational Linguistics.

\bibitem{song2024br2}
Peiyang Song, Kaiyu Yang, and Anima Anandkumar (2024). \textit{Lean Copilot: Large Language Models as Copilots for Theorem Proving in Lean}. NeuS.

\bibitem{liu2024ker}
Haochen Liu, Song Wang, Yaochen Zhu, et al. (2024). \textit{Knowledge Graph-Enhanced Large Language Models via Path Selection}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{sun2024z6b}
Yuhong Sun, Zhangyue Yin, Qipeng Guo, et al. (2024). \textit{Benchmarking Hallucination in Large Language Models Based on Unanswerable Math Word Problem}. International Conference on Language Resources and Evaluation.

\bibitem{ling2024hqv}
Chen Ling, Xujiang Zhao, Wei Cheng, et al. (2024). \textit{Uncertainty Quantification for In-Context Learning of Large Language Models}. North American Chapter of the Association for Computational Linguistics.

\bibitem{li2024jbb}
Moxin Li, Wenjie Wang, Fuli Feng, et al. (2024). \textit{Think Twice Before Trusting: Self-Detection for Large Language Models through Comprehensive Answer Reflection}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{smith2023i8w}
Andrew L Smith, Felix Greaves, and T. Panch (2023). \textit{Hallucination or Confabulation? Neuroanatomy as metaphor in Large Language Models}. PLOS Digital Health.

\bibitem{shah20242sx}
Savyasachi V. Shah (2024). \textit{Accuracy, Consistency, and Hallucination of Large Language Models When Analyzing Unstructured Clinical Notes in Electronic Medical Records.}. JAMA Network Open.

\bibitem{pan2024hm4}
Zhenyu Pan, Haozheng Luo, Manling Li, et al. (2024). \textit{Chain-of-Action: Faithful and Multimodal Question Answering through Large Language Models}. International Conference on Learning Representations.

\bibitem{zhang2024o58}
Hengran Zhang, Ruqing Zhang, J. Guo, et al. (2024). \textit{Are Large Language Models Good at Utility Judgments?}. Annual International ACM SIGIR Conference on Research and Development in Information Retrieval.

\bibitem{tang2024cxa}
Jianheng Tang, Qifan Zhang, Yuhan Li, et al. (2024). \textit{GraphArena: Benchmarking Large Language Models on Graph Computational Problems}. arXiv.org.

\bibitem{zhou2024d14}
Xiongtao Zhou, Jie He, Yuhua Ke, et al. (2024). \textit{An Empirical Study on Parameter-Efficient Fine-Tuning for MultiModal Large Language Models}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{zhao2024g9c}
Ruilin Zhao, Feng Zhao, Long Wang, et al. (2024). \textit{KG-CoT: Chain-of-Thought Prompting of Large Language Models over Knowledge Graphs for Knowledge-Aware Question Answering}. International Joint Conference on Artificial Intelligence.

\bibitem{pan2024uot}
Zhenyu Pan, Haozheng Luo, Manling Li, et al. (2024). \textit{Conv-CoA: Improving Open-domain Question Answering in Large Language Models via Conversational Chain-of-Action}. arXiv.org.

\bibitem{zhu2024hll}
Derui Zhu, Dingfan Chen, Qing Li, et al. (2024). \textit{PoLLMgraph: Unraveling Hallucinations in Large Language Models via State Transition Dynamics}. NAACL-HLT.

\bibitem{zhang20252at}
Wan Zhang, and Jing Zhang (2025). \textit{Hallucination Mitigation for Retrieval-Augmented Large Language Models: A Review}. Mathematics.

\bibitem{chen2024kgu}
Lida Chen, Zujie Liang, Xintao Wang, et al. (2024). \textit{Teaching Large Language Models to Express Knowledge Boundary from Their Own Signals}. Proceedings of the 3rd Workshop on Towards Knowledgeable Foundation Models (KnowFM).

\bibitem{dernbach2024w0b}
Stefan Dernbach, Khushbu Agarwal, Alejandro Zuniga, et al. (2024). \textit{GLaM: Fine-Tuning Large Language Models for Domain Knowledge Graph Alignment via Neighborhood Partitioning and Generative Subgraph Encoding}. AAAI Spring Symposia.

\bibitem{wu202407f}
Jiageng Wu, Xian Wu, and Jie Yang (2024). \textit{Guiding Clinical Reasoning with Large Language Models via Knowledge Seeds}. International Joint Conference on Artificial Intelligence.

\bibitem{ahn2024r1o}
Jaewoo Ahn, Taehyun Lee, Junyoung Lim, et al. (2024). \textit{TimeChara: Evaluating Point-in-Time Character Hallucination of Role-Playing Large Language Models}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{mou2024fsy}
Xinyi Mou, Zejun Li, Hanjia Lyu, et al. (2024). \textit{Unifying Local and Global Knowledge: Empowering Large Language Models as Political Experts with Knowledge Graphs}. The Web Conference.

\bibitem{xu2024f68}
Derong Xu, Ziheng Zhang, Zhihong Zhu, et al. (2024). \textit{Editing Factual Knowledge and Explanatory Ability of Medical Large Language Models}. International Conference on Information and Knowledge Management.

\bibitem{hu2024fnt}
Xiangkun Hu, Dongyu Ru, Lin Qiu, et al. (2024). \textit{RefChecker: Reference-based Fine-grained Hallucination Checker and Benchmark for Large Language Models}. arXiv.org.

\bibitem{mukherjee2024o5w}
Subhojyoti Mukherjee, Anusha Lalitha, Sailik Sengupta, et al. (2024). \textit{Multi-Objective Alignment of Large Language Models Through Hypervolume Maximization}. arXiv.org.

\bibitem{jiao2024l4e}
Qirui Jiao, Daoyuan Chen, Yilun Huang, et al. (2024). \textit{Enhancing Multimodal Large Language Models with Vision Detection Models: An Empirical Study}. arXiv.org.

\bibitem{ding20245e3}
Hao Ding, Ziwei Fan, Ingo Ghring, et al. (2024). \textit{Reasoning and Planning with Large Language Models in Code Development}. Knowledge Discovery and Data Mining.

\bibitem{wang2024swy}
Chengpeng Wang, Wuqi Zhang, Zian Su, et al. (2024). \textit{Sanitizing Large Language Models in Bug Detection with Data-Flow}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{chen2024md6}
Zhuo Chen, Jiawei Liu, Haotan Liu, et al. (2024). \textit{Black-Box Opinion Manipulation Attacks to Retrieval-Augmented Generation of Large Language Models}. arXiv.org.

\bibitem{wang2023ynd}
Xiaohua Wang, Yuliang Yan, Longtao Huang, et al. (2023). \textit{Hallucination Detection for Generative Large Language Models by Bayesian Sequential Estimation}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{yeo2024g7d}
Wei Jie Yeo, Teddy Ferdinan, Przemysaw Kazienko, et al. (2024). \textit{Self-training Large Language Models through Knowledge Detection}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{hu2024fld}
Sihao Hu, Tiansheng Huang, and Ling Liu (2024). \textit{PokeLLMon: A Human-Parity Agent for Pokemon Battles with Large Language Models}. arXiv.org.

\bibitem{zhang2024ia4}
Zhenhong Zhang, Jiajing Chen, Weiyan Shi, et al. (2024). \textit{Contrastive Learning for Knowledge-Based Question Generation in Large Language Models}. 2024 5th International Conference on Intelligent Computing and Human-Computer Interaction (ICHCI).

\bibitem{yang2024iia}
Dingkang Yang, Dongling Xiao, Jinjie Wei, et al. (2024). \textit{Improving Factuality in Large Language Models via Decoding-Time Hallucinatory and Truthful Comparators}. AAAI Conference on Artificial Intelligence.

\bibitem{yu2023ine}
Xiaodong Yu, Hao Cheng, Xiaodong Liu, et al. (2023). \textit{ReEval: Automatic Hallucination Evaluation for Retrieval-Augmented Large Language Models via Transferable Adversarial Attacks}. NAACL-HLT.

\bibitem{gu2024eig}
Zishan Gu, Changchang Yin, Fenglin Liu, et al. (2024). \textit{MedVH: Towards Systematic Evaluation of Hallucination for Large Vision Language Models in the Medical Context}. arXiv.org.

\bibitem{liu2024kf2}
Lihui Liu, Zihao Wang, Ruizhong Qiu, et al. (2024). \textit{Logic Query of Thoughts: Guiding Large Language Models to Answer Complex Logic Queries with Knowledge Graphs}. arXiv.org.

\bibitem{woo2024dtm}
B. Woo, Tom Huynh, Arthur Tang, et al. (2024). \textit{Transforming nursing with large language models: from concept to practice.}. European Journal of Cardiovascular Nursing.

\bibitem{zhao20246wi}
Xiutian Zhao, Ke Wang, and Wei Peng (2024). \textit{Measuring the Inconsistency of Large Language Models in Preferential Ranking}. KNOWLLM.

\bibitem{qian2024mj9}
Xinying Qian, Ying Zhang, Yu Zhao, et al. (2024). \textit{TimeR^4 : Time-aware Retrieval-Augmented Large Language Models for Temporal Knowledge Graph Question Answering}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{butler20242xs}
J. Butler, James Puleo, Michael Harrington, et al. (2024). \textit{From technical to understandable: Artificial Intelligence Large Language Models improve the readability of knee radiology reports.}. Knee Surgery, Sports Traumatology, Arthroscopy.

\bibitem{sahoo202420w}
N. R. Sahoo, Ashita Saxena, Kishan Maharaj, et al. (2024). \textit{Addressing Bias and Hallucination in Large Language Models}. International Conference on Language Resources and Evaluation.

\bibitem{zuo20242i0}
Kaiwen Zuo, and Yirui Jiang (2024). \textit{MedHallBench: A New Benchmark for Assessing Hallucination in Medical Large Language Models}. arXiv.org.

\bibitem{parente2024vlq}
D. J. Parente (2024). \textit{Generative Artificial Intelligence and Large Language Models in Primary Care Medical Education.}. Family Medicine.

\bibitem{zhao2024h5n}
Haiyan Zhao, Fan Yang, Himabindu Lakkaraju, et al. (2024). \textit{Opening the Black Box of Large Language Models: Two Views on Holistic Interpretability}. arXiv.org.

\bibitem{zhou2024b0u}
Yue Zhou, Henry Peng Zou, Barbara Di Eugenio, et al. (2024). \textit{Large Language Models Are Involuntary Truth-Tellers: Exploiting Fallacy Failure for Jailbreak Attacks}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{adewumi2024lv9}
Tosin P. Adewumi, Nudrat Habib, Lama Alkhaled, et al. (2024). \textit{On the Limitations of Large Language Models (LLMs): False Attribution}. arXiv.org.

\bibitem{toroghi2024mxf}
Armin Toroghi, Willis Guo, Mohammad Mahdi Torabi pour, et al. (2024). \textit{Right for Right Reasons: Large Language Models for Verifiable Commonsense Knowledge Graph Question Answering}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{taveekitworachai2024aql}
Pittawat Taveekitworachai, Febri Abdullah, and R. Thawonmas (2024). \textit{Null-Shot Prompting: Rethinking Prompting Large Language Models With Hallucination}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{wan2024mh1}
Fanqi Wan, Xinting Huang, Leyang Cui, et al. (2024). \textit{Mitigating Hallucinations of Large Language Models via Knowledge Consistent Alignment}. arXiv.org.

\bibitem{alsadat2024i78}
Shayan Meshkat Alsadat, Jean-Raphael Gaglione, D. Neider, et al. (2024). \textit{Using Large Language Models to Automate and Expedite Reinforcement Learning with Reward Machine}. American Control Conference.

\bibitem{cao2024o9a}
Qingxing Cao, Junhao Cheng, Xiaodan Liang, et al. (2024). \textit{VisDiaHalBench: A Visual Dialogue Benchmark For Diagnosing Hallucination in Large Vision-Language Models}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{benkirane202494i}
Kenza Benkirane, Laura Gongas, Shahar Pelles, et al. (2024). \textit{Machine Translation Hallucination Detection for Low and High Resource Languages using Large Language Models}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{jin2024jpw}
Lifeng Jin, Baolin Peng, Linfeng Song, et al. (2024). \textit{Collaborative decoding of critical tokens for boosting factuality of large language models}. arXiv.org.

\bibitem{mu2024f3b}
Yida Mu, Peizhen Bai, Kalina Bontcheva, et al. (2024). \textit{Addressing Topic Granularity and Hallucination in Large Language Models for Topic Modelling}. arXiv.org.

\bibitem{yu2024pp9}
Jun Yu, Yunxiang Zhang, Zerui Zhang, et al. (2024). \textit{RAG-Guided Large Language Models for Visual Spatial Description with Adaptive Hallucination Corrector}. ACM Multimedia.

\bibitem{amirizaniani2024cad}
Maryam Amirizaniani, Jihan Yao, Adrian Lavergne, et al. (2024). \textit{LLMAuditor: A Framework for Auditing Large Language Models Using Human-in-the-Loop}. Unpublished manuscript.

\bibitem{ling2024qto}
Chen Ling, Xujiang Zhao, Wei Cheng, et al. (2024). \textit{Uncertainty Decomposition and Quantification for In-Context Learning of Large Language Models}. arXiv.org.

\bibitem{sarmah2023cuq}
Bhaskarjit Sarmah, Dhagash Mehta, Stefano Pasquali, et al. (2023). \textit{Towards reducing hallucination in extracting information from financial reports using Large Language Models}. International Conference on AI-ML-Systems.

\bibitem{nathani202338c}
Deepak Nathani, David Wang, Liangming Pan, et al. (2023). \textit{MAF: Multi-Aspect Feedback for Improving Reasoning in Large Language Models}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{chataigner2024cr0}
Cl'ea Chataigner, Afaf Tak, and G. Farnadi (2024). \textit{Multilingual Hallucination Gaps in Large Language Models}. arXiv.org.

\bibitem{liu2025xwv}
Q. Liu, Xinlong Chen, Yue Ding, et al. (2025). \textit{Attention-guided Self-reflection for Zero-shot Hallucination Detection in Large Language Models}. arXiv.org.

\bibitem{song2024v5n}
Jongyoon Song, Sangwon Yu, and Sungroh Yoon (2024). \textit{Large Language Models are Skeptics: False Negative Problem of Input-conflicting Hallucination}. arXiv.org.

\bibitem{barkley202472d}
Liam Barkley, and Brink van der Merwe (2024). \textit{Investigating the Role of Prompting and External Tools in Hallucination Rates of Large Language Models}. arXiv.org.

\bibitem{huang2024c9t}
Chao-Wei Huang, and Yun-Nung Chen (2024). \textit{FactAlign: Long-form Factuality Alignment of Large Language Models}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{malin2024fin}
B. Malin, Tatiana Kalganova, and Nikoloas Boulgouris (2024). \textit{A review of faithfulness metrics for hallucination assessment in Large Language Models}. IEEE Journal on Selected Topics in Signal Processing.

\bibitem{yuan2024o7d}
Hongbang Yuan, Pengfei Cao, Zhuoran Jin, et al. (2024). \textit{Whispers that Shake Foundations: Analyzing and Mitigating False Premise Hallucinations in Large Language Models}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{pandit20257jx}
Shrey Pandit, Jiawei Xu, Junyuan Hong, et al. (2025). \textit{MedHallu: A Comprehensive Benchmark for Detecting Medical Hallucinations in Large Language Models}. arXiv.org.

\bibitem{bellinileite2023y38}
Samuel C. Bellini-Leite (2023). \textit{Dual Process Theory for Large Language Models: An overview of using Psychology to address hallucination and reliability issues}. Adaptive Behavior.

\bibitem{omar2025us3}
M. Omar, V. Sorin, J. Collins, et al. (2025). \textit{Large Language Models Are Highly Vulnerable to Adversarial Hallucination Attacks in Clinical Decision Support: A Multi-Model Assurance Analysis}. medRxiv.

\bibitem{lee2024i72}
Yi-Lun Lee, Yi-Hsuan Tsai, and Wei-Chen Chiu (2024). \textit{Delve into Visual Contrastive Decoding for Hallucination Mitigation of Large Vision-Language Models}. arXiv.org.

\bibitem{li2023irg}
Junyi Li, Xiaoxue Cheng, Wayne Xin Zhao, et al. (2023). \textit{HELMA: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models}. Unpublished manuscript.

\bibitem{zhang2023pb6}
Chen Zhang (2023). \textit{User-Controlled Knowledge Fusion in Large Language Models: Balancing Creativity and Hallucination}. arXiv.org.

\bibitem{irulandi2023xlg}
Muneeswaran Irulandi, Shreya Saxena, Siva Prasad, et al. (2023). \textit{Minimizing Factual Inconsistency and Hallucination in Large Language Models}. arXiv.org.

\bibitem{li2024ncc}
Derong Xu Xinhang Li, Ziheng Zhang, Zhenxi Lin, et al. (2024). \textit{Harnessing Large Language Models for Knowledge Graph Question Answering via Adaptive Multi-Aspect Retrieval-Augmentation}. arXiv.org.

\bibitem{guo2024tlu}
Hongyi Guo, Zhihan Liu, Yufeng Zhang, et al. (2024). \textit{Can Large Language Models Play Games? A Case Study of A Self-Play Approach}. arXiv.org.

\bibitem{xie20247zk}
Zikai Xie (2024). \textit{Order Matters in Hallucination: Reasoning Order as Benchmark and Reflexive Prompting for Large-Language-Models}. arXiv.org.

\bibitem{amirizaniani2024493}
Maryam Amirizaniani, Jihan Yao, Adrian Lavergne, et al. (2024). \textit{Developing a Framework for Auditing Large Language Models Using Human-in-the-Loop}. arXiv.org.

\bibitem{yang2025n54}
Tianyun Yang, Ziniu Li, Juan Cao, et al. (2025). \textit{Understanding and Mitigating Hallucination in Large Vision-Language Models via Modular Attribution and Intervention}. International Conference on Learning Representations.

\bibitem{agarwal202418c}
Vibhor Agarwal, Yulong Pei, Salwa Alamir, et al. (2024). \textit{CodeMirage: Hallucinations in Code Generated by Large Language Models}. arXiv.org.

\bibitem{rrv2024gw0}
Aswin Rrv, Nemika Tyagi, Md Nayem Uddin, et al. (2024). \textit{Chaos with Keywords: Exposing Large Language Models Sycophantic Hallucination to Misleading Keywords and Evaluating Defense Strategies}. Unpublished manuscript.

\bibitem{li2024hl9}
Mingchen Li, Zaifu Zhan, Han Yang, et al. (2024). \textit{Benchmarking Retrieval-Augmented Large Language Models in Biomedical NLP: Application, Robustness, and Self-Awareness}. arXiv.org.

\bibitem{wang2024t4o}
Shirui Wang, Bohan Xie, Ling Ding, et al. (2024). \textit{SeCor: Aligning Semantic and Collaborative Representations by Large Language Models for Next-Point-of-Interest Recommendations}. ACM Conference on Recommender Systems.

\bibitem{hegselmann20249q4}
S. Hegselmann, Zejiang Shen, Florian Gierse, et al. (2024). \textit{A Data-Centric Approach To Generate Faithful and High Quality Patient Summaries with Large Language Models}. ACM Conference on Health, Inference, and Learning.

\bibitem{gao2024ncr}
Jun Gao, Huan Zhao, Wei Wang, et al. (2024). \textit{EventRL: Enhancing Event Extraction with Outcome Supervision for Large Language Models}. arXiv.org.

\bibitem{tsai2024klg}
Yao-Hung Tsai, Walter Talbott, and Jian Zhang (2024). \textit{Efficient Non-Parametric Uncertainty Quantification for Black-Box Large Language Models and Decision Planning}. arXiv.org.

\bibitem{chen2024qs5}
Xinxi Chen, Li Wang, Wei Wu, et al. (2024). \textit{Honest AI: Fine-Tuning "Small" Language Models to Say "I Don't Know", and Reducing Hallucination in RAG}. arXiv.org.

\bibitem{liu2025juo}
MingShan Liu, Shi Bo, and Jialing Fang (2025). \textit{Enhancing Mathematical Reasoning in Large Language Models with Self-Consistency-Based Hallucination Detection}. arXiv.org.

\bibitem{zhang2024htn}
Taolin Zhang, Qizhou Chen, Dongyang Li, et al. (2024). \textit{DAFNet: Dynamic Auxiliary Fusion for Sequential Model Editing in Large Language Models}. Annual Meeting of the Association for Computational Linguistics.

\bibitem{guo2024hgn}
Yuhang Guo, and Zhiyu Wan (2024). \textit{Performance Evaluation of Multimodal Large Language Models (LLaVA and GPT-4-based ChatGPT) in Medical Image Classification Tasks}. IEEE International Conference on Healthcare Informatics.

\bibitem{luo2024uh8}
Weiqing Luo, Chonggang Song, Lingling Yi, et al. (2024). \textit{KELLMRec: Knowledge-Enhanced Large Language Models for Recommendation}. arXiv.org.

\bibitem{hamid2024pwn}
Oussama H. Hamid (2024). \textit{Beyond Probabilities: Unveiling the Delicate Dance of Large Language Models (LLMs) and AI-Hallucination}. Conference on Cognitive and Computational Aspects of Situation Management.

\bibitem{zheng20240qd}
Xinxin Zheng, Feihu Che, Jinyang Wu, et al. (2024). \textit{KS-LLM: Knowledge Selection of Large Language Models with Evidence Document for Question Answering}. arXiv.org.

\bibitem{rawte2024bu6}
Vipula Rawte, Aman Chadha, Amit P. Sheth, et al. (2024). \textit{Tutorial Proposal: Hallucination in Large Language Models}. International Conference on Language Resources and Evaluation.

\bibitem{yin2024iau}
Zhibo Yin (2024). \textit{A review of methods for alleviating hallucination issues in large language models}. Applied and Computational Engineering.

\bibitem{tang2025mfi}
Zilu Tang, Rajen Chatterjee, and Sarthak Garg (2025). \textit{Mitigating Hallucinated Translations in Large Language Models with Hallucination-focused Preference Optimization}. North American Chapter of the Association for Computational Linguistics.

\bibitem{das2024jdt}
Souvik Das, Lifeng Jin, Linfeng Song, et al. (2024). \textit{Entropy Guided Extrapolative Decoding to Improve Factuality in Large Language Models}. International Conference on Computational Linguistics.

\bibitem{zhang2024h4a}
Yuxiang Zhang, Jing Chen, Junjie Wang, et al. (2024). \textit{ToolBeHonest: A Multi-level Hallucination Diagnostic Benchmark for Tool-Augmented Large Language Models}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{xu2024t34}
Derong Xu, Ziheng Zhang, Zhihong Zhu, et al. (2024). \textit{Mitigating Hallucinations of Large Language Models in Medical Information Extraction via Contrastive Decoding}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{zhang2025p1z}
Hongjie Zhang, Hourui Deng, Jie Ou, et al. (2025). \textit{Mitigating spatial hallucination in large language models for path planning via prompt engineering}. Scientific Reports.

\bibitem{ahmadi2024j88}
Ali Ahmadi (2024). \textit{Unravelling the Mysteries of Hallucination in Large Language Models: Strategies for Precision in Artificial Intelligence Language Generation}. Asian Journal of Computer Science and Technology.

\bibitem{abdelghafour2024efh}
M. Abdelghafour, Mohammed Mabrouk, and Zaki Taha (2024). \textit{Hallucination Mitigation Techniques in Large Language Models}. International Journal of Intelligent Computing and Information Sciences.

\bibitem{zhou20253zv}
Xiaoling Zhou, Mingjie Zhang, Zhemg Lee, et al. (2025). \textit{HaDeMiF: Hallucination Detection and Mitigation in Large Language Models}. International Conference on Learning Representations.

\bibitem{karbasi2025j7n}
Amin Karbasi, Omar Montasser, John Sous, et al. (2025). \textit{(Im)possibility of Automated Hallucination Detection in Large Language Models}. arXiv.org.

\bibitem{mubarak2024lx6}
Hamdy Mubarak, Hend Suliman Al-Khalifa, and Khaloud Suliman Alkhalefah (2024). \textit{Halwasa: Quantify and Analyze Hallucinations in Large Language Models: Arabic as a Case Study}. International Conference on Language Resources and Evaluation.

\bibitem{zhang2024sbu}
Wenbo Zhang, Zihang Xu, and Hengrui Cai (2024). \textit{Recognizing Limits: Investigating Infeasibility in Large Language Models}. Unpublished manuscript.

\bibitem{omar2025cc3}
Mahmud Omar, Vera Sorin, Jeremy D. Collins, et al. (2025). \textit{Multi-model assurance analysis showing large language models are highly vulnerable to adversarial hallucination attacks during clinical decision support}. Communications Medicine.

\bibitem{gao20242nu}
Zhengjie Gao, Xuanzi Liu, Yuanshuai Lan, et al. (2024). \textit{A Brief Survey on Safety of Large Language Models}. Journal of computer & information technology.

\bibitem{pester20242zt}
Andreas Pester, Ahmed Tammaa, Christian Gtl, et al. (2024). \textit{Conversational Agents, Virtual Worlds, and Beyond: A Review of Large Language Models Enabling Immersive Learning}. IEEE Global Engineering Education Conference.

\bibitem{wu202415r}
Kangxi Wu, Liang Pang, Huawei Shen, et al. (2024). \textit{Enhancing Training Data Attribution for Large Language Models with Fitting Error Consideration}. Conference on Empirical Methods in Natural Language Processing.

\bibitem{tu2024v40}
Yahan Tu, Rui Hu, and Jitao Sang (2024). \textit{ODE: Open-Set Evaluation of Hallucinations in Multimodal Large Language Models}. Computer Vision and Pattern Recognition.

\end{thebibliography}

\end{document}