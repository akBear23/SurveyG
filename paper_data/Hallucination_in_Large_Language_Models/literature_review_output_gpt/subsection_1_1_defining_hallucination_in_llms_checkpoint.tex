\subsection{Defining Hallucination in LLMs}

Hallucination in large language models (LLMs) refers to the generation of plausible but factually incorrect or nonsensical information, posing significant challenges for their deployment in critical applications. This phenomenon has garnered increasing attention in recent years, leading to a rich body of research aimed at understanding, quantifying, and mitigating hallucinations in LLMs.

The foundational work by \cite{maynez2020h3q} systematically categorized hallucinations into intrinsic and extrinsic types, emphasizing the inadequacy of traditional metrics for evaluating model outputs. This paper highlighted the need for semantic-aware evaluation methods, paving the way for more nuanced approaches to understanding hallucination. Following this, \cite{du2023qu7} introduced a framework that quantifies hallucination levels and attributes them to specific model deficiencies, marking a significant advancement in the ability to diagnose and address hallucinations in LLMs.

As the field progressed, researchers began to explore practical mitigation strategies. For instance, \cite{yao20229uz} proposed the ReAct paradigm, which synergizes reasoning and acting in LLMs to ground responses in external knowledge, effectively reducing hallucination rates. This approach was complemented by \cite{trivedi2022qsf}, who introduced a method that interleaves retrieval with chain-of-thought reasoning, further enhancing the factual accuracy of LLM outputs. Both studies illustrate a shift towards integrating external knowledge sources to combat hallucination, a theme that continued to evolve in subsequent research.

In the realm of multi-modal models, \cite{li2023249} investigated object hallucination in large vision-language models, revealing that inconsistencies between generated text and visual input can lead to substantial hallucination. This work underscored the necessity for tailored evaluation methods that account for the unique challenges posed by multi-modal contexts, thereby expanding the understanding of hallucination beyond text-based models.

The theoretical underpinnings of hallucination were further explored by \cite{xu2024n76}, who provided a formal proof that hallucination is an inherent limitation of all computable LLMs. This groundbreaking insight shifted the discourse from attempts at complete elimination of hallucination to a focus on management and mitigation, thus framing hallucination as an unavoidable aspect of LLM functionality.

Recent advancements have also focused on automated detection methods. For example, \cite{manakul20236ex} introduced SelfCheckGPT, a zero-resource black-box hallucination detection method that leverages the stochastic nature of LLM outputs to identify inconsistencies. This approach highlights the ongoing trend towards developing detection mechanisms that do not rely on internal model access, making them applicable to proprietary systems.

Despite these advancements, challenges remain. Papers like \cite{gao2023ht7} and \cite{chen2023h04} emphasize the importance of verifiable citation generation and retrieval-augmented generation, respectively, yet they also reveal that current methods still struggle with preventing hallucination effectively. The need for robust benchmarks, such as those proposed by \cite{oh2024xa3} and \cite{chen2024c4k}, is evident as they aim to evaluate hallucination in more complex, dialogue-based contexts.

In conclusion, while significant strides have been made in defining, quantifying, and mitigating hallucination in LLMs, the field still faces unresolved issues. Future research must continue to explore innovative detection and correction strategies, refine evaluation benchmarks, and address the inherent limitations of LLMs, particularly as they are deployed in high-stakes environments where accuracy and trustworthiness are paramount.
```