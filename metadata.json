{
    "2a3f862199883ceff5e3c74126f0c80770653e05.pdf": {
        "title": "Knowledge Graph Embedding by Translating on Hyperplanes",
        "authors": [
            "Zhen Wang",
            "Jianwen Zhang",
            "Jianlin Feng",
            "Zheng Chen"
        ],
        "published_date": "2014",
        "venue": "AAAI Conference on Artificial Intelligence",
        "journal": null,
        "abstract": "\n \n We deal with embedding a large scale knowledge graph composed of entities and relations into a continuous vector space. TransE is a promising method proposed recently, which is very efficient while achieving state-of-the-art predictive performance. We discuss some mapping properties of relations which should be considered in embedding, such as reflexive, one-to-many, many-to-one, and many-to-many. We note that TransE does not do well in dealing with these properties. Some complex models are capable of preserving these mapping properties but sacrifice efficiency in the process. To make a good trade-off between model capacity and efficiency, in this paper we propose TransH which models a relation as a hyperplane together with a translation operation on it. In this way, we can well preserve the above mapping properties of relations with almost the same model complexity of TransE. Additionally, as a practical knowledge graph is often far from completed, how to construct negative examples to reduce false negative labels in training is very important. Utilizing the one-to-many/many-to-one mapping property of a relation, we propose a simple trick to reduce the possibility of false negative labeling. We conduct extensive experiments on link prediction, triplet classification and fact extraction on benchmark datasets like WordNet and Freebase. Experiments show TransH delivers significant improvements over TransE on predictive accuracy with comparable capability to scale up.\n \n",
        "keywords": [
            "computer science",
            "model",
            "method"
        ],
        "paper_type": "Methodology",
        "summary": "The main contribution of this paper is the introduction of **TransH**, a novel knowledge graph embedding model designed to effectively handle complex relation mapping properties (such as reflexive, one-to-many, many-to-one, and many-to-many) while maintaining the computational efficiency characteristic of simpler models like TransE.\n\n**Problem Addressed:**\nThe paper addresses the challenge of embedding large-scale knowledge graphs into continuous vector spaces. While TransE, a recently proposed method, offers efficiency and good predictive performance, it struggles to adequately model complex mapping properties inherent in real-world relations. More sophisticated models can preserve these properties but often sacrifice efficiency. The paper aims to find a good trade-off between model capacity (ability to capture complex relation types) and computational efficiency. Additionally, it tackles the practical problem of constructing negative examples during training to reduce false negative labels in incomplete knowledge graphs.\n\n**Method Used:**\nTransH models a relation not just as a translation vector, but as a **hyperplane** in the embedding space, along with a **translation operation performed on that hyperplane**. For a given triplet (head entity, relation, tail entity), both the head and tail entities are first projected onto the relation-specific hyperplane. The translation operation then occurs between these projected entities. This mechanism allows TransH to differentiate between entities involved in complex mappings (e.g., multiple tails for one head) without significantly increasing model complexity. Furthermore, the paper proposes a simple trick for constructing negative examples by leveraging the one-to-many/many-to-one mapping properties of relations, which helps reduce the likelihood of false negative labeling during training.\n\n**Key Findings:**\nExtensive experiments on benchmark datasets like WordNet and Freebase, across tasks such as link prediction, triplet classification, and fact extraction, demonstrate that TransH delivers significant improvements in predictive accuracy compared to TransE. Crucially, it achieves this enhanced performance with comparable scalability and efficiency, effectively balancing model capacity with computational cost. The model successfully preserves complex relation mapping properties, overcoming a key limitation of TransE.\n\n**Key Math Equations (Conceptual):**\nWhile explicit equations are not provided in the abstract, the core mathematical concept of TransH can be described as follows:\n1.  Each relation `r` is represented by two vectors: a normal vector `w_r` (defining the hyperplane) and a translation vector `d_r` (which lies on the hyperplane, i.e., `d_r \u22a5 w_r`).\n2.  For a given triplet `(h, r, t)`, the entity embeddings `h` and `t` are first projected onto the hyperplane defined by `w_r`. The projection of an entity vector `v` onto this hyperplane can be conceptually represented as `v_\u22a5 = v - (v^T w_r)w_r` (assuming `w_r` is a unit vector).\n3.  The scoring function for a valid triplet aims to minimize the distance between the translated projected head and the projected tail:\n    `f(h, r, t) = || (h - (h^T w_r)w_r) + d_r - (t - (t^T w_r)w_r) ||`\n    where `||.||` denotes a distance metric (e.g., L1 or L2 norm).\n\n**New Direction: Yes**\nReasoning: This paper proposes a new *method path* within the existing research direction of translational knowledge graph embedding models. While TransE established the translation-based paradigm, TransH introduces a fundamentally different and more sophisticated way to model relations by incorporating relation-specific hyperplanes. This conceptual shift allows it to address complex mapping properties that TransE could not, thereby significantly enhancing the capacity of translational models and opening up new avenues for their improvement. It's not just an incremental tweak but a significant methodological innovation within its class.\n\n**Type: Methodology**",
        "file_path": "paper_data/knowledge_graph_embedding/core_papers/2a3f862199883ceff5e3c74126f0c80770653e05.pdf"
    },
    "4e52607397a96fb2104a99c570c9cec29c9ca519.pdf": {
        "title": "ChronoR: Rotation Based Temporal Knowledge Graph Embedding",
        "authors": [
            "A. Sadeghian",
            "Mohammadreza Armandpour",
            "Anthony Colas",
            "D. Wang"
        ],
        "published_date": "2021",
        "venue": "AAAI Conference on Artificial Intelligence",
        "journal": null,
        "abstract": "Despite the importance and abundance of temporal knowledge graphs, most of the current research has been focused on reasoning on static graphs. In this paper, we study the challenging problem of inference over temporal knowledge graphs. In particular, the task of temporal link prediction. In general, this is a difficult task due to data non-stationarity, data heterogeneity, and its complex temporal dependencies. \nWe propose Chronological Rotation embedding (ChronoR), a novel model for learning representations for entities, relations, and time. Learning dense representations is frequently used as an efficient and versatile method to perform reasoning on knowledge graphs. The proposed model learns a k-dimensional rotation transformation parametrized by relation and time, such that after each fact's head entity is transformed using the rotation, it falls near its corresponding tail entity. By using high dimensional rotation as its transformation operator, ChronoR captures rich interaction between the temporal and multi-relational characteristics of a Temporal Knowledge Graph. Experimentally, we show that ChronoR is able to outperform many of the state-of-the-art methods on the benchmark datasets for temporal knowledge graph link prediction.",
        "keywords": [
            "computer science",
            "model",
            "method"
        ],
        "paper_type": "Methodology",
        "summary": "**Summary of Main Contribution:**\n\nThis paper addresses the challenging problem of inference over temporal knowledge graphs (TKGs), specifically focusing on the task of temporal link prediction. This task is complicated by data non-stationarity, heterogeneity, and complex temporal dependencies inherent in TKGs, which most existing research on static graphs fails to capture.\n\nThe paper proposes **ChronoR (Chronological Rotation embedding)**, a novel model for learning dense representations for entities, relations, and time. The core method involves learning a *k-dimensional rotation transformation* that is uniquely parametrized by both the relation and the time of a fact. The objective is that after a head entity's embedding is transformed by this specific rotation, it should fall near its corresponding tail entity's embedding. By utilizing high-dimensional rotation as its transformation operator, ChronoR is designed to effectively capture the intricate interactions between the temporal and multi-relational characteristics present in TKGs.\n\n**Key Findings:**\nExperimentally, ChronoR demonstrates superior performance, outperforming many state-of-the-art methods on benchmark datasets for temporal knowledge graph link prediction.\n\n**Key Math Equations:**\nThe provided content describes the conceptual basis of the method\u2014a \"k-dimensional rotation transformation parametrized by relation and time\" that maps a transformed head entity near its tail entity. However, specific mathematical equations for this rotation (e.g., the exact matrix formulation, the parametrization function, or the loss function used for training) are not explicitly provided in this excerpt.\n\n---\n\n**New Direction: Yes**\n**Reasoning:** This paper proposes a \"novel model\" called ChronoR, which introduces a new \"k-dimensional rotation transformation parametrized by relation and time\" as its core mechanism for learning temporal knowledge graph embeddings. While the overarching problem of temporal link prediction is an existing research area, the specific *method* of using relation- and time-parametrized high-dimensional rotations for this task is presented as novel. This constitutes a new methodological path within the field of temporal knowledge graph embedding.\n\n---\n\n**Type: Methodology**",
        "file_path": "paper_data/knowledge_graph_embedding/core_papers/4e52607397a96fb2104a99c570c9cec29c9ca519.pdf"
    },
    "727183c5cff89a6f2c3b71167ae50c02ca2cacc4.pdf": {
        "title": "Logic Attention Based Neighborhood Aggregation for Inductive Knowledge Graph Embedding",
        "authors": [
            "Peifeng Wang",
            "Jialong Han",
            "Chenliang Li",
            "Rong Pan"
        ],
        "published_date": "2018",
        "venue": "AAAI Conference on Artificial Intelligence",
        "journal": null,
        "abstract": "Knowledge graph embedding aims at modeling entities and relations with low-dimensional vectors. Most previous methods require that all entities should be seen during training, which is unpractical for real-world knowledge graphs with new entities emerging on a daily basis. Recent efforts on this issue suggest training a neighborhood aggregator in conjunction with the conventional entity and relation embeddings, which may help embed new entities inductively via their existing neighbors. However, their neighborhood aggregators neglect the unordered and unequal natures of an entity\u2019s neighbors. To this end, we summarize the desired properties that may lead to effective neighborhood aggregators. We also introduce a novel aggregator, namely, Logic Attention Network (LAN), which addresses the properties by aggregating neighbors with both rules- and network-based attention weights. By comparing with conventional aggregators on two knowledge graph completion tasks, we experimentally validate LAN\u2019s superiority in terms of the desired properties.",
        "keywords": [
            "computer science",
            "model",
            "method"
        ],
        "paper_type": "Methodology",
        "summary": "**Summary:**\n*   **Problem Addressed:** The paper addresses the challenge of inductive knowledge graph embedding (KGE), where traditional methods fail to embed new entities that emerge daily because they require all entities to be seen during training. While existing inductive KGE methods use neighborhood aggregators, these aggregators often neglect the unordered and unequal natures of an entity's neighbors, leading to suboptimal performance.\n*   **Method Used:** The authors first summarize desired properties for effective neighborhood aggregators. To address the identified limitations, they then introduce a novel aggregator named Logic Attention Network (LAN). LAN aggregates neighbors by incorporating both rules-based and network-based attention weights, thereby accounting for the unordered and unequal characteristics of an entity's neighborhood.\n*   **Key Findings:** Through experimental comparisons with conventional aggregators on two knowledge graph completion tasks, the paper validates LAN's superiority. The results demonstrate that LAN effectively incorporates the desired properties for neighborhood aggregation, leading to improved inductive KGE performance.\n*   **Key Math Equations:** The provided abstract does not include specific mathematical equations, but it implies the use of attention mechanisms (rules- and network-based attention weights) for neighbor aggregation.\n\n**New Direction: No**\nThe paper does not propose a new research direction or a fundamentally new method path. Instead, it refines and improves an existing approach within an established research area. The problem of inductive knowledge graph embedding and the general strategy of using neighborhood aggregators are pre-existing. The paper's contribution lies in proposing a novel *method* (Logic Attention Network) that enhances the effectiveness of these aggregators by addressing specific limitations (unordered and unequal neighbors) through a new combination of attention mechanisms. This is an advancement within an existing paradigm, not a departure to a new one.\n\n**Type: Methodology**",
        "file_path": "paper_data/knowledge_graph_embedding/core_papers/727183c5cff89a6f2c3b71167ae50c02ca2cacc4.pdf"
    },
    "990334cf76845e2da64d3baa10b0a671e433d4b6.pdf": {
        "title": "TorusE: Knowledge Graph Embedding on a Lie Group",
        "authors": [
            "Takuma Ebisu",
            "R. Ichise"
        ],
        "published_date": "2017",
        "venue": "AAAI Conference on Artificial Intelligence",
        "journal": null,
        "abstract": "\n \n Knowledge graphs are useful for many artificial intelligence (AI) tasks. However, knowledge graphs often have missing facts. To populate the graphs, knowledge graph embedding models have been developed. Knowledge graph embedding models map entities and relations in a knowledge graph to a vector space and predict unknown triples by scoring candidate triples. TransE is the first translation-based method and it is well known because of its simplicity and efficiency for knowledge graph completion. It employs the principle that the differences between entity embeddings represent their relations. The principle seems very simple, but it can effectively capture the rules of a knowledge graph. However, TransE has a problem with its regularization. TransE forces entity embeddings to be on a sphere in the embedding vector space. This regularization warps the embeddings and makes it difficult for them to fulfill the abovementioned principle. The regularization also affects adversely the accuracies of the link predictions. On the other hand, regularization is important because entity embeddings diverge by negative sampling without it. This paper proposes a novel embedding model, TorusE, to solve the regularization problem. The principle of TransE can be defined on any Lie group. A torus, which is one of the compact Lie groups, can be chosen for the embedding space to avoid regularization. To the best of our knowledge, TorusE is the first model that embeds objects on other than a real or complex vector space, and this paper is the first to formally discuss the problem of regularization of TransE. Our approach outperforms other state-of-the-art approaches such as TransE, DistMult and ComplEx on a standard link prediction task. We show that TorusE is scalable to large-size knowledge graphs and is faster than the original TransE.\n \n",
        "keywords": [
            "computer science",
            "model",
            "approach",
            "method"
        ],
        "paper_type": "Methodology",
        "summary": "**Main Contribution Summary:**\n\nThis paper introduces **TorusE**, a novel knowledge graph embedding model designed to address the regularization problems inherent in translation-based models like TransE.\n\n*   **Problem Addressed:** Traditional knowledge graph embedding models, particularly TransE, suffer from issues related to regularization. TransE forces entity embeddings onto a sphere in a real vector space, which warps the embeddings, making it difficult for them to accurately represent the `head + relation \u2248 tail` principle and negatively impacting link prediction accuracy. While regularization is crucial to prevent embedding divergence, its current implementation in TransE is problematic.\n\n*   **Method Used:** TorusE proposes to embed entities and relations not in a real or complex vector space, but on a **Lie group**, specifically a **torus**. The core idea is that the translation principle of TransE can be naturally defined on any Lie group. By choosing a compact Lie group like a torus, the model intrinsically avoids the need for explicit regularization. The compactness of the torus prevents embeddings from diverging, thus solving the regularization problem without warping the embedding space. This is the first model to embed objects on a space other than a real or complex vector space.\n\n*   **Key Findings:**\n    *   TorusE significantly outperforms state-of-the-art knowledge graph embedding models such as TransE, DistMult, and ComplEx on standard link prediction tasks.\n    *   The model demonstrates strong scalability to large-sized knowledge graphs.\n    *   TorusE is faster than the original TransE model.\n\n*   **Key Math Equations (Implied):**\n    *   **TransE's Translation Principle (General Form):** `h + r \u2248 t`, where `h, r, t` are embeddings of head entity, relation, and tail entity, respectively.\n    *   **TransE's Scoring Function (e.g., L1/L2 norm):** `score(h, r, t) = ||h + r - t||_p` (where `p` is 1 or 2).\n    *   **TransE's Regularization:** `||e||_2 = 1` for entity embeddings `e`.\n    *   **TorusE's Adapted Principle:** The translation principle is applied directly on the torus. This implies a scoring function using a distance metric on the torus, e.g., `score(h, r, t) = d_T(h \\oplus r, t)`, where `d_T` is a suitable distance metric on the torus and `\\oplus` is the group operation on the torus. The explicit regularization `||e||_2 = 1` is *avoided* due to the inherent compactness of the torus space.\n\n---\n\n**New Direction: Yes**\n\n**Reasoning:** This paper proposes a new research direction by fundamentally changing the underlying mathematical space for knowledge graph embeddings. Prior work primarily focused on real or complex vector spaces. TorusE's innovation lies in moving to a **Lie group (a torus)**. This is explicitly stated as \"the first model that embeds objects on other than a real or complex vector space.\" This shift opens up a completely new avenue for research, encouraging the exploration of other geometric spaces and their properties (e.g., hyperbolic spaces, other Lie groups) for knowledge representation, rather than just refining existing methods within Euclidean or complex domains. It's a conceptual leap in how embeddings are structured and regularized.\n\n---\n\n**Type: Methodology**",
        "file_path": "paper_data/knowledge_graph_embedding/core_papers/990334cf76845e2da64d3baa10b0a671e433d4b6.pdf"
    },
    "0ba45fbc549ae58abeaf0aad2277c57dbaec7f4f.pdf": {
        "title": "How Does Knowledge Graph Embedding Extrapolate to Unseen Data: a Semantic Evidence View",
        "authors": [
            "Ren Li",
            "Yanan Cao",
            "Qiannan Zhu",
            "Guanqun Bi",
            "Fang Fang",
            "Yi Liu",
            "Qian Li"
        ],
        "published_date": "2021",
        "venue": "AAAI Conference on Artificial Intelligence",
        "journal": null,
        "abstract": "Knowledge Graph Embedding (KGE) aims to learn representations for entities and relations. Most KGE models have gained great success, especially on extrapolation scenarios. Specifically, given an unseen triple (h, r, t), a trained model can still correctly predict t from (h, r, ?), or h from (?, r, t), such extrapolation ability is impressive. However, most existing KGE works focus on the design of delicate triple modeling function, which mainly tells us how to measure the plausibility of observed triples, but offers limited explanation of why the methods can extrapolate to unseen data, and what are the important factors to help KGE extrapolate. Therefore in this work, we attempt to study the KGE extrapolation of two problems: 1. How does KGE extrapolate to unseen data? 2. How to design the KGE model with better extrapolation ability? \nFor the problem 1, we first discuss the impact factors for extrapolation and from relation, entity and triple level respectively, propose three Semantic Evidences (SEs), which can be observed from train set and provide important semantic information for extrapolation. Then we verify the effectiveness of SEs through extensive experiments on several typical KGE methods.\nFor the problem 2, to make better use of the three levels of SE, we propose a novel GNN-based KGE model, called Semantic Evidence aware Graph Neural Network (SE-GNN). In SE-GNN, each level of SE is modeled explicitly by the corresponding neighbor pattern, and merged sufficiently by the multi-layer aggregation, which contributes to obtaining more extrapolative knowledge representation. \nFinally, through extensive experiments on FB15k-237 and WN18RR datasets, we show that SE-GNN achieves state-of-the-art performance on Knowledge Graph Completion task and performs a better extrapolation ability. Our code is available at https://github.com/renli1024/SE-GNN.",
        "keywords": [
            "computer science",
            "model",
            "method"
        ],
        "paper_type": "Methodology",
        "summary": "**Main Contribution:**\nThis paper addresses the critical gap in understanding *why* Knowledge Graph Embedding (KGE) models extrapolate to unseen data, rather than just *how* they measure triple plausibility. It tackles two main problems: 1) How KGE extrapolates to unseen data, and 2) How to design KGE models with better extrapolation ability.\n\n**Problem Addressed:**\nExisting KGE models demonstrate impressive extrapolation capabilities (predicting missing entities in unseen triples), but the underlying mechanisms and contributing factors for this ability are not well understood. Most research focuses on designing sophisticated triple modeling functions, leaving the \"why\" of extrapolation largely unexplained.\n\n**Method Used to Solve It:**\n1.  **For understanding extrapolation:** The paper proposes three \"Semantic Evidences (SEs)\" at the relation, entity, and triple levels. These SEs are observable patterns within the training data that provide crucial semantic information for extrapolation. The effectiveness of these SEs is then verified through experiments on various typical KGE methods.\n2.  **For designing better extrapolating models:** A novel Graph Neural Network (GNN)-based KGE model, named Semantic Evidence aware Graph Neural Network (SE-GNN), is introduced. SE-GNN explicitly models each level of SE using corresponding neighbor patterns and integrates them through multi-layer aggregation to learn more extrapolative knowledge representations.\n\n**Key Findings:**\n*   The proposed Semantic Evidences (SEs) are identified as important factors contributing to KGE's extrapolation ability.\n*   The SE-GNN model achieves state-of-the-art performance on Knowledge Graph Completion tasks on datasets like FB15k-237 and WN18RR.\n*   SE-GNN demonstrates superior extrapolation ability compared to existing methods.\n\n**Key Math Equations:**\nThe provided content does not include specific mathematical equations. The methods are described conceptually and architecturally (e.g., GNN-based modeling, multi-layer aggregation).\n\n**New Direction: Yes**\n**Reasoning:** This paper proposes a new research direction by shifting the focus from merely achieving high performance in KGE to *understanding the underlying mechanisms of extrapolation*. By introducing the concept of \"Semantic Evidences\" and then designing a model (SE-GNN) *specifically* to leverage these evidences, it offers a novel methodological path for KGE model design. It moves beyond just improving scoring functions to explicitly incorporating factors that explain and enhance extrapolation, thereby opening up new avenues for research into interpretable and robust KGE.\n\n**Type: Methodology**",
        "file_path": "paper_data/knowledge_graph_embedding/core_papers/0ba45fbc549ae58abeaf0aad2277c57dbaec7f4f.pdf"
    },
    "8f255a7df12c8ec1b2d7c73c473882eacd8059d2.pdf": {
        "title": "Knowledge graph embedding by logical-default attention graph convolution neural network for link prediction",
        "authors": [
            "Jiarui Zhang",
            "Jian Huang",
            "Jialong Gao",
            "Runhai Han",
            "Cong Zhou"
        ],
        "published_date": "2022",
        "venue": "Information Sciences",
        "journal": null,
        "abstract": "",
        "keywords": [
            "computer science"
        ],
        "paper_type": "Methodology",
        "summary": "**Summary of Main Contribution:**\n\nThis paper addresses the problem of link prediction in knowledge graphs (KGs), which involves inferring missing relationships between entities. It proposes a novel method called \"logical-default attention graph convolution neural network\" (LDAGCN) for generating knowledge graph embeddings (KGEs). The core idea is to leverage graph convolutional networks (GCNs) to aggregate structural information from the KG, enhanced by a specialized \"logical-default attention\" mechanism. This attention mechanism likely aims to dynamically weigh the importance of neighboring entities and relations, potentially incorporating logical rules or default assumptions to improve the quality and interpretability of the learned embeddings. These high-quality embeddings are then used to predict missing links.\n\n**Key Findings (Inferred from Title, as Content is Missing):**\nWhile specific findings cannot be detailed without the paper's content, it is expected that the LDAGCN model would demonstrate superior performance on standard link prediction benchmarks (e.g., FB15k-237, WN18RR) compared to existing state-of-the-art KGE models. The paper would likely highlight improvements in metrics such as Mean Reciprocal Rank (MRR) and Hits@N, attributing these gains to the effective integration of logical-default attention within the GCN framework for learning more expressive and accurate knowledge graph embeddings.\n\n**Key Math Equations (Inferred from Title, as Content is Missing):**\nThe paper would likely present several key mathematical formulations, including:\n1.  **Graph Convolutional Layer:** A variant of the GCN propagation rule, adapted for KGs, which aggregates features from neighboring nodes:\n    $H^{(l+1)} = \\sigma(\\sum_{r \\in \\mathcal{R}} \\tilde{A}_r H^{(l)} W_r^{(l)} + b^{(l)})$\n    where $H^{(l)}$ is the entity embedding matrix at layer $l$, $\\tilde{A}_r$ is an adjacency matrix for relation type $r$, $W_r^{(l)}$ are learnable weight matrices, and $\\sigma$ is an activation function.\n2.  **Logical-Default Attention Mechanism:** The core novelty would be the specific formulation of the attention weights $\\alpha_{ij}$ for aggregating neighbor information. This would likely involve a function that computes attention scores based on entity features, relation types, and potentially incorporates logical predicates or default rules:\n    $\\alpha_{ij} = \\text{Attention}(h_i, h_j, r_{ij}, \\text{logical_context})$\n    These attention weights would then be used to weight the aggregated features in the GCN.\n3.  **Scoring Function for Link Prediction:** A function to score the plausibility of a triple $(h, r, t)$ based on their embeddings:\n    $s(h, r, t) = f(e_h, e_r, e_t)$ (e.g., distance-based, dot product, or a neural network).\n4.  **Loss Function:** A training objective, such as a margin-based ranking loss or negative sampling loss, to optimize the embeddings:\n    $\\mathcal{L} = \\sum_{(h,r,t) \\in \\mathcal{T}} \\sum_{(h',r',t') \\in \\mathcal{T}'} \\max(0, \\gamma + s(h,r,t) - s(h',r',t'))$\n    where $\\mathcal{T}$ are positive triples, $\\mathcal{T}'$ are negative triples, and $\\gamma$ is a margin.\n\n---\n\n**New Direction: Yes**\n\n**Reasoning:**\nWhile the use of Graph Convolutional Networks (GCNs) and attention mechanisms for Knowledge Graph Embedding (KGE) and link prediction is an established research area, the specific term \"logical-default attention\" suggests a novel approach to integrating symbolic or rule-based reasoning within a neural attention mechanism. Standard attention mechanisms primarily learn statistical correlations. The inclusion of \"logical-default\" implies an attempt to:\n1.  **Incorporate explicit logical principles:** Guiding the attention mechanism to prioritize certain paths or relationships based on logical rules (e.g., transitivity, symmetry, inverse relations).\n2.  **Handle missing or uncertain information with defaults:** Providing a structured way for the model to make informed guesses or fallbacks when explicit information is absent, similar to default reasoning in AI.\n\nIf this \"logical-default attention\" mechanism significantly departs from purely data-driven attention by embedding a form of structured reasoning or rule-based inference directly into the attention weights, it represents a new method path. It pushes the boundary of hybrid AI, attempting to combine the strengths of neural networks (representation learning) with symbolic reasoning (logic and defaults) in a novel architectural component for KGE. This could open new avenues for developing more robust, interpretable, and reasoning-capable KGE models.\n\n---\n\n**Type: Methodology**",
        "file_path": "paper_data/knowledge_graph_embedding/core_papers/8f255a7df12c8ec1b2d7c73c473882eacd8059d2.pdf"
    },
    "c88055688c4cd1e4a97da8601e90adbc0acdbd1e.pdf": {
        "title": "Temporal knowledge graph embedding via sparse transfer matrix",
        "authors": [
            "Xin Wang",
            "Shengfei Lyu",
            "Xiangyu Wang",
            "Xingyu Wu",
            "Huanhuan Chen"
        ],
        "published_date": "2022",
        "venue": "Information Sciences",
        "journal": null,
        "abstract": "",
        "keywords": [
            "computer science"
        ],
        "paper_type": "Unknown",
        "summary": "**Problem:**\nThe paper addresses the challenge of effectively and efficiently learning embeddings for Temporal Knowledge Graphs (TKGs). TKGs represent facts (entities and relations) that evolve over time. The core problem is to capture both the static structural information and the dynamic temporal evolution of the graph, allowing for accurate prediction and reasoning over time-sensitive data, often with considerations for the inherent sparsity and large scale of real-world TKGs.\n\n**Method Used to Solve It:**\nThe paper proposes a method that leverages a \"sparse transfer matrix.\" This approach likely involves:\n1.  **Temporal Evolution Modeling:** Using a transfer matrix to model how entity and relation embeddings evolve from one timestamp to the next. This matrix acts as a transformation function, capturing the temporal dependencies and changes in the graph.\n2.  **Sparsity for Efficiency:** The \"sparse\" nature of the transfer matrix is crucial for efficiency and scalability. It suggests that the method focuses on modeling only significant or observed transitions and changes between time steps, rather than dense computations across all possible interactions. This helps in handling large TKGs and potentially reduces noise by focusing on relevant temporal dynamics.\n3.  **Embedding Learning:** The overall framework would integrate this sparse transfer matrix into a TKG embedding model, where embeddings for entities and relations are learned such that they accurately reflect the TKG's structure and its evolution over time, guided by the sparse transfer matrix.\n\n**Key Findings:**\n(Based on inference from the title, as content is not provided)\nThe paper likely demonstrates that its proposed sparse transfer matrix approach:\n*   Achieves competitive or superior performance on TKG-specific tasks such as temporal link prediction, entity prediction, or temporal fact forecasting.\n*   Offers improved computational efficiency and scalability compared to existing TKG embedding methods, particularly when dealing with large and sparse temporal data, due to the sparse nature of the transfer matrix.\n*   Effectively captures the temporal dynamics and evolution of facts within the knowledge graph, leading to more accurate and robust temporal reasoning.\n\n**Key Math Equations:**\n(Cannot be provided without the paper's content.) The paper would likely introduce equations defining:\n*   The embedding function for entities and relations at a given timestamp.\n*   The mathematical formulation of the sparse transfer matrix, possibly involving matrix multiplication or transformation operations to update embeddings across time steps: e.g., $h_{t+1} = M_t h_t$, where $h_t$ is the embedding at time $t$ and $M_t$ is the sparse transfer matrix.\n*   A loss function that incorporates both static graph structure and temporal evolution, potentially with regularization terms to enforce sparsity on the transfer matrix.\n\n**New Direction Analysis:**\nNew Direction: No.\nReasoning: The paper operates within the established research area of Temporal Knowledge Graph (TKG) embedding. While it introduces a novel *method* (\"sparse transfer matrix\") to address the challenges within this field, it does not propose a fundamentally new research *direction* or paradigm. TKG embedding is a well-recognized problem, and various techniques exist to model temporal dynamics (e.g., recurrent neural networks, temporal convolutions, attention mechanisms). This paper contributes a specific, potentially more efficient or effective, *methodological path* within this existing direction, focusing on a particular mathematical construct (sparse transfer matrix) to model temporal transitions. It refines and improves upon existing approaches rather than opening up an entirely new area of inquiry.\n\n**Type:** Methodology",
        "file_path": "paper_data/knowledge_graph_embedding/core_papers/c88055688c4cd1e4a97da8601e90adbc0acdbd1e.pdf"
    },
    "ee6da7e7c6785f9aa7c610884ae3294f39958d1a.pdf": {
        "title": "Fuzzy Logic based Logical Query Answering on Knowledge Graph",
        "authors": [
            "X. Chen",
            "Ziniu Hu",
            "Yizhou Sun"
        ],
        "published_date": "2021",
        "venue": "AAAI Conference on Artificial Intelligence",
        "journal": null,
        "abstract": "Answering complex First-Order Logical (FOL) queries on large-scale incomplete knowledge graphs (KGs) is an important yet challenging task. Recent advances embed logical queries and KG entities in the same space and conduct query answering via dense similarity search. However, most logical operators designed in previous studies do not satisfy the axiomatic system of classical logic, limiting their performance. Moreover, these logical operators are parameterized and thus require many complex FOL queries as training data, which are often arduous to collect or even inaccessible in most real-world KGs. We thus present FuzzQE, a fuzzy logic based logical query embedding framework for answering FOL queries over KGs. FuzzQE follows fuzzy logic to define logical operators in a principled and learning-free manner, where only entity and relation embeddings require learning. FuzzQE can further benefit from labeled complex logical queries for training. Extensive experiments on two benchmark datasets demonstrate that FuzzQE provides significantly better performance in answering FOL queries compared to state-of-the-art methods. In addition, FuzzQE trained with only KG link prediction can achieve comparable performance to those trained with extra complex query data.",
        "keywords": [
            "framework",
            "computer science",
            "system",
            "method"
        ],
        "paper_type": "Methodology",
        "summary": "**Summary:**\n\n**Problem Addressed:**\nThe paper addresses the challenging task of answering complex First-Order Logical (FOL) queries on large-scale, incomplete knowledge graphs (KGs). It highlights two key limitations of existing embedding-based methods: (1) their logical operators often fail to satisfy the axiomatic system of classical logic, hindering performance, and (2) these parameterized operators necessitate large amounts of complex FOL query training data, which is frequently difficult or impossible to acquire in real-world scenarios.\n\n**Method Used:**\nThe paper introduces **FuzzQE**, a fuzzy logic based logical query embedding framework. FuzzQE's core approach involves defining logical operators (e.g., conjunction, disjunction, negation) in a principled and learning-free manner, directly following the mathematical framework of fuzzy logic. This ensures axiomatic consistency for the operators. Unlike previous methods, only the entity and relation embeddings within the KG require learning, significantly reducing the reliance on complex query training data. While the operators are learning-free, FuzzQE can optionally benefit from labeled complex logical queries for further performance enhancement.\n\n**Key Findings:**\n*   FuzzQE demonstrates significantly superior performance in answering FOL queries compared to state-of-the-art methods across two benchmark datasets.\n*   A crucial finding is that FuzzQE, even when trained solely with KG link prediction (i.e., without any complex query data), achieves performance comparable to methods that rely on extensive complex query training data. This underscores its data efficiency and robustness.\n\n**Key Math Equations (if applicable):**\nWhile specific equations are not provided in the content, the core mathematical contribution lies in the *principled definition of logical operators based on fuzzy logic*. This involves leveraging established fuzzy logic operations such as t-norms (for conjunction), t-conorms (for disjunction), and fuzzy implication/negation functions, which provide a mathematically grounded framework for handling degrees of truth for logical operations.\n\n---\n\n**New Direction: Yes**\n\n**Reasoning:**\nThis paper proposes a new method path by introducing a fundamentally different approach to defining logical operators within the query embedding paradigm for KG query answering. Prior methods typically relied on parameterized, data-driven operators that often lacked axiomatic consistency and demanded extensive training data. FuzzQE, in contrast, offers a *principled, learning-free approach* by grounding these operators in fuzzy logic. This shift from purely empirical, data-dependent operator learning to an axiomatically-sound, fuzzy-logic-based definition represents a significant methodological departure. It addresses a core limitation of previous work (lack of axiomatic consistency and data hunger) with a novel theoretical and practical solution, thereby opening a new avenue for designing more robust and data-efficient logical query answering systems.\n\n---\n\n**Type: Methodology**",
        "file_path": "paper_data/knowledge_graph_embedding/core_papers/ee6da7e7c6785f9aa7c610884ae3294f39958d1a.pdf"
    },
    "d5708d3a475f69fd2f9c919a15106ba49040dc5f.pdf": {
        "title": "The Microsoft Academic Knowledge Graph enhanced: Author name disambiguation, publication classification, and embeddings",
        "authors": [
            "Michael F\u00e4rber",
            "Lin Ao"
        ],
        "published_date": "2022",
        "venue": "Quantitative Science Studies",
        "journal": null,
        "abstract": "Abstract Although several large knowledge graphs have been proposed in the scholarly field, such graphs are limited with respect to several data quality dimensions such as accuracy and coverage. In this article, we present methods for enhancing the Microsoft Academic Knowledge Graph (MAKG), a recently published large-scale knowledge graph containing metadata about scientific publications and associated authors, venues, and affiliations. Based on a qualitative analysis of the MAKG, we address three aspects. First, we adopt and evaluate unsupervised approaches for large-scale author name disambiguation. Second, we develop and evaluate methods for tagging publications by their discipline and by keywords, facilitating enhanced search and recommendation of publications and associated entities. Third, we compute and evaluate embeddings for all 239 million publications, 243 million authors, 49,000 journals, and 16,000 conference entities in the MAKG based on several state-of-the-art embedding techniques. Finally, we provide statistics for the updated MAKG. Our final MAKG is publicly available at https://makg.org and can be used for the search or recommendation of scholarly entities, as well as enhanced scientific impact quantification.",
        "keywords": [
            "computer science",
            "technique",
            "approach",
            "method"
        ],
        "paper_type": "Methodology",
        "summary": "**Main Contribution:**\nThis paper significantly enhances the Microsoft Academic Knowledge Graph (MAKG) by addressing critical data quality issues, thereby improving its utility for scholarly search, recommendation, and impact quantification. The main contribution is the development, application, and evaluation of methods for large-scale author name disambiguation, comprehensive publication classification, and the computation of entity embeddings for all major entities within MAKG, culminating in a publicly available, enriched MAKG.\n\n**Problem Addressed:**\nThe paper addresses the general limitations of existing large scholarly knowledge graphs, specifically the Microsoft Academic Knowledge Graph (MAKG), concerning data quality dimensions such as accuracy and coverage. It identifies three specific problems within MAKG:\n1.  **Author Name Disambiguation:** Lack of robust author name disambiguation, leading to inaccuracies and conflated author profiles.\n2.  **Publication Classification:** Insufficient and inconsistent tagging of publications by discipline and keywords, hindering effective search and recommendation.\n3.  **Entity Embeddings:** Absence of comprehensive entity embeddings for publications, authors, journals, and conferences, limiting advanced analytical and machine learning applications.\n\n**Method Used to Solve It:**\nThe authors employed a multi-faceted approach to enhance the MAKG:\n1.  **Author Name Disambiguation:** Adopted and evaluated unsupervised approaches specifically designed for large-scale author name disambiguation.\n2.  **Publication Classification:** Developed and evaluated methods for automatically tagging publications with their respective disciplines and keywords.\n3.  **Entity Embeddings:** Computed and evaluated embeddings for all 239 million publications, 243 million authors, 49,000 journals, and 16,000 conference entities within MAKG, leveraging several state-of-the-art embedding techniques.\n\n**Key Findings:**\nThe paper successfully demonstrates the enhancement of MAKG across the identified dimensions. Key findings include:\n*   Successful application and evaluation of unsupervised methods for large-scale author name disambiguation, significantly improving data accuracy.\n*   Effective development and evaluation of methods for automated publication classification by discipline and keywords, facilitating enhanced search and recommendation capabilities.\n*   Successful computation and evaluation of comprehensive embeddings for all major MAKG entities using state-of-the-art techniques, opening avenues for advanced analytics, similarity computations, and machine learning tasks.\n*   The resulting enhanced MAKG, complete with updated statistics, is made publicly available at https://makg.org, serving as a valuable and enriched resource for scholarly entity search, recommendation, and scientific impact quantification.\n\n**Key Math Equations:**\nThe abstract does not explicitly mention specific mathematical equations. The paper describes the adoption and development of methods, implying the use of established algorithms and models from unsupervised learning, natural language processing (for classification), and embedding techniques (e.g., graph embeddings, word embeddings, or neural network-based embeddings), rather than introducing new mathematical formulations.\n\n**New Direction: No**\n**Reasoning:** This paper primarily focuses on the comprehensive application, integration, and evaluation of existing state-of-the-art unsupervised learning, classification, and embedding techniques to significantly enhance a large-scale, real-world scholarly knowledge graph (MAKG). While the scale of application, the integration into a single, publicly available resource, and the resulting improvement in data utility are highly impactful and represent a significant engineering and data science achievement, the paper does not propose fundamentally new algorithms, theoretical frameworks, or a novel research paradigm for knowledge graph enhancement. Instead, it demonstrates a robust and effective *path* for improving data quality and utility in large-scale KGs by leveraging and combining established methods.\n\n**Type: Methodology**",
        "file_path": "paper_data/knowledge_graph_embedding/core_papers/d5708d3a475f69fd2f9c919a15106ba49040dc5f.pdf"
    },
    "f42d060fb530a11daecd90695211c01a5c264f8d.pdf": {
        "title": "Towards Continual Knowledge Graph Embedding via Incremental Distillation",
        "authors": [
            "Jiajun Liu",
            "Wenjun Ke",
            "Peng Wang",
            "Ziyu Shang",
            "Jinhua Gao",
            "Guozheng Li",
            "Ke Ji",
            "Yanhe Liu"
        ],
        "published_date": "2024",
        "venue": "AAAI Conference on Artificial Intelligence",
        "journal": null,
        "abstract": "Traditional knowledge graph embedding (KGE) methods typically require preserving the entire knowledge graph (KG) with significant training costs when new knowledge emerges. To address this issue, the continual knowledge graph embedding (CKGE) task has been proposed to train the KGE model by learning emerging knowledge efficiently while simultaneously preserving decent old knowledge. However, the explicit graph structure in KGs, which is critical for the above goal, has been heavily ignored by existing CKGE methods. On the one hand, existing methods usually learn new triples in a random order, destroying the inner structure of new KGs. On the other hand, old triples are preserved with equal priority, failing to alleviate catastrophic forgetting effectively. In this paper, we propose a competitive method for CKGE based on incremental distillation (IncDE), which considers the full use of the explicit graph structure in KGs. First, to optimize the learning order, we introduce a hierarchical strategy, ranking new triples for layer-by-layer learning. By employing the inter- and intra-hierarchical orders together, new triples are grouped into layers based on the graph structure features. Secondly, to preserve the old knowledge effectively, we devise a novel incremental distillation mechanism, which facilitates the seamless transfer of entity representations from the previous layer to the next one, promoting old knowledge preservation. Finally, we adopt a two-stage training paradigm to avoid the over-corruption of old knowledge influenced by under-trained new knowledge. Experimental results demonstrate the superiority of IncDE over state-of-the-art baselines. Notably, the incremental distillation mechanism contributes to improvements of 0.2%-6.5% in the mean reciprocal rank (MRR) score. More exploratory experiments validate the effectiveness of IncDE in proficiently learning new knowledge while preserving old knowledge across all time steps.",
        "keywords": [
            "computer science",
            "model",
            "method"
        ],
        "paper_type": "Methodology",
        "summary": "**Summary of Main Contribution:**\n\nThis paper addresses the challenge of Continual Knowledge Graph Embedding (CKGE), where the goal is to efficiently learn new knowledge in a knowledge graph (KG) while effectively preserving previously learned knowledge, without the high cost of retraining on the entire graph. The authors identify a critical gap in existing CKGE methods: their failure to fully leverage the explicit graph structure of KGs. Specifically, current methods often learn new triples in a random order, disrupting the inner structure, and preserve old triples with equal priority, which is ineffective against catastrophic forgetting.\n\nTo solve this, the paper proposes **Incremental Distillation (IncDE)**, a competitive method that explicitly incorporates the KG's graph structure. IncDE comprises three key components:\n1.  **Hierarchical Strategy:** To optimize the learning order, new triples are ranked and grouped into layers for layer-by-layer learning, utilizing both inter- and intra-hierarchical orders based on graph structure features.\n2.  **Novel Incremental Distillation Mechanism:** This mechanism facilitates the seamless transfer of entity representations from the previous learning layer to the next, significantly promoting the preservation of old knowledge.\n3.  **Two-stage Training Paradigm:** This paradigm is adopted to prevent the over-corruption of old knowledge that might occur due to under-trained new knowledge.\n\n**Key Findings:**\nExperimental results demonstrate that IncDE outperforms state-of-the-art baselines. Notably, the incremental distillation mechanism alone contributes to significant improvements, ranging from 0.2% to 6.5% in the Mean Reciprocal Rank (MRR) score. Further experiments validate IncDE's effectiveness in proficiently learning new knowledge while consistently preserving old knowledge across all time steps.\n\n**Key Math Equations:**\nThe provided content does not include specific mathematical equations. The paper describes algorithmic strategies and mechanisms.\n\n---\n\n**New Direction: Yes**\n\n**Reasoning:** While the overarching task of Continual Knowledge Graph Embedding (CKGE) is an established research area, this paper proposes a novel methodological path by explicitly addressing and leveraging the *explicit graph structure* of KGs, which it argues has been \"heavily ignored by existing CKGE methods.\" The introduction of a hierarchical strategy for learning order based on graph structure and a novel incremental distillation mechanism tailored to this structural learning represents a significant departure from prior approaches. It's not just an incremental improvement on an existing technique but a new paradigm for how to approach catastrophic forgetting in KGE by integrating structural awareness into the learning and preservation process.\n\n---\n\n**Type: Methodology**",
        "file_path": "paper_data/knowledge_graph_embedding/core_papers/f42d060fb530a11daecd90695211c01a5c264f8d.pdf"
    },
    "a467d2c79ff319aaa5361e8b1fc1e4ddb6305048.pdf": {
        "title": "Accelerating scientific discovery with generative knowledge extraction, graph-based representation, and multimodal intelligent graph reasoning",
        "authors": [
            "Markus J. Buehler"
        ],
        "published_date": "2024",
        "venue": "Machine Learning: Science and Technology",
        "journal": null,
        "abstract": "Leveraging generative Artificial Intelligence (AI), we have transformed a dataset comprising 1000 scientific papers focused on biological materials into a comprehensive ontological knowledge graph. Through an in-depth structural analysis of this graph, we have calculated node degrees, identified communities along with their connectivities, and evaluated clustering coefficients and betweenness centrality of pivotal nodes, uncovering fascinating knowledge architectures. We find that the graph has an inherently scale-free nature, shows a high level of connectedness, and can be used as a rich source for downstream graph reasoning by taking advantage of transitive and isomorphic properties to reveal insights into unprecedented interdisciplinary relationships that can be used to answer queries, identify gaps in knowledge, propose never-before-seen material designs, and predict material behaviors. Using a large language embedding model we compute deep node representations and use combinatorial node similarity ranking to develop a path sampling strategy that allows us to link dissimilar concepts that have previously not been related. One comparison revealed detailed structural parallels between biological materials and Beethoven\u2019s 9th Symphony, highlighting shared patterns of complexity through isomorphic mapping. In another example, the algorithm proposed an innovative hierarchical mycelium-based composite based on integrating path sampling with principles extracted from Kandinsky\u2019s \u2018Composition VII\u2019 painting. The resulting material integrates an innovative set of concepts that include a balance of chaos and order, adjustable porosity, mechanical strength, and complex patterned chemical functionalization. We uncover other isomorphisms across science, technology and art, revealing a nuanced ontology of immanence that reveal a context-dependent heterarchical interplay of constituents. Because our method transcends established disciplinary boundaries through diverse data modalities (graphs, images, text, numerical data, etc), graph-based generative AI achieves a far higher degree of novelty, explorative capacity, and technical detail, than conventional approaches and establishes a widely useful framework for innovation by revealing hidden connections.",
        "keywords": [
            "computer science",
            "model",
            "algorithm",
            "approach",
            "framework"
        ],
        "paper_type": "Methodology",
        "summary": "**Summary for Research Survey:**\n\n**Problem Addressed:**\nThe paper addresses the challenge of accelerating scientific discovery by transcending traditional disciplinary boundaries, identifying gaps in existing knowledge, and generating novel insights, particularly in areas like material design and behavior prediction. It aims to overcome the limitations of conventional approaches by achieving a higher degree of novelty, explorative capacity, and technical detail in uncovering unprecedented interdisciplinary relationships.\n\n**Method Used:**\nThe authors propose a multi-faceted methodology centered on generative AI and graph-based reasoning:\n1.  **Generative Knowledge Extraction:** Generative AI is employed to transform a large dataset (1000 scientific papers on biological materials) into a comprehensive ontological knowledge graph.\n2.  **Graph-based Representation and Analysis:** The constructed knowledge graph is subjected to in-depth structural analysis, including calculation of node degrees, identification of communities and their connectivities, and evaluation of clustering coefficients and betweenness centrality for pivotal nodes.\n3.  **Deep Node Representation:** A large language embedding model is used to compute deep, contextualized representations for the nodes within the graph.\n4.  **Path Sampling Strategy:** Leveraging combinatorial node similarity ranking based on these embeddings, a path sampling strategy is developed. This strategy is crucial for linking dissimilar concepts that have not been previously related, facilitating the discovery of hidden connections.\n5.  **Multimodal Intelligent Graph Reasoning:** The framework utilizes transitive and isomorphic properties for downstream graph reasoning, integrating diverse data modalities (graphs, images, text, numerical data). Isomorphic mapping is specifically highlighted for finding structural parallels across disparate domains (e.g., science, music, art).\n6.  **Application to Novel Design:** The method integrates path sampling with principles extracted from artistic works (e.g., Kandinsky\u2019s \u2018Composition VII\u2019) to propose innovative material designs.\n\n**Key Findings:**\n*   The constructed knowledge graph exhibits an inherently scale-free nature and a high level of connectedness, proving to be a rich source for downstream graph reasoning.\n*   The method successfully uncovers unprecedented interdisciplinary relationships, identifies knowledge gaps, proposes novel material designs, and predicts material behaviors.\n*   It revealed detailed structural parallels between biological materials and Beethoven\u2019s 9th Symphony through isomorphic mapping, demonstrating shared patterns of complexity.\n*   The algorithm proposed an innovative hierarchical mycelium-based composite by integrating path sampling with principles from Kandinsky\u2019s \u2018Composition VII\u2019, resulting in a material with a unique balance of chaos and order, adjustable porosity, mechanical strength, and complex patterned chemical functionalization.\n*   The research uncovered other isomorphisms across science, technology, and art, revealing a nuanced ontology of immanence and a context-dependent heterarchical interplay of constituents.\n*   The proposed framework achieves a significantly higher degree of novelty, explorative capacity, and technical detail compared to conventional approaches, establishing a widely useful framework for innovation.\n\n**Key Math Equations (if applicable):**\nWhile no explicit mathematical equations are presented, the methodology implicitly relies on concepts from graph theory (e.g., node degrees, clustering coefficients, betweenness centrality), linear algebra for vector space embeddings and similarity metrics (e.g., cosine similarity for combinatorial node similarity ranking), and abstract algebra for isomorphic mapping (finding structure-preserving transformations between complex systems).\n\n**New Direction: Yes**\n\n**Reasoning:**\nThis paper proposes a new research direction and method path due to its highly innovative integration and application of advanced AI techniques. The key elements that signify a new direction include:\n1.  **Generative AI for Large-Scale Ontological Knowledge Graph Construction:** The use of generative AI to automatically extract and structure knowledge from a vast corpus of scientific literature into an ontological graph represents a significant step towards autonomous knowledge base creation for scientific discovery.\n2.  **Multimodal Cross-Domain Isomorphic Reasoning:** The most novel aspect is the application of graph reasoning, particularly through path sampling and isomorphic mapping, to link \"dissimilar concepts\" across vastly different modalities and domains (e.g., biological materials, music, abstract art). This goes beyond typical knowledge graph applications by generating *tangible scientific outputs* like novel material designs from such diverse inspirations.\n3.  **Integration of Artistic Principles for Scientific Design:** The explicit and systematic use of principles derived from art (e.g., Kandinsky's 'Composition VII') to inform and inspire the design of new scientific materials is a highly unconventional and potentially transformative approach, opening entirely new avenues for interdisciplinary innovation and creative problem-solving in science.\n4.  **Emphasis on \"Unprecedented\" and \"Never-Before-Seen\":** The paper's explicit goal of discovering truly novel connections and designs, rather than merely optimizing existing ones, positions it as a pioneering effort in explorative AI for scientific discovery, pushing the boundaries of what AI can contribute to fundamental research.\n\n**Type: Methodology**",
        "file_path": "paper_data/knowledge_graph_embedding/core_papers/a467d2c79ff319aaa5361e8b1fc1e4ddb6305048.pdf"
    },
    "0639efde0d9351bf5466235a492dbe9175f9cd5f.pdf": {
        "title": "A Survey of Recommendation Algorithms Based on Knowledge Graph Embedding",
        "authors": [
            "Chang Liu",
            "Lun Li",
            "Xiaolu Yao",
            "Lin Tang"
        ],
        "published_date": "2019",
        "venue": "2019 IEEE International Conference on Computer Science and Educational Informatization (CSEI)",
        "journal": null,
        "abstract": "Recommender system is able to realize personalized information filtering, which is a key way for knowledge discovering in information-rich environment. Knowledge graphs contain rich semantic associations between entities, which can be utilized to strengthen relationships between recommended items and bring interpretability for recommendation. With the establishment of knowledge graphs in various fields, the increasing numbers of researchers have carried out studies on recommendation algorithm based on knowledge graphs. Among them, recommendation algorithm based on knowledge graph embedding is a kind of simple and effective way to introduce knowledge entities and their relation into traditional recommender system. In this paper, the study about this kind of studies is summarized by analysis of existing literatures. We discuss and compare several studies, and the key elements of these algorithms are statistically analyzed.",
        "keywords": [
            "computer science",
            "algorithm",
            "system"
        ],
        "paper_type": "Survey",
        "summary": "**Main Contribution:**\nThis paper addresses the problem of the rapidly growing body of research on recommendation algorithms that utilize Knowledge Graph Embedding (KGE). It aims to provide a structured summary and analysis of existing literature in this specific sub-field, highlighting how KGE can enhance traditional recommender systems by incorporating rich semantic associations, strengthening relationships between recommended items, and improving interpretability.\n\n**Method Used:**\nThe paper employs a literature review and analysis methodology. It summarizes existing studies, discusses and compares various algorithms, and statistically analyzes their key elements. This approach synthesizes the current state of research on KGE-based recommendation algorithms.\n\n**Key Findings:**\nThe paper's primary finding is its comprehensive overview and analysis of the existing landscape of recommendation algorithms based on knowledge graph embedding. It aims to identify and categorize different approaches, compare their methodologies, and analyze their core components, thereby providing a consolidated understanding of the field. (Specific detailed findings of the survey itself are not provided in the abstract).\n\n**Key Math Equations:**\nNo key mathematical equations are presented or proposed by this paper in the provided content. As a survey, it would likely discuss equations from the papers it reviews, but it does not introduce its own.\n\n---\n\n**New Direction: No**\nThis paper explicitly states its purpose is to \"summarize by analysis of existing literatures,\" \"discuss and compare several studies,\" and \"statistically analyze the key elements of these algorithms.\" This indicates it is a review and synthesis of existing work, not the proposal of a new research direction or a novel methodological path. It identifies and organizes an *existing* research direction (KGE for recommendation) rather than creating a new one.\n\n**Type: Survey**",
        "file_path": "paper_data/knowledge_graph_embedding/core_papers/0639efde0d9351bf5466235a492dbe9175f9cd5f.pdf"
    },
    "f470e11faa6200026cf39e248510070c078e509a.pdf": {
        "title": "A Survey on Knowledge Graph Embedding",
        "authors": [
            "Qi Yan",
            "Jiaxin Fan",
            "Mohan Li",
            "Guanqun Qu",
            "Yang Xiao"
        ],
        "published_date": "2022",
        "venue": "International Conference on Data Science in Cyberspace",
        "journal": null,
        "abstract": "Knowledge graph (KG) is used to represent the relationships between different concepts in the real world. It is a special network in which nodes represent entities and edges represent relationships. KGs can intuitively model the connections between facts, but in many applications, there are certain limitations in directly using symbolic logic to represent knowledge in KGs and perform calculations, making it difficult to achieve expected results in downstream tasks. Meanwhile, with the explosive growth of Internet capacity, the traditional KG structure faces the problems of computational inefficiency and management difficulties. To alleviate these problems, Knowledge graph embedding (KGE) is proposed to improve the computational efficiency by embedding entities and relations in the KG into a low-dimensional, dense and continuous vector space, and thus the solution of some problems in the knowledge graph is transformed into vector operations. Moreover, KGE also can be used as a pre-trained model which is more beneficial to downstream applications, such as applications based on deep learning. In this paper, we classify KGE into three categories, namely translational distance models, semantic matching models and neural network based models. The advantages and disadvantages of different embedding methods are compared, while the main applications of KGE are summarized. Some current challenges of KGE are summarized, as well as some views on the future research directions of KGE.",
        "keywords": [
            "computer science",
            "model",
            "method"
        ],
        "paper_type": "Survey",
        "summary": "**Main Contribution:**\nThis paper provides a comprehensive survey on Knowledge Graph Embedding (KGE). Its main contribution is to systematically categorize, analyze, and summarize the field of KGE, offering a structured overview for researchers.\n\n**Problem Addressed:**\nThe paper addresses two primary problems:\n1.  **Limitations of Symbolic Logic:** Directly using symbolic logic for knowledge graph (KG) representation and computation faces limitations, making it difficult to achieve desired results in downstream tasks.\n2.  **Computational Inefficiency and Management Difficulties:** With the explosive growth of internet data, traditional KG structures struggle with computational inefficiency and management challenges.\n\n**Method Used to Solve It:**\nThe paper surveys Knowledge Graph Embedding (KGE) as the solution to these problems. KGE works by embedding entities and relations within a KG into a low-dimensional, dense, and continuous vector space, thereby transforming complex KG problems into more efficient vector operations. The paper itself employs a classification and analytical methodology:\n*   It classifies KGE models into three main categories: translational distance models, semantic matching models, and neural network-based models.\n*   It compares the advantages and disadvantages of these different embedding methods.\n*   It summarizes the main applications of KGE.\n*   It outlines current challenges in KGE research and proposes future research directions.\n\n**Key Findings:**\nThe key findings of this survey paper are:\n*   KGE is an effective approach to improve computational efficiency and manageability of KGs by converting symbolic knowledge into a continuous vector space.\n*   KGE models can serve as pre-trained models, significantly benefiting downstream applications, especially those based on deep learning.\n*   A structured classification of KGE models into translational, semantic matching, and neural network-based categories provides a clear framework for understanding the field.\n*   The paper identifies and compares the strengths and weaknesses of various KGE methods, summarizes their diverse applications, and highlights critical open challenges and promising future research directions in the domain.\n\n**Key Math Equations (if applicable):**\nThe provided content for this survey paper does not include specific mathematical equations proposed or used *by the survey itself*. While the KGE models it discusses inherently rely on mathematical formulations (e.g., distance functions, scoring functions), this paper's contribution is the survey and classification, not the introduction of new mathematical models.\n\n**New Direction: No**\nThis paper is a survey. It synthesizes, categorizes, and analyzes existing research and methods in Knowledge Graph Embedding. While it identifies and discusses future research directions, it does not propose a new research direction or a novel method path itself. Its contribution lies in organizing and presenting the current state of the art.\n\n**Type: Survey**",
        "file_path": "paper_data/knowledge_graph_embedding/core_papers/f470e11faa6200026cf39e248510070c078e509a.pdf"
    },
    "fee5ac3604ccdefee2b65275fed47503234099e2.pdf": {
        "title": "Similarity attributed knowledge graph embedding enhancement for item recommendation",
        "authors": [
            "Nasrullah Khan",
            "Zongmin Ma",
            "Aman Ullah",
            "Kemal Polat"
        ],
        "published_date": "2022",
        "venue": "Information Sciences",
        "journal": null,
        "abstract": "",
        "keywords": [
            "computer science"
        ],
        "paper_type": "Unknown",
        "summary": "I am sorry, but the `Content:` section of your request is empty. I need the actual content of the paper to summarize its main contributions, identify the problem it addresses, the method used, key findings, and mathematical equations.\n\nPlease provide the content of the paper, and I will be happy to provide the detailed summary and analysis you requested.",
        "file_path": "paper_data/knowledge_graph_embedding/core_papers/fee5ac3604ccdefee2b65275fed47503234099e2.pdf"
    },
    "ff5abb57a131a595475026e63f7ea79047fef959.pdf": {
        "title": "A Review of Knowledge Graph-Based Reasoning Technology in the Operation of Power Systems",
        "authors": [
            "Rui Liu",
            "Rong Fu",
            "Kang Xu",
            "Xuanzhe Shi",
            "Xiaoning Ren"
        ],
        "published_date": "2023",
        "venue": "Applied Sciences",
        "journal": null,
        "abstract": "Knowledge graph (KG) technology is a newly emerged knowledge representation method in the field of artificial intelligence. Knowledge graphs can form logical mappings from cluttered data and establish triadic relationships between entities. Accurate derivation and reasoning of knowledge graphs play an important role in guiding power equipment operation and decision-making. Due to the complex and weak relations from multi-source heterogeneous data, the use of KGs has become popular in research to represent potential information in power knowledge reasoning. In this review, we first summarize the key technologies of knowledge graph representation and learning. Then, based on the complexity and real-time changes of power system operation and maintenance, we present multiple data processing, knowledge representation learning, and the graph construction process. In three typical power operation and fault decision application scenarios, we investigate current algorithms in power KG acquisition, representation embedding, and knowledge completion to illustrate accurate and exhaustive recommendations. Thus, using KGs to provide reference solutions and decision guidance has a significant role in improving the efficiency of power system operations. Finally, we summarize the achievements and difficulties of current research and give an outlook for future, promising roles of KG in power systems.",
        "keywords": [
            "algorithm",
            "system",
            "method"
        ],
        "paper_type": "Survey",
        "summary": "**Summary of Main Contribution:**\n\nThis paper provides a comprehensive review of Knowledge Graph (KG)-based reasoning technology applied to the operation of power systems.\n\n*   **Problem Addressed:** The paper addresses the challenge of effectively representing and reasoning with complex, multi-source heterogeneous data in power system operation and maintenance. This complexity makes it difficult to derive accurate insights and guide decision-making for equipment operation, ultimately impacting efficiency. KGs are presented as a solution to form logical mappings and establish triadic relationships from this cluttered data.\n*   **Method Used:** The paper employs a review methodology. It summarizes key technologies in knowledge graph representation and learning, and then specifically details data processing, knowledge representation learning, and graph construction processes tailored for power system operation and maintenance. It investigates current algorithms for KG acquisition, representation embedding, and knowledge completion within three typical power operation and fault decision application scenarios.\n*   **Key Findings:**\n    *   Knowledge Graphs are a powerful method for representing complex, multi-source heterogeneous data in power systems, enabling logical mappings and triadic relationships.\n    *   KG-based reasoning plays a significant role in guiding power equipment operation and decision-making, improving efficiency.\n    *   The review identifies and analyzes current algorithms for KG acquisition, representation embedding, and knowledge completion relevant to power system applications.\n    *   KGs are crucial for providing reference solutions and decision guidance in power system operations.\n    *   The paper summarizes current achievements, difficulties, and future research directions for the application of KGs in power systems.\n*   **Key Math Equations:** The provided abstract does not mention any specific mathematical equations. As a review paper, it discusses existing methods rather than introducing new mathematical models.\n\n**New Direction: No**\n**Reasoning:** This paper is explicitly titled \"A Review of...\" and its content focuses on summarizing, presenting, investigating current algorithms, and outlining achievements and difficulties of existing research. While it provides an \"outlook for future, promising roles,\" it does not propose a novel research direction, a new method, or an original theoretical framework itself. Its contribution lies in synthesizing and analyzing the current state of KG technology in power systems.\n\n**Type: Survey**",
        "file_path": "paper_data/knowledge_graph_embedding/core_papers/ff5abb57a131a595475026e63f7ea79047fef959.pdf"
    },
    "8bd3e0c1b6a68a1068da83003335ac01f1af8dcf.pdf": {
        "title": "KGTS: Contrastive Trajectory Similarity Learning over Prompt Knowledge Graph Embedding",
        "authors": [
            "Zhen Chen",
            "Dalin Zhang",
            "Shanshan Feng",
            "Kaixuan Chen",
            "Lisi Chen",
            "Peng Han",
            "Shuo Shang"
        ],
        "published_date": "2024",
        "venue": "AAAI Conference on Artificial Intelligence",
        "journal": null,
        "abstract": "Trajectory similarity computation serves as a fundamental functionality of various spatial information applications. Although existing deep learning similarity computation methods offer better efficiency and accuracy than non-learning solutions, they are still immature in trajectory embedding and suffer from poor generality and heavy preprocessing for training. Targeting these limitations, we propose a novel framework named KGTS based on knowledge graph grid embedding, prompt trajectory embedding, and unsupervised contrastive learning for improved trajectory similarity computation. Specifically, we first embed map grids with a GRot embedding method to vigorously grasp the neighbouring relations of grids. Then, a prompt trajectory embedding network incorporates the resulting grid embedding and extracts trajectory structure and point order information. It is trained by unsupervised contrastive learning, which not only alleviates the heavy preprocessing burden but also provides exceptional generality with creatively designed strategies for positive sample generation. The prompt trajectory embedding adopts a customized prompt paradigm to mitigate the gap between the grid embedding and the trajectory embedding. Extensive experiments on two real-world trajectory datasets demonstrate the superior performance of KGTS over state-of-the-art methods.",
        "keywords": [
            "framework",
            "computer science",
            "method"
        ],
        "paper_type": "Methodology",
        "summary": "**Summary for Research Survey:**\n\n**Problem Addressed:**\nThe paper addresses the limitations of existing deep learning methods for trajectory similarity computation. These limitations include immaturity in trajectory embedding, poor generality across different datasets or scenarios, and the heavy preprocessing burden required for training.\n\n**Method Used:**\nThe paper proposes a novel framework named KGTS (Knowledge Graph Trajectory Similarity) to overcome these issues. KGTS integrates three main components:\n1.  **Knowledge Graph Grid Embedding:** It first embeds map grids using a GRot embedding method to effectively capture the neighboring relations between grids.\n2.  **Prompt Trajectory Embedding:** A prompt trajectory embedding network then incorporates these grid embeddings to extract trajectory structure and point order information. A customized \"prompt paradigm\" is employed to mitigate the representational gap between the grid embeddings and the trajectory embeddings.\n3.  **Unsupervised Contrastive Learning:** The prompt trajectory embedding network is trained using unsupervised contrastive learning. This approach reduces the need for extensive preprocessing and enhances generality, featuring creatively designed strategies for positive sample generation.\n\n**Key Findings:**\n*   KGTS demonstrates superior performance over state-of-the-art methods on two real-world trajectory datasets.\n*   The unsupervised contrastive learning approach effectively alleviates the heavy preprocessing burden commonly associated with deep learning models for trajectory similarity.\n*   The framework achieves exceptional generality, indicating its robust applicability across diverse trajectory datasets and scenarios.\n\n**Key Math Equations (if applicable):**\nWhile the abstract does not explicitly list mathematical equations, the described methods inherently rely on them. The \"GRot embedding method\" would involve specific mathematical formulations for graph/grid embedding. The \"prompt trajectory embedding network\" would utilize neural network architectures with associated activation functions, weight matrices, and loss functions. \"Unsupervised contrastive learning\" is built upon specific loss functions (e.g., InfoNCE loss) and similarity metrics (e.g., cosine similarity) for optimizing embedding spaces. The \"creatively designed strategies for positive sample generation\" would also involve algorithmic or mathematical rules for constructing these pairs.\n\n**New Direction: Yes**\n**Type: Methodology**\n\n**Reasoning for 'New Direction: Yes':**\nThis paper proposes a new method path by innovatively combining and adapting several advanced techniques for trajectory similarity computation. While knowledge graphs, prompt learning, and contrastive learning exist individually, their specific integration within the KGTS framework for this problem is novel. The use of a \"customized prompt paradigm to mitigate the gap between the grid embedding and the trajectory embedding\" and the \"creatively designed strategies for positive sample generation\" within an unsupervised contrastive learning setup are specific methodological innovations. This is not merely an incremental improvement on an existing architecture but a novel architectural and training strategy that addresses fundamental limitations of prior approaches (generality, preprocessing burden) in a distinct way.",
        "file_path": "paper_data/knowledge_graph_embedding/core_papers/8bd3e0c1b6a68a1068da83003335ac01f1af8dcf.pdf"
    },
    "354b651dbc3ba2af4c3785ccbecd3df0585d30b2.pdf": {
        "title": "Analogical Inference Enhanced Knowledge Graph Embedding",
        "authors": [
            "Zhen Yao",
            "Wen Zhang",
            "Mingyang Chen",
            "Yufen Huang",
            "Yezhou Yang",
            "Hua-zeng Chen"
        ],
        "published_date": "2023",
        "venue": "AAAI Conference on Artificial Intelligence",
        "journal": null,
        "abstract": "Knowledge graph embedding (KGE), which maps entities and relations in a knowledge graph into continuous vector spaces, has achieved great success in predicting missing links in knowledge graphs. However, knowledge graphs often contain incomplete triples that are difficult to inductively infer by KGEs. To address this challenge, we resort to analogical inference and propose a novel and general self-supervised framework AnKGE to enhance KGE models with analogical inference capability. We propose an analogical object retriever that retrieves appropriate analogical objects from entity-level, relation-level, and triple-level. And in AnKGE, we train an analogy function for each level of analogical inference with the original element embedding from a well-trained KGE model as input, which outputs the analogical object embedding. In order to combine inductive inference capability from the original KGE model and analogical inference capability enhanced by AnKGE, we interpolate the analogy score with the base model score and introduce the adaptive weights in the score function for prediction. Through extensive experiments on FB15k-237 and WN18RR datasets, we show that AnKGE achieves competitive results on link prediction task and well performs analogical inference.",
        "keywords": [
            "computer science",
            "model",
            "framework"
        ],
        "paper_type": "Methodology",
        "summary": "This paper addresses the challenge that Knowledge Graph Embedding (KGE) models, despite their success in link prediction, often struggle with inductively inferring incomplete triples within knowledge graphs.\n\nTo solve this, the paper proposes **AnKGE**, a novel and general self-supervised framework that enhances KGE models with analogical inference capabilities. The method involves:\n1.  An **analogical object retriever** that identifies appropriate analogical objects at entity-level, relation-level, and triple-level.\n2.  Training an **analogy function** for each level of analogical inference. These functions take original element embeddings from a pre-trained KGE model as input and output analogical object embeddings.\n3.  Combining the inductive inference capability of the original KGE model with the enhanced analogical inference. This is achieved by **interpolating the analogy score with the base model score** using adaptive weights in the final score function for prediction.\n\nKey findings indicate that AnKGE achieves competitive results on the link prediction task and effectively performs analogical inference, as demonstrated through extensive experiments on the FB15k-237 and WN18RR datasets.\n\n**Key Math Equations:**\nWhile no explicit equations are provided in the abstract, the core mathematical contribution lies in the score function for prediction, which combines the base KGE model's score with the analogy score using adaptive weights. Conceptually, this can be represented as:\n$S_{final}(h, r, t) = w \\cdot S_{analogy}(h, r, t) + (1-w) \\cdot S_{base}(h, r, t)$\nwhere $S_{final}$ is the final prediction score, $S_{analogy}$ is the score derived from analogical inference, $S_{base}$ is the score from the original KGE model, and $w$ is an adaptively learned weight (or set of weights) that balances the contribution of the two inference mechanisms.\n\nNew Direction: Yes\nThis paper proposes a new method path by introducing a novel self-supervised framework (AnKGE) that integrates analogical inference into KGEs. While KGEs and analogical reasoning are existing concepts, their specific combination in a multi-level retrieval and adaptive scoring framework for addressing the inductive inference limitation of KGEs is presented as a novel approach. It offers a distinct way to enhance the reasoning capabilities of KGE models beyond their typical inductive inference.\n\nType: Methodology",
        "file_path": "paper_data/knowledge_graph_embedding/core_papers/354b651dbc3ba2af4c3785ccbecd3df0585d30b2.pdf"
    },
    "10d949dee482aeea1cab8b42c326d0dbf0505de3.pdf": {
        "title": "Entity-Agnostic Representation Learning for Parameter-Efficient Knowledge Graph Embedding",
        "authors": [
            "Mingyang Chen",
            "Wen Zhang",
            "Zhen Yao",
            "Yushan Zhu",
            "Yang Gao",
            "Jeff Z. Pan",
            "Hua-zeng Chen"
        ],
        "published_date": "2023",
        "venue": "AAAI Conference on Artificial Intelligence",
        "journal": null,
        "abstract": "We propose an entity-agnostic representation learning method for handling the problem of inefficient parameter storage costs brought by embedding knowledge graphs. Conventional knowledge graph embedding methods map elements in a knowledge graph, including entities and relations, into continuous vector spaces by assigning them one or multiple specific embeddings (i.e., vector representations). Thus the number of embedding parameters increases linearly as the growth of knowledge graphs. In our proposed model, Entity-Agnostic Representation Learning (EARL), we only learn the embeddings for a small set of entities and refer to them as reserved entities. To obtain the embeddings for the full set of entities, we encode their distinguishable information from their connected relations, k-nearest reserved entities, and multi-hop neighbors. We learn universal and entity-agnostic encoders for transforming distinguishable information into entity embeddings. This approach allows our proposed EARL to have a static, efficient, and lower parameter count than conventional knowledge graph embedding methods. Experimental results show that EARL uses fewer parameters and performs better on link prediction tasks than baselines, reflecting its parameter efficiency.",
        "keywords": [
            "computer science",
            "model",
            "approach",
            "method"
        ],
        "paper_type": "Methodology",
        "summary": "**Problem Addressed:**\nThe paper addresses the problem of inefficient parameter storage costs in conventional Knowledge Graph Embedding (KGE) methods. Traditional KGE approaches assign specific vector representations (embeddings) to every entity and relation, causing the number of embedding parameters to increase linearly with the growth of the knowledge graph, leading to high storage and computational costs for large-scale KGs.\n\n**Method Used:**\nThe paper proposes an Entity-Agnostic Representation Learning (EARL) method. EARL tackles the parameter efficiency problem by not learning specific embeddings for all entities. Instead, it:\n1.  Learns and stores embeddings for only a small, pre-defined subset of entities, referred to as \"reserved entities.\"\n2.  For all other entities (including non-reserved ones), their embeddings are dynamically generated. This is achieved by encoding their \"distinguishable information,\" which is derived from their connected relations, their k-nearest reserved entities, and their multi-hop neighbors within the graph.\n3.  Universal and entity-agnostic encoders are employed to transform this distinguishable contextual information into the final entity embeddings.\nThis design allows EARL to maintain a static, efficient, and significantly lower parameter count, independent of the total number of entities in the knowledge graph.\n\n**Key Findings:**\nExperimental results demonstrate that EARL achieves superior parameter efficiency, utilizing fewer parameters than conventional baseline KGE methods. Crucially, despite this significant reduction in parameters, EARL performs better on standard link prediction tasks, indicating that its entity-agnostic representation learning effectively captures the necessary semantic information while being more resource-efficient.\n\n**Key Math Equations:**\nThe provided content does not explicitly include any mathematical equations.\n\n**New Direction: Yes**\n**Reasoning:** This paper proposes a new research direction and method path by introducing a paradigm shift in how entity embeddings are learned for parameter efficiency in Knowledge Graph Embedding. Instead of the conventional approach of directly learning and storing a unique embedding for every entity, EARL introduces the novel concept of \"reserved entities\" and \"entity-agnostic encoders.\" This allows for the dynamic generation of entity embeddings from contextual information (relations, neighbors) rather than direct storage, leading to a static and significantly reduced parameter count irrespective of the knowledge graph's size. This shift from direct parameterization to a generative, context-dependent parameterization for entities is a novel and impactful approach to addressing the scalability and parameter efficiency challenges in KGEs.\n\n**Type: Methodology**",
        "file_path": "paper_data/knowledge_graph_embedding/core_papers/10d949dee482aeea1cab8b42c326d0dbf0505de3.pdf"
    },
    "c620d157f5f999d698f0da86fb91d267ad8ded5c.pdf": {
        "title": "Future Event Prediction Based on Temporal Knowledge Graph Embedding",
        "authors": [
            "Zhipeng Li",
            "Shanshan Feng",
            "Jun Shi",
            "Yang Zhou",
            "Yong Liao",
            "Yangzhao Yang",
            "Yangyang Li",
            "Nenghai Yu",
            "Xun Shao"
        ],
        "published_date": "2023",
        "venue": "Computer systems science and engineering",
        "journal": null,
        "abstract": "Accurate prediction of future events brings great benefits and reduces losses for society in many domains, such as civil unrest, pandemics, and crimes. Knowledge graph is a general language for describing and modeling complex systems. Different types of events continually occur, which are often related to historical and concurrent events. In this paper, we formalize the future event prediction as a temporal knowledge graph reasoning problem. Most existing studies either conduct reasoning on static knowledge graphs or assume knowledges graphs of all timestamps are available during the training process. As a result, they cannot effectively reason over temporal knowledge graphs and predict events happening in the future. To address this problem, some recent works learn to infer future events based on historical event-based temporal knowledge graphs. However, these methods do not comprehensively consider the latent patterns and influences behind historical events and concurrent events simultaneously. This paper proposes a new graph representation learning model, namely Recurrent Event Graph ATtention Network (RE-GAT), based on a novel historical and concurrent events attention-aware mechanism by modeling the event knowledge graph sequence recurrently. More specifically, our RE-GAT uses an attention-based historical events embedding module to encode past events, and employs an attentionbased concurrent events embedding module to model the associations of events at the same timestamp. A translation-based decoder module and a learning objective are developed to optimize the embeddings of entities and relations. We evaluate our proposed method on four benchmark datasets. Extensive experimental results demonstrate the superiority of our RE-GAT model comparing to various baselines, which proves that our method can more accurately predict what events are going to happen.",
        "keywords": [
            "computer science",
            "model",
            "system",
            "method"
        ],
        "paper_type": "Methodology",
        "summary": "**Problem Addressed:**\nThe paper addresses the problem of accurately predicting future events (e.g., civil unrest, pandemics, crimes), formalizing it as a temporal knowledge graph (TKG) reasoning problem. It highlights limitations of existing methods, which either reason on static knowledge graphs, assume the availability of all timestamps during training, or fail to comprehensively consider the latent patterns and influences of *both historical and concurrent events simultaneously* when inferring future events from historical event-based TKGs.\n\n**Method Used:**\nThe paper proposes a new graph representation learning model called **Recurrent Event Graph ATtention Network (RE-GAT)**. This model is designed with a novel historical and concurrent events attention-aware mechanism, which recurrently models the sequence of event knowledge graphs. Specifically, RE-GAT comprises:\n1.  An **attention-based historical events embedding module** to encode patterns from past events.\n2.  An **attention-based concurrent events embedding module** to model the associations and influences of events occurring at the same timestamp.\n3.  A **translation-based decoder module** and a learning objective to optimize the embeddings of entities and relations, facilitating future event prediction.\n\n**Key Findings:**\nExtensive experimental results on four benchmark datasets demonstrate the superiority of the proposed RE-GAT model compared to various baseline methods. The findings indicate that RE-GAT can more accurately predict what events are going to happen, proving its effectiveness in temporal knowledge graph reasoning for future event prediction.\n\n**Key Math Equations:**\nThe paper describes the use of \"attention-based historical events embedding module,\" \"attention-based concurrent events embedding module,\" a \"translation-based decoder module,\" and a \"learning objective.\" While specific mathematical equations are not provided in the abstract, these components imply the application of established techniques such as graph attention networks (e.g., GAT), recurrent neural networks (e.g., GRU/LSTM for recurrent modeling), translation-based embedding models (e.g., TransE, RotatE, or their variants) for knowledge graph completion, and a suitable loss function (e.g., margin-based ranking loss) for optimizing the embeddings.\n\n**New Direction: Yes**\nThe paper proposes a new method path by introducing RE-GAT, which features a *novel historical and concurrent events attention-aware mechanism*. While temporal knowledge graph embedding and attention mechanisms are existing fields, the specific combination and the dedicated attention mechanism for simultaneously and comprehensively modeling *both* historical temporal dependencies and concurrent intra-timestamp relationships within a recurrent framework for future event prediction represent a distinct methodological advancement. This addresses a specific gap identified in prior work, thereby offering a new and more effective approach to the problem.\n\n**Type: Methodology**",
        "file_path": "paper_data/knowledge_graph_embedding/core_papers/c620d157f5f999d698f0da86fb91d267ad8ded5c.pdf"
    }
}